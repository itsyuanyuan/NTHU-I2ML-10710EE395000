title_abstract
"Evaluating similarity measures: a large-scale study in the orkut social network.  Online information services have grown too large for users to navigate without the help of automated tools such as collaborative filtering, which makes recommendations to users based on their collective past behavior. While many similarity measures have been proposed and individually evaluated, they have not been evaluated relative to each other in a large real-world environment. We present an extensive empirical comparison of six distinct measures of similarity for recommending online communities to members of the Orkut social network. We determine the usefulness of the different recommendations by actually measuring users' propensity to visit and join recommended communities. We also examine how the ordering of recommendations influenced user selection, as well as interesting social issues that arise in recommending communities within a real social network."
"Web Search for a Planet: The Google Cluster Architecture.  Amenable to extensive parallelization, Google's Web search application lets different queries run on different processors and, by partitioning the overall index, also lets a single query use multiple processors. To handle this workload, Google's architecture features clusters of more than 15,000 commodity class PCs with fault-tolerant software. This architecture achieves superior performance at a fraction of the cost of a system built from fewer, but more expensive, high-end servers."
"The Price of Performance: An Economic Case for Chip Multiprocessing.  Over the last decade we have witnessed a succession of increasingly power inefficient CPU designs. In this article we examine some economic aspects of building a large scale computing infrastructure, and how such power trends, if continued, might threaten the affordability of computing. We further argue that chip multiprocessing constitute our best hope for reverting these power inefficiency trends, and that chip multiprocessing architectures are a very good match to the computational requirements of large scale internet services."
"The Google File System.  We have designed and implemented the Google File System, a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients. While sharing many of the same goals as previous distributed file systems, our design has been driven by observations of our application workloads and technological environment, both current and anticipated, that reflect a marked departure from some earlier file system assumptions. This has led us to reexamine traditional choices and explore radically different design points. The file system has successfully met our storage needs. It is widely deployed within Google as the storage platform for the generation and processing of data used by our service as well as research and development efforts that require large data sets. The largest cluster to date provides hundreds of terabytes of storage across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients. In this paper, we present file system interface extensions designed to support distributed applications, discuss many aspects of our design, and report measurements from both micro-benchmarks and real world use."
"Interpreting the Data: Parallel Analysis with Sawzall.  Very large data sets often have a flat but regular structure and span multiple disks and machines. Examples include telephone call records, network logs, and web document repositories. These large data sets are not amenable to study using traditional database techniques, if only because they can be too large to fit in a single relational database. On the other hand, many of the analyses done on them can be expressed using simple, easily distributed computations: filtering, aggregation, extraction of statistics, and so on. We present a system for automating such analyses. A filtering phase, in which a query is expressed using a new programming language, emits data to an aggregation phase. Both phases are distributed over hundreds or even thousands of computers. The results are then collated and saved to a file. The design -- including the separation into two phases, the form of the programming language, and the properties of the aggregators -- exploits the parallelism inherent in having data and computation distributed across many machines."
"Query-Free News Search.  Many daily activities present information in the form of a stream of text, and often people can benefit from additional information on
the topic discussed. TV broadcast news can be treated as one such stream of text; in this paper we discuss finding news articles on the
web that are relevant to news currently being broadcast. We evaluated a variety of algorithms for this problem, looking at the impact of inverse document frequency, stemming, compounds, history, and query length on the relevance and coverage of news articles returned in real time during a broadcast. We also evaluated several postprocessing techniques for improving the precision, including reranking using additional terms, reranking by document similarity, and filtering on document similarity. For the best algorithm, 84%-91% of the articles found were relevant, with at least 64% of the articles being on the exact topic of the broadcast. In addition, a relevant article was found for at least 70% of the topics."
"Searching the Web by Voice.  Spoken queries are a natural medium for searching the Web in settings where typing on a keyboard is not practical. This paper describes a speech interface to the Google search engine. We present experiments with various statistical language models, concluding that a unigram model with collocations provides the best combination of broad coverage, predictive power, and real-time performance. We also report accuracy results of the prototype system."
"Who Links to Whom: Mining Linkage between Web Sites.  Previous studies of the web graph structure have focused on the graph structure at the level of individual pages. In actuality the web is a hierarchically nested graph, with domains, hosts and web sites introducing intermediate levels of affiliation and administrative control. To better understand the growth of the web we need to understand its macro-structure, in terms of the linkage between web sites. In this paper we approximate this by studying the graph of the linkage between hosts on the web. This was done based on snapshots of the web taken by Google in Oct 1999, Aug 2000 and Jun 2001. The connectivity between hosts is represented by a directed graph, with hosts as nodes and weighted edges representing the count of hyperlinks between pages on the corresponding hosts. We demonstrate how such a ""hostgraph"" can be used to study connectivity properties of hosts and domains over time, and discuss a modified ""copy model"" to explain observed link weight distributions as a function of subgraph size. We discuss changes in the web over time in the size and connectivity of web sites and country domains. We also describe a data mining application of the hostgraph: a related host finding algorithm which achieves a precision of 0.65 at rank 3."
"PowerPoint: Shot with its own bullets.  Imagine a world with almost no pronouns or punctuation. A world where any complex thought must be broken into seven-word chunks, with colourful blobs between them. It sounds like the futuristic dystopia of Kurt Vonnegut's short story Harrison Bergeron, in which intelligent citizens receive ear-splitting broadcasts over headsets so that they cannot gain an unfair advantage over their less intelligent peers. But this world is no fiction??it is the present-day reality of a PowerPoint presentation, a reality that is repeated an estimated 30 million times a day."
"The Chubby lock service for loosely-coupled distributed systems.  We describe our experiences with the Chubby lock service, which is intended to provide coarse-grained locking as well as reliable (though low-volume) storage for a loosely-coupled distributed system. Chubby provides an interface much like a distributed file system with advisory locks, but the design emphasis is on availability and reliability, as opposed to high performance. Many instances of the service have been used for over a year, with several of them each handling a few tens of thousands of clients concurrently. The paper describes the initial design and expected use, compares it with actual use, and explains how the design had to be modified to accommodate the differences."
"Bigtable: A Distributed Storage System for Structured Data.  Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google Earth, and Google Finance. These applications place very different demands on Bigtable, both in terms of data size (from URLs to web pages to satellite imagery) and latency requirements (from backend bulk processing to real-time data serving). Despite these varied demands, Bigtable has successfully provided a flexible, high-performance solution for all of these Google products. In this paper we describe the simple data model provided by Bigtable, which gives clients dynamic control over data layout and format, and we describe the design and implementation of Bigtable."
"High-efficiency power supplies for home computers and servers.  The focus of our message is efficiency: power efficiency and programming efficiency. There are several hard technical problems surrounding power efficiency of computers, but we've found one that is actually not particularly challenging and could have a huge impact on the energy used by home computers and low-end servers: increasing power supply efficiency."
"Theoretical Advantages of Lenient Learners in Multiagent Systems.  This paper presents the dynamics of multiple reinforcement learning agents from an Evolutionary Game Theoretic perspective. We provide a Replicator Dynamics model for traditional multiagent Q-learning, and we then extend these differential equations to account for lenient learners: agents that forgive possible mistakes of their teammates that resulted in lower rewards. We use this extended formal model to visualize the basins of attraction of both traditional and lenient multiagent Q-learners in two benchmark coordination problems. The results indicate that lenience provides learners with more accurate estimates for the utility of their actions, resulting in higher likelihood of convergence to the globally optimal solution. In addition, our research supports the strength of EGT as a backbone for multiagent reinforcement learning."
"Structured Data Meets the Web: A Few Observations.  The World Wide Web is witnessing an increase in the amount of structured content -- vast heterogeneous collections of structured data are on the rise due to the Deep Web, annotation schemes like Flickr, and sites like Google Base. While this phenomenon is creating an opportunity for structured data management, dealing with heterogeneity on the web-scale presents many new challenges. In this paper we articulate challenges based on our experience with addressing them at Google, and offer some principles for addressing them in a general fashion."
"Search Worms.  Worms are becoming more virulent at the same time as operating system improvements try to contain them.Recent research demonstrates several effective methods to detect and prevent randomly scanning worms from spreading [2, 13]. As a result, worm authors are looking for new ways to acquire vulnerable targets without relying on randomly scanning for them. It is often possible to find vulnerable web servers by sending carefully crafted queries to search engines. Search worms1 automate this approach and spread by using popular search engines to find new attack vectors. These worms not only put significant load on search engines, they also evade detection mechanisms that assume random scanning. From the point of view of a search engine, signatures against search queries are only a temporary measure as many different search queries lead to the same results. In this paper, we present our experience with search worms and a framework that allows search engines to quickly detect new worms and take automatic countermeasures. We argue that signature-based filtering of search queries is ill-suited for protecting against search worms and show how we prevent worm propagation without relying on query signatures. We illustrate our approach with measurements and numeric simulations."
"An Assertional Correctness Proof of a Self-Stabilizing l-Exclusion Algorithm.  A formal correctness proof of a self-stabilizing l-exclusion algorithm (SLEX) is described. The analyzed algorithm is an improvement of the SLEX due to Abraham, Dolev, Herman, and Koll, since our version satisfies a stronger liveness property. The proof is formulated in Linear-Time Temporal Logic and utilizes a history variable to model access to regular registers. The proof consists of a safety part and a liveness part. Our analysis provides some new insight in the correctness of the algorithm: (1) Our proof is constructive. That is, we explicitly formulate auxiliary quantities required to establish some of the properties. This contrasts with the operational arguments of Abraham et al., where many quantities are not explicitly formulated and the validity of the above mentioned properties is established by disproving their non-existence. (2) We characterize processes (and their minimum number) identified by some process as attempting to enter the critical section. (3) A novel proof rule for reasoning about programs in the presence of disabled processes is presented to structure the liveness proof."
"Statistical Machine Translation for Query Expansion in Answer Retrieval.  This paper presents a novel approach to query expansion in answer retrieval that uses Statistical Machine Translation (SMT) techniques to bridge the lexical gap between questions and answers. SMT-based query expansion is performed on the one hand by using a SMT-based full-sentence paraphraser to introduce synonyms in the context the full query, and on the other hand by training an SMT model on question-answer pairs and expanding queries by answer terms taken from translations of full queries. We compare these global, context-aware query expansion techniques with a baseline tfidf model and local query expansion on a database of 10 million question-answer pairs extracted from FAQ pages. Experimental results show a significant improvement of SMT-based query expansion over both baselines."
"Failure Trends in a Large Disk Drive Population.  It is estimated that over 90% of all new information produced 
in the world is being stored on magnetic media, most of it in hard disk drives. Despite their importance, there is relatively little published work on the failure patterns of disk drives, and the key factors that affect their lifetime. Most available data are either based on extrapolation from accelerated aging experiments or from relatively modest sized field studies. Moreover, larger population studies rarely have the infrastructure in place to collect health signals from components in operation, which is critical information for detailed failure analysis. We present data collected from detailed observations of a large disk drive population in a production Internet services deployment. The population observed is many times larger than that of previous studies. In addition to presenting failure statistics, we analyze the correlation between failures and several parameters generally believed to impact longevity. Our analysis identifies several parameters from the drive??s self monitoring facility (SMART) that correlate highly with failures. Despite this high correlation, we conclude that models based on SMART parameters alone are unlikely to be useful for predicting individual drive failures. Surprisingly, we found that temperature and activity levels were much less correlated with drive failures than previously reported."
"Web-scale Data Integration: You can only afford to Pay As You Go.  The World Wide Web is witnessing an increase in the amount of structured content - vast heterogeneous collections of
structured data are on the rise due to the Deep Web, annotation schemes like Flickr, and sites like Google Base. 
While this phenomenon is creating an opportunity for structured data management, dealing with heterogeneity on the web-scale presents many new challenges. In this paper, we highlight these challenges in two scenarios - the Deep Web and Google Base. We contend that traditional data integration techniques are no longer valid in the face of such heterogeneity and scale. We propose a new data integration architecture, PAYGO, which is inspired by the concept of dataspaces and emphasizes pay-as-you-go data management as means for achieving web-scale data integration."
"Evaluating Static Analysis Defect Warnings on Production Software.  Classification of static analysis warnings into false positive, trivial or serious bugs: Experience on Java JDK and Google codebase"
"Globally Minimal Surfaces by Continuous Maximal Flows.  In this paper we address the computation of globally minimal curves and surfaces for image segmentation and stereo reconstruction. We present a solution, simulating a continuous maximal flow by a novel system of partial differential equations. Existing methods are either grid-biased (graph-based methods) or sub-optimal (active contours and surfaces). The solution simulates the flow of an ideal fluid with isotropic velocity constraints. Velocity constraints are defined by a metric derived from image data. An auxiliary potential function is introduced to create a system of partial differential equations. It is proven that the algorithm produces a globally maximal continuous flow at convergence, and that the globally minimal surface may be obtained trivially from the auxiliary potential. The bias of minimal surface methods toward small objects is also addressed. An efficient implementation is given for the flow simulation. The globally minimal surface algorithm is applied to segmentation in 2D and 3D as well as to stereo matching. Results in 2D agree with an existing minimal contour algorithm for planar images. Results in 3D segmentation and stereo matching demonstrate that the new algorithm is robust and free from grid bias."
"How difficult is it to develop a perfect spell-checker? A cross-linguistic analysis through complex network approach.  The difficulties involved in spelling error detection and correction in a language have been investigated in this work through the conceptualization of SpellNet - a weighted network of words, where edges indicate orthographic proximity between two words. We construct SpellNets for three languages - Bengali, English and Hindi. Through appropriate mathematical analysis and/or intuitive justification, we interpret the different topological metrics of SpellNet from the perspective of the issues related to spell-checking. We make many interesting observations, the most significant being that the probability of making a read word error in a language is proportionate to the average weighted degree of SpellNet, which is found to be highest for Hindi, followed by Bengali and English."
"Studies in Lower Bounding Probability of Evidence using the Markov Inequality.  Computing the probability of evidence even with
known error bounds is NP-hard. In this paper we
address this hard problem by settling on an easier
problem. We propose an approximation that provides
high confidence lower bounds on probability
of evidence. Our proposed approximation is a
randomized importance sampling based scheme
that uses the Markov inequality. However, a
straight-forward application of the Markov inequality
may lead to poor lower bounds. We,
therefore propose several heuristic measures to
improve its performance in practice. Empirical
evaluation of our scheme with state-of-the-art
lower bounding schemes reveals the promise of
our approach."
"Keeping the Web in Web 2.0: An HCI Approach to Designing Web Applications.  We will discuss javascript programming and AJAX, the
dominant tools for developing sophisticated applications on the web.  You will come away with a general understanding of the building blocks and capabilities AJAX applications; and will have a headstart on learning to apply these tools to your own projects."
"Biometric Person Authentication IS A Multiple Classifier Problem.  Several papers have already shown the interest of using multiple classifiers
in order to enhance the performance of biometric person authentication systems.
In this paper, we would like to argue that the core task of Biometric
Person Authentication is actually a multiple classifier problem as such:
indeed, in order to reach state-of-the-art performance, we argue that
all current systems , in one way or another, try to solve several tasks
simultaneously and that without such joint training (or sharing),
they would not succeed as well.
We explain hereafter this perspective, and according to it, we propose
some ways to take advantage of it, ranging from more parameter
sharing to similarity learning."
"Learning the Inter-frame Distance for Discriminative Template-based Keyword Detection.  This paper proposes a discriminative approach to template-based
keyword detection. We introduce a method to learn the distance 
used to compare acoustic frames, a crucial element for template 
matching approaches. The proposed algorithm estimates the distance 
from data, with the objective to produce a detector maximizing the 
Area Under the receiver operating Curve (AUC), i.e. the standard 
evaluation measure for the keyword detection problem. The experiments 
performed over a large corpus, SpeechDatII, suggest that our model 
is effective compared to an HMM system, e.g. the proposed approach 
reaches 93.8\% of averaged AUC compared to 87.9\% for the HMM."
"Budget Optimization in Search-Based Advertising Auctions.  Internet search companies sell advertisement slots based on users' search queries via an auction. While there has been previous work onthe auction process and its game-theoretic aspects, most of it focuses on the Internet company. In this work, we focus on the advertisers, who must solve a complex optimization problem to decide how to place bids on keywords to maximize their return (the number of user clicks on their ads) for a given budget. We model the entire process and study this budget optimization problem. While most variants are NP-hard, we show, perhaps surprisingly, that simply randomizing between two uniform strategies that bid equally on all the keywordsworks well. More precisely, this strategy gets at least a 1-1/?? fraction of the maximum clicks possible. As our preliminary experiments show, such uniform strategies are likely to be practical. We also present inapproximability results, and optimal algorithms for variants of the budget optimization problem."
Selective Disclosure.  Selective disclosure for the non-cryptographer.
"An Active Approach to Measuring Routing Dynamics Induced by Autonomous Systems.  We present an active measurement study of the routing dynamics induced by AS-path prepending, a common method for controlling the inbound traffic of a multi-homed ISP. Unlike other inter-domain inbound traffic engineering methods, AS-path prepending not only provides network resilience but does not increase routing table size. Unfortunately, ISPs often perform prepending on a trail-and-error basis, which can lead to suboptimal results and to a large amount of network churn. We study these effects by actively injecting prepended routes into the Internet routing system using the RIPE NCC RIS route collectors and observing the resulting changes from almost 200 publicly-accessible sources of BGP
information. Our results show that our prepending methods are simple and effective and that a small number of ASes is often responsible for large amounts of the route changes caused by prepending. Furthermore, we show that our methods are able to reveal hidden prepending policies to prepending and tie-breaking decisions made by ASes; this is useful for further predicting the behavior of prepending."
"Investigating prefix propagation through active BGP probing.  To devise effective network engineering strategies and to assess the quality of upstream providers, network operators would greatly benefit from the knowledge of which Internet paths might be traversed by the traffic flows entering their networks in the case of network faults or when traffic engineering measures are used. However, current methodologies do not provide this information. This paper presents methodologies to discover alternate paths that might be selected in the presence of network faults or different routing policies and to deduce the routing policies of other operators. The techniques are validated through extensive experimentation on the Internet."
"A Generative Model for Distance Patterns in Music.  In order to cope for the difficult problem of long term dependencies in
sequential data in general, and in musical data in particular, a generative
model for distance patterns especially
designed for music is introduced. A specific implementation of
the model when considering Hamming distances over rhythms is
described. The proposed model consistently outperforms a standard
Hidden Markov Model in terms of conditional prediction accuracy over
two different music databases."
"Power Provisioning for a Warehouse-sized Computer.  Large-scale Internet services require a computing infrastructure that can be appropriately described as a warehouse-sized computing system. The cost of building datacenter facilities capable of delivering a given power capacity to such a computer can rival the recurring energy consumption costs themselves. Therefore, there are strong economic incentives to operate facilities as close as possible to maximum capacity, so that the non-recurring facility costs can be best amortized. That is difficult to achieve in practice because of uncertainties in equipment power ratings and because power consumption tends to vary significantly with the actual computing activity. Effective power provisioning strategies are needed to determine how much computing equipment can be safely and efficiently hosted within a given power budget. In this paper we present the aggregate power usage characteristics of large collections of servers (up to 15 thousand) for different classes of applications over a period of approximately six months. Those observations allow us to evaluate opportunities for maximizing the use of the deployed power capacity of datacenters, and assess the risks of over-subscribing it. We find that even in well-tuned applications there is a noticeable gap (7 - 16%) between achieved and theoretical aggregate peak power usage at the cluster level (thousands of servers). The gap grows to almost 40% in whole datacenters. This headroom can be used to deploy additional compute equipment within the same power budget with minimal risk of exceeding it. We use our modeling framework to estimate the potential of power management schemes to reduce peak power and energy usage. We find that the opportunities for power and energy savings are significant, but greater at the cluster-level (thousands of servers) than at the rack-level (tens). Finally we argue that systems need to be power efficient across the activity range, and not only at peak performance levels."
"Paxos Made Live - An Engineering Perspective (2006 Invited Talk).  We describe our experience in building a fault-tolerant data-base using
the Paxos consensus algorithm.  Despite the existing literature in
the field, building such a database proved to be non-trivial.  We
describe selected algorithmic and engineering problems encountered,
and the solutions we found for them.  Our measurements indicate that
we have built a competitive system."
"Emacspeak --- The Complete Audio Desktop.  A desktop is a workspace that one uses to organize the tools of one's trade. Graphical desktops provide rich visual interaction for performing day-to-day computing tasks; the goal of the audio desktop is to enable similar efficiencies in an eyes-free environment. Thus, the primary goal of an audio desktop is to use the expressiveness of auditory output (both verbal and nonverbal) to enable the end user to perform a full range of computing tasks:"
"Practical Gammatone-Like Filters for Auditory Modeling.  This paper deals with continuous-time filter transfer functions that resemble tuning curves at particular set of places on the basilar 
membrane of the biological cochlea and that are suitable for practical VLSI implementations. The resulting filters can be used in 
a filterbank architecture to realize cochlea implants or auditory processors of increased biorealism. To put the reader into context, 
the paper starts with a short review on the gammatone filter and then exposes two of its variants, namely, the differentiated all-pole 
gammatone filter (DAPGF) and one-zero gammatone filter (OZGF), filter responses that provide a robust foundation for modeling 
cochlea transfer functions. The DAPGF and OZGF responses are attractive because they exhibit certain characteristics suitable for 
modeling a variety of auditory data: level-dependent gain, linear tail for frequencies well below the center frequency, asymmetry, 
and so forth. In addition, their form suggests their implementation by means of cascades of N identical two-pole systems which 
render them as excellent candidates for efficient analog or digital VLSI realizations. We provide results that shed light on their char- 
acteristics and attributes and which can also serve as ??design curves?? for fitting these responses to frequency-domain physiological 
data. The DAPGF and OZGF responses are essentially a ??missing link?? between physiological, electrical, and mechanical models 
for auditory filtering."
"Theoretical Convergence Guarantees for Cooperative Coevolutionary Algorithms.  Cooperative coevolutionary algorithms have the potential to significantly speed up the search process by dividing the space into parts that can be each conquered separately.  Unfortunately, recent research presented theoretical and empirical arguments that these algorithms might not be fit for optimization tasks, as they might tend to drift to suboptimal solutions in the search space. This paper details an extended formal model for cooperative coevolutionary algorithms, and uses it to demonstrate that these algorithms will converge to the globally optimal solution, if properly set and if given enough resources. We also present an intuitive graphical visualization for the basins of attraction to optimal and suboptimal solutions in the search space."
"Theoretical Advantages of Lenient Learners: An Evolutionary Game Theoretic Perspective.  This paper presents the dynamics of multiple learning agents from an evolutionary game theoretic perspective. We provide replicator dynamics models for cooperative coevolutionary algorithms and for traditional multiagent Q-learning, and we extend these differential equations to account for lenient learners: agents that forgive possible mismatched teammate actions that resulted in low rewards. We use these extended formal models to study the convergenceguarantees for these
algorithms, and also to visualize the basins of attraction to optimal and suboptimal solutions in two benchmark coordination problems. We demonstrate that lenience provides learners with more accurate information about the benefits of performing their actions, resulting in higher likelihood of convergence to the globally optimal solution. In addition, our analysis indicates that the choice of
learning algorithm has an insignificant impact on the overall performance of multiagent learning algorithms; rather, the performance of these algorithms depends primarily on the level of lenience that the agents exhibit to one another. Finally, our research supports the strength and generality of evolutionary game theory as a backbone for multiagent learning."
"A Discriminative Kernel-based Approach to Retrieval Images from Text Queries.  This paper proposes a discriminative model for the retrieval
of images from text queries. Contrary to previous research,
this approach does not rely on an intermediate annotation task.
Instead, it addresses the retrieval problem directly, and learns
from a criterion related to the final ranking performance of the
retrieval model. Moreover, our learning procedure builds upon
recent work on the online learning of kernel-based classifiers,
yielding an efficient, scalable training algorithm. The experiments
performed over stock photography data show the advantage of our
discriminative ranking approach over state-of-the-art alternatives
(e.g. our model yields $26.3\%$ average precision over
the standard Corel benchmark, which should be compared to $22.0\%$,
for the best alternative model evaluated)."
"The Need for Open Source Software in Machine Learning.  Open source tools have recently reached a level of maturity which makes them suitable for building large-scale real-world systems. At the same time, the field of machine learning has developed a large body of powerful learning algorithms for diverse applications.  However, the true potential of these methods is not utilized, since existing implementations are not openly shared, resulting in software with low usability, and weak interoperability. We argue that this situation can be significantly improved by increasing incentives for researchers to publish their software under an open source model. Additionally, we outline the problems authors are faced with when trying to publish algorithmic implementations of machine learning methods.  We believe that a resource of peer reviewed software accompanied by short articles would be highly valuable to both the machine learning and the general scientific community."
Delegating Responsibility in Digital Systems: Horton's Who Done It.  with apologies to Dr. Seuss
"Delay Learning and Polychronization for Reservoir Computing.  We propose a multi-scale learning rule for spiking neuron networks, in the vein of the recently emerging field of reservoir computing. The reservoir is a network model of spiking neurons, with random topology and driven by STDP (Spike-Time-Dependent Plasticity), a temporal Hebbian unsupervised learning mode, biologically observed. The model is further driven by a supervised learning algorithm, based on a margin criterion, that effects the synaptic delays linking the network to the readout neurons, with classification as a goal task. The network processing and the resulting performance can be explained by the concept of polychronization, proposed by Izhikevich (2006, Neural Computation, 18,1), on physiological bases. The model emphasizes the computational capabilities of this concept."
"Imagers as sensors: Correlating plant CO2 uptake with digital visible-light imagery.  There exist many natural phenomena where direct measurement is either impossible or extremely invasive. To obtain approximate measurements of these phenomena we can build prediction models based on other sensing modalities such as features extracted from data collected by an imager. These models are derived from controlled experiments performed under laboratory conditions, and can then be applied to the associated event in nature. In this paper we explore various different methods for generating such models and discuss their accuracy, robustness, and computational complexity. Given sufficiently computationally simple models, we can eventually push their computation down towards the sensor nodes themselves to reduce the amount of data required to both flow through the network and be stored in a database. The addition of these models turn in-situ imagers into powerful biological sensors, and image databases into useful records of biological activity."
"More Bang for Their Bucks: Assessing New Features for Online Advertisers.  Online search systems that display ads continually offer new features that advertisers can use to fine-tune and enhance their ad campaigns. An important question is whether a new feature actually helps advertisers. In an ideal world for statisticians, we would answer this question by running a statistically designed experiment. But that would require randomly assigning a set of advertisers to the treatment
group and forcing them to use the feature, which is not realistic. Accordingly, in the real world, new features for
advertisers are seldom evaluated with a traditional experimental protocol. Instead, customer service representatives (CSRs) select advertisers who are invited to be among the first to test a new feature (i.e., white-listed), and then each white-listed advertiser chooses whether or not to use the new feature.  Neither the CSR nor the advertiser chooses at random. This paper addresses the problem of drawing valid inferences from white-list trials about the effects of new features on advertiser happiness. We are guided by three principles. First, statistical procedures for white-list trials are likely to be applied in an automated way, so they should be robust to violations of modeling assumptions. Second, standard analysis tools should be preferred over custom-built ones, both for clarity and for robustness. Standard tools have withstood the test of time and have been thoroughly debugged. Finally, it should be easy to compute reliable confidence intervals for the estimator. We review an estimator that has all these attributes, allowing us to make valid inferences about the effects of a new feature on advertiser happiness.  In the example in this paper, the new feature was introduced during the holiday shopping season,
thereby further complicating the analysis."
"Predicting Accurate and Actionable Static Analysis Warnings: An Experimental Approach.  Static analysis tools report software defects that may or may not be detected by other verification methods. Two challenges complicating the adoption of these tools are spurious false positive warnings and legitimate warnings that are not acted on. This paper reports automated support to help address these challenges using logistic regression models that predict the foregoing types of warnings from signals in the warnings and implicated code. Because examining many potential signaling factors in large software development settings can be expensive, we use a screening methodology to quickly discard factors with low predictive power and cost-effectively build predictive models. Our empirical evaluation indicates that these models can achieve high accuracy in predicting accurate and actionable static analysis warnings, and suggests that the models are competitive with alternative models built without screening."
"Probabilistic Models for Melodic Prediction.  Chord progressions are the building blocks from which tonal music is constructed. The choice of a particular representation for chords has a strong impact on statistical modeling of the dependence between chord symbols and the actual sequences of notes in polyphonic music.  Melodic prediction is used in this paper as a benchmark task to evaluate the quality of four chord representations using two probabilistic model architectures derived from Input/Output Hidden Markov Models (IOHMMs). Likelihoods and conditional and unconditional prediction error rates are used as complementary measures of the quality of each of the proposed chord representations. We observe empirically that different chord representations are optimal depending on the chosen evaluation metric. Also, representing chords only by their roots appears to be a good compromise in most of the reported experiments."
"Span-program-based quantum algorithm for evaluating formulas.  We give a quantum algorithm for evaluating formulas over an extended gate set, including all two- and three-bit binary gates (e. g., NAND, 3-majority). The algorithm is optimal on read-once formulas for which each gate??s inputs are balanced in a certain sense. The main new tool is a correspondence between a classical linear-algebraic model of computation, ??span programs,?? and weighted bipartite graphs. A span program??s evaluation
corresponds to an eigenvalue-zero eigenvector of the associated graph. A quantum computer can therefore evaluate the span program by applying spectral estimation to the graph. For example, the classical complexity of evaluating the balanced ternary majority formula is unknown, and the natural generalization of randomized alpha-beta pruning is known to be suboptimal. In contrast, our algorithm generalizes the optimal quantum AND-OR formula
evaluation algorithm and is optimal for evaluating the balanced ternary majority formula."
"Paper interface to electronic medical records: a case of usage-driven technology appropriation.  We conducted a 6-month project with a physical therapy clinic, involving equal parts
ethnographic fieldwork and rapid prototyping. It differed from most reported user-informed
design by having an explicit dual purpose. On the one hand, the prototype should provide
significant, measurable improvements for the field site. On the other hand, the project sponsor
did not intend to develop the prototype into a product but rather identify future opportunities
and needs in the small-to-medium health care sector, requirements for next generation
multifunction peripherals (MFPs), and business applications of existing technology. Thus, the
project simultaneously investigated specific solutions for a specific work practice while looking
for key technologies to address future needs. This paper provides a detailed account of the
process and results, highlighting particular contingencies that come with a dual-purpose
exploration, as well as the benefits of a small, focused team that ??oscillates?? between research
and deployment."
"Relating Documents via User Activity: The Missing Link.  In this paper we describe a system for creating and exposing relationships between documents: a user??s interaction with digital objects (like documents) is interpreted as links ?? to be discovered and maintained by the system. Such relationships are created automatically, requiring no priming by the user. Using a very simple set of heuristics  we demonstrate the uniquely useful relationships that can be established between documents that have been touched by the user. Furthermore, this mechanism for relationship building is media agnostic, thus discovering relationships that would not be found by conventional content based approaches. We describe a proof-of-concept implementation of this basic idea and discuss a couple of natural expansions of the scope of user activity monitoring"
"Age-based Packet Arbitration in Large k-ary n-cubes.  As applications scale to increasingly large processor counts, the interconnection network is frequently the limiting factor in application performance. In order to achieve application scalability, the interconnect must maintain high bandwidth while minimizing variation in packet latency. As the offered load in the network increases with growing problem sizes and processor
counts, so does the expected maximum packet latency in the
network, directly impacting performance of applications with any synchronized communication. Age-based packet arbitration reduces the variance in packet latency as well as average latency. This paper describes the Cray XT router packet aging algorithm which allows globally fair arbitration by incorporating <i>age</i> in the packet output arbitration. We describe the parameters of the aging algorithm and how to arrive at appropriate settings. We show that an efficient aging algorithm reduces both the
average packet latency and the variance in packet latency on communication-intensive benchmarks."
"The Case for Energy-Proportional Computing.  In current servers, the lowest energy-efficiency region corresponds to their most common operating mode. Addressing this perfect mismatch will require significant rethinking of components and systems. To that end, we propose that energy proportionality should become a primary design goal. Energy-proportional designs would enable large energy savings in servers, potentially doubling their efficiency in real-life use. Achieving energy proportionality will require significant improvements in the energy usage profile of every system component, particularly the memory and disk subsystems. Although our experience in the server space motivates these observations, we believe that energy-proportional computing also will benefit other types of computing devices."
"An Overview of the Tesseract OCR Engine.  The Tesseract OCR engine, as was the HP Research
Prototype in the UNLV Fourth Annual Test of OCR
Accuracy[1], is described in a comprehensive
overview. Emphasis is placed on aspects that are novel
or at least unusual in an OCR engine, including in
particular the line finding, features/classification
methods, and the adaptive classifier."
"Tradeoffs in Retrofitting Security: An Experience Report.  In 1973, John Reynold??s and James Morris?? Gedanken language retrofit object-capability
security into an Algol-like base. Today, there are active projects retrofitting Java,
Javascript, Python, Mozart/Oz, OCaml, Perl, and Pict. These represent a variety of
approaches, with different tradeoffs regarding legacy compatibility, safety, and
expressivity. In this talk I propose a taxonomy of these approaches, and discuss some of the
lessons learned to date."
"Corrupted DNS Resolution Paths: The Rise of a Malicious Resolution Authority.  We study and document an important development in how attackers are using Internet resources: the creation of malicious DNS resolution paths. In this growing form of attack, victims are forced to use rogue DNS servers for all resolution. To document the rise of this ""second secret authority"" on the Internet, we studied instances of aberrant DNS resolution on a university campus. We found dozens of viruses that corrupt resolution paths, and noted that hundreds of URLs discovered per week performed drive-by alterations of host DNS settings."
"Large Scale Content-Based Audio Retrieval from Text Queries.  In content-based audio retrieval, the goal is to find sound recordings (audio documents) based on their acoustic features. This content-based approach differs from retrieval approaches that index media files using metadata such as file names and user tags. In this paper, we propose a machine learning approach for retrieving sounds that is novel in that it (1) uses free-form text queries rather sound sample based queries, (2) searches by audio content rather than via textual meta data, and (3) can scale to very large number of audio documents and very rich query vocabulary.  We handle generic sounds, including a wide variety of sound effects, animal vocalizations and natural scenes.  We test a scalable approach based on a passive-aggressive model for image retrieval (PAMIR), and compare it to two state-of-the-art approaches; Gaussian mixture models (GMM) and support vector machines (SVM). We test our approach on two large real-world datasets: a collection of
short sound effects, and a noisier and larger collection of
user-contributed user-labeled recordings (25K files, 2000 terms
vocabulary). We find that all three methods achieved very good
retrieval performance. For instance, a positive document is retrieved
in the first position of the ranking more than half the time, and on
average there are more than 4 positive documents in the first 10
retrieved, for both datasets. PAMIR completed both training and
retrieval of all data in less than 6 hours for both datasets, on a
single machine. It was one to three orders of magnitude faster than
the competing approaches. This approach should therefore scale to much
larger datasets in the future."
"Improving Word Alignment with Bridge Languages.  We describe an approach to improve Statistical Machine Translation (SMT) performance using multi-lingual, 
parallel, sentence-aligned corpora in several bridge languages. Our approach consists of a simple method for utilizing a bridge language to create a word alignment system and a  procedure for combining  word alignment systems from multiple bridge languages. The final translation is obtained by consensus decoding that combines
hypotheses obtained using all bridge language word alignments. We present experiments showing that multilingual, parallel text in Spanish, French, Russian,
and Chinese can be utilized in this framework to improve translation performance on an Arabic-to-English task."
"A Support Vector Approach to Censored Targets.  Censored targets, such as the time to events in survival analysis, can generally be represented by intervals on the real line. In this paper, we propose a novel support vector technique (named SVCR) for regression on censored targets. SVCR inherits the strengths of support vector methods, such as a globally optimal solution by convex programming, fast training speed and strong generalization capacity. In contrast to ranking approaches to survival analysis, our approach is able not only to achieve superior ordering performance, but also to predict the survival time very well. Experiments show a significant performance improvement when the majority of the training data is censored. Experimental results on several survival analysis datasets demonstrate that SVCR is very competitive against classical survival analysis models."
"Experiences Using Static Analysis to Find Bugs.  Static analysis examines code in the absence of input data and without running the code, and can detect potential security violations (e.g., SQL injection), runtime errors (e.g., dereferencing a null pointer) and logical
inconsistencies (e.g., a conditional test that cannot possibly be true). While there is a rich body of literature on algorithms and analytical frameworks used by such tools, reports describing experiences with such tools in industry are much harder to come by. In this paper, we describe FindBugs, an open source static analysis tool for Java, and experience using it in production settings. FindBugs does not push the envelope in terms of the sophistication of its analysis techniques. Rather, it is designed to evaluate what kinds of defects can be effectively detected with relatively simple techniques and to help us understand how such tools can be incorporated into the software development process. FindBugs has become very popular, downloaded more than 500,000 times and used by many major companies and software projects.  We report on experience running FindBugs against Sun??s JDK implementation, using Findbugs at Google where it has been used for more than a year and a half and incorporated into their standard development process, and preliminary results from a survey of FindBugs users."
"Predictive Models for Music.  Modeling long-term dependencies in time series has proved very difficult to achieve with traditional machine learning methods. This problem occurs when considering music data. In this paper, we introduce predictive models for melodies. We decompose melodic modeling into two subtasks. We first propose a rhythm model based on the distributions of distances between subsequences. Then, we define a generative model for melodies given chords and rhythms based on modeling sequences of Narmour features. The rhythm model consistently outperforms a standard Hidden Markov Model in terms of conditional prediction accuracy on two different music databases.  Using a similar evaluation procedure, the proposed melodic model consistently outperforms an Input/Output Hidden Markov Model.  Furthermore, these models are able to generate realistic melodies given appropriate musical contexts."
"Distributed divide-and-conquer techniques for effective DDoS attack defenses.  Distributed Denial-of-Service (DDoS) attacks have emerged as a popular means of causing mass targeted service disruptions, often for extended periods of time. The relative ease and low costs of launching such attacks, supplemented by the current woeful state of any viable defense mechanism, have made them one of the top threats to the Internet community today. While distributed packet logging and/or packet marking have been explored in the past for DDoS attack traceback/mitigation, we propose to advance the state of the art by using a novel distributed divide-and-conquer approach in designing a new data dissemination architecture that efficiently tracks attack sources. The main focus of our work is to tackle the three disjoint aspects of the problem, namely attack tree construction, attack path frequency detection, and packet to path association, independently and to use succinct recurrence relations to express their individual implementations. We also evaluate the network traffic and storage overhead induced by our proposed deployment on real-life Internet topologies, supporting hundreds of victims each subject to thousands of high-bandwidth flows simultaneously, and conclude that we can truly achieve single packet traceback guarantees with minimal overhead and high efficiency."
"Translating Queries into Snippets for Improved Query Expansion.  User logs of search engines have recently been applied successfully to improve various aspects of web search quality. In this paper, we will apply pairs of user queries and snippets of clicked results to train a machine translation model to bridge the ``lexical gap'' between query and document space. We show that the combination of a query-to-snippet translation model with a large n-gram language
model trained on queries achieves improved contextual query expansion compared to a system based on term correlations."
"Markovian Mixture Face Recognition with discriminative face alignment.  A typical automatic face recognition system is composed
of three parts: face detection, face alignment and face
recognition. Conventionally, these three parts are processed
in a bottom-up manner: face detection is performed first,
then the results are passed to face alignment, and finally
to face recognition. The bottom-up approach is one extreme
of vision approaches. The other extreme approach
is top-down. In this paper, we proposed a stochastic mixture
approach for combining bottom-up and top-down face
recognition: face recognition is performed from the results
of face alignment in a bottom-up way, and face alignment
is performed based on the results of face recognition in
a top-down way. By modeling the mixture face recognition
as a stochastic process, the recognized person is decided
probabilistically according to the probability distribution
coming from the stochastic face recognition, and the
recognition problem becomes that ??who the most probable
person is when the stochastic process of face recognition
goes on for a long time or ideally for an infinite duration??.
This problem is solved with the theory of Markov chains
by modeling the stochastic process of face recognition as a
Markov chain. As conventional face alignment is not suitable
for this mixture approach, discriminative face alignment
is proposed. And we also prove that the stochastic
mixture face recognition results only depend on discriminative
face alignment, not on conventional face alignment.
The effectiveness of our approach is shown by extensive experiments."
"A Distance Model for Rhythms.  Modeling long-term dependencies in time series has proved very
difficult to achieve with traditional machine learning methods. This
problem occurs when considering music data. In this paper, we
introduce a model for rhythms based on the distributions
of distances between subsequences.  A specific implementation of the
model when considering Hamming distances over a simple rhythm
representation is described. The proposed model consistently
outperforms a standard Hidden Markov Model in terms of conditional
prediction accuracy on two different music databases."
"Face Tracking and Recognition with Visual Constraints in Real-World Videos.  We address the problem of tracking and recognizing
faces in real-world, noisy videos. We track faces using
a tracker that adaptively builds a target model reflecting
changes in appearance, typical of a video setting. However,
adaptive appearance trackers often suffer from drift, a gradual
adaptation of the tracker to non-targets. To alleviate this
problem, our tracker introduces visual constraints using a
combination of generative and discriminative models in a
particle filtering framework. The generative term conforms
the particles to the space of generic face poses while the discriminative
one ensures rejection of poorly aligned targets.
This leads to a tracker that significantly improves robustness
against abrupt appearance changes and occlusions,
critical for the subsequent recognition phase. Identity of the
tracked subject is established by fusing pose-discriminant
and person-discriminant features over the duration of a
video sequence. This leads to a robust video-based face recognizer
with state-of-the-art recognition performance. We
test the quality of tracking and face recognition on realworld
noisy videos from YouTube as well as the standard
Honda/UCSD database. Our approach produces successful
face tracking results on over 80% of all videos without
video or person-specific parameter tuning. The good tracking
performance induces similarly high recognition rates:
100% on Honda/UCSD and over 70% on the YouTube set
containing 35 celebrities in 1500 sequences."
"Large-Scale Manifold Learning.  This paper examines the problem of extracting low-dimensional manifold structure given millions of high-dimensional face images. Specifically, we address the computational challenges of nonlinear dimensionality reduction via Isomap and Laplacian Eigenmaps, using a graph containing about 18 million nodes and 65 million edges. Since most manifold learning techniques rely on spectral decomposition, we first analyze two approximate spectral decomposition techniques for large dense matrices (Nystrom and Column-sampling), providing the first direct theoretical and empirical comparison between these techniques. We next
show extensive experiments on learning low-dimensional
embeddings for two large face datasets: CMU-PIE (35
thousand faces) and a web dataset (18 million faces). Our
comparisons show that the Nystrom approximation is superior
to the Column-sampling method. Furthermore, approximate
Isomap tends to perform better than Laplacian
Eigenmaps on both clustering and classification with the
labeled CMU-PIE dataset."
Amdahl's Law in the Multicore Era.  Augmenting Amdahl??s Law with a corollary for multicore hardware makes it relevant to future generations of chips with multiple processor cores. Obtaining optimal multicore performance will require further research in both extracting more parallelism and making sequential cores faster.
"Physics, Topology, Logic and Computation: A Rosetta Stone.  In physics, Feynman diagrams are used to reason about quantum processes. In the 1980s, it became clear that underlying these diagrams is a powerful analogy between quantum physics and topology: namely, a linear operator behaves very much like a ""cobordism"". Similar diagrams can be used to reason about logic, where they represent proofs, and computation, where they represent programs. With the rise of interest in quantum cryptography and quantum computation, it became clear that there is extensive network of analogies between physics, topology, logic and computation. In this expository paper, we make some of these analogies precise using the concept of ""closed symmetric monoidal category"". We assume no prior knowledge of category theory, proof theory or computer science."
"Sequence Kernels for Predicting Protein Essentiality.  The problem of identifying the minimal gene set required to sustain life is of crucial importance in understanding cellular mechanisms and designing therapeutic drugs. This work describes several kernel-based solutions for predicting essential genes that outperform existing models while using less training data. Our first solution is based on a semi-manually designed kernel derived from the Pfam database, which includes several Pfam domains. We then present novel and general {\em domain-based} sequence kernels that capture sequence similarity with respect to several domains made of large sets of protein sequences. We show how to deal with the large size of the problem -- several thousands of domains with individual domains sometimes containing thousand of sequences -- by representing and efficiently computing these kernels using automata.  We report results of extensive experiments demonstrating that they compare favorably with the Pfam kernel in predicting protein essentiality, while requiring no manual tuning."
"A New ELF Linker.  gold is a new ELF linker recently added to the GNU binutils.  I discuss why it made sense to write a new linker rather than extend the existing one.  I describe the architecture of the linker, and new features.  I present performance measurements.  I discuss future plans for the linker.  I discuss the use of C++ in writing system tools."
"Analysis of a Mixed-Use Urban WiFi Network: When Metropolitan becomes Neapolitan.  In this paper, we study the usage of the Google WiFi network deployed in Mountain View, California. We find that usage naturally falls into three categories, based almost entirely on client device type. Moreover, each of these classes of use has significant geographical, and transportation areas of the city. Finally, we find a diverse set of mobility patterns that map well to the archetypal use cases for traditional access technologies."
"Emotional Memory and Adaptive Personalities.  Believable agents designed for long-term interaction with human users need to adapt to them in a way which appears emotionally plausible while maintaining a consistent personality. For short-term interactions in restricted environments, scripting and state machine techniques can create agents with emotion and personality, but these methods are labor intensive, hard to extend, and brittle in new environments.  Fortunately, research in memory, emotion and personality in humans and animals points to a solution to this problem.  Emotions focus an animal??s attention on things it needs to care about, and strong emotions trigger enhanced formation of memory, enabling the animal to adapt its emotional response to the objects and situations in its environment.  In humans this process becomes reflective: emotional stress or frustration can trigger re-evaluating past behavior with respect to personal standards, which in turn can lead to setting new strategies or goals.   To aid the authoring of adaptive agents, we present an artificial intelligence model inspired by these psychological results in which an emotion model triggers case-based emotional preference learning and behavioral adaptation guided by personality models.  Our tests of this model on robot pets and embodied characters show that emotional adaptation can extend the range and increase the behavioral sophistication of an agent without the need for authoring additional hand-crafted behaviors."
"Online Effects of Offline Ads.  We propose a methodology for assessing how
ad campaigns in offline media such as print, audio and TV affect online interest in the advertiser's brand. Online interest can be measured by daily counts of the number of search queries that contain brand related keywords, by the number of visitors to the advertiser's web pages, by the number of pageviews at the advertiser's websites, or by the total duration of visits to the advertiser's website. An increase in outcomes like these in designated market areas (DMAs) where the offline ad appeared suggests heightened interest in the advertised product, as long as there would have been no such increase if the ad had not appeared. We propose a regression analysis to estimate the incremental value of the ad campaign beyond the baseline interest that would have been seen if the campaign had not been shown. A small print ad campaign illustrates the method."
"Google TV Search: Dual-Wielding Search and Discovery in a Large-Scale Product.  In 2006 Google designed and implemented a TV Show search system which featured a dual-navigation control combining a powerful search system with an interactive, personalize-able TV listings grid.  The dual-control system allowed users to move fluidly between explicit search and general discovery, obtain detailed information without leaving the global context and construct a personal channel for easy access and recommendations.  As a complete system, TV Search integrated with Google's general search page through a mini-listings onebox and also exported information out to Google calendar, Gmail and TiVo<sup>???</sup>. Building this TV Search system as a large-scale product meant addressing many technological, business and fine-grained user experience details. This paper describes the design process including the early designs, the user studies and the details of the final system including the mini-guide, main page dual-control and the integrated services on the detail pages."
"Guarded Program Transformations Using JTL.  There is a growing research interest in employing the logic paradigm for making queries on software in general, and OOP software in particular. We describes a side-effect-free technique of using the paradigm for the general task of program transformation. Our technique offers a variety of applications, such as implementing generic structures (without erasure) in JAVA, a Lint-like program checker, and more. By allowing the transformation target to be a different language than the source (program translation), we show how the language can be employed for tasks like the generation of database schemas or XML DTDs that match JAVA classes."
"User Preference and Search Engine Latency.  Presented at the 2008 Quality and Productivity Research Conference in Madison, WI."
"A Generative Model for Rhythms.  Modeling music involves capturing long-term dependencies in time series, which has proved very difficult to achieve with traditional statistical methods. The same problem occurs when only considering rhythms. In this paper, we introduce a generative model for rhythms based on the distributions of distances between subsequences.  A specific implementation of the model when considering Hamming distances over a simple rhythm representation is described. The proposed model consistently outperforms a standard Hidden Markov Model in terms of conditional prediction accuracy on two different music databases."
"Scalable Attribute-Value Extraction from Semi-Structured Text.  This paper describes a general methodology for extracting attribute-value pairs from web pages.  It consists of two phases: <i>candidate generation</i>, in which syntactically likely attribute-value pairs are annotated; and <i>candidate filtering</i>, in which semantically improbable annotations are removed.  We describe three types of candidate generators and two types of candidate filters, all of which are designed to be massively parallelizable.  Our methods can handle 1 billion web pages in less than 6 hours with 1,000 machines.  The best generator and filter combination achieves 70% F-measure compared to a hand-annotated corpus."
"Detecting influenza epidemics using search engine query data.  Seasonal influenza epidemics are a major public health concern, causing tens of millions of respiratory illnesses and 250,000 to 500,000 deaths worldwide each year. In addition to seasonal influenza, a new strain of influenza virus against which no previous immunity exists and that demonstrates human-to-human transmission could result in a pandemic with millions of fatalities. Early detection of disease activity, when followed by a rapid response, can reduce the impact of both seasonal and pandemic influenza. One way to improve early detection is to monitor health-seeking behaviour in the form of queries to online search engines, which are submitted by millions of users around the world each day. Here we present a method of analysing large numbers of Google search queries to track influenza-like illness in a population. Because the relative frequency of certain queries is highly correlated with the percentage of physician visits in which a patient presents with influenza-like symptoms, we can accurately estimate the current level of weekly influenza activity in each region of the United States, with a reporting lag of about one day. This approach may make it possible to use search queries to detect influenza epidemics in areas with a large population of web search users."
An Empirical Study on Computing Consensus Translations from Multiple Machine Translation Systems.  This paper presents an empirical study on how different selections of input translation systems affect translation quality in system combination. We give empirical evidence that the systems to be combined should be of similar quality and need to be almost uncorrelated in order to be beneficial for system combination. Experimental results are presented for composite translations computed from large numbers of different research systems as well as a set of translation systems derived from one of the best-ranked machine translation engines in the 2006 NIST machine translation evaluation.
"Session Viewer: Visual Exploratory Analysis of Web Session Logs.  Large-scale session log analysis typically includes statistical methods and detailed log examinations. While both methods have merits, statistical methods can miss previously unknown subpopulations in the data and detailed analyses may have selection biases. We therefore built Session Viewer, a visualization tool to facilitate and bridge between statistical and detailed analyses. Taking
a multiple-coordinated view approach, Session Viewer shows
multiple session populations at the Aggregate, Multiple, and Detail data levels to support different analysis styles. To bridge between the statistical and the detailed analysis levels, Session Viewer provides fluid traversal between data levels and side-by-side comparison at all data levels. We describe an analysis of a large-scale web usage study to demonstrate the use of Session Viewer, where we quantified the importance of grouping sessions based on task type."
"Automatic Speech and Speaker Recognition: Large Margin and Kernel Methods.  This is the first book dedicated to uniting research related to speech and speaker recognition based on the recent advances in large margin and kernel methods. The first part of the book presents theoretical and practical foundations of large margin and kernel methods, from support vector machines to large margin methods for structured learning. The second part of the book is dedicated to acoustic modeling of continuous speech recognizers, where the grounds for practical large margin sequence learning are set. The third part introduces large margin methods for discriminative language modeling. The last part of the book is dedicated to the application of keyword spotting, speaker verification and spectral clustering. The book is an important reference to researchers and practitioners in the field of modern speech and speaker recognition. The purpose of the book is twofold; first, to set the theoretical foundation of large margin and kernel methods relevant to speech recognition domain; second, to propose a practical guide on implementation of these methods to the speech recognition domain. The reader is presumed to have basic knowledge of large margin and kernel methods and of basic algorithms in speech and speaker recognition."
"Discriminative Keyword Spotting.  This chapter introduces a discriminative method for detecting and spotting keywords in spoken utterances. Given a word represented as a sequence of phonemes and a spoken utterance, the keyword spotter predicts the best time span of the phoneme sequence in the spoken utterance along with a confidence. If the prediction confidence is above certain level the keyword is declared to be spoken in the utterance within the predicted time span, otherwise the keyword is declared as not spoken. The problem of keyword spotting training is formulated as a discriminative task where the model parameters are chosen so the utterance in which the keyword is spoken would have higher confidence than any other spoken utterance in which the keyword is not spoken. It is shown theoretically and empirically that the proposed training method resulted with a high area under the receiver operating (ROC) (ROC) curve, the most common measure to evaluate keyword spotters. We present an iterative algorithm to train the keyword spotter efficiently. The proposed approach contrasts with standard spotting strategies based on HMMs, for which the training procedure does not maximize a loss directly related to the spotting performance. Several experiments performed on TIMIT and WSJ corpora show the advantage of our approach over HMM-based alternatives."
"Kernel Based Text-Independnent Speaker Verification.  The goal of a person authentication system is to authenticate the claimed identity of a user. When this authentication is based on the voice of the user, without respect of what the user exactly said, the system is called a text-independent speaker verification system.  Speaker verification systems are increasingly often used to secure personal information, particularly for mobile phone based applications. Furthermore, text-independent versions of speaker verification systems are the most used for their simplicity, as they do not require complex speech recognition modules.  The most common approach to this task is based on Gaussian Mixture Models (GMMs), which do not take into account any temporal information. GMMs have been intensively used thanks to their good performance, especially with the use of the Maximum A Posteriori (MAP) adaptation algorithm. This approach is based on the density estimation of an impostor data distribution, followed by its adaptation to a specific client data set. Note that the estimation of these densities is not the final goal of speaker verification systems, which is rather to discriminate the client and impostor classes; hence discriminative approaches might appear good candidates for this task as well.  As a matter of fact, Support Vector Machine (SVM) based systems have been the subject of several recent publications in the speaker verification community, in which they obtain similar to or even better performance than GMMs on several text-independent speaker verification tasks. In order to use SVMs or any other discriminant approaches for speaker verification, several modifications from the classical techniques need to be performed. The purpose of this chapter is to present an overview of discriminant approaches that have been used successfully for the task of text-independent speaker verification, to analyze their difference and their similarities with each other and with classical generative approaches based on GMMs.  An open-source version of the C++ source code used to performed all experiments described in this chapter can be found at http://speaker.abracadoudou.com."
"Restoring Punctuation and Capitalization in Transcribed Speech.  Adding punctuation and capitalization greatly improves the readability of automatic speech transcripts. We discuss an approach for performing both tasks in a single pass using a purely text-based n-gram language model. We study the effect on performance of varying the n-gram order (from n = 3 to n = 6) and the amount of training data (from 58 million to 55 billion tokens). Our results show that using larger training data sets consistently improves performance, while increasing the n-gram order does not help nearly as much."
"Activity Motifs Reveal Principles of Timing in Transcriptional Control of the Yeast Metabolic Network.  Significant insight about biological networks arises from the study of network motifs??overly abundant network subgraphs, but such wiring patterns do not specify when and how potential routes within a cellular network are used. To address this limitation, we introduce activity motifs, which capture patterns in the dynamic use of a network. Using this framework to analyze transcription in Saccharomyces cerevisiae metabolism, we find that cells use different timing activity motifs to optimize transcription timing in response to changing conditions: forward activation to produce metabolic compounds efficiently, backward shutoff to rapidly stop production of a detrimental product and synchronized activation for co-production of metabolites required for the same reaction. Measuring protein abundance over a time course reveals that mRNA timing motifs also occur at the protein level. Timing motifs significantly overlap with binding activity motifs, where genes in a linear chain have ordered binding affinity to a transcription factor, suggesting a mechanism for ordered transcription. Finely timed transcriptional regulation is therefore abundant in yeast metabolism, optimizing the organism's adaptation to new environmental conditions."
"Cooperative Coevolution and Univariate Estimation of Distribution Algorithms.  In this paper, we discuss a curious relationship between Cooperative Coevolutionary Algorithms (CCEAs) and Univariate EDAs.  Inspired by the theory of CCEAs, we also present a new EDA with theoretical convergence guarantees, and some preliminary experimental results in comparison with existing Univariate EDAs."
"Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation.  We present Minimum Bayes-Risk (MBR) decoding over translation lattices that compactly encode
a huge number of translation hypotheses. We describe conditions on the loss function
that will enable efficient implementation of MBR decoders on lattices. We introduce
an approximation to the BLEU score~\cite{papineni01} that satisfies these conditions. The MBR decoding under this approximate BLEU is realized using Weighted Finite State Automata. Our experiments show that the Lattice MBR decoder yields moderate, consistent gains in
translation performance over N-best MBR decoding on Arabic-to-English, Chinese-to-English and English-to-Chinese translation tasks. We conduct a range of experiments to
understand why Lattice MBR improves upon N-best MBR and also study the impact of various parameters on MBR performance."
"Lattice-based Minimum Error Rate Training for Statistical Machine Translation.  Minimum Error Rate Training (MERT) is an effective means to estimate the feature function weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training. To accomplish this, the training procedure determines for each feature function its exact error surface on a given set of candidate translations. The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum. Typically, candidates in MERT are represented as N-best lists which contain the N most probable translation hypotheses produced by a decoder. In this paper, we present a novel algorithm
that allows for efficiently constructing and representing the exact error surface of all translations that are encoded in a phrase lattice. Compared to N-best MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes. The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system. Experiments
conducted on the NIST 2008 translation tasks show significant runtime improvements and moderate BLEU score gains over N-best MERT."
"Undo and Erase Events as Indicators of Usability Problems.  One approach to reduce the costs of usability testing is to facilitate the automatic detection of critical incidents: serious breakdowns in interaction that stand out during software use. This research evaluates the use of undo and erase events as indicators of critical incidents in Google SketchUp (a 3D-modeling application), measuring an indicator??s usefulness by the numbers and types of usability problems discovered. Our evaluation also compares problems identified using undo and erase events to problems identified using the user-reported critical incident technique [CITE]. In a within-subjects experiment with 37 participants, undo and erase episodes together revealed over 80% of the problems rated as severe, one third of which would not have been discovered by self-report alone. Moreover, problems found by all three techniques were rated as significantly more severe than those identified by only a subset of techniques. These results suggest that undo and erase events will serve as a useful complement to user reported critical incidents for low cost usability evaluation of design-oriented applications like Google SketchUp."
"The Mobile Web in Developing Countries.  The mobile web in developing countries has received increasing attention within the last few years,
both as a potential means of bridging the digital divide as well as a lucrative market opportunity.
However, while the realized gains so far as well as the potential are indeed tremendous, significant
challenges remain to be overcome. Mobile data usage, particularly for advanced data applications,
faces difficulties that are different from those posed by the initial expansion of voice services. The
needs and environments of developing countries are very diverse, with as many significant differences
perhaps as similarities, making it difficult to replicate country-specific solutions. In addition, while
one traditional migration route of functionality ?? from the desktop to the handheld ?? may be viable in
the industrialized world, it is not clear that this is the likely best approach in developing countries.
What does seem clear is that there is a definite and significant need for further research examining the
characteristics and challenges of the mobile web in developing countries at all layers, ranging from
applications to networking. We sketch examples of such research issues, and mention specific roles
the W3C could potentially play. This brief position paper presents these hypotheses with the goal of
stimulating discussion at the workshop."
"Learning to hash: forgiving hash functions and applications	
Learning to hash: forgiving hash functions and applications.  The problem of efficiently finding similar items in a large corpus of high-dimensional data points arises in many real-world tasks, such as music, image, and video retrieval. Beyond the scaling difficulties that arise with lookups in large data sets, the complexity in these domains is exacerbated by an imprecise definition of similarity. In this paper, we describe a method to learn a similarity function from only weakly labeled positive examples. Once learned, this similarity function is used as the basis of a hash function to severely constrain the number of points considered for each lookup. Tested on a large real-world audio dataset, only a tiny fraction of the points (~0.27%) are ever considered for each lookup. To increase efficiency, no comparisons in the original high-dimensional space of points are required. The performance far surpasses, in terms of both efficiency and accuracy, a state-of-the-art Locality-Sensitive-Hashing-based (LSH) technique for the same problem and data set."
"VisualRank: Applying PageRank to Large-Scale Image Search.  Because of the relative ease in understanding and processing text, commercial image-search systems often rely on
techniques that are largely indistinguishable from text search. Recently, academic studies have demonstrated the effectiveness of
employing image-based features to provide either alternative or additional signals to use in this process. However, it remains uncertain
whether such techniques will generalize to a large number of popular Web queries and whether the potential improvement to search
quality warrants the additional computational cost. In this work, we cast the image-ranking problem into the task of identifying
??authority?? nodes on an inferred visual similarity graph and propose VisualRank to analyze the visual link structures among images.
The images found to be ??authorities?? are chosen as those that answer the image-queries well. To understand the performance of such
an approach in a real system, we conducted a series of large-scale experiments based on the task of retrieving images for 2,000 of the
most popular products queries. Our experimental results show significant improvement, in terms of user satisfaction and relevancy, in
comparison to the most recent Google Image Search results. Maintaining modest computational cost is vital to ensuring that this
procedure can be used in practice; we describe the techniques required to make this system practical for large-scale deployment in
commercial search engines."
"Parallelizing Support Vector Machines on Distributed Computers.  We also open-source this work at: 
<a href=""http://code.google.com/p/psvm/"">http://code.google.com/p/psvm/</a>."
"Reading the Markets: Forecasting Public Opinion of Political Candidates by News Analysis.  Media reporting shapes public opinion which can in turn influence events, particularly in political elections, in which candidates both respond to and shape public perception of their campaigns. We use computational linguistics to automatically predict the impact of news on public perception of political candidates. Our system uses daily newspaper articles to predict shifts in public opinion as reflected in prediction markets. We discuss various types of features designed for this problem. The news system improves market prediction over baseline market systems."
"Confidence-Weighted Linear Classification.  We introduce confidence-weighted linear classifiers, which add parameter confidence information to linear classifiers. Online learners in this setting update both classifier parameters and the estimate of their confidence. The particular online algorithms we study here maintain a Gaussian distribution over parameter vectors and update the mean and covariance of the distribution with each instance. Empirical evaluation on a range of NLP tasks show that our algorithm improves over other state of the art online and batch methods, learns faster in the online setting, and lends itself to better classifier combination after parallel training."
"Parallel Spectral Clustering.  Spectral clustering algorithm has been shown to be more eective in nding clusters than most traditional algorithms. However, spectral clustering suers from a scalability problem in both memory use and computational time when a dataset size is large. To perform clustering on large datasets, we propose to parallelize both memory use and computation on distributed computers. Through an empirical study on a large document dataset of 193,844 data instances and a large photo dataset of 637,137, we demonstrate that our parallel algorithm can effectively alleviate the scalability problem."
Asymptotic Performance of the Non-Forced Idle Time Scheduling Policies in the Presence of Variable Demand for Resources.  http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4797599&amp;tag=1
"Privacy Protection in Video Surveillance.  <a href=""http://www.springer.com/computer/computer+imaging/book/978-1-84882-300-6"">An edited book dealing with various aspects of privacy protection in automatic video surveillance systems.</a> Chapters deal with redaction/obscuration, cryptography, detection, integration with RFID, performance analysis, social issues and acceptance."
"Learning Invariant Features Using Inertial Priors.  We address the technical challenges involved in combining key features from several theories of the visual cortex in a single coherent model. The resulting model is a hierarchical Bayesian network factored into modular component networks embedding variable-order Markov models. Each component network has an associated receptive field corresponding to components residing in the level directly below it in the hierarchy.  The variable-order Markov models account for features that are invariant to naturally occurring transformations in their inputs. These invariant features give rise to increasingly stable, persistent representations as we ascend the hierarchy.  The receptive fields of proximate components on the same level overlap to restore selectivity that might otherwise be lost to invariance."
"Scalable Inference in Hierarchical Generative Models.  We address the technical challenges involved in combining key features from several theories of the visual cortex in a single coherent model. The resulting model is a hierarchical Bayesian network factored into modular component networks embedding variable-order Markov models. Each component network has an associated receptive ?eld corresponding to components residing in the level directly below it in the hierarchy. The variable-order Markov models account for features that are invariant to naturally occurring transformations in their inputs. These invariant features give rise to increasingly stable, persistent representations as we ascend the hierarchy. The receptive ?elds of proximate components on the same level overlap to restore selectivity that might otherwise be lost to invariance."
"On the Prospects for Building a Working Model of the Visual Cortex.  Human-level visual performance has remained largely beyond the reach of engineered systems despite decades of research and significant advances in problem formulation, algorithms and computing power. We posit that significant progress can be made by combining existing technologies from machine vision, insights from theoretical neuroscience and large-scale distributed computing. Such claims have been made before and so it is quite reasonable to ask what are the new ideas we bring to the table that might make a difference this time around. From a theoretical standpoint, our primary point of departure from current practice is our reliance on exploiting time in order to turn an otherwise intractable unsupervised problem into a locally semi-supervised, and plausibly tractable, learning problem. From a pragmatic perspective, our system architecture follows what we know of cortical neuroanatomy and provides a solid foundation for scalable hierarchical inference. This combination of features provides the framework for implementing a wide range of robust object-recognition capabilities."
"A Computational Model of the Cerebral Cortex.  Our current understanding of the primate cerebral cortex (neocortex) and in particular the posterior, sensory association cortex has matured to a point where it is possible to develop a family of graphical models that capture the structure, scale and power of the neocortex for purposes of associative recall, sequence prediction and pattern completion among other functions.  Implementing such  models using readily available computing clusters is now within the grasp of many labs and would provide scientists with the opportunity to experiment with both hard-wired connection schemes and structure-learning algorithms inspired by animal learning and developmental studies.  While neural circuits involving structures external to the neocortex such as the thalamic nuclei are less well understood, the availability of a computational model on which to test hypotheses would likely accelerate our understanding of these circuits.  Furthermore, the existence of an agreed-upon cortical substrate would not only facilitate our understanding of the brain but enable researchers to combine lessons learned from biology with state-of-the-art graphical-model and machine-learning techniques to design hybrid systems that combine the best of biological and traditional computing approaches."
"Revisiting Graphemes with Increasing Amounts of Data.  Letter units, or graphemes, have been reported in the literature as a surprisingly effective substitute to the more traditional phoneme units, at least in languages that enjoy a strong correspondence between pronunciation and orthography. For English however, where letter symbols have less acoustic consistency, previously reported results fell short of systems using highly-tuned pronunciation lexicons. Grapheme units simplify system design, but since graphemes map to a wider set of acoustic realizations than phonemes, we should expect grapheme-based acoustic models to require more training data to capture these variations. In this paper, we compare the rate of improvement of
grapheme and phoneme systems trained with datasets ranging from 450 to 1200 hours of speech. We consider various grapheme unit configurations, including using letter-specific, onset, and coda units. We show that the grapheme systems improve faster and, depending on the lexicon, reach or surpass
the phoneme baselines with the largest training set. Index Terms?? Acoustic modeling, graphemes, directory assistance, speech recognition."
"The Goals and Challenges of Click Fraud Penetration Testing Systems.  It is important for search and pay-per-click engines to penetration test their click fraud detection systems, in order to find potential vulnerabilities and correct them before fraudsters can exploit them. In this paper, we describe: (1) some goals and desirable qualities of a click fraud penetration testing system, based on our experience, and (2) our experiences with the challenges of building and using a click fraud penetration testing system called Camelot that has been in use at Google."
"Web-derived Pronunciations.  Pronunciation information is available in large quantities on the Web, in the form of IPA and ad-hoc transcriptions. We describe techniques for extracting candidate pronunciations from Web pages and associating them with orthographic words, filtering out poorly extracted pronunciations, normalizing IPA pronunciations to better conform to a common transcription standard, and generating phonemic from ad-hoc transcriptions. We show improvements on a letter-to-phoneme task when using web-derived vs. Pronlex pronunciations."
"Discriminative Keyword Spotting.  This paper proposes a new approach for keyword spotting, which is based on large margin and kernel methods rather than on HMMs. Unlike previous approaches, the proposed method employs a discriminative learning procedure, in which the learning phase aims at achieving a high area under the ROC curve, as this quantity is the most common measure to evaluate keyword spotters. The keyword spotter we devise is based on mapping the input acoustic representation of the speech utterance along with the target keyword into a vector space. Building on techniques used for large margin and kernel methods for predicting whole sequences, our keyword spotter distills to a classifier in this vector-space, which separates speech utterances in which the keyword is uttered from speech utterances in which the keyword is not uttered. We describe a simple iterative algorithm for training the keyword spotter and discuss its formal properties, showing theoretically that it attains high area under the ROC curve. Experiments on read speech with the TIMIT corpus show that the resulted discriminative system outperforms the conventional context-independent HMM-based system. Further experiments using the TIMIT trained model, but tested on both read (HTIMIT, WSJ) and spontaneous speech (OGI-Stories), show that without further training or adaptation to the new corpus our discriminative system outperforms the conventional context-independent HMM-based system."
"Version Control with Subversion, Second Edition.  The official guide and reference manual for the popular open source revision control technology."
"It's Time To Retire the ""n &gt;= 30"" rule..  The old rule of using z or t tests or confidence intervals if n &gt;= 30 is a relic of the pre-computer era, and should be discarded in favor of bootstrap-based diagnostics. The diagnostics will surprise many statisticians, who don't realize how lousy the classical inferences are.  For example, 95% confidence intervals should miss 2.5% on each side, and we might expect the actual non-coverage to be within 10% of that. Using a t interval, this requires n &gt; 5000 for a moderately-skewed (exponential) population.  There are better confidence intervals and tests, bootstrap and others. The bootstrap also offers pedagogical benefits in teaching sampling distributions and other statistical concepts, offering actual distributions that can be viewed using histograms and other familiar techniques."
"Learning to be a software engineer in a complex organization: A case study focusing on apprenticeship/practice based learning for getting new engineers productive in contributing to the Google codebase. Purpose: This paper seeks to analyse the effectiveness and impact of how Google currently trains its new software engineers (??Nooglers??) to become productive in the software engineering community. The research focuses on the institutions and support for practice-based learning and cognitive apprenticeship in the Google environment.
Design/methodology/approach: The study uses a series of semi-structured interviews with 24 Google stakeholders. These interviews are complemented by observations, document analysis, and review of existing survey and statistical data.
Findings:It is found that Google offers a ""te-of-the-art onboarding program and benchmark qualities that provide legitimate peripheral participation. The research reveals how Google empowers programmers to ??feel at home?? using company coding practices, as well as maximizing peer-learning and collaborative practices. These practices reduce isolation, enhance collegiality, and increase employee morale and job satisfaction.
Research limitations/implications: The case study describes the practices in one company.
Practical implications: The research documented in the paper can be used as a benchmark for other onboarding and practice-based learning set-ups.
Originality/value: This is the first research that gives insights into the practice-based learning and onboarding practices at Google. The practices are assessed to be state-of-the-art and the insights therefore relevant for benchmarking exercises of other companies.
<h2 class=""ntent__title "">
          Abstract
        </h2>
<div class=""content__body"">
<dl><dt>Purpose</dt><dd>This paper seeks to analyse the effectiveness and impact of how Google currently trains its new software engineers (??Nooglers??) to become productive in the software engineering community. The research focuses on the institutions and support for practice-based learning and cognitive apprenticeship in the Google environment.</dd>
<dt>Design/methodology/approach</dt><dd>The study uses a series of semi-structured interviews with 24 Google stakeholders. These interviews are complemented by observations, document analysis, and review of existing survey and statistical data.</dd>
<dt>Findings</dt><dd>It is found that Google offers a state-of-the-art onboarding program and benchmark qualities that provide le"
"Forecasting Web Page Views: Methods and Observations.  Web sites must forecast Web page views in order to plan computer resource allocation and estimate upcoming revenue and advertising growth. In this paper, we focus on extracting trends and seasonal patterns from page view series, two dominant factors in the variation of such series. We investigate the Holt-Winters procedure and a state space model for making relatively short-term prediction. It is found that Web page views exhibit strong impulsive changes occasionally. The impulses cause large prediction errors long after their occurrences. A method is developed to identify impulses and to alleviate their damage on prediction. We also develop a long-range trend and season extraction method, namely the Elastic Smooth Season Fitting (ESSF) algorithm, to compute scalable and smooth yearly seasons. ESSF derives the yearly season by minimizing the residual sum of squares under smoothness regularization, a quadratic optimization problem. It is shown that for long-term prediction, ESSF improves accuracy significantly over other methods that ignore the yearly seasonality."
"Native Client: A Sandbox for Portable, Untrusted x86 Native Code.  Native Client is an open-source research technology for running x86 native code in web applications, with the goal of maintaining the browser neutrality, OS portability, and safety that people expect from web apps. We released this project in December 2008 to get feedback from the security and broader open-source communities. We believe that Native Client technology will someday help web developers to create richer and more dynamic browser-based applications."
"The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre-Training.  Whereas theoretical work suggests that deep architectures might be more efficient at representing highly-varying functions, training deep architectures was unsuccessful until the recent advent of algorithms based on unsupervised pre-training.  Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. Answering these questions
is important if learning in deep architectures is to be further improved. We attempt to shed some light on these questions through extensive simulations. The experiments confirm and clarify the advantage of unsupervised pre-training. They demonstrate the robustness of the training procedure with respect to the
random initialization, the positive effect of pre-training in terms of optimization and its role as a regularizer. We empirically show the influence of pre-training with respect to architecture depth, model capacity, and number of training examples."
"Isolating Web Programs in Modern Browser Architectures.  Many of today's web sites contain substantial amounts of client-side code, and consequently, they act more like programs than simple documents.  This creates robustness and performance challenges for web browsers.  To give users a robust and responsive platform, the browser must identify program boundaries and provide isolation between them. We provide three contributions in this paper.  First, we present abstractions of web programs and program instances, and we show that these abstractions clarify how browser components interact and how appropriate program boundaries can be identified.  Second, we identify backwards compatibility tradeoffs that constrain how web content can be divided into programs without disrupting existing web sites.  Third, we present a multi-process browser architecture that isolates these web program instances from each other, improving fault tolerance, resource management, and performance.  We discuss how this architecture is implemented in Google Chrome, and we provide a quantitative performance evaluation examining its benefits and costs."
"Timing properties of gene expression responses to environmental changes. Cells respond to environmental perturbations with changes in their gene expression that are coordinated in magnitude and time. Timing information about individual genes, rather than clusters, provides a refined way to view and analyze responses, but it is hard to estimate accurately. To analyze response timing of individual genes, we developed a parametric model that captures the typical temporal responses: an abrupt early response followed by a second transition to a steady state. This?impulse?model explicitly represents natural temporal properties such as the onset and the offset time, and can be estimated robustly, as demonstrated by its superior ability to impute missing values in gene expression data. Using response time of individual genes, we identify relations between gene function and their response timing, showing, for example, how cytosolic ribosomal genes are only repressed after the mitochondrial ribosome is activated. We further demonstrate a strong relation between the binding affinity of a transcription factor and the activation timing of its targets, suggesting that graded binding affinities could be a widely used mechanism for controlling expression timing. See online Supplementary Material at?www.liebertonline.com."
"Program Representation for General Intelligence.  Traditional machine learning systems work with relatively flat, uniform data representations, such as feature vectors, time-series, and context-free grammars. However, reality often presents us with data which are best understood in terms of relations, types, hierarchies, and complex functional forms. One possible representational scheme for coping with this sort of complexity is computer programs. This immediately raises the question of how programs are to be best represented. We propose an answer in the context of ongoing work towards artificial general intelligence."
"Please Permit Me: Stateless Delegated Authorization in Mashups.  Mashups have emerged as a Web 2.0 phenomenon, connecting disjoint applications together to provide unified services. However, scalable access control for mashups is difficult. To enable a mashup to gather data from legacy applications and services, users must give the mashup their login names and passwords for those services. This all-or-nothing approach violates the principle of least privilege (not to mention the terms of service) and leaves users vulnerable to misuse of their credentials by malicious mashups. In this paper, we introduce Permits - a stateless approach to access rights delegation in mashups - and describe our complete implementation of a permit-based authorization delegation service."
"Mobile User Experience Research: Challenges, Methods &amp; Tools.  The main goal of this CHI 2009 workshop was to bring together researchers from industry and academia, designers, and creators of mobile research tools to discuss methods, tools and infrastructure for mobile UX and HCI research. To achieve this goal, we:"
"Hybrid Page Layout Analysis via Tab-Stop Detection.  A new hybrid page layout analysis algorithm is proposed, which uses bottom-up methods to form an initial data-type hypothesis and locate the tab-stops that were used when the page was formatted. The detected tab-stops are used to deduce the column layout of the page. The column layout is then applied in a top-down manner to impose structure and reading-order on the detected regions.
 The complete C++ source code implementation is available as part of the Tesseract open source OCR engine at http://code.google.com/p/tesseract-ocr."
"Web Derived Pronunciations for Spoken Term Detection.  Indexing and retrieval of speech content in various forms such as broadcast news, customer care data and on-line media has gained a lot of interest for a wide range of applications, from customer analytics to on-line media search. For most retrieval applications, the speech content is typically first converted to a lexical or phonetic representation using automatic speech recognition (ASR). The first step in searching through indexes built on these representations is the generation of pronunciations for named entities and foreign language query terms. This paper summarizes the results of the work conducted during the 2008 JHU Summer Workshop by the Multilingual Spoken Term Detection team, on mining the web for pronunciations and analyzing their impact on spoken term detection. We will first present methods to use the vast amount of pronunciation information available on the Web, in the form of IPA and ad-hoc transcriptions. We describe techniques for extracting candidate pronunciations from Web pages and associating them with orthographic words, filtering out poorly extracted pronunciations, normalizing IPA pronunciations to better conform to a common transcription standard, and generating phonemic representations from ad-hoc transcriptions. We then present an analysis of the effectiveness of using these pronunciations to represent Out-Of-Vocabulary (OOV) query terms on the performance of a spoken term detection (STD) system. We will provide comparisons of Web pronunciations against automated techniques for pronunciation generation as well as pronunciations generated by human experts. Our results cover a range of speech indexes based on lattices, confusion networks and one-best transcriptions at both word and word fragments levels."
"Google??s Auction for Radio and TV Ads.  This document describes the auction system used by Google for allocation and pricing of TV ads and Radio ads. It is based on a simultaneous ascending auction, and has been in use
since September 2008."
"Large Scale Online Learning of Image Similarity Through Ranking.  Learning a measure of similarity between pairs of objects is an important generic problem in machine learning. It is particularly useful in large scale applications like searching for an image that is similar to a given image or finding videos that are relevant to a given video. In these tasks, users look for objects that are not only visually similar but also semantically related to a given object.   Unfortunately, the approaches that exist today for learning such semantic similarity do not scale to large datasets. This is both because typically their CPU and storage requirements grow quadratically with the sample size, and because many methods impose complex positivity constraints on the space of learned similarity functions. The current paper presents OASIS, an Online Algorithm for
Scalable Image Similarity learning that learns a bilinear similarity measure over sparse representations. OASIS is an online dual approach using the passive-aggressive family of learning algorithms with a large margin criterion and an efficient hinge loss cost. Our experiments show that OASIS is both fast and accurate at a wide range of scales: for a dataset with thousands of images, it achieves better results than existing state-of-the-art methods, while being an order of magnitude faster. For large, web scale, datasets, OASIS can be trained on more than two million images from 150K text queries within 3 days on a single CPU.  On this large scale dataset, human evaluations showed that 35% of the ten nearest neighbors of a given test image, as found by OASIS, were semantically relevant to that
image. This suggests that query independent similarity could be accurately learned even for large scale datasets that could not be handled before."
"Using a Market Economy to Provision Compute Resources Across Planet-wide  Clusters.  We present a practical, market-based solution to the resource provisioning problem in a set of heterogeneous resource clusters. We focus on provisioning rather than immediate scheduling decisions to allow users to change long-term job specifications based on market feedback. Users enter bids to purchase quotas, or bundles of resources for long-term use. These requests are mapped into a simulated clock auction which determines uniform, fair resource prices that balance supply and demand. The reserve prices for resources sold by the operator in this auction are set based on current utilization, thus guiding the users as they set their bids towards under-utilized
resources. By running these auctions at regular time intervals, prices fluctuate like those in a real-world economy and provide motivation for users to engineer systems that can best take advantage of available resources. These ideas were implemented in an experimental resource market at Google. Our preliminary results demonstrate an efficient transition of users from more congested resource pools to less congested resources. The disparate engineering costs for users to reconfigure their jobs to run on less expensive resource pools was evidenced by the large price premiums some users were willing to pay for more expensive resources. The final resource allocations illustrated how this framework can lead to significant, beneficial changes in user behavior, reducing the excessive shortages and surpluses of more traditional allocation methods."
"Balancing Usability and Security in a Video CAPTCHA.  We present a technique for using a content-based video labeling task as a CAPTCHA. Our video CAPTCHAs are generated from YouTube videos, which contain labels (tags) supplied by the person that uploaded the video. They are graded using a video's tags, as well as tags from related videos. In a user study involving 184 participants, we were able to increase the average human success rate on our video CAPTCHA from roughly 70% to 90%, while keeping the average success rate of a tag frequency-based attack fixed at around 13%. Through a different parameterization of the challenge generation and grading algorithms, we were able to reduce the success rate of the same attack to 2%, while still increasing the human success rate from 70% to 75%. The usability and security of our video CAPTCHA appears to be comparable to existing CAPTCHAs, and a majority of participants (60%) indicated that they found the video CAPTCHAs more enjoyable than traditional CAPTCHAs in which distorted text must be transcribed."
"Causeway: a message-oriented distributed debugger.  An increasing number of developers face the difficult task of debugging distributed asynchronous programs. This trend has outpaced the development of adequate debugging tools and currently, the best option for many is an ad hoc patchwork of sequential tools and printf debugging. This paper presents Causeway, a postmortem distributed debugger that demonstrates a novel approach to understanding the behavior of a distributed program. Our message-oriented approach borrows an effective strategy from sequential debugging: To find the source of unintended side- effects, start with the chain of expressed intentions. We show how Causeway's integrated views - describing both distributed and sequential computation - help users navigate causal pathways as they pursue suspicions. We highlight Causeway's innovative features which include adaptive, customizable event abstraction mechanisms and graphical views that follow message flow across process and machine boundaries."
"Cost-efficient Dragonfly Topology for Large-scale Systems.  Evolving technology and increasing pin-bandwidth motivate the use of high-radix routers to reduce the diameter, 
latency, and cost of interconnection networks. This migration from low-radix to high-radix routers is demonstrated with the recent introduction of high-radix routers and they are expected to impact networks used in large-scale systems such as multicomputers and data centers. As a result, a scalable and a cost-efficient topology is needed to properly exploit high-radix routers. 
High-radix networks require longer cables than their low-radix counterparts. Because cables dominate network 
cost, the number of cables, and particularly the number of long, global cables should be minimized to realize an 
efficient network. In this paper, we introduce the dragonfly topology which uses a group of high-radix routers as a virtual router to increase the effective radix of the network. With this organization, each minimally routed packet traverses at most one global channel. By reducing global channels, a dragonfly reduces cost by 20% compared to a flattened butterfly and by 52% compared to a folded Clos network in configurations with at least 16K nodes. The paper also introduces two new variants of global adaptive routing that enable load-balanced routing in the dragonfly. Each router in a dragonfly must make an adaptive routing decision based on the state of a global channel connected to a different router. Because of the indirect nature of this routing decision, conventional adaptive routing algorithms give degraded performance. We introduce the use of selective virtual-channel discrimination and the use of credit round-trip latency to both sense and signal channel congestion. The combination of these two methods gives throughput and latency that approaches that of an ideal adaptive routing algorithm."
"Cost-efficient Dragonfly Topology for Large-scale Systems.  It is more efficient to use increasing pin bandwidth by creating high-radix routers with a large number of narrow ports instead of low-radix routers with fewer wide ports. Building networks using high-radix routers lowers cost and improves performance, but also presents many challenges. The dragonfly topology minimizes network cost by reducing the number of global channels required."
"Achieving Predictable Performance through Better Memory Controller Placement in Many-Core CMPs.  In the near term, Moore's law will continue to provide an increasing number of transistors and therefore an increasing number of on-chip cores. Limited pin bandwidth prevents the integration of a large number of memory controllers on-chip. With many cores, and few memory controllers, where to locate the memory controllers in the on-chip interconnection fabric becomes an important and as yet unexplored question. In this paper, we show how the location of the memory controllers can reduce contention (hot spots) in the on-chip fabric, as well as lower the variance in reference latency which provides for predictable performance of memory-intensive applications regardless of the processing core on which a thread is scheduled. We explore the design space of on-chip fabrics to find optimal memory controller placement relative to different topologies (i.e. mesh and torus), routing algorithms, and workloads."
"What's Up CAPTCHA? A CAPTCHA Based On Image Orientation.  We present a new CAPTCHA which is based on identifying an
image's upright orientation. This task requires analysis of the often complex contents of an image, a task which humans usually perform well and machines generally do not. Given a large repository of images, such as those from a web search result, we use a suite of automated orientation detectors to prune those images that can be automatically set upright easily. We then apply a social feedback mechanism to verify that the remaining images have a human-recognizable upright orientation. The main advantages of our CAPTCHA technique over the traditional text recognition techniques are that it is language-independent, does not require text-entry (e.g. for a mobile device), and employs another domain for CAPTCHA generation beyond character obfuscation. This CAPTCHA lends itself to rapid implementation and has an almost limitless supply of images. We conducted extensive experiments to measure the viability of this technique."
"Optimizing the update packet stream for web applications.  The Internet has evolved to an extent where users now expect any-where any-time and any-form access to their personalized data and applications of choice. However providing a coherent (seamless) user experience across multiple devices has been relatively hard to achieve. While the 'how to sync' problem has been well studied in literature, the complementary 'when to sync' problem has remained relatively unexplored. While frequent updates providing higher user satisfaction/retention are naturally more desirable than sparse updates, the steadily escalating resource costs are a significant bottleneck. We thus propose extensions to the traditional periodic refresh model based on an adaptive 'smart sync approach' that enables variable rate updates closely modeling expected user behavior over time. An experimental evaluation of the proposed mechanism on a sizeable subset of users of the GMAIL web interface indicates that the proposed refresh policy can achieve the best of both worlds - limited resource provisioning and minimal user-perceived delays."
"DRAM Errors in the Wild: A Large-Scale Field Study.  The Internet has evolved to an extent where users now expect any-where any-time and any-form access to their personalized data and applications of choice. However providing a coherent (seamless) user experience across multiple devices has been relatively hard to achieve. While the 'how to sync' problem has been well studied in literature, the complementary 'when to sync' problem has remained relatively unexplored. While frequent updates providing higher user satisfaction/retention are naturally more desirable than sparse updates, the steadily escalating resource costs are a significant bottleneck. We thus propose extensions to the traditional periodic refresh model based on an adaptive 'smart sync approach' that enables variable rate updates closely modeling expected user behavior over time. An experimental evaluation of the proposed mechanism on a sizeable subset of users of the GMAIL web interface indicates that the proposed refresh policy can achieve the best of both worlds - limited resource provisioning and minimal user-perceived delays."
"Erlang for Concurrent Programming.  Designed for concurrency from the ground up, the Erlang language can be a valuable too to help solve concurrent problems."
"Optimal Traversal Planning in Road Networks with Navigational Constraints.  A frequent query in geospatial planning and decision making
domains (e.g., emergency response, data acquisition, street
cleaning), is to ??nd an optimal traversal plan (OTP) that
traverses an entire area (e.g., a city) by navigating through all its streets. The optimality is de??ned in terms of the time it takes to complete the traversal. This time depends on the number of times each street segment is traversed as well as the navigation time such as the time spent on changing direction at each intersection.
While the problem roots in the classic problems of graph
theory, real-world geospatial constraints of road network introduce new application-speci??c challenges. In this paper, we propose two algorithms to ??nd OTP of a directed road network. Our greedy algorithm employs a classic graph traversal algorithm. During the traversal, it utilizes a set of heuristics at each intersection to minimize the total travel time. Our near-optimal algorithm, however, reduces an OTP problem to an Asymmetric Traveling Salesman Problem (ATSP) by extracting the dual graph of the original network in which each edge is represented by a graph node. Using an approximate solution for ATSP, our algorithm finds a near optimal answer. Our experiments using real-world road networks verify that our near-optimal algorithm outperforms the greedy algorithm in terms of the overall cost of its generated traversal by a factor of two, while its complexity is tolerable in real-world cases."
"Do you know your IQ? A research agenda for information quality in systems.  Information quality (IQ) is a measure of how fit information is for a purpose. Sometimes called Quality of Information (QoI) by analogy with Quality of Service (QoS), it quantifies whether the right information is being used to make a decision or take an action. Failure to understand whether information is of adequate quality can lead to bad decisions and catastrophic effects. The results can include system outages, increased costs, lost revenue -- and worse. Quantifying information quality can help improve decision making, but the ultimate goal should be to select or construct information sources that have the appropriate balance between information quality and the cost of providing it. In this paper, we provide a brief introduction to the field, argue the case for applying information quality metrics in the systems domain, and propose a research agenda to explore this space."
RFC5635 - Remote Triggered Black Hole filtering with uRPF.  Remote Triggered Black Hole (RTBH) filtering is a popular and effective technique for the mitigation of denial-of-service attacks. This document expands upon destination-based RTBH filtering by outlining a method to enable filtering by source address as well.
"LSH Banding for Large-Scale Retrieval with Memory and Recall Constraints.  Locality Sensitive Hashing (LSH) is widely used for efficient retrieval of candidate matches in very large audio, video, and image systems.  However, extremely large reference databases necessitate a guaranteed limit on the memory used by the table lookup itself, no matter how the entries crowd different parts of the signature space, a guarantee that LSH does not give.  In this paper, we provide such guaranteed limits, primarily through the design of the LSH bands.  When combined with data-adaptive bin splitting (needed on only 0.04% of the occupied bins), this approach provides the required guarantee on memory usage.  At the same time, it avoids the reduced recall that more extensive use of bin splitting would give."
"Capacity of Steganographic Channels.  This work investigates a central problem in steganography, that is: How much data can safely be hidden without being detected? To answer this question, a formal definition of steganographic capacity is presented. Once this has been defined, a general formula for the capacity is developed. The formula is applicable to a very broad spectrum of channels due to the use of an information-spectrum approach. This approach allows for the analysis of arbitrary steganalyzers as well as nonstationary, nonergodic encoder and attack channels. After the general formula is presented, various simplifications are applied to gain insight into example hiding and detection methodologies. Finally, the context and applications of the work are summarized in a general discussion."
"Why Silent Updates Boost Security.  Security fixes and feature improvements don't benefit the end user of software if the update mechanism and strategy is not effective. In this paper we analyze the effectiveness of different Web browsers update mechanisms; from Chrome's silent update mechanism to Opera's update requiring a full re-installation. We use anonymized logs from Google's world wide distributed Web servers. An analysis of the logged HTTP user-agent string that Web browsers report when requesting any Web page is used to measure the daily browser version shares in active use. To the best of our knowledge, this is the first global scale measurement of Web browser update effectiveness comparing four different Web browser update strategies. Our measurements prove that silent updates and little dependency on the underlying operating system are most effective to get users of Web browsers to surf the Web with the latest browser version. However, there is still room for improvement as we found. Chrome's advantageous silent update mechanism has been open sourced in April 2009. We recommend any software vendor to seriously consider deploying silent updates as this benefits both the vendor and the user, especially for widely used attack-exposed applications like Web browsers and browser plug-ins."
"Characterizing End-to-End Packet Reordering with UDP Traffic.  Packet reordering is an Internet event that degrades the performance of both TCP and UDP-based applications. In this paper, we present an end-to-end measurement study of packet reordering of UDP traffic. The goal of the measurement study, performed on PlanetLab, was to answer four main questions: how prevalent is reordering across end-to-end paths, what are the time scales of reordered packets, how correlated is reordering with traffic load, and does the size of a transmitted packet affect the likelihood of reordering? Overall, our analysis shows that current UDP traffic reordering is consistent to prior 1990's studies on TCP traffic, despite increased Internet load and technology advancements, and it adds to the previous results by identifying additional reordering characteristics. More specifically, we show that packet reordering is asymmetric as well as temporal and site-dependent, packet size does influence the likelihood of reordering, that there exists a time-of-the-day dependency, and reordering primarily exists at two time-scales (a few milliseconds or multiple tens of milliseconds.)"
"Computers and iPhones and Mobile Phones, oh my! A logs-based comparison of search users on different devices.  We present a logs-based comparison of search patterns across three platforms: computers, iPhones and conventional mobile phones. Our goal is to understand how mobile search users differ from computer-based search users, and we focus heavily on the distribution and variability of tasks that users perform from each platform. The results suggest that search usage is much more focused for the average mobile user than for the average computer-based user. However, search behavior on high-end phones resembles computer-based search behavior more so than mobile search behavior. A wide variety of implications follow from these findings. First, there is no single search interface which is suitable for all mobile phones. We suggest that for the higher-end phones, a close integration with the standard computer-based interface (in terms of personalization and available feature set) would be beneficial for the user, since these
phones seem to be treated as an extension of the users' computer. For all other phones, there is a huge opportunity for personalizing the search experience for the user's ""mobile needs"", as these users are likely to repeatedly search for a single type of information need on their phone."
"Named Entity Transcription with Pair n-Gram Models.  We submitted results for each of the eight shared tasks. Except for Japanese name kanji restoration, which uses a noisy channel model, our Standard Run submissions were produced by generative long-range pair ngram models, which we mostly augmented with publicly available data (either from LDC datasets or mined from Wikipedia) for the Non-Standard Runs."
"Algorithms for Secretary Problems on Graphs and Hypergraphs.  We examine several online matching problems, with applications to Internet advertising reservation systems. Consider an edge-weighted bipartite graph G, with partite sets L, R. We develop an 8-competitive algorithm for the following secretary problem: Initially given R, and the size of L, the algorithm receives the vertices of L sequentially, in a random order. When a vertex l \in L is seen, all edges incident to l are revealed, together with their weights. The algorithm must immediately either match l to an available vertex of R, or decide that l will remain unmatched. Dimitrov and Plaxton show a 16-competitive algorithm for the transversal matroid secretary problem, which is the special case with weights on vertices, not edges. (Equivalently, one may assume that for each l \in L, the weights on all edges incident to l are identical.) We use a similar algorithm, but simplify and improve the analysis to obtain a better competitive ratio for the more general problem. Perhaps of more interest is the fact that our analysis is easily extended to obtain competitive algorithms for similar problems, such as to find disjoint sets of edges in hypergraphs where edges arrive online. We also introduce secretary problems with adversarially chosen groups. Finally, we give a 2e-competitive algorithm for the secretary problem on graphic matroids, where, with edges appearing online, the goal is to find a maximum-weight acyclic subgraph of a given graph."
"Automatic, Efficient, Temporally-Coherent Video Enhancement for Large Scale Applications.  A fast and robust method for video contrast enhancement is presented.
The method uses the histogram of each frame, along with upper and lower
bounds computed per shot in order to enhance the current frame. This
ensures that the artifacts introduced during the enhancement is reduced
to a minimum. Traditional methods that do not compute per-shot estimates
tend to over-enhance parts of the video such as fades and transitions.
Our method does not suffer from this problem, which is essential for a
fully automatic algorithm. We present the parameters for our methods
which yielded the best human feedback, which showed that out of 208 videos,
203 were enhanced, while the remaining 5 were of too poor quality to
be enhanced. Additionally, we present a visual comparison of our work with
the recently-proposed Weighted Thresholded Histogram Equalization (WTHE) algorithm."
"Large Scale Online Learning  of Image Similarity Through Ranking: Extended Abstract.  Learning a measure of similarity between pairs of objects is a fundamental problem in machine learning. Pairwise similarity plays a crucial role in classification algorithms like nearest neighbors, and is practically important for applications like searching for images that are similar to a given image or finding videos that are relevant to a given video. In these tasks, users look for objects that are both visually similar and semantically related to a given object.  Unfortunately, current approaches for learning semantic similarity are limited to small scale datasets, because their complexity grows quadratically with the sample size, and because they impose costly positivity constraints on the learned similarity functions.  To address real-world large-scale AI problem, like learning similarity over all images on the web, we need to develop new algorithms that scale to many samples, many classes, and many features.  The current abstract presents OASIS, an {\em Online Algorithm for Scalable Image Similarity} learning that learns a bilinear similarity measure over sparse representations. OASIS is an online dual approach using the passive-aggressive family of learning algorithms with a large margin criterion and an efficient hinge loss cost. Our experiments show that OASIS is both fast and accurate at a wide range of scales: for a dataset with thousands of images, it achieves better results than existing state-of-the-art methods, while being an order of magnitude faster. Comparing OASIS with different symmetric variants, provides unexpected insights into the effect of symmetry on the quality of the similarity. For large, web scale, datasets, OASIS can be trained on more than two million images from 150K text queries within two days on a single CPU.  Human evaluations showed that 35\% of the ten top images ranked by OASIS were semantically relevant to a query image. This suggests that query-independent similarity could be accurately learned even for large-scale datasets that could not be handled before."
"Sound Ranking Using Auditory Sparse-Code Representations.  The task of ranking sounds from text queries is a
good test application for machine-hearing techniques, and particularly
for comparison and evaluation of alternative sound representations in
a large-scale setting.  We have adapted a machine-vision system,
``passive-aggressive model for image retrieval''
(PAMIR), which
efficiently learns, using a ranking-based cost function, a linear
mapping from a very large sparse feature space to a large
query-term space.
Using this system allows us to focus on comparison of different
auditory front ends and different ways of extracting sparse features
from high-dimensional auditory images.  In addition to two main
auditory-image models, we also include and compare a family of more
conventional MFCC front ends.  The experimental results show a
significant advantage for the auditory models over vector-quantized MFCCs.
The two auditory models tested use the adaptive pole-zero filter
cascade (PZFC) auditory filterbank and sparse-code feature extraction
from stabilized auditory images via multiple vector quantizers. The
models differ in their implementation of the strobed temporal
integration used to generate the stabilized image. Using ranking
precision-at-top-k performance measures, the best results are about
70% top-1 precision and 35% average precision, using a test corpus
of thousands of sound files and a query vocabulary of hundreds of
words."
"Collaborative Filtering for Orkut Communities: Discovery of User Latent Behavior.  Users of social networking services can connect with each other by forming communities for online interaction. Yet as the number of communities hosted by such websites grows over time, users have even greater need for effective community recommendations in order to meet more users. In this paper, we investigate two algorithms from very different domains and evaluate their effectiveness for personalized community recommendation. First is association rule mining (ARM), which discovers associations between sets of communities that are shared across many users. Second is latent Dirichlet allocation (LDA), which models user-community co-occurrences using latent aspects. In comparing LDA with ARM, we are interested in discovering whether modeling low-rank latent structure is more effective for recommendations than directly mining rules from the observed data. We experiment on an Orkut data set consisting of 492,104 users and 118,002 communities. We show that LDA consistently performs better than ARM using the top-k recommendations ranking metric, and we analyze examples of the latent information learned by LDA to explain this finding. To efficiently handle the large-scale data set, we parallelize LDA on distributed computers and demonstrate our parallel implementation's scalability with varying numbers of machines."
"The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines.  As computation continues to move into the cloud, the computing platform of interest no longer resembles a pizza box or a refrigerator, but a warehouse full of computers. These new large datacenters are quite different from traditional hosting facilities of earlier times and cannot be viewed simply as a collection of co-located servers. Large portions of the hardware and software resources in these facilities must work in concert to efficiently deliver good levels of Internet service performance, something that can only be achieved by a holistic approach to their design and deployment. In other words, we must treat the datacenter itself as one massive warehouse-scale computer (WSC). We describe the architecture of WSCs, the main factors influencing their design, operation, and cost structure, and the characteristics of their software base. We hope it will be useful to architects and programmers of today's WSCs, as well as those of future many-core platforms which may one day implement the equivalent of today's WSCs on a single board."
"An Online Algorithm for Large Scale Image Similarity Learning.  Learning a measure of similarity between pairs of objects is a
  fundamental problem in machine learning. It stands in the core of
  classifications methods like kernel machines, and is particularly
  useful for applications like searching for images that are similar
  to a given image or finding videos that are relevant to a given
  video. In these tasks, users look for objects that are not only
  visually similar but also semantically related to a given
  object. Unfortunately, current approaches for learning similarity do
  not scale to large datasets, especially when imposing metric
  constraints on the learned similarity.
  We describe OASIS, a method for learning pairwise similarity that is
  fast and scales linearly with the number of objects and the number of
  non-zero features. Scalability is achieved through online learning of a
  bilinear model over sparse representations using a large margin
  criterion and an efficient hinge loss cost. OASIS is accurate at a
  wide range of scales: on a standard benchmark with thousands of
  images, it is more precise than state-of-the-art methods, and faster
  by orders of magnitude. On 2 millions images collected from the web,
  OASIS can be trained within 3 days on a single CPU. The non-metric
  similarities learned by OASIS can be transformed into metric
  similarities, achieving higher precisions than similarities that are
  learned as metrics in the first place. This suggests an approach for
  learning a metric from data that is larger by two orders of magnitude
  than was handled before."
"Affiliation Networks.  In the last decade, structural properties of several naturally arising networks (the Internet, social networks, the web graph, etc.) have been studied intensively with a view to understanding their evolution. In recent empirical work, Leskovec, Kleinberg, and Faloutsos identify two new and surprising properties of the evolution of many real-world networks: densification (the ratio of edges to vertices grows over time), and shrinking diameter (the diameter reduces over time to a constant). These properties run counter to conventional wisdom, and are certainly inconsistent with graph models prior to their work. In this paper, we present the first model that provides a simple, realistic, and mathematically tractable generative model that intrinsically explains all the well-known properties of the social networks, as well as densification and shrinking diameter. Our model is based on ideas studied empirically in the social sciences, primarily on the groundbreaking work of Breiger (1973) on bipartite models of social networks that capture the affiliation of agents to societies. We also present algorithms that harness the structural consequences of our model. Specifically, we show how to overcome the bottleneck of densification in computing shortest paths between vertices by producing sparse subgraphs that preserve or approximate shortest distances to all or a distinguished subset of vertices. This is a rare example of an algorithmic benefit derived from a realistic graph model. Finally, our work also presents a modular approach to connecting random graph paradigms (preferential attachment, edge-copying, etc.) to structural consequences (heavy-tailed degree distributions, shrinking diameter, etc.)"
"The One-Way Communication Complexity of Hamming Distance.  Consider the following version of the Hamming distance problem for
{1,-1}-vectors of length  n:  the promise is that the distance is 
either at least  (n/2)+sqrt{n}  or at most  (n/2)-sqrt{n}, and
the goal is to find out which of these two cases occurs.
Woodruff (Proc. ACM-SIAM Symposium on Discrete Algorithms, 2004)
gave a linear lower bound for the randomized one-way communication
complexity of this problem.  In this note we give a simple proof 
of this result. Our proof uses a simple reduction from the indexing
problem and avoids the VC-dimension arguments used in the previous
paper. As shown by Woodruff (loc. cit.), this implies an<br/>
Omega(1/epsilon^2)-space lower bound for approximating frequency
moments within a factor  1+epsilon  in the data stream model."
"How opinions are received by online communities: A case study on Amazon.com helpfulness votes.  There are many on-line settings in which users publicly express opinions. A number of these offer mechanisms for other users to evaluate these opinions; a canonical example is Amazon.com, where reviews come with annotations like <code>26 of 32 people found the following review helpful.'' Opinion evaluation appears in many off-line settings as well, including market research and political campaigns. Reasoning about the evaluation of an opinion is fundamentally different from reasoning about the opinion itself: rather than asking,</code>What did Y think of X?'', we are asking, <code>What did Z think of Y's opinion of X?'' Here we develop a framework for analyzing and modeling opinion evaluation, using a large-scale collection of Amazon book reviews as a dataset. We find that the perceived helpfulness of a review depends not just on its content but also but also in subtle ways on how the expressed evaluation relates to other evaluations of the same product. As part of our approach, we develop novel methods that take advantage of the phenomenon of review</code>plagiarism'' to control for the effects of text in opinion evaluation, and we provide a simple and natural mathematical model consistent with our findings. Our analysis also allows us to distinguish among the predictions of competing theories from sociology and social psychology, and to discover unexpected differences in the collective opinion-evaluation behavior of user populations from different countries."
"Optimizing Programs with Intended Semantics.  Modern object-oriented languages have complex features that cause
programmers to overspecify their programs. This overspecification
hinders automatic optimizers, since they must preserve the
overspecified semantics. If an optimizer knew which semantics the
programmer intended, it could do a better job. Making a programmer clarify his intentions by placing assumptions
into the program is rarely practical.  This is because the programmer
does not know which parts of the programs' overspecified semantics
hinder the optimizer.  Therefore, the programmer has to guess
which assumption to add.  Since the programmer can add many
different assumptions to a large program, he will need to place
many such assumptions before he guesses right and helps the optimizer. We present IOpt, a practical optimizer that uses a
specification of the programmers' intended semantics to enable
additional optimizations.  That way, our optimizer can significantly
improve the performance of a program. We present case studies in which
we use IOpt to speed up two programs by over 50%. To make specifying the intended semantics practical, IOpt
communicates with the programmer. IOpt  identifies which
assumptions the programmer textit{should} place, and where he should
place them.  IOpt ranks each assumption by (i) the likelyhood that
the assumption conforms to the programmers' intended semantics and
(ii) how much the assumption will help IOpt  improve the programs'
performance.  IOpt proposes ranked assumptions to the programmer,
who just picks those that conform to his intended semantics.  With
this approach, IOpt keeps the programmers' specification burden
low. Our case studies show that the programmer just needs to add a few
assumptions to realize the 50% speedup."
"Wireless Techniques in Optical Transport.  Abstract
    The field of optical communications is undergoing a transformation from analog to digital.  Advanced signal processing techniques which have been widely used in wireless communications and local access loops are now being applied to long haul optical transmission networks.  In this paper, we discuss the implications of such transformations and postulate a new paradigm for optical transport in future high speed optical backbone networks."
"Sound Retrieval and Ranking Using Sparse Auditory Representations.  To create systems that understand the sounds that humans are exposed
to in everyday life, we need to represent sounds with features that
can discriminate among many different sound classes. Here, we use a
sound-ranking framework to quantitatively evaluate such
representations in a large scale task. We have adapted a
machine-vision method, the ``passive-aggressive model for image
retrieval'' (PAMIR), which efficiently learns a linear mapping from a
very large sparse feature space to a large query-term space. Using
this approach we compare different auditory front ends and different
ways of extracting sparse features from high-dimensional auditory
images. We tested auditory models that use adaptive pole--zero filter
cascade (PZFC) auditory filterbank and sparse-code feature extraction
from stabilized auditory images via multiple vector quantizers.  In
addition to auditory image models, we also compare a family of more
conventional Mel-Frequency Cepstral Coefficient (MFCC) front ends. The
experimental results show a significant advantage for the auditory
models over vector-quantized MFCCs. Ranking thousands of sound files
with a query vocabulary of thousands of words, the best precision at
top-1 was 73% and the average precision was 35%, reflecting a 18%
improvement over the best competing MFCC."
Large-scale Privacy Protection in Google Street View.  The full paper will appear from IEEE.
"Good Abandonment in Mobile and PC Internet Search.  Query abandonment by search engine users is generally considered to be a negative signal. In this paper, we explore the concept of good abandonment. We define a good abandonment as an abandoned query for which the user's information need was successfully addressed by the search results page, with no need to click on a result or refine the query. We present an analysis of abandoned internet search queries across two modalities (PC and mobile) in three locales. The goal is to approximate the prevalence of good abandonment, and to identify types of information needs that may lead to good abandonment, across different locales and modalities. Our study has three key findings: First, queries potentially indicating good abandonment make up a significant portion of all abandoned queries. Second, the good abandonment rate from mobile search is significantly higher than that from PC search, across all locales tested. Third, classified by type of information need, the major classes of good abandonment vary dramatically by both locale and modality. Our findings imply that it is a mistake to uniformly consider query abandonment as a negative signal. Further, there is a potential opportunity for search engines to drive
additional good abandonment, especially for mobile search
users, by improving search features and result snippets."
"Combined Orientation and Script Detection using the Tesseract OCR Engine.  This paper proposes a simple but effective algorithm to estimate the script and dominant page orientation of the text contained in an image. A candidate set of shape classes for each script is generated using synthetically rendered text and used to train a fast shape classifier. At run time, the classifier is applied independently to connected components in the image for each possible orientation of the component, and the accumulated confidence scores are used to determine the best estimate of page orientation and script. Results demonstrate the effectiveness of the approach on a dataset of 1846 documents containing a diverse set of images in 14 scripts and any of four possible page orientations."
"Succinct approximate counting of skewed data.  Practical data analysis relies on the ability to count
observations of objects succinctly and efficiently.
Unfortunately the space usage of an exact estimator
grows with the size of the a priori set from
which objects are drawn while the time required
to maintain such an estimator grows with the size
of the data set. We present static and on-line approximation
schemes that avoid these limitations
when approximate frequency estimates are acceptable.
Our Log-Frequency Sketch extends the approximate
counting algorithm of Morris [1978] to
estimate frequencies with bounded relative error
via a single pass over a data set. It uses constant
space per object when the frequencies follow a
power law and can be maintained in constant time
per observation. We give an (??,??)-approximation
scheme which we verify empirically on a large natural
language data set where, for instance, 95 percent
of frequencies are estimated with relative error
less than 0.25 using fewer than 11 bits per object in
the static case and 15 bits per object on-line."
"Finding Images and Line Drawings in Document-Scanning Systems.  This work addresses the problem of finding images and line-drawings in scanned pages. It is a crucial processing step in the creation of a large-scale system to detect and index images found in books and historic documents. Within the scanned pages that contain both text and images, the images are found through the use of local-feature extraction, applied across the full scanned page. This is followed by a novel learning system to categorize the local features into either text or image. The discrimination is based on using multiple classifiers trained via stochastic sampling of weak classifiers for each AdaBoost stage. The approach taken in sampling includes stochastic hill climbing across weak detectors, allowing us to reduce our classification error by as much as 25% relative to more naive stochastic sampling. Stochastic hill climbing in the weak classifier space is possible due to the manner in which we parameterize the weak classifier space. Through the use of this system, we improve image detection by finding more line-drawings, graphics, and photographs, as well as reducing the number of spurious detections due to misclassified text, discoloration, and scanning artifacts."
"Why does Unsupervised Pre-training Help Deep Learning?.  Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the influence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments confirm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pre-training guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training."
"A Generalized Composition Algorithm for Weighted Finite-State Transducers.  This paper describes a weighted finite-state transducer composition algorithm that generalizes the notion of the composition filter and present filters that remove useless epsilon paths and push forward labels and weights along epsilon paths. This filtering allows us to compose together large speech recognition context-dependent lexicons and language models much more efficiently in time and space than previously possible. We present experiments on Broadcast News and Google Search by Voice that demonstrate a 5% to 10% overhead for dynamic, runtime composition compared to a static, offline composition of the recognition transducer. To our knowledge, this is the first such system with such small overhead."
"Adaptive Dynamic of Realistic Small World Networks.  Continuing in the steps of Jon Kleinberg??s and others celebrated work on decentralized search, we conduct an experimental analysis of destination sampling, a dynamic algorithm that produces small-world networks. We find that the algorithm adapts robustly to a wide variety of situations in realistic geographic networks with synthetic test data and with real world data, even when vertices are unevenly and non-homogeneously distributed. We investigate the same algorithm in the case where some vertices are more popular destinations for searches than others, for example obeying power-laws. We find that the algorithm adapts and adjusts the networks according to the distributions, leading to improved performance. The ability of the dynamic process to adapt and create small worlds in such diverse settings suggests a possible mechanism by which such networks appear in nature."
"Moving Beyond End-to-End Path Information to Optimize CDN Performance.  Continuing in the steps of Jon Kleinberg??s and others celebrated work on decentralized search, we conduct an experimental analysis of destination sampling, a dynamic algorithm that produces small-world networks. We find that the algorithm adapts robustly to a wide variety of situations in realistic geographic networks with synthetic test data and with real world data, even when vertices are unevenly and non-homogeneously distributed. We investigate the same algorithm in the case where some vertices are more popular destinations for searches than others, for example obeying power-laws. We find that the algorithm adapts and adjusts the networks according to the distributions, leading to improved performance. The ability of the dynamic process to adapt and create small worlds in such diverse settings suggests a possible mechanism by which such networks appear in nature."
"Personalized News Recommendation Based on Click Behavior.  Online news reading has become very popular as the web
provides access to news articles from millions of sources
around the world. A key challenge of news service website
is help users to find news articles that are interesting to
read. In this paper, we present our research on developing
personalized news recommendation system in Google
News. The recommendation system builds profiles of user??s
news interests based on user??s click behavior on the
website. To understand the news interest change over time,
we first conducted a large-scale log analysis of the click
behavior of Google News users. Based on the log analysis,
we developed a Bayesian framework for predict user??s
current news interests, which considers both the activities
of that particular user and the news trend demonstrated in
activities of a group of users. We combine the information
filtering mechanism using learned user profile with an
existing collaborative filtering mechanism to generate
personalized news recommendation. The combined method
was deployed in Google News. Experiments on the live
traffic of Google News website demonstrated that the
combined method improves the quality of news
recommendation and attracts more frequent visit to the
website."
"Differential Synchronization.  This paper describes the Differential Synchronization (DS) method for keeping documents synchronized.  The key feature of DS is that it is simple and well suited for use in both novel and existing state-based applications without requiring application redesign.  DS uses deltas to make efficient use of bandwidth, and is fault-tolerant, allowing copies to converge in spite of occasional errors.  We consider practical implementation of DS and describe some techniques to improve its performance in a browser environment."
"Automatic Generation of Research Trails in Web History.  The web is large and complex, and in the process of navigating it, we often lose our way. Research trailing is a method to organize web contents that we have spent some effort on into distinct research sessions. Research trails are automatically constructed by filtering and organizing users?? activity history, using a combination of semantic and temporal criteria for grouping similar web activity. The design of research trails was informed by an ethnographic study of ordinary people doing research on the web; it addresses the specific challenges of establishing and maintaining context when the research process is fragmented and the research question is still in formation. This paper motivates and describes our algorithms for generating high quality research trails. Research trails can be applied in several contexts: as the underlying mechanism for a research task browser, or as feed to an ambient display of history information while searching. A prototype was built to assess the utility of the first option, a research trail browser."
"An Audio Indexing System for Election Video Material.  In the 2008 presidential election race in the United States, the prospective candidates made extensive use of YouTube to post video material. We developed a scalable system that transcribes this material and makes the content searchable (by indexing the meta-data and transcripts of the videos) and allows the user to navigate through the video material based on content. The system is available as an iGoogle gadget as well as a Labs product. Given the large exposure, special emphasis was put on the scalability and reliability of the system. This paper describes the design and implementation of this system."
"Origins of Homophily in an Evolving Social Network.  In the 2008 presidential election race in the United States, the prospective candidates made extensive use of YouTube to post video material. We developed a scalable system that transcribes this material and makes the content searchable (by indexing the meta-data and transcripts of the videos) and allows the user to navigate through the video material based on content. The system is available as an iGoogle gadget as well as a Labs product. Given the large exposure, special emphasis was put on the scalability and reliability of the system. This paper describes the design and implementation of this system."
"Adapting Software Fault Isolation to Contemporary CPU Architectures.  Software Fault Isolation (SFI) is an effective approach
to sandboxing binary code of questionable provenance,
an interesting use case for native plugins in a Web
browser. We present software fault isolation schemes for
ARM and x86-64 that provide control-flow and memory
integrity with average performance overhead of under
5% on ARM and 7% on x86-64. We believe these are the
best known SFI implementations for these architectures,
with significantly lower overhead than previous systems
for similar architectures. Our experience suggests that
these SFI implementations benefit from instruction-level
parallelism, and have particularly small impact for workloads that are data memory-bound, both properties that
tend to reduce the impact of our SFI systems for future
CPU implementations."
"FlumeJava: Easy, Efficient Data-Parallel Pipelines.  MapReduce and similar systems significantly ease the task of writing
data-parallel code. However, many real-world computations require
a pipeline of MapReduces, and programming and managing
such pipelines can be difficult. We present FlumeJava, a Java library
that makes it easy to develop, test, and run efficient dataparallel
pipelines. At the core of the FlumeJava library are a couple
of classes that represent immutable parallel collections, each
supporting a modest number of operations for processing them in
parallel. Parallel collections and their operations present a simple,
high-level, uniform abstraction over different data representations
and execution strategies. To enable parallel operations to run effi-
ciently, FlumeJava defers their evaluation, instead internally constructing
an execution plan dataflow graph. When the final results
of the parallel operations are eventually needed, FlumeJava first optimizes
the execution plan, and then executes the optimized operations
on appropriate underlying primitives (e.g., MapReduces). The
combination of high-level abstractions for parallel data and computation,
deferred evaluation and optimization, and efficient parallel
primitives yields an easy-to-use system that approaches the effi-
ciency of hand-optimized pipelines. FlumeJava is in active use by
hundreds of pipeline developers within Google.
Categories and Subject Descriptors D.1.3 [Concurrent Programming]:
Parallel Programming
General Terms Algorithms, Languages, Performance
Keywords data-parallel programming, MapReduce, Java"
"Finding Meaning on YouTube: Tag Recommendation and Category Discovery.  We present a system that automatically recommends tags for YouTube
videos solely based on their audiovisual content. We also propose a novel framework
for unsupervised discovery of video categories that exploits knowledge mined
from the World-Wide Web text documents/searches. First, video content to tag
association is learned by training classifiers that map audiovisual
content-based features from millions of videos on YouTube.com to existing
uploader-supplied tags for these videos.  When a new video is uploaded, the
labels provided by these classifiers are used to automatically suggest tags
deemed relevant to the video. Our system has learned a vocabulary of over 20,000 tags.
Secondly, we mined large volumes of Web pages and search queries to discover a
set of possible text entity categories and a set of associated is-A
relationships that map individual text entities to categories. Finally, we
apply these is-A relationships mined from web text on the tags learned from
audiovisual content of videos to automatically synthesize a reliable set of
categories most relevant to videos -- along with a mechanism to predict these
categories for new uploads. We then present rigorous rating studies that
establish that: (a) the average relevance of tags automatically recommended by
our system matches the average relevance of the uploader-supplied tags at the
same or better coverage and (b) the average precision@K of video categories
discovered by our system is 70% with K=5."
"Table Detection in Heterogeneous Documents.  Detecting tables in document images is important since not
only do tables contain important information, but also most
of the layout analysis methods fail in the presence of tables
in the document image. Existing approaches for table de-
tection mainly focus on detecting tables in single columns
of text and do not work reliably on documents with varying
layouts. This paper presents a practical algorithm for table
detection that works with a high accuracy on documents
with varying layouts (company reports, newspaper articles,
magazine pages, . . . ). An open source implementation of the
algorithm is provided as part of the Tesseract OCR engine.
Evaluation of the algorithm on document images from pub-
licly available UNLV dataset shows competitive performance
in comparison to the table detection module of a commercial
OCR system."
"Large Scale Graph Transduction.  We consider the issue of scalability of graph-based semi-supervised learning (SSL) algorithms. In this context, we propose a fast graph node ordering algorithm that improves parallel spatial locality by being cache cognizant. This approach allows for a linear speedup on a shared-memory parallel machine to be achievable, and thus means that graph-based SSL can scale to very large data sets. We use the above algorithm an a multi-threaded implementation to solve a SSL problem on a 120 million node graph in a reasonable amount of time."
"Measuring Advertising Quality on Television: Deriving Meaningful Metrics from Audience Retention Data.  This paper introduces a measure of television ad quality based on audience retention, using logistic regression techniques to normalize such scores against expected audience behavior. By adjusting for features such as time of day, network, recent user behavior, and household demographics, we are able to isolate ad quality from these extraneous factors.  We introduce the current model used in our production system, as well as two new competing models that show some improvement.  We also devise metrics for calculating a model??s predictive power and variance, allowing us to determine which of our models performs best.  We conclude with discussions of retention score applications for advertisers to evaluate their ad strategies, and potential as an aid in future ad pricing."
"sos: Searching Help Pages of R Packages.  The sos package provides a means to quickly and flexibly search the help pages of contributed packages, finding
functions and datasets in seconds or minutes that could not be found in hours or days by any other means we know.  Its findFn function accesses Jonathan Baron's R Site Search database and returns the matches in a data frame of class ""findFn"", which can be further manipulated by other sos functions to produce, for example, an Excel file that starts with a summary sheet that makes it relatively easy to prioritize alternative packages for further study. As such, it provides a very powerful way to do a literature search for functions and packages relevant to a particular topic of interest and could become virtually mandatory for authors of new packages or papers in publications such as The R Journal and the Journal of Statistical Software."
"Large Scale Image Annotation: Learning to Rank with Joint Word-Image Embeddings.  Image annotation datasets are becoming larger and larger,
with tens of millions of images and
tens of thousands of possible annotations.
We propose a strongly performing method  that scales to such
datasets
by simultaneously learning to optimize precision at k of the ranked list of annotations for a given image
 and learning a low-dimensional joint embedding
   space for both images and annotations.
Our method both outperforms several baseline
methods and, in comparison to them,
is faster and consumes less memory.
We also demonstrate how our method
 learns an interpretable model, where annotations with
alternate spellings
or even languages are close in the embedding space. Hence, even when our model does not predict
the exact annotation given by a human labeler,
it often predicts similar annotations, a fact that we try to quantify by measuring
the newly introduced ``sibling'' precision metric, where our method also obtains excellent results."
"Metric Embeddings with Relaxed Guarantees.  We consider the problem of embedding finite metrics with slack: We seek to produce embeddings with small dimension and distortion while allowing a (small) constant fraction of all distances to be arbitrarily distorted. This definition is motivated by recent research in the networking community, which achieved striking empirical success at embedding Internet latencies with low distortion into low-dimensional Euclidean space, provided that some small slack is allowed. Answering an open question of Kleinberg, Slivkins, and Wexler [in Proceedings of the 45th IEEE Symposium on Foundations of Computer Science, 2004], we show that provable guarantees of this type can in fact be achieved in general: Any finite metric space can be embedded, with constant slack and constant distortion, into constant-dimensional Euclidean space. We then show that there exist stronger embeddings into $\ell_1$ which exhibit gracefully degrading distortion: There is a single embedding into $\ell_1$ that achieves distortion at most $O(\log\frac{1}{\epsilon})$ on all but at most-1.5pt an $\epsilon$ fraction of distances simultaneously for all $\epsilon&gt;0$. We extend this with distortion1pt $O(\log\frac{1}{\epsilon})^{1/p}$ to maps into general $\ell_p$, $p\geq1$, for several classes of metrics, including those with bounded doubling dimension and those arising from the shortest-path metric of a graph with an excluded minor. Finally, we show that many of our constructions are tight and give a general technique to obtain lower bounds for $\epsilon$-slack embeddings from lower bounds for low-distortion embeddings."
"QuickSuggest: Character Prediction for Improved Text Entry on Web Appliances.  As traditional media and information devices integrate with the web,
they must abruptly support a vastly larger database of relevant items.
Many devices such as internet-capable televisions and set-top boxes
support traditional remote controls with on-screen keyboards for text
input. These input methods are not well suited for text entry but are
difficult to displace. To make these devices work well in a rich
information environment such as the WWW, we must develop ways to
improve text entry through this input bottleneck. We introduce
QuickSuggest which significantly improves text entry speed for
on-screen keyboards, much like query suggestions in a search text box
can improve query input. QuickSuggest uses the same simple
Up/Down/Left/Right/Enter interface common to remote controls, gaming
devices and car input controls used to enter text.
The paper describes QuickSuggest's novel adaptive
user interface to make text entry more efficient and demonstrates
quantitative improvements from simulation results on millions of user
queries. User experiments also show ease of use and efficiency with no
learning curve. Our results suggest that very simple input devices can
be used to enter text covering a large vocabulary with surprising
ease."
"Evaluating IPv6 adoption in the Internet.  As IPv4 address space approaches exhaustion, large networks are deploying IPv6 or preparing for deployment. However, there is little data available about the quantity and quality of IPv6 connectivity. We describe a methodology to measure IPv6 adoption from the perspective of a Web site operator and to evaluate the impact that adding IPv6 to a Web site will have on its users. We apply our methodology to the Google Web site and present results collected over the last year. Our data show that IPv6 adoption, while growing significantly, is still low, varies considerably by country, and is heavily influenced by a small number of
large deployments. We find that native IPv6 latency is comparable to IPv4 and provide statistics on IPv6 transition mechanisms used."
"Discontinuous Seam-Carving for Video Retargeting.  We introduce a new algorithm for video retargeting that uses discontinuous seam-carving in both space and time for resizing videos. We propose a novel appearance-based temporal coherence formulation that allows for frame-by-frame processing and results in temporally discontinuous seams, as opposed to geometrically smooth and continuous seams. This formulation optimizes the difference in appearance of the resultant retargeted frame to the optimal temporally coherent one, and allows for carving around fast moving salient regions. Additionally, we generalize the idea of appearance-based coherence to the spatial domain by introducing piece-wise spatial seams. Our spatial coherence measure minimizes the change in gradients during retargeting, which preserves spatial detail better than minimization of color difference alone. We also show that retargeting based on per-frame saliency (gradient-based or feature-based) does not always lead to desirable results and propose a novel automatically computed measure of spatio-temporal saliency. As needed, the user can also augment the saliency by interactive region-brushing. Our retargeting algorithm processes the video sequentially, which allows us to deal with streaming videos.  We demonstrate results over a wide range of video examples and evaluate the effectiveness of each component of our algorithm."
"Efficient Hierarchical Graph-Based Video Segmentation.  We present an efficient and scalable technique for spatio-temporal segmentation of long video sequences using a hierarchical graph-based algorithm. We begin by over-segmenting a volumetric video graph into space-time regions grouped by appearance. We then construct a ``region graph"" over the obtained segmentation and iteratively repeat this process over multiple levels to create a tree of spatio-temporal segmentations. This hierarchical approach generates high quality segmentations which are temporally coherent with stable region boundaries. Additionally, the resulting segmentation hierarchy allows subsequent applications to choose from varying levels of granularity. We further improve segmentation quality by using dense optical flow when constructing the initial graph. We also propose two novel approaches to improve the scalability of our technique: (a) a parallel out-of-core algorithm that can process volumes much larger than an in-core algorithm, and (b) a clip-based processing algorithm that divides the video into overlapping clips in time, and segments them successively while enforcing consistency. We can segment video shots as long as 40 seconds without compromising quality, and even support a streaming mode for arbitrarily long videos, albeit without the ability to process them hierarchically."
"MapReduce: The programming model and practice.  Inspired by similar concepts in functional languages dated as early as 60's, Google first introduced MapReduce in 2004. Now, MapReduce has become the most popular framework for large-scale data processing at Google and it is becoming the framework of choice on many off-the-shelf clusters."
"Google Fusion Tables: Data Management, Integration, and Collaboration in the Cloud.  Google Fusion Tables is a cloud-based service for data management and integration. Fusion Tables enables users to upload tabular data les (spreadsheets, CSV, KML), currently of up to 100MB. The system provides several ways of visualizing the data (e.g., charts, maps, and timelines) and the ability to filter and aggregate the data. It supports the integration of data from multiple sources by performing joins across tables that may belong to dierent users. Users can keep the data private, share it with a select set of collaborators, or make it public and thus crawlable by search engines. The discussion feature of Fusion Tables allows collaborators to conduct detailed discussions of the data at the level of tables and individual rows, columns, and cells. This paper describes the inner workings of Fusion Tables, including the storage of data in the system and the tight integration with
the Google Maps infrastructure."
"Google Fusion Tables: Web-Centered Data Management and Collaboration.  It has long been observed that database management systems focus on traditional business applications, and that few people use a database management system outside their workplace. Many have wondered what it will take to enable the use of data management technology by a broader class of users and for a much wider range of applications. Google Fusion Tables represents an initial answer to the question of how data management functionality that focused on enabling new users and applications would look in today's computing environment. This paper characterizes such users and applications and highlights the resulting principles, such as seamless Web integration, emphasis on ease of use, and incentives for data sharing, that underlie the design of Fusion Tables. We describe key novel features, such as the support for data acquisition, collaboration, visualization, and web-publishing."
"Object views: Fine-grained sharing in browsers.  Browsers do not currently support the secure sharing of JavaScript objects between principals. We present this problem as the need for object views, which are consistent and controllable versions of objects. Multiple views can be made for the same object and customized for the recipients. We implement object views with a JavaScript library that wraps shared objects and interposes on all access attempts. Developers can control the fine-grained behavior of objects with an aspect system that accepts programmatic policies. The security challenge is to fully mediate access to objects shared through a view and prevent privilege escalation. To facilitate simple document sharing, we build a policy system for declaratively defining policies for document object views. Notably, our document policy system makes it possible to hide elements without breaking document structure invariants. We discuss how object views can be deployed in two settings: same-origin sharing with rewriting-based JavaScript isolation systems like Google Caja, and inter-origin sharing between browser frames over a message-passing channel."
"Sparse Spatiotemporal Coding for Activity Recognition.  We present a new approach to learning sparse, spatiotemporal features and demonstrate the utility of the approach by applying the resulting sparse codes to the problem of activity recognition. Learning features that discriminate among human activities in video is difficult in part because the stable space-time events that reliably characterize the relevant motions are rare. To overcome this problem, we adopt a multi-stage approach to activity recognition. In the initial preprocessing stage, we first whiten and apply local contrast normalization to each frame of the video. We then apply an additional set of filters to identify and extract salient space-time volumes that exhibit smooth periodic motion. We collect a large corpus of these space-time volumes as training data for the unsupervised learning of a sparse, over-complete basis using a variant of the two-phase analysis-synthesis algorithm of Olshausen and Field [1997]. We treat the synthesis phase, which consists of reconstructing the input as sparse a mostly coefficient zero and most importantly the time required for reconstruction in subsequent use production we adapted existing algorithms to exploit potential parallelism through the use of readily-available SIMD hardware. To obtain better codes, we developed a new approach to learning sparse, spatiotemporal codes in which the number of basis vectors, their orientations, velocities and the size of their receptive fields change over the duration of unsupervised training. The algorithm starts with a relatively small, initial basis with minimal temporal extent. This initial basis is obtained through conventional sparse coding techniques and is expanded over time by recursively constructing a new basis consisting of basis vectors with larger temporal extent that proportionally conserve regions of previously trained weights. These proportionally conserved weights are combined with the result of adjusting newly added weights to represent a greater range of primitive motion features. The size of the current basis is determined probabilistically by sampling from existing basis vectors according to their activation on the training set. The resulting algorithm produces bases consisting of filters that are bandpass, spatially oriented and temporally diverse in terms of their transformations and velocities. We demonstrate the utility of our approach by using it to recognize human activity in video."
"Compression Progress, Pseudorandomness, &amp; Hyperbolic Discounting.  General intelligence requires open-ended exploratory learning. The principle of compression progress proposes that agents should derive intrinsic reward from maximizing ""interestingness"", the first derivative of compression progress over the agent's history. Schmidhuber posits that such a drive can explain ""essential aspects of ... curiosity, creativity, art, science, music, [and] jokes"", implying that such phenomena might be replicated in an artificial general intelligence programmed with such a drive. I pose two caveats: 1) as pointed out by Rayhawk, not everything that can be considered ""interesting"" according to this definition is interesting to humans; 2) because of (irrational) hyperbolic discounting of future rewards, humans have an additional preference for rewards that are structured to prevent premature satiation, often superseding intrinsic preferences for compression progress."
"Robust Symbolic Regression with Affine Arithmetic.  We use affine arithmetic to improve both the performance and the robustness of genetic programming for symbolic regression. During evolution, we use affine arithmetic to analyze expressions generated by the genetic operators, giving an estimate of their output range given the ranges of their inputs over the training data. These estimated output ranges allow us to discard trees that contain asymptotes as well as those whose output is too far from the desired output range determined by the training instances. We also perform linear scaling of outputs before fitness evaluation. Experiments are performed on 15 problems, comparing the proposed system with a baseline genetic programming system with protected operators, and with a similar system based on interval arithmetic. Results show integrating affine arithmetic with an implementation of standard genetic programming reduces the number of fitness evaluations during training and improves generalization performance, minimizes overfitting, and completely avoids extreme errors on unseen test data."
"PLANET: Massively Parallel Learning of Tree Ensembles with MapReduce.  Classification and regression tree learning on massive datasets is a common data mining task at Google, yet many state of the art tree learning algorithms require training data to reside in memory on a single machine. While more scalable implementations of tree learning have been proposed, they typically require specialized parallel computing architectures. In contrast, the majority of Google??s computing infrastructure is based on commodity hardware. In this paper, we describe PLANET: a scalable distributed framework for learning tree models over large datasets. PLANET defines tree learning as a series of distributed computations, and implements each one using the MapReduce model of distributed computation. We show how this framework supports scalable construction of classification and regression trees, as well as ensembles of such models. We discuss the benefits and challenges of using a MapReduce compute cluster for tree learning, and demonstrate the scalability of this approach by applying it to a real world learning task from the domain of computational advertising."
"Measuring the User Experience on a Large Scale: User-Centered Metrics for Web Applications.  More and more products and services are being deployed
on the web, and this presents new challenges and
opportunities for measurement of user experience on a large
scale. There is a strong need for user-centered metrics for
web applications, which can be used to measure progress
towards key goals, and drive product decisions. In this
note, we describe the HEART framework for user-centered
metrics, as well as a process for mapping product goals to
metrics. We include practical examples of how HEART
metrics have helped product teams make decisions that are
both data-driven and user-centered. The framework and
process have generalized to enough of our company??s own
products that we are confident that teams in other
organizations will be able to reuse or adapt them. We also
hope to encourage more research into metrics based on
large-scale behavioral data."
"Children's Roles Using Keyword Search Interfaces in the Home.  Children want to find information about their world, but
there are barriers to finding what they seek. Young people
have varying abilities to formulate multi-step queries and
comprehend search results. Challenges in understanding
where to type, confusion about what tools are available, and
frustration with how to parse the results page all have led to
a lack of perceived search success for children 7-11 years
old. In this paper, we describe seven search roles children
display as information seekers using Internet keyword
interfaces, based on a home study of 83 children ages 7, 9,
and 11. These roles are defined not only by the children??s
search actions, but also by who influences their searching,
their perceived success, and trends in age and gender.
These roles suggest a need for new interfaces that expand
the notion of keywords, scaffold results, and develop a
search culture among children."
"Behind the Scenes of Google Maps Navigation: Enabling actionable user feedback at scale.  This case study describes an Android-based feedback mechanism, created to gain structured input on prototypes of Google Maps Navigation, a mobile GPS navigation system, during real-world usage. We note the challenges faced, common to many mobile projects, and how we addressed them. We describe the user flow for submitting feedback; the resulting feedback report from the team's perspective; our triaging process for the high volume of incoming data; and the results &amp; benefits gleaned from using this system. Learnings and recommendations are provided, to aid mobile teams who may be interested in developing a similar system for their working prototype, particularly if real-world testing is required."
"Rhythms and plasticity: television temporality at home.  Digital technologies have enabled new temporalities of media consumption in the home. Through a field study of home television viewing practices, we investigated temporal orderings of television watching. In contrast to traditional pictures of television use, our evidence suggests that rhythms across households play an important role in shaping television watching. Further, we found a flexibility and openness within the patterns of television viewing that we refer to as ??plasticity.?? Our data suggest that plasticity and rhythms co-exist and together compose the qualitative experience of domestic television time; an understanding of both aspects of temporality suggests an approach for the design of future television technologies."
"Best of Both Worlds: Improving Gmail Labels with the Affordances of Folders.  Gmail??s filing system for email conversations is based 
around labels, which are more flexible and powerful 
than folders. With its original user interface, many 
users did not discover labels, and wondered why Gmail 
had no folders. The Gmail team redesigned the user 
interface for labeling to make it more discoverable and 
understandable, and to add the most useful 
functionality of folders. The new design works for the 
simple use case (a conversation with only one label), 
while still making the more complex use case (multiple 
labels) easily available. It has been launched to millions 
of users worldwide and has resulted in much higher 
adoption of labels, especially by new users of Gmail."
"The Nocebo Effect on the Web: An Analysis of Fake Anti-Virus Distribution.  We present a study of Fake Anti-Virus attacks on the web. Fake AV software masquerades as a legitimate security product with the goal of deceiving victims into paying registration fees to seemingly remove malware from their computers. Our analysis of 240 million web pages collected by Google's malware detection infrastructure over a 13 month period discovered over 11,000 domains involved in Fake AV distribution. We show that the Fake AV threat is rising in prevalence, both absolutely, and relative to other forms of web-based malware. Fake AV currently accounts for 15% of all malware we detect on the web. Our investigation reveals several characteristics that distinguish Fake AVs from other forms of web-based malware and shows how these characteristics have changed over time. For instance, Fake AV attacks occur frequently via web sites likely to reach more users including spam web sites and on-line Ads. These attacks account for 60% of the malware discovered on domains that include trending keywords. As of this writing, Fake AV is responsible for 50% of all malware delivered via Ads, which represents a five-fold increase from just a year ago."
"Lightweight Feedback-Directed Cross-Module Optimization.  Cross-module inter-procedural compiler optimization (IPO) and Feedback-Directed Optimization (FDO) are two important compiler techniques delivering solid performance gains.  The combination of IPO and FDO delivers peak performance, but also multiplies both
techniques' usability problems. In this paper, we present LIPO, a novel static IPO framework, which  integrates IPO and FDO. Compared to existing approaches, LIPO no
longer requires writing of the compiler's intermediate representation, eliminates the link-time inter-procedural optimization phase entirely, and minimizes code re-generation overhead, thus improving scalability by an order of magnitude.  Compared to an FDO baseline, and without further specific tuning, LIPO improves performance of SPEC2006 INT
by 2.5%, and of SPEC2000 INT by 4.4%, with up to 23% for one benchmarks. We confirm
our scalability results on a set of large industrial applications, demonstrating 2.9% 
performance improvements on average. Compile time overhead for full builds is less than 30%, incremental builds take a few seconds on average, and storage requirements increase by only 24%, all compared to the FDO baseline."
"Dapper, a Large-Scale Distributed Systems Tracing Infrastructure.  Modern Internet services are often implemented as complex, large-scale distributed systems. These applications are constructed from collections of software modules that may be developed by different teams, perhaps in different programming languages, and could span many thousands of machines across multiple physical facili- ties. Tools that aid in understanding system behavior and reasoning about performance issues are invaluable in such an environment. Here we introduce the design of Dapper, Google??s production distributed systems tracing infrastructure, and describe how our design goals of low overhead, application-level transparency, and ubiquitous deployment on a very large scale system were met. Dapper shares conceptual similarities with other tracing systems, particularly Magpie [3] and X-Trace [12], but certain design choices were made that have been key to its success in our environment, such as the use of sampling and restricting the instrumentation to a rather small number of common libraries. The main goal of this paper is to report on our experience building, deploying and using the system for over two years, since Dapper??s foremost measure of success has been its usefulness to developer and operations teams. Dapper began as a self-contained tracing tool but evolved into a monitoring platform which has enabled the creation of many different tools, some of which were not anticipated by its designers. We describe a few of the analysis tools that have been built using Dapper, share statistics about its usage within Google, present some example use cases, and discuss lessons learned so far."
"Taming Hardware Event Samples for FDO Compilation.  Feedback-directed optimization (FDO) is effective in improving application runtime performance, but has not been widely adopted due to the tedious dual-compilation model, the difficulties in generating representative training data sets, and the high runtime overhead of profile collection. The use of hardware-event sampling to generate estimated edge profiles overcomes these drawbacks. Yet, hardware event samples are typically not precise at the instruction or basic-block granularity. These inaccuracies lead to missed performance when compared to instrumentation-based FDO. In this paper, we use multiple hardware event profiles and supervised learning techniques to generate heuristics for improved precision of basic-block-level sample profiles, and to further improve the smoothing algorithms used to construct edge profiles. We demonstrate that sampling-based FDO can achieve an average of 78% of the performance gains obtained using instrumentation-based exact edge profiles for SPEC2000 benchmarks, matching or beating instrumentation-based FDO in many cases. The overhead of collection is only 0.74% on average, while compiler based instrumentation incurs 6.8%??53.5% overhead (and 10x overhead on an industrial web search application), and dynamic instrumentation incurs 28.6%??1639.2% overhead."
"Demand Dispatch - Using Real-Time Control of Demand to help Balance Generation and Load.  DEMAND RESPONSE (DR) TRADITIONALLY REFERS TO THE ability to curtail some electrical loads at peak times to alleviate the need for peaking generation sources. Basically, it means being able to turn loads off on command. Progress in communication protocols and technology has been extraordinary in the past decade, making cheap, fast communication widespread. Over the next decade, we expect inexpensive broadband to become ubiquitous. In addition, more and more electrical loads are equipped for communication as well as control. Together, these trends enable a new way of thinking about DR, which we call demand dispatch."
"A theory of learning from different domains.  Discriminative learning methods for classification perform well when training and test data are drawn from the same distribution. Often, however, we have plentiful labeled training data from a source domain but wish to learn a classifier which performs well on a target domain with a different distribution and little or no labeled training data. In this work we investigate two questions. First, under what conditions can a classifier trained from source data be expected to perform well on target data? Second, given a small amount of labeled target data, how should we combine it during training with the large amount of labeled source data to achieve the lowest target error at test time?   We address the first question by bounding a classifier's target error in terms of its source error and the divergence between the two domains. We give a classifier-induced divergence measure that can be estimated from finite, unlabeled samples from the domains. Under the assumption that there exists some hypothesis that performs well in both domains, we show that this quantity together with the empirical source error characterize the target error of a source-trained classifier.    We answer the second question by bounding the target error of a model which minimizes a convex combination of the empirical source and target errors. Previous theoretical work has considered minimizing just the source error, just the target error, or weighting instances from the two domains equally. We show how to choose the optimal combination of source and target error as a function of the divergence, the sample sizes of both domains, and the complexity of the hypothesis class. The resulting bound generalizes the previously studied cases and is always at least as tight as a bound which considers minimizing only the target error or an equal weighting of source and target errors."
"A transcription factor affinity-based code for mammalian transcription initiation.  The recent arrival of large-scale cap analysis of gene expression (CAGE) data sets in mammals provides a wealth of quantitative information on coding and noncoding RNA polymerase II transcription start sites (TSS). Genome-wide CAGE studies reveal that a large fraction of TSS exhibit peaks where the vast majority of associated tags map to a particular location ( approximately 45%), whereas other active regions contain a broader distribution of initiation events. The presence of a strong single peak suggests that transcription at these locations may be mediated by position-specific sequence features. We therefore propose a new model for single-peaked TSS based solely on known transcription factors (TFs) and their respective regions of positional enrichment. This probabilistic model leads to near-perfect classification results in cross-validation (auROC = 0.98), and performance in genomic scans demonstrates that TSS prediction with both high accuracy and spatial resolution is achievable for a specific but large subgroup of mammalian promoters. The interpretable model structure suggests a DNA code in which canonical sequence features such as TATA-box, Initiator, and GC content do play a significant role, but many additional TFs show distinct spatial biases with respect to TSS location and are important contributors to the accurate prediction of single-peak transcription initiation sites. The model structure also reveals that CAGE tag clusters distal from annotated gene starts have distinct characteristics compared to those close to gene 5'-ends. Using this high-resolution single-peak model, we predict TSS for approximately 70% of mammalian microRNAs based on currently available data."
Privacy protection and face recognition.  Invited chapter in second edition of Handbook of Face recognition ed Stan Li &amp; Anil K. Jain. Covers privacy protecting technologies applied to face detection and recognition.
"Suggesting Friends Using the Implicit Social Graph.  Although users of online communication tools rarely categorize their contacts into groups such as ""family"", ""co-workers"", or ""jogging buddies"", they nonetheless implicitly cluster contacts, by virtue of their interactions with them, forming implicit groups. In this paper, we describe the implicit social graph which is formed by users' interactions with contacts and groups of contacts, and which is distinct from explicit social graphs in which users explicitly add other individuals as their ""friends"". We introduce an interaction-based metric for estimating a user's affinity to his contacts and groups.  We then describe a novel friend suggestion algorithm that uses a user's implicit social graph to generate a friend group, given a small seed set of contacts which the user has already labeled as friends. We show experimental results that demonstrate the importance of both implicit group relationships and  interaction-based affinity ranking in suggesting friends. Finally, we discuss two applications of the Friend Suggest algorithm that have been released as Gmail Labs features."
"MAC Reforgeability.  Message Authentication Codes (MACs) are core algorithms deployed in virtually every security protocol in common usage.  In these protocols, the integrity and authenticity of messages rely entirely on the security of the MAC; we examine cases in which this security is lost.
In this paper, we examine the notion of ""reforgeability"" for MACs, and motivate its utility in the context of {power, bandwidth, CPU}-constrained computing environments.  We first give a definition for this new notion, then examine some of the most widely-used and well-known MACs under our definition in a variety of adversarial settings, finding in nearly all cases a failure to meet the new notion.  We examine simple counter-measures to increase resistance to reforgeability, using state and truncating the tag length, but find that both are not simultaneously applicable to modern MACs.  In response, we give a tight security reduction for a new MAC, WMAC, which we argue is the ""best fit"" for resource-limited devices."
"Example-based Image Compression.  The current standard image-compression approaches rely on fairly simple predictions, using either block- or wavelet-based methods. While many more sophisticated texture-modeling approaches have been proposed, most do not provide a significant improvement in compression rate over the current standards at a workable encoding complexity level. We re-examine this area, using example-based texture prediction. We find that we can provide consistent and significant improvements over JPEG, reducing the bit rate by more than 20% for many PSNR levels. These improvements require consideration of the differences between residual energy and prediction/residual compressibility when selecting a texture prediction, as well as careful control of the computational complexity in encoding."
"Fast Covariance Computation and Dimensionality Reduction for Sub-Window Features in Images.  This paper presents algorithms for efficiently computing the covariance matrix for features that form sub-windows in a large multi-dimensional image. For example, several image processing applications, e.g. texture analysis/synthesis, image retrieval, and compression, operate upon patches within an image. These patches are usually projected onto a low-dimensional feature space using dimensionality reduction techniques such as Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA), which in-turn requires computation of the covariance matrix from a set of features. Covariance computation is usually the bottleneck during PCA or LDA (O(nd^2) where n is the number of pixels in the image and d is the dimensionality of the vector). Our approach reduces the complexity of covariance computation by exploiting the redundancy between feature vectors corresponding to overlapping patches. Specifically, we show that the covariance between two feature components can be reduced to a function of the relative displacement between those components in patch space. One can then employ a lookup table to store covariance values by relative displacement. By operating in the frequency domain, this lookup table can be computed in O(n log n) time. We allow the patches to sub-sample the image, which is useful for hierarchical processing and also enables working with filtered responses over these patches, such as local gist features. We also propose a method for fast projection of sub-window patches onto the low-dimensional space."
"A Comparison of Features for Automatic Readability Assessment.  Several sets of explanatory variables ?? including shallow, language modeling, POS, syntactic, and discourse features ?? are compared and evaluated in terms of their impact on predicting the grade level of reading material for primary school students. We find that features based on in-domain language models have the highest predictive power. Entity-density (a discourse feature) and POS-features, in particular nouns, are individually very useful but highly correlated. Average sentence length (a shallow feature) is more useful ?? and less expensive to compute ?? than individual syntactic features. A judicious combination of features examined here results in a significant improvement over the state of the art."
"Ignore These At Your Peril: Ten principles for trust design.  Online trust has been discussed for more than 10 years, yet little practical guidance has emerged that has proven to be applicable across contexts or useful in the long run. 'Trustworthy UI design guidelines' created in the late 90ies to address the then big question of online trust: how to get shoppers online,  are now happily employed by people preparing phishing scams. In this paper we summarize, in practical terms, a conceptual framework for online trust we've established in 2005. Because of its abstract nature it is still useful as a lens through which to view the current big questions of the online trust debate - largely focused on usable security and phishing attacks. We then deduct practical 10 rules for providing effective trust support to help practitioners and researchers of usable security."
"Energy Proportional Datacenter Networks.  Numerous studies have shown that datacenter computers rarely operate at full utilization, leading to a number of proposals for creating servers that are energy proportional with respect to the computation that they are performing. In this paper, we show that as servers themselves become more energy proportional, the datacenter network can become a significant fraction (up to 50%) of cluster power. In this paper we propose several ways to design a high-performance datacenter network whose power consumption is more proportional to the amount of traffic it is moving --- that is, we propose energy proportional datacenter networks.
We first show that a flattened butterfly topology itself is inherently more power efficient than the other commonly proposed topology for high-performance datacenter networks. We then exploit the characteristics of modern plesiochronous links to adjust their power and performance envelopes dynamically. Using a network simulator, driven by both synthetic workloads and production datacenter traces, we characterize and understand design tradeoffs, and demonstrate an 85% reduction in power --- which approaches the ideal energy-proportionality of the network. Our results also demonstrate two challenges for the designers of future network switches: 1) We show that there is a significant power advantage to having independent control of each unidirectional channel comprising a network link, since many traffic patterns show very asymmetric use, and 2) system designers should work to optimize the high-speed channel designs to be more energy efficient by choosing optimal data rate and equalization technology. Given these assumptions, we demonstrate that energy proportional datacenter communication is indeed possible."
"Search by Voice in Mandarin Chinese.  In this paper we describe our efforts to build a Mandarin Chinese voice search system. We describe our strategies for data collection, language, lexicon and acoustic modeling, as well as issues related to text normalization that are an integral part of building voice search systems. We show excellent performance on typical spoken search queries under a variety of accents and acoustic conditions. The system has been in operation since October 2009 and has received very positive user reviews."
"Generalization Bounds for Learning Kernels.  This paper presents several novel generalization
bounds for the problem of learning kernels based
on a combinatorial analysis of the Rademacher
complexity of the corresponding hypothesis sets.
Our bound for learning kernels with a convex
combination of p base kernels using L1 regularization
admits only a ???log p dependency on the
number of kernels, which is tight and considerably
more favorable than the previous best bound
given for the same problem. We also give a novel
bound for learning with a non-negative combination
of p base kernels with an L2 regularization
whose dependency on p is also tight and only in
p^(1/4). We present similar results for Lq regularization
with other values of q, and outline the relevance
of our proof techniques to the analysis of
the complexity of the class of linear functions.
Experiments with a large number of kernels further
validate the behavior of the generalization
error as a function of p predicted by our bounds."
"Two-Stage Learning Kernel Algorithms.  This paper examines two-stage techniques for
learning kernels based on a notion of alignment.
It presents a number of novel theoretical, algorithmic,
and empirical results for alignmentbased
techniques. Our results build on previous
work by Cristianini et al. (2001), but we adopt
a different definition of kernel alignment and
significantly extend that work in several directions:
we give a novel and simple concentration
bound for alignment between kernel matrices;
show the existence of good predictors for kernels
with high alignment, both for classification
and for regression; give algorithms for learning a
maximum alignment kernel by showing that the
problem can be reduced to a simple QP; and report
the results of extensive experimentswith this
alignment-based method in classification and regression
tasks, which show an improvement both
over the uniformcombination of kernels and over
other state-of-the-art learning kernel methods."
"Half Transductive Ranking.  We study the standard retrieval task of ranking
a fixed set of items given a previously unseen
query and pose it as the half transductive
ranking problem. The task is transductive
as the set of items is fixed. Transductive
representations (where the vector representation
of each example is learned) allow
the generation of highly nonlinear embeddings
that capture object relationships without
relying on a specific choice of features,
and require only relatively simple optimization.
Unfortunately, they have no direct outof-
sample extension. Inductive approaches
on the other hand allow for the representation
of unknown queries. We describe algorithms
for this setting which have the advantages
of both transductive and inductive approaches,
and can be applied in unsupervised
(either reconstruction-based or graph-based)
and supervised ranking setups. We show empirically
that our methods give strong performance
on all three tasks."
"Study on Interaction between Entropy Pruning and Kneser-Ney Smoothing.  The paper presents an in-depth analysis of a less known interaction between Kneser-Ney smoothing and entropy pruning
that leads to severe degradation in language model performance under aggressive pruning regimes. Experiments in a data-rich setup such as google.com voice search show a significant impact in WER as well: pruning Kneser-Ney and Katz models to 0.1% of their original impacts speech recognition accuracy significantly, approx. 10% relative. Any third party with LDC membership should be able to reproduce our experiments using the scripts available at http://code.google.com/p/kneser-ney-pruning-experiments."
"Adaptive Bound Optimization for Online Convex Optimization.  We introduce a new online convex optimization algorithm that
adaptively chooses its regularization function based on the loss
functions observed so far.  This is in contrast to previous algorithms
that use a fixed regularization function such as L2-squared, and
modify it only via a single time-dependent parameter.  Our algorithm's
regret bounds are worst-case optimal, and for certain realistic
classes of loss functions they are much better than existing bounds.
These bounds are problem-dependent, which means they can exploit the
structure of the actual problem instance.  Critically, however, our
algorithm does not need to know this structure in advance.  Rather, we
prove competitive guarantees that show the algorithm provides a bound
within a constant factor of the best possible bound (of a certain
functional form) in hindsight."
"Automatically Learning Source-side Reordering Rules for Large Scale Machine Translation.  We describe an approach to automatically learn reordering rules to be applied as a preprocessing step in phrase-based machine 
translation. We learn rules for 8 different language pairs, showing
BLEU improvements for all of them, and demonstrate that many
important order transformations (SVO to SOV or VSO, head-modifier, verb
movement) can be captured by this approach."
"Unsupervised Discovery and Training of Maximally Dissimilar Cluster Models.  One of the difficult problems of acoustic modeling for Automatic
Speech Recognition (ASR) is how to adequately model
the wide variety of acoustic conditions which may be present
in the data. The problem is especially acute for tasks such as
Google Search by Voice, where the amount of speech available
per transaction is small, and adaptation techniques start showing
their limitations. As training data from a very large user
population is available however, it is possible to identify and
jointly model subsets of the data with similar acoustic qualities.
We describe a technique which allows us to perform this
modeling at scale on large amounts of data by learning a treestructured
partition of the acoustic space, and we demonstrate
that we can significantly improve recognition accuracy in various
conditions through unsupervised Maximum Mutual Information
(MMI) training. Being fully unsupervised, this technique
scales easily to increasing numbers of conditions."
"Not gone, but forgotten: Helping users re-find web pages by identifying those which are most likely to be lost.  We describe LostRank, a project in its formative stage which
aims to produce a way to rank results in re-finding search
engines according to the likelihood of their being lost to the user. To this end, we have explored a number of ideas, including applying users' temporal document access patterns
to determine the documents that are both important and
have not been recently accessed (indicating greater potential for loss), understanding users' topical access patterns to determine the topics that are more unfamiliar and hence more difficult to re-find documents within, and assessing users' difficulties in originally finding documents in order to predict future difficulties in re-finding them. As a position paper, we use this as an opportunity to describe early work, invite collaboration with others, and further the case for the use of temporal access patterns as a source for assisting users' re-finding of personal documents."
"Evaluating TV Ad Campaigns Using Set-Top Box Data.  Google has developed new metrics based on set-top box data for predicting the future audience retention of TV ads. This paper examines how to use these metrics to judge the effectiveness of TV ad campaigns. More specifically, we analyze how these metrics can inform future campaign targeting and placement goals."
"How Surfers Watch: Measuring audience response to video advertising online.  For several years, Google has been analyzing television set-top box data to measure audience response to specific TV ads. This paper presents how similar techniques can be applied to online video advertising on YouTube. As more and more video programming is made available online, it will become increasingly important to understand how to engage with online viewers through video advertising. Furthermore, we find that viewing behavior is even more effected by specific video ad creatives online than it is on TV. This suggests that online viewing can become a valuable source data on viewer response to video ad creatives more generally."
"Making Privacy a Fundamental Component of Web Resources.  We present a social network inspired and access control list based sharing model for web resources. We have specified it as an extension for OpenSocial 1.0 and implemented a proof of concept in Orkut as well as a mobile social photo sharing
application using it. The paper explains important design decisions and how the model can be leveraged to make privacy a core component and enabler for sharing resources on the web and beyond using capabilities of mobile devices."
"Overlapping Experiment Infrastructure: More, Better, Faster Experimentation.  At Google, experimentation is practically a mantra; we evaluate almost every change that potentially affects what our users experience.  Such changes include not only obvious user-visible changes such as modifications to a user interface, but also more subtle changes such as different machine learning algorithms that might affect ranking or content selection. Our insatiable appetite for experimentation has led us to tackle the problems of how to run more experiments, how to run experiments that produce better decisions, and how to run them faster. In this paper, we describe Google??s overlapping experiment infrastructure that is a key component to solving these problems. In addition, because an experiment infrastructure alone is insufficient, we also discuss the associated tools and educational processes required to use it effectively. We conclude by describing trends that show the success of this overall experimental environment. While the paper specifically describes the experiment system and experimental processes we have in place at Google, we believe they can be generalized and applied by any entity interested in using experimentation to improve search engines and other web applications."
"Using the Wave Protocol to Represent Individuals?? Health Records.  There are several challenges in aggregating health records from multiple sources, including merging data, preserving proper attribution, and allowing corrections. unfortunately, standards for exchanging medical records data, such as CCR and CCD, tend to focus on representing particular clinical data as some subset of a patient??s complete record. This provides a snapshot of a patient record, but there is very little to describe how a sequence of changes to the record should be interpreted as a coherent whole.there is something available that gives us the data aggregation, conflict resolution, and audit trail that what we want: the Wave federation protocol."
"Technology Companies are Best Positioned to Offer Health Record Trusts.  The current health system lacks assurances to patients of data retention and privacy control. We argue that this is due to discrepancies in how health data is reported and consumed and contrast this with how financial credit data is reported and consumed. To address these health system gaps in protection of medical data, we would like to evangelize the implementation of health record trusts. Finally, we argue that Personal Health Records (PHRs) are the closest to offering the main features of health record trusts."
"Evaluating Online Ad Campaigns in a Pipeline: Causal Models at Scale.  Display ads proliferate on the web, but are they effective? Or are
they irrelevant in light of all the other advertising that people see?
We describe a way to answer these questions, quickly and accurately, 
without randomized experiments, surveys, focus groups or expert data analysts. Doubly robust estimation protects against the selection bias 
that is inherent in observational data, and a nonparametric test that 
is based on irrelevant outcomes provides further defense. Simulations 
based on realistic scenarios show that the resulting estimates are 
more robust to selection bias than traditional alternatives, such as 
regression modeling or propensity scoring. Moreover, computations 
are fast enough that all processing, from data retrieval through 
estimation, testing, validation and report generation, proceeds in an 
automated pipeline, without anyone needing to see the raw data."
PseudoID: Enhancing Privacy in Federated Login.  PseudoID is a federated login system that protects users from disclosure of private login data held by identity providers. We offer a proof of concept implementation of PseudoID based on blind digital signatures that is backward-compatible with a popular federated login system named OpenID. We also propose several extensions and discuss some of the practical challenges that must be overcome to further protect user privacy in federated login systems.
"Statistical verification of probabilistic properties with unbounded until.  We consider statistical (sampling-based) solution methods for verifying probabilistic properties with unbounded until.  Statistical solution methods for probabilistic verification use sample execution trajectories for a system to verify properties with some level of confidence.  The main challenge with properties that are expressed using unbounded until is to ensure termination in the face of potentially infinite sample execution trajectories.  We describe two alternative solution methods, each one with its own merits.  The first method relies on reachability analysis, and is suitable primarily for large Markov chains where reachability analysis can be performed efficiently using symbolic data structures, but for which numerical probability computations are expensive.  The second method employs a termination probability and weighted sampling.  This method does not rely on any specific structure of the model, but error control is more challenging.  We show how the choice of termination probability---when applied to Markov chains---is tied to the subdominant eigenvalue of the transition probability matrix, which relates it to iterative numerical solution techniques for the same problem."
"Statistical Language Modeling.  Many practical applications such as automatic speech recognition, statistical machine translation, spelling correction resort to variants of the well established source-channel model for producing the correct string of words W given an input speech signal, sentence in foreign language, or typed text with possible mistakes, respectively. A basic component of such systems is a statistical language model which estimates the prior
probability values for strings of words W."
"Performance Trade-offs Implementing Refactoring Support for Objective-C.  When we started implementing a refactoring tool for real-world C programs, we recognized that preprocessing and parsing in straightforward and accurate ways would result in unacceptably slow analysis times and an overly-complex parsing system.  Instead, we traded some accuracy so we could parse, analyze, and change large, real programs while still making the refactoring experience feel interactive and fast.  Our tradeoffs fell into three categories: using different levels of accuracy in different parts of the analysis, recognizing that the collected wisdom about C programs didn't hold for Objective-C programs, and finding ways to exploit delays in typical interaction with the tool."
"Google-Wide Profiling: A Continuous Profiling Infrastructure for Data Centers.  Google-Wide Profiling (GWP), a continuous profiling infrastructure for data centers, provides performance insights for cloud applications. With negligible overhead, GWP provides stable, accurate profiles and a datacenter-scale tool for traditional performance analyses. Furthermore, GWP introduces novel applications of its profiles, such as application- platform affinity measurements and identification of platform-specific, microarchitectural peculiarities."
"Feedback-Directed Optimizations in GCC with Estimated Edge Profiles from Hardware Event Sampling.  Traditional feedback-directed optimization (FDO) in GCC uses static instrumentation to collect edge and value profiles. This method has shown good application performance gains, but is not commonly used in practice due to the high runtime overhead of profile collection, the tedious dual-compile usage model, and difficulties in generating representative training data sets. In this paper, we show that edge frequency estimates can be successfully constructed with heuristics using profile data collected by sampling of  hardware events, incurring low runtime overhead (e.g., less then 2%), and requiring no instrumentation, yet achieving competitive performance gains. We describe the motivation, design, and implementation of FDO using sample profiles in GCC and also present our initial experimental results with SPEC2000int C benchmarks that show approximately 70% to 90% of the performance gains obtained using traditional FDO with exact edge profiles."
"Say What? Why users choose to speak their web queries.  The context in which a speech-driven application is used (or
conversely not used) can be an important signal for recognition engines, and for spoken interface design. Using
large-scale logs from a widely deployed spoken system, we
analyze on an aggregate level factors that are correlated with a decision to speak a web search query rather than type it. We find the factors most predictive of spoken queries are whether a query is made from an unconventional keyboard, for a search topic relating to a users' location, or for a search topic that can be answered in a ??hands-free?? fashion. We also find, contrary to our intuition, that longer queries have a higher probability of being typed than shorter queries."
"Beyond ??Near-Duplicates??: Learning Hash Codes for Efficient Similar-Image Retrieval.  Finding similar images in a large database is an important, but often computationally expensive, task.  In this paper, we present a two-tier similar-image retrieval system with the efficiency characteristics found in simpler systems designed to recognize near-duplicates.  We compare the efficiency of lookups based on random projections and learned hashes to 100-times-more-frequent exemplar sampling. Both approaches significantly improve on the results from exemplar sampling, despite having significantly lower computational costs. Learned-hash keys provide the best result, in terms of both recall and efficiency."
"Max-Cover in Map-Reduce.  The NP-hard Max-k-cover problem requires selecting k sets from a collection so as to maximize the size of the union. This classic problem occurs commonly in many settings in web search and advertising. For moderately-sized instances, a greedy algorithm gives an approximation of (1-1/e). However, the greedy algorithm requires updating scores of arbitrary elements after each step, and hence becomes intractable for large datasets. We give the first max cover algorithm designed for today's large-scale commodity clusters. Our algorithm has provably almost the same approximation as greedy, but runs much faster. Furthermore, it can be easily expressed in the MapReduce programming paradigm, and requires only polylogarithmically many passes over the data. Our experiments on five large problem instances show that our algorithm is practical and can achieve good speedups compared to the sequential greedy algorithm."
"Research trails: getting back where you left off.  In this paper, we present a prototype system that helps users in early-stage web research to create and reestablish context across fragmented work process, without requiring them to explicitly collect and organize the material they visit. The system clusters a user's web history and shows it as research trails. We present two user interaction models with the research trails. The first interaction model is implemented as a standalone application, which presents a hierarchical view of research trails. The second interaction model is integrated with the web browser. It shows the user's research trails as selectable and manipulable visual streams when they open a new tab. Thereby, the NewTab page serves as a springboard in the browser for a user resuming an ongoing task."
"Universally optimal privacy mechanisms for minimax agents.  A scheme that publishes aggregate information about sensitive data must resolve the trade-off between utility to information consumers and privacy of the database participants. Differential privacy is a well-established definition of privacy--this is a universal guarantee against all attackers, whatever their side-information or intent. Can we have a similar universal guarantee for utility? There are two standard models of utility considered in decision theory: Bayesian and minimax. Ghosh et. al. show that a certain ""geometric mechanism"" gives optimal utility to all Bayesian information consumers. In this paper, we prove a similar result for minimax information consumers. Our result also works for a wider class of information consumers which includes Bayesian information consumers and subsumes the result from [8]. We model information consumers as minimax (risk-averse) agents, each endowed with a loss-function which models their tolerance to inaccuracies and each possessing some side-information about the query. Further, information consumers are rational in the sense that they actively combine information from the mechanism with their side-information in a way that minimizes their loss. Under this assumption of rational behavior, we show that for every fixed count query, the geometric mechanism is universally optimal for all minimax information consumers. Additionally, our solution makes it possible to release query results, when information consumers are at different levels of privacy, in a collusion-resistant manner."
"User browsing models: relevance versus examination.  There has been considerable work on user browsing models for search engine results, both organic and sponsored. The click-through rate (CTR) of a result is the product of the probability of examination (will the user look at the result) times the perceived relevance of the result (probability of a click given examination). Past papers have assumed that when the CTR of a result varies based on the pattern of clicks in prior positions, this variation is solely due to changes in the probability of examination. We show that, for sponsored search results, a substantial portion of the change in CTR when conditioned on prior clicks is in fact due to a change in the relevance of results for that query instance, not just due to a change in the probability of examination. We then propose three new user browsing models, which attribute CTR changes solely to changes in relevance, solely to changes in examination (with an enhanced model of user behavior), or to both changes in relevance and examination. The model that attributes all the CTR change to relevance yields substantially better predictors of CTR than models that attribute all the change to examination, and does only slightly worse than the model that attributes CTR change to both relevance and examination. For predicting relevance, the model that attributes all the CTR change to relevance again does better than the model that attributes the change to examination. Surprisingly, we also find that one model might do better than another in predicting CTR, but worse in predicting relevance. Thus it is essential to evaluate user browsing models with respect to accuracy in predicting relevance, not just CTR."
"Juggler: Virtual Networks for Fun and Profit.  There are many situations in which an additional network interface??or two??can provide benefits to a mobile user. Additional interfaces can support parallelism in network flows, improve handoff times, and provide sideband communication with nearby peers. Unfortunately, such benefits are outweighed by the added costs of an additional physical interface. Instead, virtual interfaces have been proposed as the solution, multiplexing a single physical interface across more than one communication endpoint. However, the switching time of existing implementations is too high for some potential applications, and the benefits of this approach to real applications are not yet clear. This paper directly addresses these two shortcomings. It describes a link-layer implementation of a virtual 802.11 networking layer, called Juggler, that achieves switching times of approximately 3 ms, and less than 400 \mu{\rm s} in certain conditions. We demonstrate the performance of this implementation on three application scenarios. By devoting 10 percent of the duty cycle to background tasks, Juggler can provide nearly instantaneous handoff between base stations or support a modest sideband channel with peer nodes, without adversely affecting foreground throughput. Furthermore, when the client issues concurrent network flows, Juggler is able to assign these flows across more than one AP, providing significant speedup when wired-side bandwidth from the AP constrains end-to-end performance."
"Fiber Optic Communication Technologies: What??s Needed for Datacenter Network Operations.  The authors review the growing trend of warehouse-scale mega-datacenter computing, the Internet transformation driven by mega-datacenter applications, and the opportunities and challenges for fiber optic communication technologies to support the growth of mega-datacenter computing in the next three to four years."
"TCP Option to Denote Packet Mood.  In an attempt to anthropomorphize the bit streams on countless physical layer networks throughout the world, we propose a TCP option to express packet mood.  This can be addressed by adding TCP Options [RFC793] to the TCP header, using ASCII characters that encode commonly used ""emoticons"" to convey packet mood."
"Machine Hearing: An Emerging Field.  (intro paragraph in lieu of abstract)  If we had machines that could hear as humans do, we would expect them to be able to easily distinguish speech from music and background noises, to pull out the speech and music parts for special treatment, to know what direction sounds are coming from, to learn which noises are typical and which are noteworthy. Hearing machines should be able to organize what they hear; learn names for recognizable objects, actions, events, places, musical styles, instruments, and speakers; and retrieve sounds by reference to those names. These machines should be able to listen and react in real time, to take appropriate action on hearing noteworthy events, to participate in ongoing activities, whether in factories, in musical performances, or in phone conversations."
"Public-Key Encryption in the Bounded-Retrieval Model.  We construct the first public-key encryption scheme in the Bounded-Retrieval Model (BRM), providing security against various forms of adversarial ""key leakage"" attacks. In this model, the adversary is allowed to learn arbitrary information about the decryption key, subject only to the constraint that the overall amount of ""leakage"" is bounded by at most L bits. The goal of the BRM is to design cryptographic schemes that can flexibly tolerate arbitrarily leakage bounds L (few bits or many Gigabytes), by only increasing the size of secret key proportionally, but keeping all the other parameters -- including the size of the public key, ciphertext, encryption/decryption time, and the number of secret-key bits accessed during decryption -?? small and independent of L. As our main technical tool, we introduce the concept of an Identity-Based Hash Proof System (IB-HPS), which generalizes the notion of hash proof systems of Cramer and Shoup [CS02] to the identity-based setting. We give three different constructions of this primitive based on: (1) bilinear groups, (2) lattices, and (3) quadratic residuosity. As a result of independent interest, we show that an IB-HPS almost immediately yields an Identity-Based Encryption (IBE) scheme which is secure against (small) partial leakage of the target identity??s decryption key. As our main result, we use IB-HPS to construct public-key encryption (and IBE) schemes in the Bounded-Retrieval Model."
"Datacenter-scale Computing.  Although the field of datacenter computing is arguably still in its relative infancy, a sizable body of work from both academia and industry is already available and some consistent technological trends have begun to emerge. This special issue presents a small sample of the work underway by researchers and professionals in this new field. The selection of articles presented reflects the key role that hardware-software codesign plays in the development of effective datacenter-scale computer systems."
"Training and Testing Low-degree Polynomial Data Mappings via Linear SVM.  Kernel techniques have long been used in SVM to handle linearly inseparable problems by transforming data to a high dimensional space, but training and testing large data sets is often time consuming. In contrast, we can efficiently train and test much larger data sets using linear SVM without kernels. In this work, we apply fast linear-SVM methods to the explicit form of polynomially mapped data and investigate implementation issues. The approach enjoys fast training and testing, but may sometimes achieve accuracy close to that of using highly nonlinear kernels. Empirical experiments show that the proposed method is useful for certain large-scale data sets. We successfully apply the proposed method to a natural language processing (NLP) application by improving the testing accuracy under some training/testing speed requirements."
"Auctions with intermediaries: extended abstract.  Inspired by online advertisement exchange systems, we study a setting where potential buyers of a unique, indivisible good attempt to purchase from a central seller via a set of intermediaries. Each intermediary has captive buyers, and runs an auction for a 'contingent' good. Based on the outcome, the intermediary bids in a subsequent upstream auction run by the seller. In this paper, we study the equilibria and incentives of intermediaries and the central seller. We find that combining the notion of optimal auction design with the double-marginalization arising from the presence of intermediaries yields new strategic elements not present in either setting individually: we show that in equilibrium, revenue-maximizing intermediaries will use an auction with a randomized reserve price chosen from an interval. We characterize the interval and the probability distribution from which this reserve price is chosen as a function of the distribution of buyers' types. Furthermore, we characterize the revenue maximizing auction for the central seller by taking into account the effect of his choice of mechanism on the mechanisms offered by the intermediaries. We find that the optimal reserve price offered by the seller decreases with the number of buyers (but remains strictly positive); in contrast to the classical optimal auction without intermediaries, where the reserve price is independent of the number of buyers."
"An Argument for Increasing TCP's Initial Congestion Window.  TCP flows start with an initial congestion window of at most four segments or approximately 4KB of data. Because most Web transactions are short-lived, the initial congestion window is a critical TCP parameter in determining how quickly flows can finish. While the global network access speeds increased dramatically on average in the past decade, the standard value of TCP??s initial congestion window has remained unchanged. In this paper, we propose to increase TCP??s initial congestion window to at least ten segments (about 15KB). Through large-scale Internet experiments, we quantify the latency benefits and costs of using a larger window, as functions of network bandwidth, round-trip time (RTT), bandwidthdelay product (BDP), and nature of applications. We show that the average latency of HTTP responses improved by approximately 10% with the largest benefits being demonstrated in high RTT and BDP networks. The latency of low bandwidth networks also improved by a significant amount in our experiments. The average retransmission rate increased by a modest 0.5%, with most of the increase coming from applications that effectively circumvent TCP??s slow start algorithm by using multiple concurrent connections. Based on the results from our experiments, we believe the initial congestion window should be at least ten segments and the same be investigated for standardization by the IETF."
"Automata Evaluation and Text Search Protocols with Simulation Based Security.  Our protocol is the first to address this problem with full security in the face of malicious adversaries. The construction is based on a novel protocol for secure oblivious automata evaluation which is of independent interest. In this problem, party P1 holds an automaton and party P2 holds an input string, and they need to decide if the automaton accepts the input, without learning anything else."
"Functional and Logic Programming.  Functional and Logic Programming, 10th International Symposium, FLOPS 2010, Sendai, Japan, April 19-21, 2010, Proceedings"
"CPU bandwidth control for CFS.  Over the past few years there has been an increasing focus on the development of features which deliver resource management within the Linux kernel. The addition of the fair group scheduler has enabled the provisioning of proportional CPU time through the specification of group weights. As the scheduler is inherently work-conserving in nature, a task or a group may consume excess CPU share in an otherwise idle system. There are many scenarios where this unbounded CPU share may lead to unacceptable utilization or latency variation.  CPU bandwidth control approaches this problem by allowing an explicit upper bound for allowable CPU bandwidth to be defined in addition to the lower bound already provided by shares. There are many enterprise scenarios where this functionality is useful. In particular are the cases of pay-per-use environments, and user facing services where provisioning is latency bounded. In this paper we detail the motivations behind this feature, the challenges involved in incorporating into CFS (Completely Fair Scheduler), and the future development road map."
"Scaling Optical Interconnects in Datacenter Networks Opportunities and Challenges for WDM.  We review the growing need for optical interconnect
bandwidth in datacenter networks, and the opportunities and
challenges for wavelength division multiplexing (WDM) to
sustain the ??last 2km?? bandwidth growth inside datacenter
networks."
"Fast Routing in Very Large Public Transportation Networks Using Transfer Patterns.  We show how to route on very large public transportation networks (up to half a billion arcs) with average query times of a few milliseconds. We take into account many realistic features like: traffic days, walking between stations, queries between geographic locations instead of a source and a target station, and multi-criteria cost functions. Our algorithm is based on two key observations: (1) many shortest paths share the same transfer pattern, i.e., the sequence of stations where a change of vehicle occurs; (2) direct connections  without change of vehicle can be looked up quickly. We precompute the respective data; in practice, this can be done in time linear in the network size, at the expense of a small fraction of non-optimal results. We have accelerated public transportation routing on Google Maps with a system based on our ideas. We report experimental results for three data sets of various kinds and sizes."
"Prediction of Advertiser Churn for Google AdWords.  Google AdWords has thousands of advertisers participating in auctions to show their advertisements. Google's business model has two goals: firrst, provide relevant information to users and second, provide advertising opportunities to advertisers to achieve their business needs. To better serve these two parties, it is important to find relevant information for
users and at the same time assist advertisers in advertising more efficiently and effectively. In this paper, we try to tackle this problem of better connecting users and advertisers from a customer relationship management point of view. More specifically, we try to retain more advertisers in AdWords by identifying and helping advertisers that are not successful in
using Google AdWords. In this work, we first propose a new definition of advertiser churn for AdWords advertisers; second we present a method to carefully select a homogeneous group of advertisers to use in understanding and predicting advertiser churn; and third we build a model to predict advertiser churn using machine learning algorithms."
"Mahout in Action.  A computer system that learns and adapts as it collects data is an extraordinarily interesting and powerful concept. With new technologies to capture, store, and process information, machine learning has moved from the academic edges of computer science to the middle of the mainstream. Mahout, an open source machine learning library, captures the core algorithms of recommendation systems, classification, and clustering in ready-to-use, scalable libraries. With Mahout, you can immediately apply the machine learning techniques that drive Amazon, Netflix, and other data-centric businesses to your own projects. Mahout in Action explores machine learning through Apache's scalable machine learning project, Mahout. Following real-world examples, it introduces practical use cases, and then illustrates how Mahout can be applied to solve them. It places particular focus on issues of scalability, and how to apply these techniques against large data sets using the Apache Hadoop framework. In this book, you'll use Mahout to dive into three practical applications of machine learning: Recommendations. Using group user history and preferences you can make accurate recommendations for individual users. This is an extremely powerful principle, because accurate recommendations are beneficial both to customers and vendors.
Clustering. Learn to automatically discover logical groupings with groups of data or data sets, such as documents or lists. This technique is especially useful to search and data mining applications.
Classification. Determining on the fly whether a thing fits a category based on its attributes and previous history can help instantaneously organize unstructured groups. For instance, you'll learn about filtering techniques that decide whether email messages should be considered ""spam.""
Mahout in Action is written primarily for developers who need to become better practitioners of machine learning techniques. It is also appropriate for researchers who understand the techniques and want to understand how to apply them effectively at scale. It assumes familiarity with Java, and some basic grounding in machine learning techniques, but no previous exposure to Mahout is necessary."
"Catching a Viral Video.  The sharing and re-sharing of videos on social sites, blogs e-mail, and other means has given rise to the phenomenon of viral videos ?? videos that become popular through internet sharing. In this paper we seek to better understand viral videos on YouTube by analyzing sharing and its relationship to video popularity using 1.5 million YouTube videos. The socialness of a video is quantified by classifying the
referrer sources for video views as social (e.g. an emailed link) or non-social (e.g. a link from related videos). By segmenting videos according to their fraction of social views, we find that viewership patterns of highly social videos is very different than less social videos. For example, the highly social videos rise to, and fall from, their peak popularity more quickly than less social videos. We also find that not all highly social videos become popular, and not all popular videos are highly social. And, despite their ability to generate large volumes of views over a short period of time, only 21% of the most popular videos (in terms of 30-day views) can be classified as viral. The observations made here lay the ground work for future work related to the creation of classification and predictive models for online videos."
"Cython: The Best of Both Worlds.  Cython is an extension to the Python language that allows explicit type declarations and is compiled directly to C. This addresses Python's large overhead for numerical loops and the difficulty of efficiently making use of existing C and Fortran code, which Cython code can interact with natively. The Cython language combines the speed of C with the power and simplicity of the Python language."
"Modeling similarity in the age of data.  The process of applying mathematics to the real world is undergoing a radical change through our ability to gather data at a massive scale. This is particularly true at Google, where we routinely process petabytes of human language, and interact with many millions of users. In this talk I describe some surprising realizations that arose from this data while trying to improve part of our search quality. It turns out that everything I thought I knew about similarity was wrong, and I should have been talking to psychologists."
"Practical Privacy Concerns in a Real World Browser.  Google Chrome has implemented a number of
HTML5 APIs, including the Geolocation
API and various storage APIs. In this paper we discuss some of our experiences on the
Google Chrome team in implementing these
APIs, as well as our thoughts around privacy
for new APIs we are considering implementing. Specifically, we discuss our ideas of how
providing access to things such as speech,
web cameras, and filesystems can be done in
ways that are understandable and in the natural flow of users."
"Query Language Modeling for Voice Search.  The paper presents an empirical exploration of google.com query stream language modeling. We describe the normalization of the typed query stream resulting in out-of-vocabulary (OoV) rates below 1% for a one million word vocabulary. We present a comprehensive set of experiments that guided the design decisions for a voice search service. In the process we re-discovered a less known interaction between Kneser-Ney smoothing and entropy pruning, and found empirical evidence that hints at non-stationarity of the query stream, as well as strong dependence on various English locales---USA, Britain and Australia."
"YouTube's Collaborative Annotations.  More and more YouTube videos no longer provide a passive viewing experience, but rather entice the viewer to interact with the video by clicking on objects with embedded links. These links are part of YouTube??s Annotations system, which enables content owners to add active overlays on top of their videos. YouTube Annotation overlays also enable adding dynamic speech bubbles and pop-ups which can function as an ever-changing layer of supplementary information and entertainment, augmenting the video experience. This paper addresses the question of whether the ability to add annotation overlays on a given video should be opened to the YouTube public. The basic dilemma in opening a video to collaborative annotations is derived from the tension between the benefits of collaboration and the risks of visual clutter and spam. We term the degree to which a video is open to external contributions as the collaboration spectrum, and describe several models that let content owners to explore this spectrum in order to find the optimal way to harness the power of the masses."
"Capsicum: practical capabilities for UNIX.  Capsicum is a lightweight operating system capabil- 
ity and sandbox framework planned for inclusion in 
FreeBSD 9. Capsicum extends, rather than replaces, 
UNIX APIs, providing new kernel primitives (sandboxed 
capability mode and capabilities) and a userspace sand- 
box API. These tools support compartmentalisation of 
monolithic UNIX applications into logical applications, 
an increasingly common goal supported poorly by dis- 
cretionary and mandatory access control. We demon- 
strate our approach by adapting core FreeBSD utilities 
and Google??s Chromium web browser to use Capsicum 
primitives, and compare the complexity and robustness 
of Capsicum with other sandboxing techniques."
"Availability in Globally Distributed Storage Systems.  Highly available cloud storage is often implemented with
complex, multi-tiered distributed systems built on top of clusters of
commodity servers and disk drives. Sophisticated management, load
balancing and recovery techniques are needed to achieve high
performance and availability amidst an abundance of failure sources
that include software, hardware, network connectivity, and power issues. While
there is a relative wealth of failure studies of individual components of
storage systems, such as disk drives, relatively little has been
reported so far on the overall availability behavior of large
cloud-based storage services. We characterize the availability properties of cloud
storage systems based on an extensive one year study of Google's
main storage infrastructure and present statistical models
that enable further insight into the impact of multiple
design choices, such as data placement and replication strategies.
With these models we compare data availability under a variety of
system parameters given the real patterns of failures observed in our fleet."
"Beyond Heuristics: Learning to Classify Vulnerabilities and Predict Exploits.  The security demands on modern system administration are enormous and getting worse. Chief among these demands, administrators must monitor the continual ongoing disclosure of software vulnerabilities that have the potential to compromise their systems in some way. Such vulnerabilities include buffer overflow errors, improperly validated inputs, and other unanticipated attack modalities. In 2008, over 7,400 new vulnerabilities were disclosed??well over 100 per week. While no enterprise is affected by all of these disclosures, administrators commonly face many outstanding vulnerabilities across the software systems they manage. A key question for systems administrators is which vulnerabilities to prioritize. From publicly available databases that document past vulnerabilities, we show how to train classifiers that predict whether and how soon a vulnerability is likely to be exploited. As input, our classifiers operate on high dimensional feature vectors that we extract from the text fields, time stamps, cross-references, and other entries in existing vulnerability disclosure reports. Compared to current industry-standard heuristics based on expert knowledge and static formulas, our classifiers predict much more accurately whether and how soon individual vulnerabilities are likely to be exploited."
"Efficient Topologies for Large-Scale Cluster Networks.  Increasing integrated-circuit pin bandwidth has motivated a corresponding increase in the degree or radix of interconnection networks and their routers. This paper describes the flattened butterfly, a cost-efficient topology for high-radix networks. On benign (load-balanced) traffic, the flattened butterfly approaches the cost/performance of a butterfly network and has roughly half the cost of a comparable performance Clos network. The advantage over the Clos is achieved by eliminating redundant hops when they are not needed for load balance. On adversarial traffic, the flattened butterfly matches the cost/performance of a folded-Clos network and provides an order of magnitude better performance than a conventional butterfly. In this case, global adaptive routing is used to switch the flattened butterfly from minimal to non-minimal routing ?? using redundant hops only when they are needed. Different routing algorithms are evaluated on the flattened butterfly and compared against alternative topologies. We also provide a detailed cost model for an interconnection network and compare the cost of the flattened butterfly to alternative topologies to show the cost advantages of the flattened butterfly."
"??Poetic?? Statistical Machine Translation: Rhyme and Meter.  As a prerequisite to translation of poetry, we
implement the ability to produce translations
with meter and rhyme for phrase-based MT,
examine whether the hypothesis space of such
a system is flexible enough to accommodate
such constraints, and investigate the impact of
such constraints on translation quality."
"On-Demand Language Model Interpolation for Mobile Speech Input.  Google offers several speech features on the Android mobile
operating system: search by voice, voice input to any text field, and an API for application developers. As a result, our speech recognition service must support a wide range of usage scenarios and speaking styles: relatively short search queries, addresses, business names, dictated SMS and e-mail messages, and a long tail of spoken input to any of the applications users may install. We present a method of on-demand language model interpolation in which contextual information about each utterance determines interpolation weights among a number of n-gram language models. On-demand interpolation results in an 11.2% relative reduction in WER compared to using a single language model to handle all traffic."
"Mining Arabic Business Reviews.  For languages with rich content over the web, business reviews are easily accessible via many known websites, e.g., Yelp.com. For languages with poor content over the web like Arabic, there are very few websites (we are actually aware  of only one that is indeed unpopular) that provide business  reviews. However, this does not mean that such reviews do not exist. They indeed exist unstructured in websites not originally intended for reviews, e.g., Forums and Blogs. Hence, there is a need to mine for those Arabic reviews from the web in order to provide them in the search results when a user searches for a business or a category of businesses.
In this paper, we show how to extract the business reviews scattered on the web written in the Arabic language. The mined reviews are analyzed to also provide their sentiments (positive, negative or neutral). This way, we provide our users the information they need about the local businesses  in the language they understand, and therefore provide a better search experience for the Middle East region, which mostly speaks Arabic."
"Building Transcribed Speech Corpora Quickly and Cheaply for Many Languages.  We present a system for quickly and cheaply building transcribed speech corpora containing utterances from many speakers in a variety of acoustic conditions. The system consists of a client application running on an Android mobile device with an intermittent Internet connection to a server. The client application collects demographic information about the speaker, fetches textual prompts from the server for the speaker to read, records the speaker??s voice, and uploads the audio and associated metadata to the server. The system has so far been used to collect over 3000 hours of transcribed audio in 17 languages around the world."
"SemWebVid - Making Video a First Class Semantic Web Citizen and a First Class Web Bourgeois.  SemWebVid is an online Ajax application that allows for the automatic generation of Resource Description Framework (RDF) video descriptions. These descriptions are based on two pillars: first, on a combination of user-generated metadata such as title, summary, and tags; and second, on closed captions which can be user-generated, or be auto-generated via speech recognition. The plaintext contents of both pillars are being analyzed using multiple Natural Language Processing (NLP) Web services in parallel whose results are then merged and where possible matched back to concepts in the sense of Linking Open Data (LOD). The final result is a deep-linkable RDF description of the video, and a ??scroll-along?? view of the video as an example of video visualization formats."
"Decision Tree State Clustering with Word and Syllable Features.  In large vocabulary continuous speech recognition, decision trees are widely used to cluster triphone states. In addition to commonly used phonetically based questions, others have proposed additional questions such as phone position within word or syllable. This paper examines using the word or syllable context itself as a feature in the decision tree, providing an elegant way of introducing word- or syllable-specific models into the system. Positive results are reported on two state-of-the-art systems: voicemail transcription and a search by voice tasks across a variety of acoustic model and training set sizes."
"Voice Search for Development.  In light of the serious problems with both illiteracy and information access in the developing world, there is a widespread belief that speech technology can play a significant role in improving the quality of life of developing-world citizens. We review the main reasons why this impact has not occurred to date, and propose that voice-search systems may be a useful tool in delivering on the original promise. The challenges that must be addressed to realize this vision are analyzed, and initial experimental results in developing voice search for two languages of South Africa (Zulu and Afrikaans) are summarized"
"Direct Construction of Compact Context-Dependency Transducers From Data.  This paper describes a new method for building compact
con-text-dependency transducers for finite-state transducer-based ASR decoders. Instead of the conventional phonetic decision-tree growing followed by FST compilation, this approach incorporates the phonetic context splitting directly into the transducer construction. The
objective function of the split optimization is augmented with a regularization term that measures the number of transducer states introduced by a split.  We give results on a large spoken-query task for various n-phone orders and other phonetic features that show this method can greatly reduce the size of the resulting context-dependency transducer with no significant impact on recognition accuracy. This permits using context sizes and features that might otherwise be unmanageable."
"Hardware Requirements for Optical Circuit Switched Data Center Networks.  Based on measurements of a prototype, we identify hardware requirements for improving the performance of hybrid electrical-packet-switched/optical-circuit-switched data center networks."
"Scalable I/O Event Handling for GHC.  We have developed a new, portable I/O event manager for the Glasgow Haskell Compiler (GHC) that scales to the needs of modern server applications. Our new code is transparently available to existing Haskell applications. Performance at lower concurrency levels is comparable with the existing implementation. We support millions of concurrent network connections, with millions of active timeouts, from a single multithreaded program, levels far beyond those achievable with the current I/O manager. In addition, we provide a public API to developers who need to create event-driven network applications."
"100GbE and Beyond for Warehouse Scale Computing.  As computation and storage continues to move from desktops to large internet services, computing platforms running such services are transforming into warehouse-scale computers. 100 Gigabit Ethernet and beyond will be instrumental in scaling the interconnection within and between these ubiquitous warehouse-scale computing infrastructures. In this paper, we describe the drivers for such interfaces and some methods of scaling Ethernet interfaces to speeds beyond 100GbE."
"Sparse coding of auditory features for machine hearing in interference.  A key problem in using the output of an auditory model as the input to a machine-learning system in a machine-hearing application is to find a good feature-extraction layer.  For systems such as PAMIR (passive-aggressive model for image retrieval) that work well with a large sparse feature vector, a conversion from auditory images to sparse features is needed.  For audio-file ranking and retrieval from text queries, based on stabilized auditory images, we took a multi-scale approach, using vector quantization to choose one sparse feature in each of many overlapping regions of different scales, with the hope that in some regions the features for a sound would be stable even when other interfering sounds were present and affecting other regions.  We recently extended our testing of this approach using sound mixtures, and found that the sparse-coded auditory-image features degrade less in interference than vector-quantized MFCC sparse features do.  This initial success suggests that our hope of robustness in interference may indeed be realizable, via the general idea of sparse features that are localized in a domain where signal components tend to be localized or stable."
"History and Future of Auditory Filter Models.  Auditory filter models have a history of over a hundred years, with explicit 
bio-mimetic inspiration at many stages along the way.  From passive analogue electric 
delay line models, through digital filter models, active analogue VLSI models, and 
abstract filter shape models, these filters have both represented and driven the state of 
progress in auditory research. Today, we are able to represent a wide range of linear 
and nonlinear aspects of the psychophysics and physiology of hearing with a rather 
simple and elegant set of circuits or computations that have a clear connection to 
underlying hydrodynamics and with parameters calibrated to human performance data. 
A key part of the progress in getting to this stage has been the experimental clarification 
of the nature of cochlear nonlinearities, and the modelling work to map these experimental 
results into the domain of circuits and systems. No matter how these models are built 
into machine-hearing systems, their bio-mimetic roots will remain key to 
their performance. In this paper we review some of these models, explain 
their advantages and disadvantages and present possible ways of implementing them. 
As an example, a continuous-time analogue CMOS implementation of the 
One Zero Gammatone Filter (OZGF) is presented together with its automatic 
gain control that models its level-dependent nonlinear behaviour."
The Cray XT4 and Seastar 3-D Torus Interconnect.  The Cray XT4 system is a distributed memory multiprocessor combining an aggressive superscalar processor (AMD64) with a bandwidth-rich 3-D torus interconnection network that scales up to 32K processing nodes. This chapter provides an overview of the Cray XT4 system architecture and a detailed discussion of its interconnection network.
"Probabilistic Distance-based Arbitration: Providing Equality of Service for Many-core CMPs.  Emerging many-core chip multiprocessors will integrate dozens of small
processing cores with an on-chip interconnect consisting of point-to-point links.  The interconnect enables the processing cores to not onl communicate, but to share common resources such as main memory resources and I/O controllers.  In this work, we propose an arbitration scheme to enable equality of service (EoS) in access to a chip's shared resources.  That is, we seek to remove any bias in a core's access to a shared resource based on its location within the CMP. We propose using probabilistic arbitration combined  with distance-based weights to achieve EoSand overcome the limitation of conventional round-robin arbiter. We describe how nonlinear weights need to be used with probabilistic arbiters and propose three different arbitration weight metrics -- fixed weight, constantly increasing weight, and variably increasing weight. By only modifying the arbitration of an on-chip router, we do not require any additional buffers or virtual channels and create a simple, low-cost mechanism for achieving EoS. We evaluate our arbitration scheme across a wide range of traffic patterns. In addition to providing EoS, the proposed arbitration has additional benefits which include providing quality-of-service features (such as differentiated service) and providing fairness in terms of both throughput and latency that approaches the global fairness achieved with age-base arbitration -- thus, providing a more stable network 
by achieving high sustained throughput beyond saturation."
"Online Learning in the Manifold of Low-Rank Matrices.  When learning models that are represented in matrix forms, enforcing a low-rank constraint can dramatically improve the memory and run time complexity, while providing a natural regularization of the model. However, naive approaches for minimizing functions over the set of low-rank matrices are either prohibitively time consuming (repeated singular value decomposition of the matrix) or numerically unstable (optimizing a factored representation of the low rank matrix)."
"Google Street View: Capturing the World at Street Level.  Street View serves millions of Google users daily with panoramic imagery captured in hundreds of cities in 20 countries across four continents. A team of Google researchers describes the technical challenges involved in capturing, processing, and serving street-level imagery on a global scale."
"Confucius and Its Intelligent Disciples: Integrating Social with Search.  Q&amp;A sites continue to flourish as a large number of users rely on them as useful substitutes for incomplete or missing search results. In this paper, we present our experience with developing Confucius, a Google Q&amp;A service launched in 21 countries and four languages by the end of 2009. Confucius employs six data mining subroutines to harness synergy between web search and social networks. We present these subroutines?? design goals, algorithms, and their effects on service quality. We also describe techniques for and experience with scaling the subroutines to mine massive data sets."
"Learning improved linear transforms for speech recognition.  This paper explores a large margin approach to learning a linear transform for
dimensionality reduction. The method assumes a trained Gaussian mixture model
for the each class to be discriminated and trains a linear transform with
respect to the model using stochastic gradient descent. Results are presented
showing improvements in state classification for individual frames and reduced
word error rate in a large vocabulary speech recognition problem after maximum likelihood training and boosted maximum mutual information training."
"The Go Frontend for GCC.  A description of the Go language frontend for gcc. This is a new frontend which is a complete implementation of the new Go programming language. The frontend is currently some 50,000 lines of C++ code, and uses its own IR which is then converted to GENERIC. I describe the structure of the frontend and the IR, issues that arise when compiling the Go language, and issues with hooking up any frontend to the gcc middle-end."
"Access and Analyze Broadband Measurements Collected using M-Lab.  Measurement Lab (M-Lab) is an open, distributed server platform for researchers, to deploy Internet measurement tools. Everybody can use M-Lab's tools to measure their own broadband connection performance. The M-Lab servers collect logs of all the users' tests and make them publicly available. As of July 2010, users have run millions of tests that have generated many terabytes of measurement data. This talk will present the public repositories of M-Lab data and will explain how to analyze M-Lab data using Google's BigQuery. BigQuery stores M-Lab's measurements logs in a table with more than 60 billions of rows. It takes less than 1 minute to run a query against the whole dataset."
"Gesture Search: A Tool for Fast Mobile Data Access.  Modern mobile phones can store a large amount of data, such as contacts, applications and music. However, it is difficult to access specific data items via existing mobile user interfaces. In this paper, we present Gesture Search, a tool that allows a user to quickly access various data items on a mobile phone by drawing gestures on its touch screen. Gesture Search contributes a unique way of combining gesture-based interaction and search for fast mobile data access. It also demonstrates a novel approach for coupling gestures with standard GUI interaction. A real world deployment with mobile phone users showed that Gesture Search enabled fast, easy access to mobile data in their day-to-day lives. Gesture Search has been released to public and is currently in use by hundreds of thousands of mobile users. It was rated positively by users, with a mean of 4.5 out of 5 for over 5000 ratings."
"Safe ICF: Pointer Safe and Unwinding Aware Identical Code Folding in Gold.  We have found that large C++ applications and shared libraries tend to have many functions whose code is identical with another function. As much as 10% of the code could theoretically be eliminated by merging such identical functions into a single copy. This optimization, Identical Code Folding (ICF), has been implemented in the gold  linker. At link time, ICF detects functions with identical object code and merges them into a single copy. ICF can be unsafe, however, as it can change the run-time behaviour of code that relies on each function having a unique address. To address this, ICF can be used in a safe mode where it identifies and folds functions whose addresses are guaranteed not to have been used in comparison operations. Further, profiling and debugging binaries with merged functions can be confusing, as the PC values of merged functions cannot be always disambiguated to point to the correct function. To address this, we propose a new call table format for the DWARF debugging information to allow tools like the debugger and profiler to disambiguate PC values of merged functions correctly by examining the call chain. Detailed experiments on the x86 platform show that ICF can reduce the text size of a selection of Google binaries, whose average text size is 64 MB, by about 6%. Also, the code size savings of ICF with the safe option is almost as good as the code savings obtained without the safe option. Further, experiments also show that the run-time performance of the optimized binaries on the x86 platform does not change."
Challenges in Automatic Speech Recognition.  ISCA Student panel presentation slides
"The Politics of Search: A Decade Retrospective..  In ??Shaping theWeb:Why the Politics of Search Engines Matters,??
Introna and Nissenbaum (2000) introduced scholars to the
political, as well as technical, issues central to the development of
online search engines. Since that time, scholars have critically evaluated
the role that search engines play in structuring the scope of
online information access for the rest of society, with an emphasis
on the implications for a democratic and diverseWeb. This article
describes the thought behind search engine regulation, online diversity,
and information bias, and it places these issues within the
context of the technical and societal changes that have occurred
in the online search industry. The author assesses which of the
initial concerns expressed about online search engines remain relevant
today and discusses how technical changes demand a new
approach to measuring online diversity and democracy. The author
concludes with a proposal to direct the research and thought
in online search going forward."
"Media agenda setting and online search traffic: Influences of online and traditional media.  This paper addresses the patterns of influence between the news media and the public, by specifically targeting breaking stories, or shocks, to a news system. Specifically, we assess media agenda setting and selective exposure by looking at the relative public attention spans to hard and soft news (as measured by query volume), in comparison with the volume of news coverage (in print, broadcast, and Web content) for these selected news events. We measure the dynamic distribution of issue coverage in the news media, and how this volume of coverage ultimately influences online search traffic. In order to assess sustained interest in a given topic, distributions of query volume and news coverage were fit with Gamma distributions of appropriate parameters. Findings indicate that there are significant differences in the public attention spans for hard and soft news issues, particularly relative to what news coverage might predict. Soft news events produced a slower rate of decline in query volume, matching the slow tapering off of issue coverage found in Web content. Conversely, for hard, substantive news issues, query volume drops off quite quickly, more closely paralleling the distribution of coverage in broadcast news."
"RDRP: Reward-Driven Request Prioritization for e-Commerce Web Sites.  Meeting client Quality-of-Service (QoS) expectations proves to be a difficult task for the providers of e-Commerce services, especially when web servers experience overload conditions, which cause increased response times and request rejections, leading to user frustration, lowered usage of the service and reduced revenues. In this paper, we propose a server-side request scheduling mechanism that addresses these problems. Our Reward-Driven Request Prioritization (RDRP) algorithm gives higher execution priority to client web sessions that are likely to bring more service profit (or any other application-specific reward). The method works by predicting future session structure by comparing its requests seen so far with aggregated information about recent client behavior, and using these predictions to preferentially allocate web server resources. Our experiments using the TPC-W benchmark application with an implementation of the RDRP techniques in the JBoss web application server show that RDRP can significantly boost profit attained by the service, while providing better QoS to clients that bring more profit."
"Improved Consistent Sampling, Weighted Minhash and L1 Sketching.  We propose a new Consistent Weighted Sampling method, where the probability of
drawing identical samples for a pair of inputs is equal to their Jaccard
similarity. Our method takes deterministic constant time per non-zero weight,
improving on the best previous approach which takes expected constant time. The
samples can be used as Weighted Minhash for efficient retrieval and compression
(sketching) under Jaccard or L1 metric. A method is presented for using simple
data statistics to reduce the running time of hash computation by two orders of
magnitude.  We compare our method with the random projection method and show
that it has better characteristics for retrieval under L1.  We present a novel
method of mapping hashes to short bit-strings, apply it to Weighted Minhash, and
achieve more accurate distance estimates from sketches than existing methods, as
long as the inputs are sufficiently distinct.  We show how to choose the optimal
number of bits per hash for sketching, and demonstrate experimental results
which agree with the theoretical analysis."
"Field verification of 40G DPSK upgrade in a legacy 10G network.  We report verification of 1,200 km field upgrade of 10 G NRZ wavelengths with 40 G DPSK channels. Non symmetric dispersion map results in pronounced intra-channel nonlinear effect, which could be significantly reduced by dispersion pre-compensation"
"FTTH look ahead ??? technologies &amp; architectures.  We review the trade-offs, challenges and potentials of various FTTH architecture options."
"Efficient Spectral Neighborhood Blocking for Entity Resolution.  In many telecom and web applications, there is a
need to identify whether data objects in the same source or
different sources represent the same entity in the real world. This problem arises for subscribers in multiple services, customers in supply chain management, and users in social networks when there lacks a unique identifier across multiple data sources to represent a real-world entity. Entity resolution is to identify and discover objects in the data sets that refer to the same entity in the real world.
We investigate the entity resolution problem for large data sets where efficient and scalable solutions are needed. We propose a novel unsupervised blocking algorithm, namely SPectrAl Neighborhood (SPAN), which constructs a fast bipartition tree for the records based on spectral clustering such that real entities can be identified accurately by neighborhood records in the tree. There
are two major novel aspects in our approach: 1) We develop a fast algorithm that performs spectral clustering without computing pairwise similarities explicitly, which dramatically improves the scalability of the standard spectral clustering algorithm; 2) We utilize a stopping criterion specified by Newman-Girvan modularity in the bipartition process. Our experimental results with both synthetic and real-world data demonstrate that SPAN is robust and outperforms other blocking algorithms in terms of
accuracy while it is efficient and scalable to deal with large data sets."
"Optimizing Utilization of Resource Pools in Web Application Servers.  Among the web application server resources, most critical for its performance are those that are held exclusively by a service request for the duration of its execution (or some significant part of it). Such exclusively-held server resources become performance bottleneck points, with failures to obtain such a resource constituting a major portion of request rejections under server overload conditions. In this paper, we propose a methodology that computes the optimal pool sizes for two such critical resources: web server threads and database connections. Our methodology uses information about incoming request flow and about fine-grained server resource utilization by service requests of different types, obtained through offline and online request profiling. In our methodology, we advocate (and show its benefits) the use of a database connection pooling mechanism that caches database connections for the duration of a service request execution (so-called request-wide database connection caching). We evaluate our methodology by testing it on the TPC-W web application. Our method is able to accurately compute the optimal number of server threads and database connections, and the value of sustainable request throughput computed by the method always lies within a 5% margin of the actual value determined experimentally."
"Stability Bounds for Stationary $\phi$-mixing and $\beta$-mixing Processes.  Most generalization bounds in learning theory are based on some measure of the complexity of the hypothesis class used, independently of any algorithm. In contrast, the notion of algorithmic stability can be used to derive tight generalization bounds that are tailored to specific learning algorithms by exploiting their particular properties. However, as in much of learning theory, existing stability analyses and bounds apply only in the scenario where the samples are independently and identically distributed. In many machine learning applications, however, this assumption does not hold. The observations received by the learning algorithm often have some inherent temporal dependence."
"Managing Crowdsourced Human Computation.  The tutorial covers an emerging topic of wide
interest: Crowdsourcing. Specifically, we cover areas of crowdsourcing related to managing structured and unstructured data in a web-related content. Many researchers and practitioners today see the great opportunity that becomes available through easily-available crowdsourcing platforms. However, most newcomers face the same questions: How can we manage the (noisy) crowds to generate high quality output? How to estimate the quality of the contributors? How can we best structure the tasks? How can we get results in small amounts of time and minimizing the necessary resources? How to setup the incentives? How should such crowdsourcing markets be setup? Their presented material will cover topics from a variety of fields, including computer science, statistics,
economics, and psychology. Furthermore, the material will include real-life examples and case studies from years of experience in running and managing crowdsourcing applications in business settings. The tutorial presenters have an extensive academic and
systems building experience and will provide the audience with data sets that can be used for hands-on tasks."
"MapReduce/Bigtable for Distributed Optimization.  For large data it can be very time consuming to run gradient based optimizat ion,for example to minimize the log-likelihood for maximum entropy models.Distributed methods are therefore appealing and a number of distributed gradientoptimization strategies have been proposed including: distributed gradient, asynchronousupdates, and iterative parameter mixtures. In this paper, we evaluatethese various strategies with regards to their accuracy and speed over MapReduce/Bigtable and discuss the techniques needed for high performance."
"Auditory Sparse Coding.  The concept of sparsity has attracted considerable interest in the
field of machine learning in the past few years.  Sparse feature
vectors contain mostly values of zero and one or a few non-zero
values.  Although these feature vectors can be classified by
traditional machine learning algorithms, such as SVM, there are various
recently-developed algorithms that explicitly take advantage of
the sparse nature of the data, leading to massive speedups in time, as
well as improved performance.  Some fields that have benefited from
the use of sparse algorithms are finance, bioinformatics, text mining, 
and image classification.
Because of their speed, these algorithms perform well on very large
collections of data; large collections are becoming 
increasingly relevant given the huge amounts of data collected and warehoused 
by Internet businesses.
We discuss the application of sparse feature vectors
in the field of audio analysis, and specifically their use in conjunction with 
preprocessing systems that model the human auditory system. We present
results that demonstrate the applicability of the combination of 
auditory-based processing and sparse coding to content-based audio analysis tasks: a search task in which 
ranked lists of sound effects are retrieved from text queries, and a music 
information retrieval (MIR) task dealing with the classification of music into 
genres."
"Practical Large-Scale Latency Estimation.  We present the implementation of a large-scale latency estimation system based on GNP and incorporated into the Google content delivery network. Our implementation does not rely on active participation of Web clients, and carefully controls the overhead incurred by latency measurements using a scalable centralized scheduler. It also requires only a small number of CDN modifications, which makes it attractive for any CDN interested in large-scale latency estimation. We investigate the issue of coordinate stability over time and show that coordinates drift away from their initial values with time, so that 25% of node coordinates become inaccurate by more than 33 milliseconds after one week. However, daily recomputations make 75% of the coordinates stay within 6 milliseconds of their initial values. Furthermore, we demonstrate that using coordinates to decide on client-to-replica redirection leads to selecting replicas closest in term of measured latency in 86% of all cases. In another 10% of all cases, clients are redirected to replicas offering latencies that are at most two times longer than optimal. Finally, collecting a huge volume of latency data and using clustering techniques enable us to estimate latencies between globally distributed Internet hosts that have not participated in our measurements at all. The results are sufficiently promising that Google may offer a public interface to the latency estimates in the future."
"Cascades of two-pole???two-zero asymmetric resonators are good models of peripheral auditory function.  A cascade of two-pole??two-zero filter stages is a good model of the auditory periphery in two distinct ways. First, in the form of the pole??zero filter cascade, it acts as an auditory filter model that provides an excellent fit to data on human detection of tones in masking noise, with fewer fitting parameters than previously reported filter models such as the roex and gammachirp models. Second, when extended to the form of the cascade of asymmetric resonators with fast-acting compression, it serves as an efficient front-end filterbank for machine-hearing applications, including dynamic nonlinear effects such as fast wide-dynamic-range compression. In their underlying linear approximations, these filters are described by their poles and zeros, that is, by rational transfer functions, which makes them simple to implement in analog or digital domains. Other advantages in these models derive from the close connection of the filter-cascade architecture to wave propagation in the cochlea. These models also reflect the automatic-gain-control function of the auditory system and can maintain approximately constant impulse-response zero-crossing times as the level-dependent parameters change.
<em>Copyright (2011) Acoustical Society of America. This article may be downloaded for personal use only. Any other use requires prior permission of the author and the Acoustical Society of America.  The article appeared in J. Acoust. Soc. Am. vol. 130 and may be found via <a href=""http://asadl.org/jasa/resource/1/jasman/v130/i6/p3893_s1"">http://asadl.org/jasa/resource/1/jasman/v130/i6/p3893_s1</a>.</em>"
"Experience report: Haskell as a reagent: results and observations on the use of Haskell in a python project.  In system administration, the languages of choice for solving automation tasks are scripting languages, owing to their flexibility, extensive library support and quick development cycle. Functional programming is more likely to be found in software development teams and the academic world. This separation means that system administrators cannot use the most effective tool for a given problem; in an ideal world, we should be able to mix and match different languages, based on the problem at hand. This experience report details our initial introduction and use of Haskell in a mature, medium size project implemented in Python. We also analyse the interaction between the two languages, and show how Haskell has excelled at solving a particular type of real-world problems."
"Megastore: Providing Scalable, Highly Available Storage for Interactive Services.  Megastore is a storage system developed to meet the requirements of today's interactive online services. Megastore blends the scalability of a NoSQL datastore with the convenience of a traditional RDBMS in a novel way, and provides both strong consistency guarantees and high availability. We provide fully serializable ACID semantics within fine-grained partitions of data. This partitioning allows us to synchronously replicate each write across a wide area network with reasonable latency and support seamless failover between datacenters. This paper describes Megastore's semantics and replication algorithm. It also describes our experience supporting a wide range of Google production services built with Megastore."
"Yield Optimization of Display Advertising with Ad Exchange.  In light of the growing market of Ad Exchanges for the real-time sale of advertising slots, publishers face new challenges in choosing between the allocation of contract-based reservation ads and spot market ads. In this setting, the publisher should take into account the tradeoff between short-term revenue from an Ad Exchange and quality of allocating reservation ads. In this paper, we formalize this combined optimization problem as a stochastic control problem and derive an efficient policy for online ad allocation in settings with general joint distribution over placement quality and exchange bids. We prove asymptotic optimality of this policy in terms of any trade-off between quality of delivered reservation ads and revenue from the exchange, and provide a rigorous bound for its convergence rate to the optimal policy. We also give experimental results on data derived from real publisher inventory, showing that our policy can achieve any pareto-optimal point on the quality vs. revenue curve. Finally, we study a parametric training-based algorithm in which instead of learning the dual variables from a sample data (as is done in non-parametric training-based algorithms), we learn the parameters of the distribution and construct those dual variables from the learned parameter values. We compare parametric and non-parametric ways to estimate from data both analytically and experimentally in the special case without the ad exchange, and show that though both methods converge to the optimal policy as the sample size grows, our parametric method converges faster, and thus performs better on smaller samples."
"Limits on the Application of Frequency-based Language Models to OCR.  Although large language models are used in speech recognition and machine translation applications, OCR systems are ??far behind?? in their use of language models. The reason for this is not the laggardness of the OCR community, but the fact that, at high accuracies, a frequency-based language model can do more damage than good, unless carefully applied. This paper presents an analysis of this discrepancy with the help of the Google Books n-gram Corpus, and concludes that noisy-channel models that closely model the underlying classifier and segmentation errors are required."
"Kernelized Structural SVM Learning for Supervised Object Segmentation.  Object segmentation needs to be driven by top-down knowledge to produce semantically meaningful results. In this paper, we propose a supervised segmentation approach that tightly integrates object-level top down information with low-level image cues. The information from the two levels is fused under a kernelized structural SVM learning framework. We defined a novel nonlinear kernel for comparing two image-segmentation masks. This kernel combines four different kernels: the object similarity kernel, the object shape kernel, the per-image color distribution kernel, and the global color distribution kernel. Our experiments show that the structured SVM algorithm finds bad segmentations of the training examples given the current scoring function and punishes these bad segmentations to lower scores than the example (good) segmentations. The result is a segmentation algorithm that not only knows what good segmentations are, but also learns potential segmentation mistakes and tries to avoid them. Our proposed approach can obtain comparable performance to other state-of-the-art top-down driven segmentation approaches yet is flexible enough to be applied to widely different domains."
Universal Gigabit Optical Access.  We review the imperatives on the optical communication technology industry to realize universal ultra high speed access to the world??s information.
"Follow-the-Regularized-Leader and Mirror Descent: Equivalence Theorems and L1 Regularization.  We prove that many mirror descent algorithms for online convex optimization (such as online gradient descent) have an equivalent interpretation as follow-the-regularized-leader (FTRL) algorithms.  This observation makes the relationships between many commonly used algorithms explicit, and provides theoretical insight on previous experimental observations.  In particular, even though the FOBOS composite mirror descent algorithm handles L1 regularization explicitly, it has been observed that the FTRL-style Regularized Dual Averaging (RDA) algorithm is even more effective at producing sparsity.  Our results demonstrate that the key difference between these algorithms is how they handle the cumulative L1 penalty.  While FOBOS handles the $L_1$ term exactly on any given update, we show that it is effectively using subgradient approximations to the L1 penalty from previous rounds, leading to less sparsity than RDA, which handles the cumulative penalty in closed form.  The FTRL-Proximal algorithm, which we introduce, can be seen as a hybrid of these two algorithms, and significantly outperforms both on a large, real-world dataset."
"Adapting Online Advertising Techniques to Television.  The availability of precise data on TV ad consumption fundamentally changes this advertising medium, and allows many techniques developed for analyzing online ads to be adapted for TV. This chapter looks in particular at how results from the emerging field of online ad quality analysis can now be applied to TV."
"Applications and Extensions of Alloy: Past, Present, and Future.  Alloy is a declarative language for lightweight modelling and analysis of software. The core of the language is based on first-order relational logic, which offers an attractive balance between analysability and expressiveness. The logic is expressive enough to capture the intricacies of real systems, but is also simple enough to support fully automated analysis with the Alloy Analyzer. The Analyzer is built on a SAT-based constraint solver and provides automated simulation, checking and debugging of Alloy specifications. Because of its automated analysis and expressive logic, Alloy has been applied in a wide variety of domains. These applications have motivated a number of extensions both to the Alloy language and to its SAT-based analysis. This paper provides an overview of Alloy in the context of its three largest application domains, lightweight modelling, bounded code verification and test-case generation, and three recent application-driven extensions, an imperative extension to the language, a compiler to executable code and a proof-capable analyser based on SMT."
"Auto-Directed Video Stabilization with Robust L1 Optimal Camera Paths.  We present a novel algorithm for automatically applying constrainable, L1-optimal camera paths to generate stabilized videos by removing undesired motions. Our goal is to compute camera paths that are composed of constant, linear and parabolic segments mimicking the camera motions employed by professional cinematographers.  To this end, our algorithm is based on a linear programming framework to minimize the first, second, and third derivatives of the resulting camera path.  Our method allows for video stabilization beyond the conventional filtering of camera paths that only suppresses high frequency jitter. 
We incorporate additional constraints on the path of the camera directly in our algorithm, allowing for stabilized and retargeted videos. Our approach accomplishes this without the need of user interaction or costly 3D reconstruction of the scene, and works as a post-process for videos from any camera or from an online source."
"Showing Relevant Ads via Lipschitz Context Multi-Armed Bandits.  We study contextual multi-armed bandit problems where the context comes from a metric space and the payoff satisfies a Lipschitz condition with respect to the metric. Abstractly, a contextual multi-armed bandit problem models a situation where, in a sequence of independent trials, an online algorithm chooses, based on a given context (side information), an action from a set of possible actions so as to maximize the total payoff of the chosen actions. The payoff depends on both the action chosen and the context. In contrast, context-free multi-armed bandit problems, a focus of much previous research, model situations where no side information is available and the payoff depends only on the action chosen. Our problem is motivated by sponsored web search, where the task is to display ads to a user of an Internet search engine based on her search query so as to maximize the click-through rate (CTR) of the ads displayed. We cast this problem as a contextual multi-armed bandit problem where queries and ads form metric spaces and the payoff function is Lipschitz with respect to both the metrics. For any $\epsilon &gt; 0$ we present an algorithm with regret $O(T^{\frac{a+b+1}{a+b+2} + \epsilon})$ where $a,b$ are the covering dimensions of the query space and the ad space respectively. We prove a lower bound $\Omega(T^{\frac{\tilde{a}+\tilde{b}+1}{\tilde{a}+\tilde{b}+2} \epsilon})$ for the regret of any algorithm where $\tilde{a}, \tilde{b}$ are packing dimensions of the query spaces and the ad space respectively. For finite spaces or convex bounded subsets of Euclidean spaces, this gives an almost matching upper and lower bound."
"Indexing the World Wide Web: The Journey So Far.  In this chapter, we describe the key indexing components of today??s web search engines. As the World Wide Web has grown, the systems and methods for indexing have changed significantly.  We present the data structures used, the features extracted, the infrastructure needed, and the options available for designing a brand new search engine.  We highlight techniques that improve relevance of results, discuss trade-offs to best utilize machine resources, and cover distributed processing concepts in this context. In 
particular, we delve into the topics of indexing phrases instead of terms, storage in memory vs. on disk, and data partitioning. We will finish with some thoughts on information organization for the newly emerging data-forms."
"The Emerging Optical Data Center.  We review the architecture of modern datacenter networks, as well as their scaling challenges; then present high-level requirements for deploying optical technologies in datacenters, particularly focusing on optical circuit switching and WDM transceivers."
"Experiences Scaling Use of Google's Sawzall.  Sawzall is a procedural language developed at Google for parallel analysis of very large data sets. Given a log sharded into many separate files, its companion tool named saw runs Sawzall interpreters to perform an analysis. Hundreds of Googlers have written thousands of saw+Sawzall programs, which form a significant minority of Google's daily data processing. Short programs grew to become longer programs, which were not easily shared nor tested. In other words, scaling naively written Sawzall led to unmaintainable programs. The simple idea of writing programs functionally, not iteratively, yielded shareable, testable programs. The functions reflect fundamental map reduction concepts: mapping, reducing, and iterating. Each can be easily tested. This case study demonstrates that developers of parallel processing systems should also simultaneously develop ways for users to decompose code into sharable pieces that reflect fundamental underlying concepts. As importantly, they must develop ways for users to easily write tests of their code."
"Power Management of Online Data-Intensive Services.  Much of the success of the Internet services model can be attributed to the popularity of a class of workloads that we call Online Data-Intensive (OLDI) services. These workloads perform significant computing over massive data sets per user request but, unlike their offline counterparts (such as MapReduce computations), they require responsiveness in the sub-second time scale at high request rates. Large search products, online advertising, and machine translation are examples of workloads in this class. Although the load in OLDI services can vary widely during the day, their energy consumption sees little variance due to the lack of energy proportionality of the underlying machinery. The scale and latency sensitivity of OLDI workloads also make them a challenging target for power management techniques. We investigate what, if anything, can be done to make OLDI systems more energy-proportional. Specifically, we evaluate the applicability of active and idle low-power modes to reduce the power consumed by the primary server components (processor, memory, and disk), while maintaining tight response time constraints, particularly on 95th-percentile latency. Using Web search as a representative example of this workload class, we first characterize a production Web search workload at cluster-wide scale. We provide a fine-grain characterization and expose the opportunity for power savings using low-power modes of each primary server component. Second, we develop and validate a performance model to evaluate the impact of processor- and memory-based low-power modes on the search latency distribution and consider the benefit of current and foreseeable low-power modes. Our results highlight the challenges of power management for this class of workloads. In contrast to other server workloads, for which idle low-power modes have shown great promise, for OLDI workloads we find that energy-proportionality with acceptable query latency can only be achieved using coordinated, full-system active low-power modes."
"Space-Filling Trees: A New Perspective on Incremental Search for Motion Planning.  This paper introduces space-filling trees and analyzes them in the context of sampling-based motion planning. Space-filling trees are analogous to space-filling curves, but have a branching, tree-like structure, and are defined by an incremental process that results in a tree for which every point in the space has a finite-length path that converges to it. In contrast to space-filling curves, individual paths in the tree are short, allowing any part of the space to be quickly reached from the root. We compare some basic constructions of space-filling trees to Rapidly-exploring Random Trees (RRTs), which underlie a number of popular algorithms used for sampling-based motion planning. We characterize several key tree properties related to path quality and the overall efficiency of exploration and conclude with a number of open mathematical questions."
"From Basecamp to Summit: Scaling Field Research Across 9 Locations.  In this case study we discuss the mechanics of running a complex field research project within one week: 32 field visits, 4 countries, 9 locations, 10+ researchers, 30+ observers. We outline the goals that lead to this  project plan, and the tools and processes we developed  to succeed under the constraints given. We discuss in particular (1) the role of ongoing in-field analysis and data sharing, (2) the role of basecamp as a centralized mission control center and real-time analysis hub, and (3) the added value of running the study and initial analysis in such a compressed time frame. We close with a reflection on the strengths and weaknesses of this approach, as well as ideas for future improvements."
"Visual and Semantic Similarity in ImageNet.  Many computer vision approaches take for granted positive answers to questions such as ??Are semantic categories visually separable??? and ??Is visual similarity correlated to semantic similarity??? In this paper, we study experimentally whether these assumptions hold and show parallels to questions investigated in cognitive science about the human visual system.  The insights gained from our analysis enable building a novel distance function between images assessing whether they are from the same basic-level category.  This function goes beyond direct visual distance as it also exploits semantic similarity measured through ImageNet.  We demonstrate experimentally that it outperforms purely visual distances."
"High Performance Datacenter Networks: Architectures, Algorithms, and Opportunities.  Datacenter networks provide the communication substrate for large parallel computer systems that form the ecosystem for high performance computing (HPC) systems and modern Internet applications. The design of new datacenter networks is motivated by an array of applications ranging from communication intensive climatology, complex material simulations and molecular dynamics to such Internet applications as Web search, language translation, collaborative Internet applications, streaming video and voice-over-IP. For both Supercomputing and Cloud Computing the network enables distributed applications to communicate and interoperate in an orchestrated and efficient way. This book describes the design and engineering tradeoffs of datacenter networks. It describes interconnection networks from topology and network architecture to routing algorithms, and presents opportunities for taking advantage of the emerging technology trends that are influencing router microarchitecture. With the emergence of ""many-core"" processor chips, it is evident that we will also need ""many-port"" routing chips to provide a bandwidth-rich network to avoid the performance limiting effects of Amdahl's Law. We provide an overview of conventional topologies and their routing algorithms and show how technology, signaling rates and cost-effective optics are motivating new network topologies that scale up to millions of hosts. The book also provides detailed case studies of two high performance parallel computer systems and their networks."
"Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections.  We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language. Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable for a wide array of resource-poor languages. We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as constraints in an unsupervised model. Across six European languages, our approach results in an average absolute improvement of 9.7\% over the state-of-the-art baseline, and 17.0\% over vanilla hidden Markov models induced with EM."
"Language Modeling for Automatic Speech Recognition Meets the Web: Google Search by Voice.  The talk presents key aspects faced when building language models (LM) for the google.com query stream, and their use for automatic speech recognition (ASR). Distributed LM tools enable us to handle a huge amount of data, and experiment with LMs that are two orders of magnitude larger than usual. An empirical exploration of the problem led us to re-discovering a less known interaction between Kneser-Ney smoothing and entropy pruning, possible non-stationarity of the query stream, as well as strong dependence on various English locales---USA, Britain and Australia. LM compression techniques allowed us to use one billion n-gram LMs in the first pass of an ASR system built on FST technology, and evaluate empirically whether a two-pass system architecture has any losses over one pass."
"MAO - an Extensible Micro-Architectural Optimizer.  Performance matters, and so does repeatability and predictability. Today's processors' micro-architectures have become so complex as to now contain many undocumented, not understood, and even puzzling performance cliffs.  Small changes in the instruction stream, such as the insertion of a single NOP instruction, can lead to significant performance deltas, with the effect of exposing compiler and performance optimization efforts to perceived unwanted randomness. This paper presents MAO, an extensible micro-architectural assembly to assembly optimizer, which seeks to address this problem for x86/64 processors. In essence, MAO is a thin wrapper around a common open source assembler infrastructure. It offers basic operations, such as creation or modification of instructions, simple data-flow analysis, and advanced infra-structure, such as loop recognition, and a repeated relaxation algorithm to compute instruction addresses and lengths.  This infrastructure enables a plethora of passes for pattern matching, alignment specific optimizations, peep-holes, experiments (such as random insertion of NOPs), and fast prototyping of more sophisticated optimizations.  MAO can be integrated into any compiler that emits assembly code, or can be used standalone. MAO can be used to discover micro-architectural details semi-automatically. Initial performance results are encouraging."
"Language-independent Compound Splitting with Morphological Operations.  Translating compounds is an important problem in machine translation. Since many compounds have not been observed during training, they pose a challenge for translation systems. Previous decompounding methods have
often been restricted to a small set of languages as they cannot deal with more complex compound forming processes. We present a novel and unsupervised method to learn the
compound parts and morphological operations needed to split compounds into their compound parts. The method uses a bilingual corpus to learn the morphological operations
required to split a compound into its parts. Furthermore, monolingual corpora are used to learn and filter the set of compound part candidates. We evaluate our method within a machine translation task and show significant improvements for various languages to show the versatility of the approach."
"Multicore Bundle Adjustment.  The emergence of multi-core computers represents a fundamental shift, with major implications for the design of computer vision algorithms. Most computers sold today have a multicore CPU with 2-16 cores and a GPU with anywhere from 4 to 128 cores. Exploiting this hardware parallelism will be key to the success and scalability of computer vision algorithms in the future. In this project, we consider the design and implementation of new inexact Newton type Bundle Adjustment algorithms that exploit hardware parallelism for efficiently solving large scale 3D scene reconstruction problems. We explore the use of multicore CPU as well as multicore GPUs for this purpose. We show that overcoming the severe memory and bandwidth limitations of current generation GPUs not only leads to more space efficient algorithms, but also to surprising savings in runtime. Our CPU based system is up to ten times and our GPU based system is up to thirty times faster than the current state of the art methods, while maintaining comparable convergence behavior."
"Deploying Google Search by Voice in Cantonese.  We describe our efforts in deploying Google search by voice for Cantonese, a southern Chinese dialect widely spoken in and around Hong Kong and Guangzhou. We collected audio data from local Cantonese speakers in Hong Kong and Guangzhou by using our DataHound smartphone application. This data was used to create appropriate acoustic models. Language models were trained on anonymized query logs from Google Web Search for Hong Kong. Because users in Hong Kong frequently mix English and Cantonese in their queries, we designed our system from the ground up to handle both languages. We report on experiments with different techniques for mapping the phoneme inventories for both languages into a common space. Based on extensive experiments we report word error rates and web scores for both Hong Kong and Guangzhou data. Cantonese Google search by voice was launched in December 2010."
"Channeling the data deluge.  With vast increases in biological data generation, mechanisms for data storage and analysis have become limiting. A data structure, semantically typed data hypercubes (SDCubes), that combines hierarchical data format version 5 (HDF5) and extensible markup language (XML) file formats, now permits the flexible storage, annotation and retrieval of large and heterogenous datasets."
"Fuzzy Computing Applications for Anti-Money Laundering and Distributed Storage System Load Monitoring.  Fuzzy computing (FC) has made a great impact in capturing human domain knowledge and modeling non-linear mapping of input-output space. In this paper, we describe the design and implementation of FC systems for detection of money laundering behaviors in financial transactions and monitoring of distributed storage system load. Our objective is to demonstrate the power of FC for real-world applications which are char- acterized by imprecise, uncertain data, and incomplete domain knowledge. For both applications, we designed fuzzy rules based on experts?? domain knowledge, depending on money laundering scenarios in transactions or the ??health?? of a distributed storage system. In addition, we developped a generic fuzzy inference engine and contributed to the open source community."
"Suggesting (More) Friends Using the Implicit Social Graph.  Although users of online communication tools rarely categorize their contacts into groups such as ""family"", ""co-workers"", or ""jogging buddies"", they nonetheless implicitly cluster contacts, by virtue of their interactions with them, forming implicit groups. In this paper, we describe the implicit social graph which is formed by users' interactions with contacts and groups of contacts, and which is distinct from explicit social graphs in which users explicitly add other individuals as their ""friends"". We introduce an interaction-based metric for estimating a user's affinity to his contacts and groups.  We then describe a novel friend suggestion algorithm that uses a user's implicit social graph to generate a friend group, given a small seed set of contacts which the user has already labeled as friends. We show experimental results that demonstrate the importance of both implicit group relationships and  interaction-based affinity ranking in suggesting friends. Finally, we discuss two applications of the Friend Suggest algorithm that have been released as Gmail features."
"Topology Discovery of Sparse Random Graphs With Few Participants.  We consider the task of topology discovery of sparse random graphs using end-to-end random
measurements (e.g., delay) between a subset of nodes, referred to as the participants. The rest of
the nodes are hidden, and do not provide any information for topology discovery. We consider
topology discovery under two routing models: (a) the participants exchange messages along
the shortest paths and obtain end-to-end measurements, and (b) additionally, the participants
exchange messages along the second shortest path. For scenario (a), our proposed algorithm
results in a sub-linear edit-distance guarantee using a sub-linear number of uniformly selected
participants. For scenario (b), we obtain a much stronger result, and show that we can achieve
consistent reconstruction when a sub-linear number of uniformly selected nodes participate. This
implies that accurate discovery of sparse random graphs is tractable using an extremely small
number of participants. Our algorithms are simple to implement, computationally efficient, and
exploit the locally tree-like property of sparse random graphs. We finally obtain a lower bound on
the number of participants required by any algorithm to reconstruct the original random graph
up to a given edit distance. We also demonstrate that while consistent discovery is tractable for
sparse random graphs using a small number of participants, in general, there are graphs which
cannot be discovered by any algorithm even with a significant number of participants, and with
the availability of end-to-end information along all the paths between the participants."
"RACEZ: A Lightweight and Non-Invasive Race Detection Tool for Production Applications.  Concurrency bugs, particularly data races, are notoriously difficult to debug and are a significant source of unreliability in multithreaded applications. Many tools to catch data races rely on program instrumentation to obtain memory instruction traces. Unfortunately, this instrumentation introduces significant runtime overhead, is extremely invasive, or has a limited domain of applicability making these tools unsuitable for many production systems. Consequently, these tools are typically used during application testing where many data races go undetected. This paper proposes RACEZ, a novel race detection mechanism which uses a sampled memory trace collected by the hardware performance monitoring unit rather than invasive instrumentation. The approach introduces only a modest overhead making it usable in production environments. We validate RACEZ using two open source server applications and the PARSEC benchmarks. Our experiments show that RACEZ catches a set of known bugs with reasonable probability while introducing only 2.8% runtime slow down on average."
"A Hierarchical Conditional Random Field Model for Labeling and Images of Street Scenes.  Simultaneously segmenting and labeling images is a fundamental problem in Computer Vision. In this paper, we
introduce a hierarchical CRF model to deal with the problem of labeling images of street scenes by several distinctive object classes. In addition to learning a CRF model
from all the labeled images, we group images into clusters
of similar images and learn a CRF model from each cluster
separately. When labeling a new image, we pick the closest
cluster and use the associated CRF model to label this image. Experimental results show that this hierarchical image labeling method is comparable to, and in many cases superior to, previous methods on benchmark data sets. In addition to segmentation and labeling results, we also showed how to apply the image labeling result to rerank Google similar images."
"Blognoon: Exploring a Topic in the Blogosphere.  We demonstrate Blognoon, a semantic blog search engine with the focus on topic exploration and navigation. Blognoon provides concept search instead of traditional keywords search and improves ranking by identifying main topics of posts. It enhances navigation over the Blogosphere with faceted interfaces and recommendations."
"The Method of Moments and Degree Distributions for Network Models.  Probability models on graphs are becoming increasingly 
important in many applications, but statistical tools for fitting such models are not yet well developed. Here we propose a general method of moments approach that can be used to fit a large class of probability models through empirical counts of certain patterns in a graph. We
establish some general asymptotic properties of empirical graph moments and prove consistency of the estimates as the graph size grows for all ranges of the average degree including ??(1). Additional results are obtained for the important special case of degree distributions."
"Indirect Content Privacy Surveys: Measuring Privacy Without Asking About It.  The strong emotional reaction elicited by privacy issues is well documented (e.g., [12, 8]). The emotional aspect of privacy makes it difficult to evaluate privacy concern, and directly asking about a privacy issue may result in an emo- tional reaction and a biased response. This effect may be partly responsible for the dramatic privacy concern ratings coming from recent surveys, ratings that often seem to be at odds with user behavior. In this paper we propose indirect techniques for measuring content privacy concerns through surveys, thus hopefully diminishing any emotional response. We present a design for indirect surveys and test the design??s use as (1) a means to measure relative privacy concerns across content types, (2) a tool for predicting unwillingness to share content (a possible indicator of privacy concern), and (3) a gauge for two underlying dimensions of privacy ?? content importance and the willingness to share content. Our evaluation consists of 3 surveys, taken by 200 users each, in which privacy is never asked about directly, but privacy warnings are issued with increasing escalation in the instruc- tions and individual question-wording. We demonstrate that this escalation results in statistically and practically signif- icant differences in responses to individual questions. In addition, we compare results against a direct privacy survey and show that rankings of privacy concerns are increasingly preserved as privacy language increases in the indirect sur- veys, thus indicating our mapping of the indirect questions to privacy ratings is accurately reflecting privacy concerns."
"Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition.  We use search engine results to address a particularly dif?cult cross-domain language processing task, the adaptation of named entity recognition (NER) from news text to web queries. The key novelty of the method is that we submit a token with context to a search engine and use similar contexts in the search results as additional information for correctly classifying the token. We achieve strong gains in NER performance on news, in-domain and out-of-domain, and on web queries."
Advertising and Traffic: Learning from online video data.  Online media portals like Google??s YouTube are generating unprecedented volumes of data on usage patterns and viewing behavior. Learn about improving online advertising by understanding how ads impact online traffic.
"Public vs. Publicized: Content Use Trends and Privacy Expectations.  From a semantic standpoint, there is a clear differentia- tion between the meanings of public and publicized con- tent. The former includes any content that is accessible by anyone, while the latter emphasizes visibility ?? publi- cized content is actively made available. As a user??s on- line experience becomes more personalized and data is increasingly pushed rather than pulled, the line between public and publicized content is inevitably blurred. In this position paper, we present quantitative evidence that despite this trend, in some settings users do not antici- pate the use of public content beyond the narrow context in which is was disclosed; they do not anticipate that the content may be publicized. While providing a ??publicized?? option for data is an important counterpart to the ability to limit access to data (e.g. through access con- trol lists), such an option must be accompanied by both greater user awareness of the ramifications of such an option and by transparency into data usage."
"Training a Parser for Machine Translation Reordering.  We propose a simple training regime that can improve the extrinsic performance of a parser, given only a corpus of sentences and a way to automatically evaluate the extrinsic quality of a candidate parse. We apply our method to train parsers that excel when used as part of a reordering component in a statistical machine translation system. We use a corpus of weakly-labeled reference reorderings to guide parser training. Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress."
"Incremental Clicks Impact Of Search Advertising.  Advertisers often wonder whether search ads cannibalize their organic traffic. In other words, if search ads were paused, would clicks on organic results increase, and make up for the loss in paid traffic? Google statisticians recently ran over 400 studies on paused accounts to answer this question. In what we call ??Search Ads Pause Studies??, our group of researchers observed organic click volume in the absence of search ads. Then they built a statistical model to predict the click volume for given levels of ad spend using spend and organic impression volume as predictors. These models generated estimates for the incremental clicks attributable to search ads (IAC), or in other words, the percentage of paid clicks that are not made up for by organic clicks when search ads are paused. The results were surprising. On average, the incremental ad clicks percentage across verticals is 89%. This means that a full 89% of the traffic generated by search ads is not replaced by organic clicks when ads are paused. This number was consistently high across verticals."
"Entire Relaxation Path for Maximum Entropy Problems.  We discuss and analyze the problem of finding a distribution that minimizes the relative entropy to a prior distribution while satisfying max-norm constraints with respect to an observed distribution. This setting generalizes the classical maximum entropy problems as it relaxes the standard constraints on the observed values. We tackle the problem by introducing a re-parametrization in which the unknown distribution is distilled to a single scalar. We then describe a homotopy between the relaxation parameter and the distribution characterizing parameter. The homotopy also reveals an aesthetic symmetry between the prior distribution and the observed distribution. We then
use the reformulated problem to describe a space and time efficient algorithm for tracking the <em>entire</em> relaxation path. Our derivations are based on a
compact geometric view of the relaxation path as a piecewise linear function in a <em>two</em> dimensional space of the relaxation-characterization parameters. We demonstrate the usability of our approach by applying the problem to Zipfian distributions over a large alphabet."
Advertising and Traffic: Learning from online video data.  Online media portals like Google??s YouTube are generating unprecedented volumes of data on usage patterns and viewing behavior. Learn about improving online advertising by understanding how ads impact online traffic.
Computing TCP's Retransmission Timer.  This document defines the standard algorithm that Transmission Control Protocol (TCP) senders are required to use to compute and manage their retransmission timer.  It expands on the discussion in Section 4.2.3.1 of RFC1122 and upgrades the requirement of supporting the algorithm from a SHOULD to a MUST.  This document obsoletes RFC 2988. This is an Internet Standards Track document.
"Large-Scale Music Annotation and Retrieval: Learning to Rank in Joint Semantic Spaces..  Music prediction tasks range from predicting tags given a song or clip of audio, predicting the name of the artist, or predicting related songs given a song, clip, artist name or tag.  That is, we are interested in every semantic relationship between the different musical concepts in our database. In realistically sized databases, the number of songs is measured in the hundreds of thousands or more, and the number of artists in the tens of thousands or more, providing a considerable challenge to standard machine learning techniques.  In this work, we propose a method that scales to such datasets which attempts to capture the semantic similarities between the database items by modeling audio, artist names, and tags in a single low-dimensional semantic embedding space. This choice of space is learnt by optimizing the set of prediction tasks of interest jointly using multi-task learning.  Our single model learnt by training on the joint objective function is shown experimentally to have improved accuracy over training on each task alone.  Our method also outperforms the baseline methods tried and, in comparison to them, is faster and consumes less memory.  We also demonstrate how our method learns an interpretable model, where the semantic space captures well the similarities of interest."
"Wsabie: Scaling Up To Large Vocabulary Image Annotation.  Image annotation datasets are becoming larger and larger, with tens of millions of images and tens of thousands of possible annotations.  We propose a strongly performing method  that scales to such datasets by simultaneously learning to optimize precision at the top of the ranked list of annotations for a given image and learning a low-dimensional joint embedding space for both images and annotations.  Our method, called Wsabie, both outperforms several baseline methods and is faster and consumes less memory."
"Exploiting Service Usage Information for Optimizing Server Resource Management.  It is often difficult to tune the performance of modern component-based Internet services because: (1) component middleware are complex software systems that expose several independently tuned server resource management mechanisms; (2) session-oriented client behavior with complex data access patterns makes it hard to predict what impact tuning these mechanisms has on application behavior; and (3) component-based Internet services themselves exhibit complex structural organization with requests of different types having widely ranging execution complexity. In this article we show that exposing and using detailed information about how clients use Internet services enables mechanisms that achieve two interconnected goals: (1) providing improved QoS to the service clients, and (2) optimizing server resource utilization. To differentiate among levels of service usage (service access) information, we introduce the notion of the service access attribute and identify four related groups of service access attributes, encompassing different aspects of service usage information, ranging from the high-level structure of client web sessions to low-level fine-grained information about utilization of server resources by different requests. To show how the identified service usage information can be collected, we implement a request profiling infrastructure in the JBoss Java application server. In the context of four representative service management problems, we show how collected service usage information is used to improve service performance, optimize server resource utilization, or to achieve other problem-specific service management goals."
"App Isolation: Get the Security of Multiple Browsers with Just One.  Many browser-based attacks can be prevented by using separate browsers for
separate web sites. However, most users access the web with only one browser.
We explain the security benefits that using multiple browsers provides in terms
of two concepts: entry-point restriction and state isolation.  We combine these
concepts into a general app isolation mechanism that can provide the same
security benefits in a single browser.  While not appropriate for all types of
web sites, many sites with high-value user data can opt in to app isolation to
gain defenses against a wide variety of browser-based attacks.  We implement
app isolation in the Chromium browser and verify its security properties using
finite-state model checking.  We also measure the performance overhead of app
isolation and conduct a large-scale study to evaluate its adoption complexity
for various types of sites, demonstrating how the app isolation mechanisms are
suitable for protecting a number of high-value Web applications, such as online
banking."
"Tenzing  A SQL Implementation On The MapReduce Framework.  Tenzing is a query engine built on top of MapReduce for ad hoc analysis of Google data. Tenzing supports a mostly complete SQL implementation (with several extensions) combined with several key characteristics such as heterogeneity, high performance, scalability, reliability, metadata awareness, low latency, support for columnar storage and structured data, and easy extensibility. Tenzing is currently used internally at Google by 1000+ employees and serves 10000+ queries per day over 1.5 petabytes of compressed data. In this paper, we describe the architecture and implementation of Tenzing, and present benchmarks of typical analytical queries."
"Warehouse-scale Computing: entering the teenage decade.  Video recording of a plenary talk delivered at the 2011 ACM Federated Computing Research Conference, focusing on some important challenges awaiting programmers and designers of Warehouse-scale Computers as it enters its second decade. June 8, 2011, San Jose, CA."
Drivers and applications of optical technologies for Internet Data Center networks.  The rise of large-scale Data Centers to power the Internet infrastructure is driving new architectural directions for optical networking. This paper highlights these architectural options and discusses technology building blocks for scaling inter-Datacenter connectivity.
"A Pole-Zero Filter Cascade Provides Good Fits to Human Masking Data and to Basilar Membrane and Neural Data.  A cascade of two-pole??two-zero filters with level-dependent pole and
zero dampings, with few parameters, can provide a good match to human
psychophysical and physiological data.  The model has been fitted to
data on detection threshold for tones in notched-noise masking,
including bandwidth and filter shape changes over a wide range of
levels, and has been shown to provide better fits with fewer parameters
compared to other auditory filter models such as gammachirps.
Originally motivated as an efficient machine implementation of auditory
filtering related to the WKB analysis method of cochlear wave
propagation, such filter cascades also provide good fits to mechanical
basilar membrane data, and to auditory nerve data, including linear
low-frequency tail response, level-dependent peak gain, sharp tuning
curves, nonlinear compression curves, level-independent zero-crossing
times in the impulse response, realistic instantaneous frequency
glides, and appropriate level-dependent group delay even with
minimum-phase response.  As part of exploring different level-dependent
parameterizations of such filter cascades, we have identified a simple
sufficient condition for stable zero-crossing times, based on the
shifting property of the Laplace transform:  simply move all the
$s$-domain poles and zeros by equal amounts in the real-$s$ direction.
Such pole-zero filter cascades are efficient front ends for machine
hearing applications, such as music information retrieval, content
identification, speech recognition, and sound indexing."
"Web 2.0 and Performance: Using Social Media to Facilitate Learning at Google.  Are you leveraging Web 2.0 technologies to solve performance problems? Google has tapped the power of online collaboration to solve business problems and engage learners. It is easier than you might think to leverage scalable and free technologies to address your organization's needs. In this hands-on session, explore case studies of how Google is using blogs, wikis, shared documents, RSS readers, and online video sharing to transform learning and performance."
"Using a Cascade of Asymmetric Resonators with Fast-Acting Compression as a Cochlear Model for Machine-Hearing Applications.  Every day, machines process many thousands of hours of audio signals through a realistic cochlear model.  They extract features, inform classifiers and recommenders, and identify copyrighted material.  The machine-hearing approach to such tasks has taken root in recent years, because hearing-based approaches perform better than we can do with more conventional sound-analysis approaches.  We use a bio-mimetic ""cascade of asymmetric resonators with fast-acting compression"" (CAR-FAC)??an efficient sound analyzer that incorporates the hearing research community's findings on nonlinear auditory filter models and cochlear wave mechanics.  The CAR-FAC is based on a pole??zero filter cascade (PZFC) model of auditory filtering, in combination with a multi-time-scale coupled automatic-gain-control (AGC) network.  It uses simple nonlinear extensions of conventional digital filter stages, and runs fast due to its low complexity.  The PZFC plus AGC network, the CAR-FAC, mimics features of auditory physiology, such as masking, compressive traveling-wave response, and the stability of zero-crossing times with signal level.  Its output ""neural activity pattern"" is converted to a ""stabilized auditory image"" to capture pitch, melody, and other temporal and spectral features of the sound."
"Dremel: Interactive Analysis of Web-Scale Datasets.  Dremel is a scalable, interactive ad hoc query system for analysis of read-only nested data. By combining multilevel execution trees and columnar data layout, it is capable of running aggregation queries over trillion-row tables in seconds. The system scales to thousands of CPUs and petabytes of data, and has thousands of users at Google. In this paper, we describe the architecture and implementation of Dremel, and explain how it complements MapReduce-based computing. We present a novel columnar storage representation for nested records and discuss experiments on few-thousand node instances of the system."
"Unary Data Structures for Language Models.  Language models are important components of speech recognition and machine translation systems.
Trained on billions of words, and consisting of billions of parameters, language models often are the
single largest components of these systems. There have been many proposed techniques to reduce the
storage requirements for language models. A technique based upon pointer-free compact storage of
ordinal trees shows compression competitive with the best proposed systems, while retaining the full
finite state structure, and without using computationally expensive block compression schemes or
lossy quantization techniques."
"Developing on Google+.  This hands on exploration of the Google+ platform walks through the full offering of Google+ APIs. Developing with Google+ takes a common sense, sequential approach to learning Google+. It focuses on concrete examples of integrating existing websites with Google+ as well as building social applications upon this new platform. In this book you will:
- Make the most of social widgets like the +1 Button and the Sharebox
- Register applications to gain access to the Google+ APIs
- Access public data directly over RESTful web services
- Use OAuth to gain access to access user specific data
- Learn about the available client libraries
- Build a small social application that integrates deeply with Google+ Whether you??re new to web design or an experienced application developer, you??ll learn everything you need to get started integrating Google+ into your new and existing projects."
The Case Against Data Lock-in.  Want to keep your users? Just make it easy for them to leave.
"Maestro: Quality-of-Service in Large Disk Arrays.  Provisioning storage in disk arrays is a difficult problem because many applications with different workload characteristics and priorities share resources provided by the array. Currently, storage in disk arrays is statically partitioned, leading to difficult choices between over-provisioning to meet peak demands and resource sharing to meet efficiency targets. In this paper, we present Maestro, a feedback controller that can manage resources on large disk arrays to provide performance differentiation among multiple applications. Maestro monitors the performance of each application and dynamically allocates the array resources so that diverse performance requirements can be met without static partitioning. It supports multiple performance metrics (e.g., latency and throughput) and application priorities so that important applications receive better performance in case of resource contention. By ensuring that high-priority applications sharing storage with other applications obtain the performance levels they require, Maestro makes it possible to use storage resources efficiently. We evaluate Maestro using both synthetic and real-world workloads on a large, commercial disk array. Our experiments indicate that Maestro can reliably adjust the allocation of disk array resources to achieve application performance targets."
"Milgram-routing in social networks..  We demonstrate how a recent model of social networks (??Affiliation Networks??) offers powerful cues in local routing within social networks, a theme made famous by sociologist Milgram??s ??six degrees of separation?? experiments. This model posits the existence of an ??interest space?? that underlies a social network; we prove that in networks produced by this model, not only do short paths exist among all pairs of nodes but natural local routing algorithms can discover them effectively. Specifically, we show that local routing can discover paths of length O(log^2 n) to targets chosen uniformly at random, and paths of length O(1) to targets chosen with probability proportional to their degrees. Experiments on the co-authorship graph derived from DBLP data confirm our theoretical results, and shed light into the power of one step of lookahead in routing algorithms for social networks."
"Sticking Together: Handcrafting Personalized Communication Interfaces..  We present I/O Stickers, adhesive sensors and actuators thatchildren can use to create personalized remote communicationinterfaces. By attaching I/O Stickers to special greeting cards,children can invent ways to communicate with long-distanceloved ones with personalized, connected messages. Childrendecorate these cards with their choice of craft materials, creativelyexpressing themselves while making a functioning interface. Thelow-bandwidth connections leave room for children to design notonly the look and function, but also the signification of theconnections. We describe the design of the I/O Stickers, a varietyof artifacts children have created, and future directions for thetoolkit. Preliminary results indicate that I/O Stickers are beginningto make a space for creative learning about communication and tomake keeping in touch playful and meaningful."
"Distributed forensics and incident response in the enterprise.  Remote live forensics has recently been increasingly used in order to facilitate rapid remote access to enterprise machines. We present the GRR Rapid Response Framework
(GRR), a new multi-platform, open source tool for enterprise forensic investigations enabling remote raw disk and memory access. GRR is designed to be scalable, opening the
door for continuous enterprise wide forensic analysis. This paper describes the architecture used by GRR and illustrates how it is used routinely to expedite enterprise forensic
investigations."
"Filtering: a method for solving graph problems in MapReduce..  The MapReduce framework is currently the de facto standard used throughout both industry and academia for petabyte scale data analysis. As the input to a typical MapReduce computation is large, one of the key requirements of the framework is that the input cannot be stored on a single machine and must be processed in parallel. In this paper we describe a general algorithmic design technique in the MapReduce framework called filtering. The main idea behind filtering is to reduce the size of the input in a distributed fashion so that the resulting, much smaller, problem instance can be solved on a single machine. Using this approach we give new algorithms in the MapReduce framework for a variety of fundamental graph problems. Specifically, we present algorithms for minimum spanning trees, maximal matchings, approximate weighted matchings, approximate vertex and edge covers and minimum cuts. In all of these cases, we will parameterize our algorithms by the amount of memory available on the machines allowing us to show tradeoffs between the memory available and the number of MapReduce rounds. For each setting we will show that even if the machines are only given substantially sublinear memory, our algorithms run in a constant number of MapReduce rounds. To demonstrate the practical viability of our algorithms we implement the maximal matching algorithm that lies at the core of our analysis and show that it achieves a significant speedup over the sequential version."
"Building Web Apps for Google TV.  By integrating the Web with traditional TV, Google TV offers developers an important new channel for content. But creating apps for Google TV requires learning some new skills??in fact, what you may already know about mobile or desktop web apps isn't entirely applicable. Building Web Apps for Google TV will help you make the transition to Google TV as you learn the tools and techniques necessary to build sophisticated web apps for this platform."
"Model-Based Aligner Combination Using Dual Decomposition.  Unsupervised word alignment is most often modeled as a Markov process that generates a sentence f conditioned on its translation e. A similar model generating e from f will make different alignment predictions. Statistical machine translation systems combine the predictions of two directional models, typically using heuristic combination procedures like grow-diag-final. This paper presents a graphical model that embeds two directional aligners into a single model. Inference can be performed via dual decomposition, which reuses the efficient inference algorithms of the directional models. Our bidirectional model enforces a one-to-one phrase constraint while accounting for the uncertainty in the underlying directional models. The resulting alignments improve upon baseline combination heuristics in word-level and phrase-level evaluations."
"Translation-Inspired OCR.  Optical character recognition is carried out using techniques
  borrowed from statistical machine translation.  In particular, the
  use of multiple simple feature functions in linear combination,
  along with minimum-error-rate training, integrated decoding, and
  $N$-gram language modeling is found to be remarkably effective,
  across several scripts and languages.  Results are presented using
  both synthetic and real data in five languages."
"The Snap Framework: A Web Toolkit for Haskell.  The authors discuss Web development in the Haskell programming language. They look at Snap, a simple, expressive Web development framework with an integrated, high-performance HTTP server."
"Practical Memory Checking with Dr. Memory.  Memory corruption, reading uninitialized memory, using freed memory, and other memory-related errors are among the most difficult programming bugs to identify and fix due to the delay and non-determinism linking the error to an observable symptom.  Dedicated memory checking tools are invaluable for finding these errors.  However, such tools are difficult to build, and because they must monitor all memory accesses by the application, they incur significant overhead.  Accuracy is another challenge: memory errors are not always straightforward to identify, and numerous false positive error reports can make a tool unusable.  A third obstacle to creating such a tool is that it depends on low-level operating system and architectural details, making it difficult to port to other platforms and
difficult to target proprietary systems like Windows. This paper presents Dr. Memory, a memory checking tool that operates on both Windows and Linux applications.  Dr. Memory handles the complex and not fully documented Windows environment, and avoids reporting false positive memory leaks that plague traditional leak locating algorithms.  Dr. Memory employs efficient instrumentation techniques; a direct comparison with the state-of-the-art Valgrind Memcheck tool reveals that Dr. Memory is twice as fast as Memcheck on average and up to four times faster on individual benchmarks."
"Dynamic Race Detection with LLVM Compiler.  Data races are among the most difficult to detect and costly 
bugs. Race detection has been studied widely, but none of the existing tools satisfies the requirements of high speed, detailed reports and wide availability at the same time. We describe our attempt to create a tool that works fast, has detailed and understandable reports and is available on a variety of platforms. The race detector is based on our previous work, ThreadSanitizer, and the instrumentation is done using the LLVM compiler. We show that applying compiler instrumentation and sampling reduces the slowdown to less than 1.5x, fast enough for interactive use."
"Transparency and Choice: Protecting Consumer Privacy in an Online World.  There have been concerns raised recently about online tracking. There are a variety of
mechanisms by which data is collected online, and for which it is used, and it is unclear which
of these are intended to be addressed by ??Do Not Track?? mechanisms. Tracking is often data
collection that helps ensure the security and integrity of data, determines relevancy of served
content and also helps create innovation opportunities. This value ought to be central in
any ??Do Not Track?? discussions."
"Collaborative Environmental In Situ Data Collection: Experiences and Opportunities for Ambient Data Integration.  Collaborative environmental in situ data collection occurs when a team of investigators goes into the field together to collect environmental data. These data might be necessary, e.g., for a biodiversity inventory, compilation of a soil density map, or to estimate above-ground forest carbon stocks. Investigators will often arrive at a location and disperse, collecting data, and then compiling it either in the field, or at a later time. Typically, an area will be divided into a set of plots, and within those, subplots. Teams of investigators will visit each of these plots with standardized forms and specialized equipment for collecting the data of interest. For example, in a forest inventory, investigators might collect data about the diameter and species of the trees in the forest, the trees?? health, fire damage and soil quality at the plot, proximity to roads, and whether any logging has taken place."
"L1 and L2 Regularization for Multiclass Hinge Loss Models.  This paper investigates the relationship between the loss function, the type of regularization, and the resulting model sparsity of discriminatively-trained multiclass linear models.  The effects on sparsity of optimizing log loss are straightforward: L2 regularization produces very dense models while L1 regularization produces much sparser models.  However, optimizing hinge loss yields more nuanced behavior.  We give experimental evidence and theoretical arguments that, for a class of problems that arises frequently in
natural-language processing, both L1- and L2-regularized hinge loss lead to sparser models than L2-regularized log loss, but less sparse models than L1-regularized log loss.  Furthermore, we give evidence and arguments that for models with only indicator features, there is a critical threshold on the weight of the regularizer below which L1- and L2-regularized hinge loss tends to produce models of similar sparsity."
"Subset Feedback Vertex Set is fixed parameter tractable.  The classical Feedback Vertex Set problem asks, for a given undirected graph G and an integer k, to find a set of at most k vertices that hits all the cycles in the graph G.
Feedback Vertex Set has attracted a large amount of research in the parameterized setting, and subsequent kernelization and fixed-parameter algorithms have been a rich source of ideas in the field. In this paper we consider a more general and difficult version of the problem, named Subset Feedback Vertex Set (SFVS in short) where an instance comes additionally with a set S of vertices, and we ask for a set of at most k vertices that hits all simple cycles passing through S.
Because of its applications in circuit testing and genetic linkage analysis SFVS was studied from the approximation algorithms perspective by Even et al. [SICOMP'00, SIDMA'00]. The question whether the SFVS problem is fixed parameter tractable was posed independently by Kawarabayashi and Saurabh in 2009.  We answer this question affirmatively.  We begin by showing that this problem is fixed-parameter tractable when parametrized by |S|.  Next we present an algorithm which reduces the size of S to O(k^3) in 2^{O(k\log k)}n^{O(1)} time using kernelization techniques such as the 2-Expansion Lemma, Menger's theorem and Gallai's theorem. These two facts allow us to give a 2^{O(k\log k)} n^{O(1)} time algorithm solving the SFVS problem, proving that it is indeed fixed parameter
tractable."
"Fast Elliptic Curve Cryptography in OpenSSL.  We present a 64-bit optimized implementation of the NIST and SECG-standardized elliptic curve P-224. Our implementation is fully integrated into OpenSSL 1.0.1: full TLS handshakes using a 1024-bit RSA certificate and ephemeral Elliptic Curve Diffie-Hellman key exchange over P-224 now run at twice the speed of standard OpenSSL, while atomic elliptic curve operations are up to 4 times faster. In addition, our implementation is immune to timing attacks - most notably, we show how to do small table look-ups in a cache-timing resistant way, allowing us to use precomputation. To put our results in context, we also discuss the various security performance trade-offs available to TLS applications."
"Quantitative Analysis of Culture Using Millions of Digitized Books.  We constructed a corpus of digitized texts containing about 4% of all books ever printed. Analysis of this corpus enables us to investigate cultural trends quantitatively. We survey the vast terrain of ??culturomics,?? focusing on linguistic and cultural phenomena that were reflected in the English language between 1800 and 2000. We show how this approach can provide insights about fields as diverse as lexicography, the evolution of grammar, collective memory, the adoption of technology, the pursuit of fame, censorship, and historical epidemiology. Culturomics extends the boundaries of rigorous quantitative inquiry to a wide array of new phenomena spanning the social sciences and the humanities."
"Dynamic Stylized Shading Primitives.  Shading appearance in illustrations, comics and graphic novels is designed to convey illumination, material and surface shape characteristics at once. Moreover, shading may vary depending on different configurations of surface distance, lighting, character expressions, timing of the action, to articulate storytelling or draw attention to a part of an object. In this paper, we present a method that imitates such expressive stylized shading techniques in dynamic 3D scenes, and which offers a simple and flexible means for artists to design and tweak the shading appearance and its dynamic behavior. The key contribution of our approach is to seamlessly vary appearance by using a combination of shading primitives that take into account lighting direction, material characteristics and surface features. We demonstrate their flexibility in a number of scenarios: minimal shading, comics or cartoon rendering, glossy and anisotropic material effects; including a variety of dynamic variations based on orientation, timing or depth. Our prototype implementation combines shading primitives with a layered approach and runs in real-time on the GPU"
"Crowdsourcing Event Detection in YouTube Videos.  Considerable efforts have been put into making video content on the Web more accessible, searchable, and navigable by research on both textual and visual analysis of the actual video content and the accompanying metadata. Nevertheless, most of the time, videos are opaque objects in websites. With Web browsers gaining more support for the HTML5 &lt;video&gt; element, videos are becoming first class citizens on the Web. In this paper we show how events can be detected on-the-fly through crowdsourcing (i) textual, (ii) visual, and (iii) behavioral analysis in YouTube videos, at scale. The main contribution of this paper is a generic crowdsourcing framework for automatic and scalable semantic annotations of HTML5 videos. Eventually, we discuss our preliminary results using traditional server-based approaches to video event detection as a baseline."
"DC Proposal: Enriching Unstructured Media Content About Events to Enable Semi-Automated Summaries, Compilations, and Improved Search by Leveraging Social Networks.  Mobile devices like smartphones together with social networks enable people to generate, share, and consume enormous amounts of media content. Common search operations, for example searching for a music clip based on artist name and song title on video platforms such as YouTube, can be achieved both based on potentially shallow human-generated metadata, or based on more profound content analysis, driven by Optical Character Recognition (OCR) or Automatic Speech Recognition (ASR). However, more advanced use cases, such as summaries or compilations of several pieces of media content covering a certain event, are hard, if not impossible to fulfill at large scale. One example of such event can be a keynote speech held at a conference, where, given a stable network connection, media content is published on social networks while the event is still going on. In our thesis, we develop a framework for media content processing, leveraging social networks, utilizing the Web of Data and fine-grained media content addressing schemes like Media Fragments URIs to provide a scalable and sophisticated solution to realize the above use cases: media content summaries and compilations. We evaluate our approach on the entity level against social media platform APIs in conjunction with Linked (Open) Data sources, comparing the current manual approaches against our semi-automated approach. Our proposed framework can be used as an extension for existing video platforms."
"Adding Meaning to Facebook Microposts via a Mash-up API and Tracking Its Data Provenance.  The social networking website Facebook offers to its users a feature called ??status updates?? (or just ??status??), which allows users to create microposts directed to all their contacts, or a subset thereof. Readers can respond to microposts, or in addition to that also click a ??Like?? button to show their appreciation for a certain micropost. Adding semantic meaning in the sense of unambiguous intended ideas to such microposts can, for example, be achieved via Natural Language Processing (NLP). Therefore, we have implemented a RESTful mash-up NLP API, which is based on a combination of several third party NLP APIs in order to retrieve more accurate results in the sense of emergence. In consequence, our API uses third party APIs opaquely in the background in order to deliver its output. In this paper, we describe how one can keep track of provenance, and credit back the contributions of each single API to the combined result of all APIs. In addition to that, we show how the existence of provenance metadata can help understand the way a combined result is formed, and optimize the result combination process. Therefore, we use the HTTP Vocabulary in RDF and the Provenance Vocabulary. The main contribution of our work is a description of how provenance metadata can be automatically added to the output of mash-up APIs like the one presented here."
"Efficient Runtime Service Discovery and Consumption with Hyperlinked RESTdesc.  Hyperlinks and forms let humans navigate with ease through websites they have never seen before. In contrast, automated agents can only perform preprogrammed actions on Web services, reducing their generality and restricting their usefulness to a specialized domain. Many of the employed services call themselves RESTful, although they neglect the hypermedia constraint as defined by Roy T. Fielding, stating that the application state should be driven by hypertext. This lack of link usage on the Web of services severely limits agents in what they can do, while connectedness forms a primary feature of the human Web. An urgent need for more intelligent agents becomes apparent, and in this paper, we demonstrate how the conjunction of functional service descriptions and hypermedia links leads to advanced, interactive agent behavior. We propose a new mode for our previously introduced semantic service description format RESTdesc, providing the mechanisms for agents to consume Web services based on links, similar to human browsing strategies. We illustrate the potential of these descriptions by a use case that shows the enhanced capabilities they offer to automated agents, and explain how this is vital for the future Web."
"A Tweet Consumers' Look At Twitter Trends.  Twitter Trends allows for a global or local view on ??what??s happening in my world right now?? from a tweet producers?? point of view. In this paper, we explore a way to complete Twitter Trends by having a closer look at the other side: the tweet consumers?? point of view. While Twitter Trends works by analyzing the frequency of terms and their velocity of appearance in tweets being written, our approach is based on the popularity of extracted named entities in tweets being read."
"Fulfilling the Hypermedia Constraint Via HTTP OPTIONS, the HTTP Vocabulary In RDF, and Link Headers.  One of the main REST design principles is the focus on media types as the core of contracts on the Web. However, not always is the service designer free to select the most appropriate media type for a task, sometimes a generic media type like application/rdf+xml (or in the worst case a binary format like image/png) with no defined or possible hypermedia controls at all has to be chosen. With this position paper we present a way how the hypermedia constraint of REST can still be fulfilled using a combination of Link headers, the OPTIONS method, and the HTTP Vocabulary in RDF."
"How Google is using Linked Data Today and Vision For Tomorrow.  In this position paper, we first discuss how modern search engines, such as Google, make use of Linked Data spread in Web pages for displaying Rich Snippets. We present an example of the technology and we analyze its current uptake. We then sketch some ideas on how Rich Snippets could be extended in the future, in particular for multimedia documents. We outline bottlenecks in the current Internet architecture that require fixing in order to enable our vision to work at Web scale."
"SemWebVid - Making Video a First Class Semantic Web Citizen and a First Class Web Bourgeois -- Semantic Web Challenge.  SemWebVid is an online Ajax application that allows for the automatic generation of Resource Description Framework (RDF) video descriptions. These descriptions are based on two pillars: first, on a combination of user-generated metadata such as title, summary, and tags; and second, on closed captions which can be user-generated, or be auto-generated via speech recognition. The plaintext contents of both pillars are being analyzed using multiple Natural Language Processing (NLP) Web services in parallel whose results are then merged and where possible matched back to concepts in the sense of Linking Open Data (LOD). The final result is a deep-linkable RDF description of the video, and a ??scroll-along?? view of the video as an example of video visualization formats."
"From Research Hypotheses to Practical Guidelines: A Proposal to Facilitate Researcher-Practitioner Interaction.  In this paper, we describe the gulf that exists between research findings and their adoption in practice. We propose ideas that have the potential to increase the collaboration between researchers and practitioners to forge a symbiotic relationship between these two worlds. Our proposal includes highlighting industry constraints in academic HCI classes, encouraging researchers to present practical implications in papers, creating a collaborative platform between researchers and practitioners, and fostering strong relationships between HCI students and industry professionals."
"Sustainability of Bits, not just Atoms.  In this paper, we discuss sustainability as it applies to digital artifacts and personal information. We continually create and/or receive new information items in the form of emails, files, photos, media, etc., but once these artifacts enter our information ecosystems, they stay permanently and are rarely deleted even if their intrinsic value is no longer the same as earlier. This impacts information seeking tasks negatively, as users must now learn to navigate a larger corpus of information, and leads to information overload. We describe the technological causes of information overload in the context of existing finding, filing, and refiling practices and information heirlooms. We conclude with an example of a solution that can address this challenge."
"Scheduling partially ordered jobs faster than 2^n.  In the SCHED problem we are given a set of n jobs, together with their processing times and precedence constraints. The task is to order the jobs so that their total completion time is minimized. SCHED is a special case of the Traveling Repairman Problem with precedences. A natural dynamic programming algorithm solves both these problems in 2^n n^O(1) time, and whether there exists an algorithms solving
SCHED in O(c^n) time for some constant c &lt; 2 was an open problem posted in 2004 by Woeginger. In this paper we answer this question positively."
"On Multiway Cut paramterized above lower bounds.  In this paper we consider two above lower bound parameterizations of Node Multiway Cut ?? above maximum  separating cut and above a natural LP-relaxation ?? and prove them to be fixed-parameter tractable. Our results imply O(4^k) algorithms for Vertex Cover above Maximum Matching and Almost 2-SAT as well as a O*(2^k) algorithm for Node Multiway Cut with a standard parameterization by the solution size, improving previous bounds for these
problems."
"YouPivot: Improving Recall with Contextual Search.  According to cognitive science literature, human memory is predicated on contextual cues (e.g., room, music) in the environment. During recall tasks, we associate information/activities/objects with contextual cues. However, computer systems do not leverage our natural process of using contextual cues to facilitate recall. We present a new interaction technique, Pivoting, that allows users to search for contextually related activities and find a target piece of information (often not semantically related). A sample motivation for contextual search would be, 'what was that website I was looking at when Yesterday by The Beatles was last playing?' Our interaction technique is grounded in the cognitive science literature, and is demonstrated in our system YouPivot. In addition, we present a new personal annotation method, called TimeMarks, to further support contextual recall and the pivoting process. In a pilot study, participants were quicker to identify websites, and preferred using YouPivot, compared to current tools. YouPivot demonstrates how principles of human memory can be applied to enhance the search of digital information."
"Thialfi: A Client Notification Service for Internet-Scale Applications.  Ensuring the freshness of client data is a fundamental problem for applications that rely on cloud infrastructure to store data and mediate sharing. Thialfi is a notification service developed at Google to simplify this task. Thialfi supports applications written in multiple programming languages and running on multiple platforms, e.g., browsers, phones, and desktops. Applications register their interest in a set of shared objects and receive notifications when those objects change. Thialfi servers run in multiple Google data centers for availability and replicate their state asynchronously. Thialfi's approach to recovery emphasizes simplicity: all server state is soft, and clients drive recovery and assist in replication. A principal goal of our design is to provide a straightforward API and good semantics despite a variety of failures, including server crashes, communication failures, storage unavailability, and data center failures. Evaluation of live deployments confirms that Thialfi is scalable, efficient, and robust. In production use, Thialfi has scaled to millions of users and delivers notifications with an average delay of less than one second."
"Simultaneous Approximations for Adversarial and Stochastic Online Budgeted Allocation.  Motivated by online ad allocation, we study the problem of simultaneous approximations for the adversarial and stochastic online budgeted allocation problem. This problem consists of a bipartite graph G = (X, Y, E), where the nodes of Y along with their corresponding capacities are known beforehand to the algorithm, and the nodes of X arrive online. When a node of X arrives, its incident edges, and their respective weights are revealed, and the algorithm can match it to a neighbor in Y. The objective is to maximize the weight of the final matching, while respecting the capacities. When nodes arrive in an adversarial order, the best competitive ratio is known to be 1 - 1/e, and it can be achieved by the Ranking [18], and its generalizations (Balance [16, 21]). On the other hand, if the nodes arrive through a random permutation, it is possible to achieve a competitive ratio of 1 -- ?? [9]. In this paper we design algorithms that achieve a competitive ratio better than 1 -- 1/e on average, while preserving a nearly optimal worst case competitive ratio. Ideally, we want to achieve the best of both worlds, i.e, to design an algorithm with the optimal competitive ratio in both the adversarial and random arrival models. We achieve this for unweighted graphs, but show that it is not possible for weighted graphs. In particular, for unweighted graphs, under some mild assumptions, we show that Balance achieves a competitive ratio of 1 -- ?? in a random permutation model. For weighted graphs, however, we prove this is not possible; we prove that no online algorithm that achieves an approximation factor of 1 -- 1/?? for the worst-case inputs may achieve an average approximation factor better than 97.6% for random inputs. In light of this hardness result, we aim to design algorithms with improved approximation ratios in the random arrival model while preserving the competitive ratio of 1 -- 1/?? in the worst case. To this end, we show the algorithm proposed by [21] achieves a competitive ratio of 0.76 for the random arrival model, while having a 1 -- 1/?? ratio in the worst case."
"Diagnosing Latency in Multi-Tier Black-Box Services.  As multi-tier cloud applications become pervasive, we need better tools for understanding their performance. This paper presents a system that analyzes observed or desired changes to end-to-end latency prole in a large distributed application, and identifies their underlying causes. It recognizes changes to system conguration, workload, or performance of individual services that lead to the observed or desired outcome. Experiments on an industrial datacenter demonstrate the utility of the system."
"Recursion in Scalable Protocols via Distributed Data Flows.  This paper proposes a new approach to representing scalable hierarchical distributed multi-party protocols, and reasoning about their behavior. The established endpoint-to-endpoint message-passing abstraction provides little support for modeling distributed algorithms in hierarchical systems, in which the hierarchy and membership dynamically evolve. This paper explains how with our new Distributed Data Flow (DDF) abstraction, hierarchical architecture can be modeled via recursion in the language. This facilitates a more concise code, and it enables automated generation of scalable hierarchical implementations for heterogeneous network environments."
"Improving Book OCR by Adaptive Language and Image Models.  In order to cope with the vast diversity of book content
and typefaces, it is important for OCR systems to leverage the
strong consistency within a book but adapt to variations across
books. In this work, we describe a system that combines two
parallel correction paths using document-specific image and
language models. Each model adapts to shapes and vocabularies
within a book to identify inconsistencies as correction hypotheses,
but relies on the other for effective cross-validation. Using the
open source Tesseract engine as baseline, results on a large
dataset of scanned books demonstrate that word error rates can
be reduced by 25% using this approach."
"Large-Scale Parallel Statistical Forecasting Computations in R.  We demonstrate the utility of massively parallel computational infrastructure for statistical computing using the MapReduce paradigm for R. This framework allows users to write computations in a high-level language that are then broken up and distributed to worker tasks in Google datacenters. Results are collected in a scalable, distributed data store and returned to the interactive user session. We apply our approach to a forecasting application that fits a variety of models, prohibiting an analytical description of the statistical uncertainty associated with the overall forecast. To overcome this, we generate simulation-based uncertainty bands, which necessitates a large number of computationally intensive realizations.  Our technique cut total run time by a factor of 300. Distributing the computation across many machines permits analysts to focus on statistical issues while answering questions that would be intractable without significant parallel computational infrastructure. We present real-world performance characteristics from our application to allow practitioners to better understand the nature of massively parallel statistical simulations in R."
"Logical Attestation: An Authorization Architecture for Trustworthy Computing.  This paper describes the design and implementation of a new operating system authorization architecture to support trustworthy computing. Called logical attestation, this architecture provides a sound framework for reasoning about run time behavior of applications. Logical attestation is based on attributable, unforgeable statements about program properties, expressed in a logic. These statements are suitable for mechanical processing, proof construction, and verification; they can serve as credentials, support authorization based on expressive authorization policies, and enable remote principals to trust software components without restricting the local user??s choice of binary implementations.
We have implemented logical attestation in a new operating system called the Nexus. The Nexus executes natively on x86 platforms equipped with secure coprocessors. It supports both native Linux applications and uses logical attestation to support new trustworthy-computing applications. When deployed on a trustworthy cloud-computing stack, logical attestation is efficient, achieves high-performance, and can run applications that provide qualitative guarantees not possible with existing modes of attestation."
"TCP Fast Open.  Today??s web services are dominated by TCP flows so short
that they terminate a few round trips after handshaking; this handshake is a significant source of latency for such flows. In this paper we describe the design, implementation, and deployment of the TCP Fast Open protocol, a new mechanism that enables data exchange during TCP??s initial handshake.
In doing so, TCP Fast Open decreases application network
latency by one full round-trip time, decreasing the delay experienced by such short TCP transfers. We address the security issues inherent in allowing data exchange during the three-way handshake, which we mitigate using a security token that verifies IP address ownership.
We detail other fall-back defense mechanisms and address
issues we faced with middleboxes, backwards compatibility
for existing network stacks, and incremental deployment.
Based on traffic analysis and network emulation, we
show that TCP Fast Open would decrease HTTP transaction
network latency by 15%and whole-page load time over 10%
on average, and in some cases up to 40%"
"Active Tuples-based Scheme for Bounding Posterior Beliefs.  The paper presents a scheme for computing lower and upper bounds on the posterior marginals in Bayesian networks with discrete variables. Its power lies in its ability to use any available scheme that bounds the probability of evidence or posterior marginals and enhance its performance in an anytime manner. The scheme uses the cutset conditioning principle to tighten existing bounding schemes and to facilitate anytime behavior, utilizing a fixed number of cutset tuples. The accuracy of the bounds improves as the number of used cutset tuples increases and so does the computation time. We demonstrate empirically the value of our scheme for bounding posterior marginals and probability of evidence using a variant of the bound propagation algorithm as a plug-in scheme."
"Extracting Patterns from Location History.  In this paper, we describe how a user's location history (recorded by tracking the user's mobile device location with his permission) is used to extract the user's location patterns. We describe how we compute the user's commonly visited places (including home and work),  and commute patterns. The analysis is displayed on the Google Latitude history dashboard [7] which is only accessible to the user."
"Models for Neural Spike Computation and Cognition.  This monograph addresses the intertwined mathematical, neurological, and cognitive mysteries of the brain. It first evaluates the mathematical performance limits of simple spiking neuron models that both learn and later recognize complex spike excitation patterns in less than one second without using training signals unique to each pattern. Simulations validate these models, while theoretical expressions validate their simpler performance parameters. These single-neuron models are then qualitatively related to the training and performance of multi-layer neural networks that may have significant feedback. The advantages of feedback are then qualitatively explained and related to a model for cognition. This model is then compared to observed mild hallucinations that arguably include accelerated time-reversed video memories. The learning mechanism for these binary threshold-firing ??cognon?? neurons is spike-timing-dependent plasticity (STDP) that depends only on whether the spike excitation pattern presented to a given single ??learning-ready?? neuron within a period of milliseconds causes that neuron to fire or ??spike??. The ??false-alarm?? probability that a trained neuron will fire for a random unlearned pattern can be made almost arbitrarily low by reducing the number of patterns learned by each neuron. Models that use and that do not use spike timing within patterns are evaluated. A Shannon mutual information metric (recoverable bits/neuron) is derived for binary neuron models that are characterized only by their probability of learning a random input excitation pattern presented to that neuron during learning readiness, and by their false-alarm probability for random unlearned patterns. Based on simulations, the upper bounds to recoverable information are ~0.1 bits per neuron for optimized neuron parameters and training. This information metric assumes that: 1) each neural spike indicates only that the responsible neuron input excitation pattern (a pattern lasts less than the time between consecutive patterns, say 30 milliseconds) had probably been seen earlier while that neuron was ??learning ready??, and 2) information is stored in the binary synapse strengths. This focus on recallable learned information differs from most prior metrics such as pattern classification performance and metrics relying on pattern-specific training signals other than the normal input spikes. This metric also shows that neuron models can recall useful Shannon information only if their probability of firing randomly is lowered between learning and recall. Also discussed are: 1) how rich feedback might permit improved noise immunity, learning and recognition of pattern sequences, compression of data, associative or content-addressable memory, and development of communications links through white matter, 2) extensions of cognon models that use spike timing, dendrite compartments, and new learning mechanisms in addition to spike timing- dependent plasticity (STDP), 3) simulations that show how simple optimized neuron models can have optimum numbers of binary synapses in the range between 200 and 10,000, depending on neural parameters, and 4) simulation results for parameters like the average bits/spike, bits/neuron/second, maximum number of learnable patterns, optimum ratios between the strengths of weak and strong synapses, and probabilities of false alarms."
"Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models.  Cross-document coreference, the task of grouping all the mentions of each entity in a document collection, arises in information extraction and automated knowledge base construction. For large collections, it is clearly
impractical to consider all possible groupings of mentions into distinct entities. To solve the problem we propose two ideas: (a) a distributed inference technique that uses parallelism to enable large scale processing, and (b) a hierarchical model of coreference that represents uncertainty over multiple granularities of entities to facilitate more effective approximate inference. To evaluate these ideas, we constructed a labeled corpus of 1:5 million disambiguated mentions in Web pages by selecting link anchors referring to Wikipedia entities. We show that the combination of the
hierarchical model with distributed inference quickly obtains high accuracy (with error reduction of 38%) on this large dataset, demonstrating the scalability of our approach."
"Measuring the Impact of Advertising on YouTube Traffic.  At YouTube, balancing ad load and user happiness is a major concern. One way we measure this tradeoff is through live experiments, which we run on a small percentage of traffic. For example, by holding back certain ad formats we can build metrics around the impact of YouTube advertising on the user experience. In this talk we will discuss the benefits and challenges of running large-scale advertising experiments."
"Question Identification on Twitter, Accepted by CIKM 2011.  In this paper, we investigate the novel problem of auto-
matic question identification in the microblog environment.
It contains two steps: detecting tweets that contain ques-
tions (we call them ??interrogative tweets??) and extracting
the tweets which really seek information or ask for help (so
called ??qweets??) from interrogative tweets. To detect inter-
rogative tweets, both traditional rule-based approach and
state-of-the-art learning-based method are employed. To
extract qweets, context features like short urls and Tweet-
specific features like Retweets are elaborately selected for
classification. We conduct an empirical study with sampled
one hour??s English tweets and report our experimental re-
sults for question identification on Twitter."
"K2Q: Generating Natural Language Questions from Keywords with User Refinements.  Garbage in and garbage out. A Q&amp;A system must receive a well formulated question that matches the user??s intent or she
has no chance to receive satisfactory answers. In this paper, we propose a keywords to questions (K2Q) system to assist a user to articulate and refine questions.
K2Q generates candidate questions and refinement words from a set of input keywords. After specifying some initial keywords, a user receives a list of candidate questions as well as a list of refinement words. The user can then select a satisfactory question, or select a refinement word
to generate a new list of candidate questions and refinement words. We propose a User Inquiry Intent (UII) model to de-
scribe the joint generation process of keywords and questions for ranking questions, suggesting refinement words, and generating questions that may not have previously
appeared. Empirical study shows UII to be useful and effective for the K2Q task."
"LaDeDa: Languages for Debuggable Distributed Algorithms.  When programming language designs are presented, the examples are
almost exclusively of correct programs. Most attention of programming
language designers is indeed on the beauty and elegance of correct
programs. For incorrect programs, great design attention is paid to
catching errors early---such as fancy static type systems---so that
many incorrect programs are never run. Due to the success of these efforts, many programs are either correct
or inadmissible, conserving on the need for programmer attention. As a
result, most of the attention working programmers spend looking at
code is spent debugging incorrect running code. Often this is code
written by others and only partially understood. What properties
should such code have? How can programming language design encourage
incorrect programs to have those properties that facilitate debugging? Distributed programs introduce additional difficult bugs of a
different character. How should distributed language design facilitate
the debugging of distributed programs? We explain how these considerations have affected four distributed
language designs (E, AmbientTalk, Joe-E/Waterken, Dr. SES) and one
distributed debugging tool (Causeway)."
"A Tale of Two (Similar) Cities: Inferring City Similarity Through Geo-Spatial Query Log Analysis.  Understanding the backgrounds and interest of the people who are consuming a piece of content, such as a  news story, video, or music, is vital for the content producer as well the advertisers who rely on the content  to provide a channel on which to advertise.  We extend traditional search-engine query log analysis, which has primarily concentrated on analyzing either single or small groups of queries or users, to examining the  complete query stream of very large groups of users ?? the inhabitants of 13,377 cities across the United  States.  Query logs can be a good representation of the interests of the city??s inhabitants and a useful characterization of the city itself.   Further, we demonstrate how query logs can be effectively used to gather city-level statistics sufficient for providing insights into the similarities and differences between  cities.  Cities that are found to be similar through the use of query analysis correspond well to the similar cities as determined through other large-scale and time-consuming direct measurement studies, such as those undertaken by the Census Bureau."
"RFC6472 - Recommendation for Not Using AS_SET and AS_CONFED_SET in BGP.  This document recommends against the use of the AS_SET and
   AS_CONFED_SET types of the AS_PATH in BGPv4.  This is done to
   simplify the design and implementation of BGP and to make the
   semantics of the originator of a route more clear.  This will also
   simplify the design, implementation, and deployment of ongoing work
   in the Secure Inter-Domain Routing Working Group."
"A Room with a View: Understanding Users' Stages in Picking a Hotel Online.  We describe how we built a model for user decision making during local search tasks, specifically hotels. We differentiate between affective and functional needs and identify the following stages and related information needs: 0: Lay of the land; 1: Generating options; 2: Scanning for attractors and detractors; 3: Due diligence. We contrast this framework with existing consumer decision-making models. We close by describing how this model influenced the development of the recently launched experiment, Google Hotel Finder"
"Catching a viral video.  The sharing and re-sharing of videos on social sites, blogs e-mail, and other means has given rise to the phenomenon of viral videos??videos that become popular through internet sharing. In this paper we seek to better understand viral videos on YouTube by analyzing sharing and its relationship to video popularity using millions of YouTube videos. The socialness of a video is quantified by classifying the referrer sources for video views as social (e.g. an emailed link, Facebook referral) or non-social (e.g. a link from related videos). We find that viewership patterns of highly social videos are very different from less social videos. For example, the highly social videos rise to, and fall from, their peak popularity more quickly than less social videos. We also find that not all highly social videos become popular, and not all popular videos are highly social. By using our insights on viral videos we are able develop a method for ranking blogs and websites on their ability to spread viral videos."
"Non-Price Equilibria in Markets of Discrete Goods.  We study markets of indivisible items in which price-based
(Walrasian) equilibria often do not exist due to the discrete
non-convex setting. Instead we consider Nash equilibria of
the market viewed as a game, where players bid for items,
and where the highest bidder on an item wins it and pays
his bid. We first observe that pure Nash-equilibria of this
game excatly correspond to price-based equilibiria (and thus
need not exist), but that mixed-Nash equilibria always do
exist, and we analyze their structure in several simple cases
where no price-based equilibrium exists. We also undertake
an analysis of the welfare properties of these equilibria showing that while pure equilibria are always perfectly efficient
(??first welfare theorem??), mixed equilibria need not be, and
we provide upper and lower bounds on their amount of inefficiency."
"Can a 2-hour Visit to a Hi-Tech Company Increase Interest in and Change Perceptions about Computer Science?.  This paper presents the ""Mind the Gap"" initiative that aims to encourage female high school pupils to study computer science (CS) in high school. This is achieved by increasing their awareness to what CS is, and exposing them to the essence of a hi-tech environment and to same gender role models. The initiative was undertaken by female software engineers at Google's Israel R&amp;D Center in collaboration with the Israeli National Center for Computer Science Teachers. We describe the initiative and its impact on the female pupils' interest in CS. One of our conclusions is that even a short visit to a hi-tech company, in this case ?? Google, has the potential to change pupils' perception of what CS is and to increase their interest in CS and their desire to study it. Our initiative can be easily adapted by other companies and can be scaled to impact a rather large
population."
"Graph cube: on warehousing and OLAP multidimensional networks.  We consider extending decision support facilities toward large sophisticated networks, upon which multidimensional attributes are associated with network entities, thereby forming the so-called multidimensional networks. Data warehouses and OLAP (Online Analytical Processing) technology have proven to be effective tools for decision support on relational data. However, they are not well equipped to handle the new yet important multidimensional networks. In this paper, we introduce Graph Cube, a new data warehousing model that supports OLAP queries effectively on large multidimensional networks. By taking account of both attribute aggregation and structure summarization of the networks, Graph Cube goes beyond the traditional data cube
model involved solely with numeric value based group-by??s,
thus resulting in a more insightful and structure-enriched
aggregate network within every possible multidimensional
space. Besides traditional cuboid queries, a new class of
OLAP queries, crossboid, is introduced that is uniquely useful in multidimensional networks and has not been studied
before. We implement Graph Cube by combining special
characteristics of multidimensional networks with the existing well-studied data cube techniques. We perform extensive experimental studies on a series of real world data sets and Graph Cube is shown to be a powerful and efficient tool for decision support on large multidimensional networks."
"The future of child-computer interaction.  In this panel, academic, non-profit, and industry professionals will ask, what does the future hold for ""child-computer interaction?"" Panelists will explore such issues as how new mobile, social, and ubiquitous technologies change children's future patterns of searching, exploration, and expression of information; how learning environments will be ever-changing because of new technologies; and the challenges and opportunities of designing for child-computer interaction."
"Dynamic cache contention detection in multi-threaded applications.  In today's multi-core systems, cache contention due to true and false sharing can cause unexpected and significant performance degradation. A detailed understanding of a given multi-threaded application's behavior is required to precisely identify such performance bottlenecks. Traditionally, however, such diagnostic information can only be obtained after lengthy simulation of the memory hierarchy. In this paper, we present a novel approach that efficiently analyzes interactions between threads to determine thread correlation and detect true and false sharing. It is based on the following key insight: although the slowdown caused by cache contention depends on factors including the thread-to-core binding and parameters of the memory hierarchy, the amount of data sharing is primarily a function of the cache line size and application behavior. Using memory shadowing and dynamic instrumentation, we implemented a tool that obtains detailed sharing information between threads without simulating the full complexity of the memory hierarchy. The runtime overhead of our approach --- a 5x slowdown on average relative to native execution --- is significantly less than that of detailed cache simulation. The information collected allows programmers to identify the degree of cache contention in an application, the correlation among its threads, and the sources of significant false sharing. Using our approach, we were able to improve the performance of some applications up to a factor of 12x. For other contention-intensive applications, we were able to shed light on the obstacles that prevent their performance from scaling to many cores."
"Designing for user experience: academia &amp; industry.  As the importance of user experience (UX) has grown, so too have attempts to define, delimit, categorize and theorize about it. In particular, there have been emerging lines of tension in User Experience that parallel the tensions in the larger field of HCI research, particularly between approaches that emphasize the need for representations and understandings of user experience that are precise, comparable, and generalizable, and third-wave approaches that emphasize the richness of situated actions, the inseparability of mind and body, and the contextual dependency of experiences. At the same time, there are tensions between the needs of industry for immediately useful and applicable techniques and methods, and academics' emphasis on verifiable, repeatable, and theoretically grounded work. In this panel, we bring together a number of these threads to discuss the necessity of designing for user experience. How can we connect the different threads of UX work, without erasing the differences between them? Is there any value in theory of UX, and if so, to whom? What actually works in designing for a user experience?"
"Near to the brain: Functional near-infrared spectroscopy as a lightweight brain imaging technique for visualization.  In order to better understand the user and visual interface, it is crucial to also understand human cognitive processes. Unfortunately,
these processes are traditionally difficult to monitor without the use
of cumbersome or expensive brain imaging equipment. In recent
years, functional near-infrared spectroscopy (fNIRS) has emerged
as a brain imaging technique that is both lightweight and easy to set
up. In this paper, we demonstrate the potential of fNIRS to examine
current visualization techniques and influence the design of visual
interfaces. To validate fNIRS as a tool for visualization research,
we present two studies based on previous work in brightness contrast in visual search and angle vs. position comparisons in form.
Our results indicate there are significant and unintuitive cognitive
differences in the prefrontal cortex during visual search tasks of
positive and negative contrast polarity. Furthermore, we are able
to differentiate between angle and position comparisons under specific experimental conditions. Finally, we outline the potential of
fNIRS to give objective, continuous, and near real-time feedback
of brain activity in future visualization research."
"Smart Phone Use by Non-Mobile Business Users.  The rapid increase in  smart phone capabilities  has introduced new opportunities for mobile information access and computing.  However,  smart phone use may still be constrained  by both device  affordances and work environments.  To understand how  current business users 
employ  smart phones and to identify opportunities for improving business smart phone use, we conducted two studies of actual and perceived performance of standard work tasks.  Our studies involved  243 smart phone users from a large corporation. We intentionally chose users who 
primarily work with desktops and laptops,  as  these  ??nonmobile?? users represent the largest population of business users. Our results go beyond the general intuition that smart phones are better for consuming than producing information: we provide  concrete measurements that show 
how fast reading is on phones and how much slower and more effortful text entry is on phones than on computers. We also demonstrate that security mechanisms are a significant barrier to wider  business smart phone use. We offer design suggestions to overcome these barriers."
"Accelerator Compiler for the VENICE Vector Processor.  This paper describes the compiler design for VENICE, a new
soft vector processor (SVP). The compiler is a new back-end
target for Microsoft Accelerator, a high-level data parallel
library for C++ and C#. This allows us to automatically
compile high-level programs into VENICE assembly code,
thus avoiding the process of writing assembly code used by
previous SVPs. Experimental results show the compiler can
generate scalable parallel code with execution times that are comparable to hand-written VENICE assembly code. On
data-parallel applications, VENICE at 100MHz on an Altera DE3 platform runs at speeds comparable to one core of
a 3.5GHz Intel Xeon W3690 processor, beating it in performance on four of six benchmarks by up to 3.2%."
"Cloud Data Protection for the Masses.  Offering strong data protection to cloud users while enabling rich applications is a challenging task. Researchers explore a new cloud platform architecture called Data Protection as a Service, which dramatically reduces the per-application development effort required to offer data protection, while still allowing rapid development and maintenance."
"Vanity or Privacy? Social Media as a Facilitator of Privacy and Trust.  In this position paper, we argue that social media
provides valuable support for the perception of one??s
self and others, and in doing so, supports privacy. In
addition we suggest that engagement, which reflects a
certain degree of trust, can be facilitated by social
information. We support our arguments with results
from a recent privacy survey and a study of social
annotations in search."
"Bubble-Up: Increasing Utilization In Modern Warehouse Scale Computers Via Sensible Co-Locations.  As much of the world??s computing continues to move into the cloud, the over-provisioning of computing resources to ensure the performance isolation of latency-sensitive tasks, such as web search, in modern datacenters is a major contributor to low machine utilization. Being unable to accurately predict performance degradation due to contention for shared resources on multicore systems has led to the heavy handed approach of simply disallowing the co-location of high-priority, latency-sensitive tasks with other tasks. Performing this precise prediction has been a challenging and unsolved problem. In this paper, we present Bubble-Up, a characterization methodology that enables the accurate prediction of the performance degradation that results from contention for shared resources in the memory subsystem. By using a bubble to apply a tunable amount of ??pressure?? to the memory subsystem on processors in production datacenters, our methodology can predict the performance interference between co-locate applications with an accuracy within 1% to 2% of the actual performance degradation. Using this methodology to arrive at ??sensible?? co-locations in Google??s
production datacenters with real-world large-scale applications, we can improve the utilization of a 500-machine cluster by 50% to 90% while guaranteeing a high quality of service of latency-sensitive applications."
"Heterogeneity in ??Homogeneous?? Warehouse-Scale Computers: A Performance Opportunity.  The class of modern datacenters recently coined as ??warehouse scale computers?? (WSCs) has traditionally been embraced as homogeneous computing platforms. However, due to frequent machine replacements and upgrades, modern WSCs are in fact composed of diverse commodity microarchitectures and machine configurations. Yet, current WSCs are designed with an assumption of homogeneity, leaving a potentially significant performance opportunity unexplored. In this paper, we investigate the key factors impacting the available heterogeneity in modern WSCs, and the benefit of exploiting this heterogeneity to maximize overall performance. We also introduce a new metric, opportunity factor, which can be used to quantify an application??s sensitivity to the heterogeneity in a given WSC. For applications that are sensitive to heterogeneity, we observe a performance improvement of up to 70% when employing our approach. In a WSC composed of state-of-the-art machines, we can improve the overall performance of the entire datacenter by 16% over the status quo."
"Deadline-Aware Datacenter TCP (D2TCP).  An important class of datacenter applications, called Online Data-Intensive (OLDI) applications, includes Web search, online retail, and advertisement. To achieve good user experience, OLDI applications operate under soft-real-time constraints (e.g., 300 ms latency) which imply deadlines for network communication within the applications. Further, OLDI applications typically employ tree-based algorithms which, in the common case, result in bursts of children-to-parent traffic with tight deadlines. Recent work on datacenter network protocols is either deadline-agnostic (DCTCP) or is deadline-aware (D3) but suffers under bursts due to race conditions. Further, D3 has the practical drawbacks of requiring changes to the switch hardware and not being able to coexist with legacy TCP.
We propose Deadline-Aware Datacenter TCP (D2TCP), a novel transport protocol, which handles bursts, is deadline-aware, and is readily deployable. In designing D2TCP, we make two contributions: (1) D2TCP uses a distributed and reactive approach for bandwidth allocation which fundamentally enables D2TCP??s properties. (2) D2TCP employs a novel congestion avoidance algorithm, which uses ECN feedback and deadlines to modulate the congestion window via a gamma-correction function. Using a small-scale implementation and at-scale simulations, we show that D2TCP reduces the fraction of missed deadlines compared to DCTCP and D3 by 75% and 50%, respectively."
"Distributed Discriminative Language Models for Google Voice Search.  This paper considers large-scale linear discriminative language models trained using a distributed perceptron algorithm. The algorithm is implemented efficiently using a
MapReduce/SSTable framework. This work also introduces the use of large amounts of unsupervised data (confidence filtered Google voice-search logs) in conjunction with a novel training procedure that regenerates word lattices for the given data with a weaker acoustic model than the one used to generate the unsupervised transcriptions for the logged data. We observe small but statistically significant improvements in recognition performance after reranking N-best lists of a standard Google voice-search data set."
"Science in the Cloud.  Scientific discovery is in transition from a focus on data collection
to an emphasis on analysis and prediction using large scale computation.
These computations can be done with unused cycles in commercial Clouds
if there is appropriate software support. Moving science into the
Cloud will promote data sharing and collaborations that will accelerate
scientific discovery in the 21st century."
"Robust Local Search for Solving RCPSP/max with Durational Uncertainty.  Scheduling problems in manufacturing, logistics and project management have frequently been modeled using the framework of Resource Constrained Project Scheduling Problems with minimum and maximum time lags (RCPSP/max). Due to the importance of these problems, providing scalable solution schedules for RCPSP/max problems is a topic of extensive research. However, all existing methods for solving RCPSP/max assume that durations of activities are known with certainty, an assumption that does not hold in real world scheduling problems where unexpected external events such as manpower availability, weather changes, etc. lead to delays or advances in completion of activities. Thus, in this paper, our focus is on providing a scalable method for solving RCPSP/max problems with durational uncertainty. To that end, we introduce the robust local search method consisting of three key ideas: (a) Introducing and studying the properties of two decision rule approximations used to compute start times of activities with respect to dynamic realizations of the durational uncertainty; (b) Deriving the expression for robust makespan of an execution strategy based on decision rule approximations; and (c) A robust local search mechanism to efficiently compute activity execution strategies that are robust against durational uncertainty. Furthermore, we also provide enhancements to local search that exploit temporal dependencies between activities. Our experimental results illustrate that robust local search is able to provide robust execution strategies efficiently."
"Large Scale Visual Semantic Extraction.  Image annotation is the task of providing textual semantic to new images, by ranking a large set of possible annotations according to how they correspond to a given image. In the large scale setting, there could be millions of images to process and hundreds of thousands of potential distinct annotations. In order to achieve such a task we propose to build a so-called ""embedding space"", into which both images and annotations can be automatically projected. In such a space, one can then find the nearest annotations to a given image, or annotations similar to a given annotation. One can even build a visio-semantic tree from these annotations, that corresponds to how concepts (annotations) are similar to each other with respect to their visual characteristics. Such a tree will be different from semantic-only trees, such as WordNet, which do not take into account the visual appearance of concepts."
"It's Our Research: Getting stakeholder buy-in for user experience research projects.  It's Our Research provides a strategic framework for people who practice UX research who wish to be heard by their stakeholders. It gives you the techniques needed to involve stakeholders throughout the process of planning, execution, analysis, and reporting UX research. Dramatically increase the chances that product managers, engineers, and management agree to do research and act upon its results."
"Impact Of Ranking Of Organic Search Results On The Incrementality Of Search Ads.  In an earlier study, we reported that on average 89% of the visits to the advertiser??s site from search ad clicks were incremental. In this research, we examine how the ranking of an advertiser??s organic listings on the search results page affects the incrementality of ad clicks expressed through Incremental Ad Clicks (IAC) and as estimated by Search Ads Pause models. A meta-analysis of 390 Search Ads Pause studies highlights the limited opportunity for clicks from organic search results to substitute for ad clicks when the ads are turned off.  On average, 81% of ad impressions and 66% of ad clicks occur in the absence of an associated organic search result.  We find that having an associated organic search result in rank one does not necessarily mean a low IAC.  On average, 50% of the ad clicks that occur with a top rank organic result are incremental, compared to 100% of the ad clicks being incremental in the absence of an associated organic result."
"Robust Trait Composition for JavaScript.  We introduce traits.js, a small, portable trait composition library for Javascript. Traits are a more robust alternative to multiple inheritance and enable object composition and reuse. traits.js is motivated by two goals: first, it is an experiment in using and extending Javascript's recently added meta-level object description format. By reusing this standard description format, traits.js can be made more interoperable with similar libraries, and even with built-in primitives. Second, traits.js makes it convenient to create ""high-integrity"" objects whose integrity cannot be violated by clients, an important property in the context of mash-ups composed from mutually suspicious scripts. We describe the design of traits.js and provide an operational semantics for TRAITS-JS, a minimal calculus that models the core functionality of the library."
"Traffic Anomaly Detection Based on the IP Size Distribution.  In this paper we present a data-driven framework for detecting machine-generated traffic based on the IP size, i.e., the number of users sharing the same source IP. Our main observation is that diverse machine-generated traffic attacks share a common characteristic: they induce an anomalous deviation from the expected IP size distribution. We develop a principled framework that automatically detects and classifies these deviations using statistical tests and ensemble learning. We evaluate our approach on a massive dataset collected at Google for 90 consecutive days. We argue that our approach combines desirable characteristics: it can accurately detect fraudulent machine-generated traffic; it is based on a fundamental characteristic of these attacks and is thus robust (e.g., to DHCP re-assignment) and hard to evade; it has low complexity and is easy to parallelize, making it suitable for large-scale detection; and finally, it does not entail profiling users, but leverages only aggregate statistics of network traffic."
"V-SMART-Join: A Scalable MapReduce Framework for All-Pair Similarity Joins of Multisets and Vectors.  This work proposes V-SMART-Join, a scalable MapReduce-based framework for discovering all pairs of similar entities. The V-SMART-Join framework is applicable to sets, multisets, and vectors. V-SMART-Join is motivated by the observed skew in the underlying distributions of Internet traffic, and is a family of 2-stage algorithms, where the first stage computes and joins the partial results, and the second stage computes the similarity exactly for all candidate pairs. The V-SMART-Join algorithms are very efficient and scalable in the number of entities, as well as their cardinalities. They were up to 30 times faster than the state of the art algorithm, VCL, when compared on a real dataset of a small size. We also established the scalability of the proposed algorithms by running them on a dataset of a realistic size, on which VCL never succeeded to finish. Experiments were run using real datasets of IPs and cookies, where each IP is represented as a multiset of cookies, and the goal is to discover similar IPs to identify Internet proxies."
"On the design of the ECMAScript Reflection API.  We describe in detail the new reflection API of the upcoming Javascript standard. The most prominent feature of this new API is its support for creating proxies: virtual objects that behave as regular objects, but whose entire ??meta-object protocol?? is implemented in Javascript itself. Next to a detailed description of the API, we describe a more general set of design principles that helped steer the API??s design, and which should be applicable to similar APIs for other languages. We also describe access control abstractions implemented in the new API, and provide an operational semantics of an extension of the untyped lambda-calculus featuring proxies."
"Shadow Removal for Aerial Imagery by Information Theoretic Intrinsic Image Analysis.  We present a novel technique for shadow removal based on an information theoretic approach to intrinsic image analysis. Our key observation is that any illumination change in the scene tends to increase the entropy of observed texture intensities. Similarly, the presence of texture in the scene increases the entropy of the illumination function. Consequently, we formulate the separation of an image into texture and illumination components as minimization of entropies of each component. We employ a non-parametric kernel-based quadratic entropy formulation, and present an efficient multi-scale iterative optimization algorithm for minimization of the resulting energy functional. Our technique may be employed either fully automatically, using a proposed learning based method for automatic initialization, or alternatively with small amount of user interaction. As we demonstrate, our method is particularly suitable for aerial images, which consist of either distinctive texture patterns, e.g. building facades, or soft shadows with large diffuse regions, e.g. cloud shadows."
"Calibration-Free Rolling Shutter Removal.  We present a novel algorithm for efficient removal of rolling shutter distortions in uncalibrated streaming videos. Our proposed method is calibration free as it does not need any knowledge of the camera used, nor does it require calibration using specially recorded calibration sequences. Our algorithm can perform rolling shutter removal under varying focal lengths, as in videos from CMOS cameras equipped with an optical zoom. We evaluate our approach across a broad range of cameras and video sequences demonstrating robustness, scaleability, and repeatability. We also conducted a user study, which demonstrates preference for the output of our algorithm over other state-of-the art methods. Our algorithm is computationally efficient, easy to parallelize, and robust to challenging artifacts introduced by various cameras with differing technologies."
"Topical clustering of search results.  Search results clustering (SRC) is a challenging algorithmic problem that requires grouping together the results returned by one or more search engines in topically coherent clusters, and labeling the clusters with meaningful phrases describing the topics of the results included in them. In this paper we propose to solve SRC via an innovative approach that consists of modeling the problem as the labeled clustering of the nodes of a newly introduced graph of topics. The topics are Wikipedia-pages identified by means of recently proposed topic annotators [9, 11, 16, 20] applied to the search results, and the edges denote the relatedness among these topics computed by taking into account the linkage of the Wikipedia-graph. We tackle this problem by designing a novel algorithm that exploits the spectral properties and the labels of that graph of topics. We show the superiority of our approach with respect to academic state-of-the-art work [6] and well-known commercial systems (CLUSTY and LINGO3G) by performing an extensive set of experiments on standard datasets and user studies via Amazon Mechanical Turk. We test several standard measures for evaluating the performance of all systems and show a relative improvement of up to 20%."
"High Performance, Low Cost, Colorless ONU for WDM-PON.  We give an overview of key technologies for realizing WDM-PON. In particular, we highlight promising developments and directions in widely tunable laser technologies for achieving a high performance, colorless ONU at the cost points required for access networks."
"Projecting Disk Usage Based on Historical Trends in a Cloud Environment.  Provisioning scarce resources among competing users and jobs remains one of the primary challenges of operating large-scale, distributed computing environments.  Distributed storage systems, in particular, typically rely on hard operator-set quotas to control disk allocation and enforce isolation for space and I/O bandwidth among disparate
users. However, users and operators are very poor at predicting future requirements and, as a result, tend to over-provision grossly. For three years, we collected detailed usage information for data stored in distributed filesystems in a large private cloud spanning dozens of clusters on multiple continents.  Specifically, we measured the disk space usage, I/O rate, and age of stored data for thousands of different engineering users and teams.  We find that although the
individual timeseries often have non-stable usage trends, regional aggregations, user classification, and ensemble forecasting methods can be combined to provide a more accurate prediction of future use for the majority of users. We applied this methodology for the storage users in one
geographic region and back-tested these techniques over the past three years to compare our forecasts against actual usage.  We find that by classifying a small subset of users with unforecastable trend changes due to known product launches, we can generate three-month out forecasts with mean absolute errors of less than ~12%.  This compares favorably to the amount of allocated but unused quota that is generally wasted with manual operator-set quotas."
"Music Models for Music-Speech Separation.  We consider the task of speech recognition with loud music background interference. We use model-based music-speech separation and train GMM models for music on the audio prior to speech. We show over 8% relative improvement in WER at 10 dB SNR for a real world Voice Search ASR system.
We investigate the relationship between ASR accuracy and the amount of music background used as prologue and the the size of music models. Our study shows that performance peaks when using a
music prologue of around 6 seconds to train the music model. We hypothesize that this is due to the dynamic nature of music and the structure of popular music. Adding more history beyond a certain point does not improve results. Additionally, we show moderately sized 8-component music GMM models suffice to model this amount of music prologue. Index Terms?? ASR, noise robustness, noise reduction, non-stationary noise, music"
"Mobile Music Modeling, Analysis and Recognition.  We present an analysis of music modeling and recognition techniques in the context of mobile music matching, substantially improving on the techniques presented in [Mohri et al., 2010]. We accomplish this by adapting the features specifically to this task, and by introducing new modeling techniques that enable using a corpus of noisy and channel-distorted data to improve mobile music recognition quality. We report the results of an extensive empirical investigation of the system's robustness under realistic channel effects and distortions. We show an improvement of recognition accuracy by explicit duration modeling of music phonemes and by integrating the expected noise environment into the training process. Finally, we propose the use of frame-to-phoneme alignment for high-level structure analysis of polyphonic music."
"Searching for Build Debt: Experiences Managing Technical Debt at Google.  With a large and rapidly changing codebase, Google software engineers are constantly paying interest on various forms of technical debt. Google engineers also make efforts to pay down that debt, whether through special Fixit days, or via dedicated teams, variously known as janitors, cultivators,
or demolition experts. We describe several related efforts to measure and pay down technical debt found in Google's BUILD files and associated dead code. We address debt found in dependency specifications, unbuildable targets, and 
unnecessary command line flags. These efforts often expose other forms of technical debt that must first be managed."
"Challenges in building large-scale information retrieval systems: invited talk.  Building and operating large-scale information retrieval systems used by hundreds of millions of people around the world provides a number of interesting challenges. Designing such systems requires making complex design tradeoffs in a number of dimensions, including (a) the number of user queries that must be handled per second and the response latency to these requests, (b) the number and size of various corpora that are searched, (c) the latency and frequency with which documents are updated or added to the corpora, and (d) the quality and cost of the ranking algorithms that are used for retrieval. In this talk I'll discuss the evolution of Google's hardware infrastructure and information retrieval systems and some of the design challenges that arise from ever-increasing demands in all of these dimensions. I'll also describe how we use various pieces of distributed systems infrastructure when building these retrieval systems. Finally, I'll describe some future challenges and open research problems in this area."
"Investigations on Exemplar-Based Features for Speech Recognition Towards Thousands of Hours of Unsupervised, Noisy Data.  The acoustic models in state-of-the-art speech recognition
systems are based on phones in context that are represented
by hidden Markov models. This modeling approach may be
limited in that it is hard to incorporate long-span acoustic
context. Exemplar-based approaches are an attractive alternative, in particular if massive data and computational power are available. Yet, most of the data at Google are unsupervised and noisy. This paper investigates an exemplar-based approach under this yet not well understood data regime. A log-linear rescoring framework is used to combine the exemplar-based features on the word level with the first-pass model. This approach guarantees at least baseline performance and focuses on the refined modeling of words with sufficient data. Experimental results for the Voice Search and the YouTube tasks are presented."
"Around the Water Cooler: Shared Discussion Topics and Contact Closeness in Social Search.  Search engines are now augmenting search results with social annotations, i.e., endorsements from users?? social network contacts. However, there is currently a dearth of published research on the effects of these annotations on user choice. This work investigates two research questions associated with annotations: 1) do some contacts affect user choice more than others, and 2) are annotations relevant across various information needs. We conduct a controlled experiment with 355 participants, using hypothetical searches and annotations, and elicit users?? choices. We find that domain contacts are preferred to close contacts, and this preference persists across a variety of information needs. Further, these contacts need not be experts and might be identified easily from conversation data."
"Understanding the Meta-Experience of Casual Games.  In this position paper, we argue that casual gamers can be segmented by ??meta-experiences?? into a typology that could inform game platform design. These meta- experiences include out-of-game immersion, social layering, and game discovery. We discuss the interviews and video diaries that have helped shape the typology."
"Recognition of Multilingual Speech in Mobile Applications.  We evaluate different architectures to recognize multilingual speech for real-time mobile applications. In particular, we show that combining the results of several recognizers greatly outperforms other solutions such as training a single large multilingual system or using an explicit language identification system to select the appropriate recognizer. Experiments are conducted on a trilingual English-French-Mandarin mobile speech task. The data set includes Google searches, Maps queries, as well as more general inputs such as email and short message dictation. Without pre-specifying the input language, the combined system achieves comparable accu- racy to that of the monolingual systems when the input language is known. The combined system is also roughly 5% absolute better than an explicit language identification approach, and 10% better than a single large multilingual system."
"Dagstuhl Seminar 09141: Web Application Security (Abstracts collection).  From 29th March to 3rd April 2009 the Dagstuhl Seminar
09141 Web Application Security was held in Schloss Dagstuhl -- Leibniz
Center for Informatics. During the seminar, several participants presented
their current research, and ongoing work and open problems were
discussed. Abstracts of the presentations given during the seminar are
put together in this paper. Links to full papers (if available) are provided
in the corresponding seminar summary document."
"Talking in Circles: Selective Sharing in Google+.  Online social networks have become indispensable tools for information sharing, but existing ??all-or-nothing?? models for sharing have made it difficult for users to target information to specific parts of their networks. In this paper, we study Google+, which enables users to selectively share content with specific ??Circles?? of people. Through a combination of log analysis with surveys and interviews, we investigate how active users organize and select audiences for shared content. We find that these users frequently engaged in selective sharing, creating circles to manage content across particular life facets, ties of varying strength, and interest-based groups. Motivations to share spanned personal and informational reasons, and users frequently weighed ??limiting?? factors (e.g. privacy, relevance, and social norms) against the desire to reach a large audience. Our work identifies implications for the design of selective sharing mechanisms in social networks."
"Vine Pruning for Efficient Multi-Pass Dependency Parsing.  Coarse-to-fine inference has been shown to be a robust approximate method for
improving the efficiency of structured prediction models while preserving their
accuracy. We propose a multi-pass coarse-to-fine architecture for dependency
parsing using linear-time vine pruning and structured prediction cascades.
Our first-, second-, and third-order models achieve accuracies comparable to
those of their unpruned counterparts, while exploring only a fraction of the
search space. We observe speed-ups of up to two orders of magnitude compared
to exhaustive search. Our pruned third-order model is twice as fast as an
unpruned first-order model and also compares favorably to a state-of-the-art
transition-based parser for multiple languages."
"A Web-Based Tool for Developing Multilingual Pronunciation Lexicons.  We present a web-based tool for generating and editing pronunciation lexicons in multiple languages. The tool is implemented as a web application on Google App Engine and can be accessed remotely from a web browser. The client application displays to users a textual prompt and interface that reconfigures based on language and task. It lets users generate pronunciations via constrained phoneme selection, which allows users with no special training to provide phonemic transcriptions efficiently and accurately."
"Google's Cross-Dialect Arabic Voice Search.  We present a large scale effort to build a commercial Automatic Speech Recognition (ASR) product for Arabic. Our goal is to support voice search, dictation, and voice control for the general Arabic-speaking public, including support for multiple Arabic dialects. We describe our ASR system design and compare recognizers for five Arabic dialects, with the potential to reach more than 125 million people in Egypt, Jordan, Lebanon, Saudi Arabia, and the United Arab Emirates (UAE). We compare systems built on diacritized vs. non-diacritized text. We also conduct cross-dialect experiments, where we train on one dialect and test on the others. Our average word error rate (WER) is 24.8% for voice search."
"Let's Parse to Prevent Pwnage.  Software that processes rich content suffers from endemic security vulnerabilities. Frequently, these bugs are due to data confusion: discrepancies in how content data is parsed, composed, and otherwise processed by different applications, frameworks, and language runtimes. Data confusion often enables code injection attacks, such as cross-site scripting or SQL injection, by leading to incorrect assumptions about the encodings and checks applied to rich content of uncertain provenance. However, even for well-structured, value-only content, data confusion can critically impact security, e.g., as shown by XML signature vulnerabilities [12]. This paper advocates the position that data confusion can be effectively prevented through the use of simple mechanisms??based on parsing??that eliminate ambiguities by fully resolving content data to normalized, clearly-understood forms. Using code injection on the Web as our motivation, we make the case that automatic defense mechanisms should be integrated with programming languages, application frameworks, and runtime libraries, and applied with little, or no, developer intervention. We outline a scalable, sustainable approach for developing and maintaining those mechanisms. The resulting tools can offer comprehensive protection against data confusion, even when multiple types of rich content data are processed and composed in complex ways."
"A Comparative Evaluation of Finger and Pen Stroke Gestures.  This paper reports an empirical investigation in which participants produced a set of stroke gestures with varying degrees of complexity and in different target sizes using both the finger and the pen. The recorded gestures were then analyzed according to multiple measures characterizing many aspects of stroke gestures. Our findings were as follows: (1) Finger drawn gestures were quite different to pen drawn gestures in basic measures including size ratio and average speed. Finger drawn gestures tended to be larger and faster than pen drawn gestures. They also differed in shape geometry as measured by, for example, aperture of closed gestures, corner shape distance and intersecting points deviation; (2) Pen drawn gestures and finger drawn gestures were similar in several measures including articulation time, indicative angle difference, axial symmetry and proportional shape distance; (3) There were interaction effects between gesture implement (finger vs. pen) and target gesture size and gesture complexity. Our findings show that half of the features we tested were performed well enough by the finger. This finding suggests that ""finger friendly"" systems should exploit these features when designing finger interfaces and avoid using the other features in which the finger does not perform as well as the pen."
"Understanding information preview in mobile email processing.  Browsing a collection of information on a mobile device is a common task, yet it can be difficult due to the small size of mobile displays. A common trade-off offered by many current mobile interfaces is to allow users to switch between an overview and detailed views of particular items. An open question is how much preview of each item to include in the overview. Using a mobile email processing task, we attempted to answer that question. We investigated participants' email processing behaviors under differing preview conditions in a semi-controlled, naturalistic study. We collected log data of participants' actual behaviors as well as their subjective impressions of different conditions. Our results suggest that a moderate level of two to three lines of preview should be the default. The overall benefit of a moderate amount of preview was supported by both positive subjective ratings and fewer transitions between the overview and individual items."
"D-Nets: Beyond Patch-Based Image Descriptors.  Despite much research on patch-based descriptors, SIFT remains the gold standard for finding correspondences across images and recent descriptors focus primarily on improving speed rather than accuracy. In this paper we propose Descriptor-Nets (D-Nets), a computationally efficient
method that significantly improves the accuracy of image matching by going beyond patch-based approaches. D-Nets constructs a network in which nodes correspond to traditional sparsely or densely sampled keypoints, and where image content is sampled from selected edges in this net. Not only is our proposed representation invariant to cropping, translation, scale, reflection and rotation, but it is also significantly more robust to severe perspective and non-linear distortions. We present several variants of our algorithm, including one that tunes itself to the image complexity and an efficient parallelized variant that employs a fixed grid. Comprehensive direct comparisons against SIFT and ORB on standard datasets demonstrate that D-Nets dominates existing approaches in terms of precision and recall while retaining computational efficiency."
"Model Recommendation for Action Recognition.  Simply choosing one model out of a large set of possibilities for a given vision task is a surprisingly difficult problem, especially if there is limited evaluation data with which to distinguish among models, such as when choosing the best ``walk'' action classifier from a large pool of classifiers tuned for different viewing angles, lighting conditions, and background clutter.  In this paper we suggest that this problem of selecting a good model can be recast as a recommendation problem, where the goal is to recommend a good model for a particular task based on how well a limited probe set of models appears to perform. Through this conceptual remapping, we can bring to bear all the collaborative filtering techniques developed for consumer recommender systems (e.g., Netflix, Amazon.com). We test this hypothesis on action recognition, and find that even when every model has been directly rated on a training set, recommendation finds better selections for the corresponding test set than the best performers on the training set."
"Smart Pricing Grows the Pie.  Some publisher advertising networks provide features intended to help advertisers bid more efficiently with a single bid in many publishers?? click auctions at once ?? Smart Pricing on the Google Display Network is one example. Typically such features involve discounting advertiser bids or prices for clicks on publisher websites according to how click values vary across sites (for some appropriate measure of advertiser value). Contrary to concerns that such features necessarily result in reduced publisher (and network) revenue we find that, in many simple cases, the modified auction dynamics produce rational incentives for advertisers to bid more ?? and spend more ?? than they would without the benefit of these features. So if advertisers act in their own interest then publishers and networks stand to make more revenue as well."
"RFC6583 - Operational Neighbor Discovery Problems.  In IPv4, subnets are generally small, made just large enough to cover the actual number of machines on the subnet.  In contrast, the default IPv6 subnet size is a /64, a number so large it covers trillions of addresses, the overwhelming number of which will be unassigned.  Consequently, simplistic implementations of Neighbor Discovery (ND) can be vulnerable to deliberate or accidental denial of service (DoS), whereby they attempt to perform address resolution for large numbers of unassigned addresses.  Such denial-of-service attacks can be launched intentionally (by an attacker) or result from legitimate operational tools or accident conditions.  As a result of these vulnerabilities, new devices may not be able to ""join"" a network, it may be impossible to establish new IPv6 flows, and existing IPv6 transported flows may be interrupted. This document describes the potential for DoS in detail and suggests possible implementation improvements as well as operational mitigation techniques that can, in some cases, be used to protect against or at least alleviate the impact of such attacks."
"Trickle: Rate Limiting YouTube Video Streaming.  YouTube traffic is bursty. These bursts trigger packet
losses and stress router queues, causing TCP??s
congestion-control algorithm to kick in. In this paper,
we introduce Trickle, a server-side mechanism that
uses TCP to rate limit YouTube video streaming. Trickle
paces the video stream by placing an upper bound on
TCP??s congestion window as a function of the streaming
rate and the round-trip time. We evaluated Trickle on
YouTube production data centers in Europe and India
and analyzed its impact on losses, bandwidth, RTT, and
video buffer under-run events. The results show that
Trickle reduces the average TCP loss rate by up to 43%
and the average RTT by up to 28% while maintaining
the streaming rate requested by the application."
"A Class-Based Agreement Model For Generating Accurately Inflected Translations.  When automatically translating from a weakly inflected source language like English to a target language with richer grammatical features such as gender and dual number, the output commonly contains morpho-syntactic agreement errors. To address this issue, we present a target-side, class-based agreement model. Agreement is promoted by scoring a sequence of fine-grained morpho-syntactic classes that are predicted during decoding for each translation hypothesis. For English-to-Arabic translation, our model yields a +1.04 BLEU average improvement over a state-of-the-art baseline. The model does not require bitext or phrase table annotations and can be easily implemented as a feature in many phrase-based decoders."
"Social Annotations in Web Search.  We ask how to best present social annotations on search results, and attempt to find an answer through mixed-method eye-tracking and interview experiments. Current practice is anchored on the assumption that faces and names draw attention; the same presentation format is used independently of the social connection strength and the search query topic. The key findings of our experiments indicate room for improvement. First, only certain social contacts are useful sources of information, depending on the search topic. Second, faces lose their well-documented power to draw attention when rendered small as part of a social search result annotation. Third, and perhaps most surprisingly, social annotations go largely unnoticed by users in general due to selective, structured visual parsing behaviors specific to search result pages. We conclude by recommending improvements to the design and content of social annotations to make them more noticeable and useful."
"Efficient Spatial Sampling of Large Geographical Tables.  Large-scale map visualization systems play an increasingly important role in presenting geographic datasets to end users. Since these datasets can be
extremely large, a map rendering system often needs to select a small
fraction of the data to visualize them in a limited space. This paper addresses the fundamental challenge of {\em thinning}:
determining appropriate samples of data to be shown on specific geographical
regions and zoom levels. Other than the sheer scale of the data, the thinning
problem is challenging because of a number of other reasons: (1) data can
consist of complex geographical shapes, (2) rendering of data needs to satisfy
certain constraints, such as data being preserved across zoom levels and
adjacent regions, and (3) after satisfying the constraints, an {\em optimal}
solution needs to be chosen based on {\em objectives} such as {\em
maximality}, {\em fairness}, and {\em importance} of data. This paper formally defines and presents a complete solution to the thinning
problem. First, we express the problem as an integer programming formulation
that efficiently solves thinning for desired objectives. Second, we
present more efficient solutions for maximality, based on DFS traversal of a
spatial tree. Third, we consider the common special case of point datasets,
and present an even more efficient randomized algorithm.  Finally, we have
implemented all techniques from this paper in Google Maps visualizations of
Fusion Tables, and we describe a set of experiments that demonstrate the
tradeoffs among the algorithms."
"Finding Related Tables.  We consider the problem of finding related tables in a large corpus of
heterogenous tables. Detecting related tables provides users a
powerful tool for enhancing their tables with additional data and
enables effective reuse of available public data. Our first
contribution is a framework that captures several types of relatedness,
including tables that are candidates for joins and tables that are
candidates for union. Our second contribution is a set of algorithms
for detecting related tables that can be either unioned or
joined.  We describe a set of experiments that demonstrate that our
algorithms produce highly related tables. We also show that
we can often improve the results of table search by
pulling up tables that are ranked much lower based on their
relatedness to top-ranked tables. Finally, we describe how to scale up
our algorithms and show the results of running it on a corpus of over
a million tables extracted from Wikipedia."
"Polyhedral clinching auctions and the adwords polytope.  A central issue in applying auction theory in practice is the problem of dealing with budget-constrained agents. A desirable goal in practice is to design incentive compatible, individually rational, and Pareto optimal auctions while respecting the budget constraints. Achieving this goal is particularly challenging in the presence of nontrivial combinatorial constraints over the set of feasible allocations. Toward this goal and motivated by AdWords auctions, we present an auction for polymatroidal environments satisfying these properties. Our auction employs a novel clinching technique with a clean geometric description and only needs an oracle access to the submodular function defining the polymatroid. As a result, this auction not only simplifies and generalizes all previous results, it applies to several new applications including AdWords Auctions, bandwidth markets, and video on demand. In particular, our characterization of the AdWords auction as polymatroidal constraints might be of independent interest. This allows us to design the first mechanism for Ad Auctions taking into account simultaneously budgets, multiple keywords and multiple slots. We show that it is impossible to extend this result to generic polyhedral constraints. This also implies an impossibility result for multiunit auctions with decreasing marginal utilities in the presence of budget constraints."
"Older adult??s perceptions of usefulness of personal health records.  Electronic personal health records (PHRs) have the potential to both make health information more accessible to patients and function as a decision-support system for patients managing chronic conditions. Age-related changes in cognition may make traditional strategies of integrating and understanding existing (i.e., paper-based) health information more difficult for older adults. The centralized and integrated nature of health information, as well as the long-term tracking capabilities present in many PHRs, may be especially beneficial for older patients?? management of health. However, older adults tend to be late adopters of technology and may be hesitant to adopt a PHR if the benefits are not made clear (perceived usefulness). Toward the design of a useful PHR, a needs analysis was conducted to determine how people currently manage their health information, what they perceive as useful, and to identify any unmet needs. This paper describes two qualitative studies examining the health information needs of both younger and older adults. The first study used a 2-week diary methodology to examine everyday health questions or concerns, while the second study examined maintenance of health information and perceptions of PHRs through the use of a three-part interview. User??s perceptions of the usefulness of PHRs are provided as recommendations for the design of e-health technology, especially those targeted for older adult healthcare consumers. The results suggest that both older and younger adults would deem a PHR useful if it provides memory support in the form of reminders, provides tools to aid in comprehension of one??s health concerns, is interactive and provides automatic functions, and is highly accessible to authorized users, yet one??s information is kept secure and private."
"Understanding Tablet Use: A Multi-Method Exploration.  Tablet ownership has grown rapidly over the last year. While market research surveys have helped us understand the demographics of tablet ownership and provided early insights into usage, there is little comprehensive research available. This paper describes a multi-method research effort that employed written and video diaries, in-home interviews, and contextual inquiry observations to learn about tablet use across three locations in the US. Our research provides an in-depth picture of frequent tablet activities (e.g., checking emails, playing games, social networking), locations of use (e.g., couch, bed, table), and contextual factors (e.g., watching TV, eating, cooking). It also contributes an understanding of why and how people choose to use tablets. Popular activities for tablet use, such as media consumption, shopping, cooking, and productivity are also explored. The findings from our research provide design implications and opportunities for enriching the tablet experience, and agendas for future research."
"Machine learning: a probabilistic perspective.  Today??s Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, using a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package--PMTK (probabilistic modeling toolkit)--that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students."
"Open Problem: Better Bounds for Online Logistic Regression.  Known algorithms applied to online logistic regression on a feasible set of L2 diameter D achieve regret bounds like O(e<sup>D</sup> log T) in one dimension, but we show a bound of O(sqrt(D) + log T) is possible in a binary 1-dimensional problem.  Thus, we pose the   following question: Is it possible to achieve a regret bound for online logistic regression that is O(poly(D)log(T))?  Even if this is not possible in general, it would be interesting to have a bound that reduces to our bound in the one-dimensional case."
"Unsupervised Translation Sense Clustering.  We propose an unsupervised method for clustering the translations of a word, such that the translations in each cluster share a common semantic sense. Words are assigned to clusters based on their usage distribution in large monolingual and parallel corpora using the soft K-Means algorithm. In addition to describing our approach, we formalize the task of translation sense clustering and describe a procedure that leverages WordNet for evaluation. By comparing our induced clusters to reference clusters generated from WordNet, we demonstrate that our method effectively identifies sense-based translation clusters and benefits from both monolingual and parallel corpora. Finally, we describe a method for annotating clusters with usage examples."
"On the Difficulty of Nearest Neighbor Search.  Fast approximate nearest neighbor (NN) search in large databases is becoming popular and several powerful learning-based formulations have been proposed recently. However, not much attention has been paid to a more fundamental question: how difficult is (approximate) nearest neighbor search in a given data set? And which data properties affect the difficulty of nearest neighbor search and how? This paper introduces the first concrete measure called Relative Contrast that can be used to evaluate the influence of several crucial data characteristics such as dimensionality, sparsity, and database size simultaneously in arbitrary normed metric spaces. Moreover, we present a theoretical analysis to show how relative contrast affects the complexity of Local Sensitive Hashing, a popular approximate NN search method. Relative contrast also provides an explanation for a family of heuristic hashing algorithms with good practical performance based on PCA. Finally, we show that most of the previous works measuring meaningfulness or difficulty of NN search can be derived as special asymptotic cases for dense vectors of the proposed
measure."
"Compact Hyperplane Hashing with Bilinear Functions.  Hyperplane hashing aims at rapidly searching nearest points to a hyperplane, and has shown practical impact in scaling up active learning with SVMs. Unfortunately, the existing randomized methods need long hash codes to achieve reasonable search accuracy and thus suffer from reduced search speed and large memory overhead. To this end, this paper proposes a novel hyperplane hashing technique which yields compact hash codes. The key idea is the bilinear form of the proposed hash functions, which leads to higher collision probability than the existing hyperplane hash functions when using random projections. To further increase the performance, we propose a learning based framework in which the bilinear functions are directly learned from the data. This results in short yet discriminative codes, and also boosts the search performance over the random projection based solutions. Large-scale active learning experiments carried out on two datasets with up to one million samples demonstrate the overall superiority of
the proposed approach."
"Are privacy concerns a turn-off? Engagement and privacy in social networks.  We describe the survey results from a representative sample of 1,075 U.S. social network users who use Facebook as their primary network. Our results show a strong association between low engagement and privacy concern. Specifically, users who report concerns around sharing control, comprehension of sharing practices or general Facebook privacy concern, also report consistently less time spent as well as less (self-reported) posting, commenting and ??Like??ing of content. The limited evidence of other significant differences between engaged users and others suggests that privacy-related concerns may be an important gate to engagement. Indeed, privacy concern and network size are the only malleable attributes that we find to have significant association with engagement. We manually categorize the privacy concerns finding that many are nonspecific and not associated with negative personal experiences. Finally, we identify some education and utility issues associated with low social network activity, suggesting avenues for increasing engagement amongst current users."
"Team Geek: A Software Developer's Guide to Working Well with Others.  As a software engineer, you??re great with computer languages, compilers, debuggers, and algorithms. And in a perfect world, those who produce the best code are the most successful. But in our perfectly messy world, success also depends on how you work with people to get your job done. In this highly entertaining book, Brian Fitzpatrick and Ben Collins-Sussman cover basic patterns and anti-patterns for working with other people, teams, and users while trying to develop software. It??s valuable information from two respected software engineers whose popular video series, ""Working with Poisonous People"", has attracted hundreds of thousands of viewers. You??ll learn how to deal with imperfect people??those irrational and unpredictable beings??in the course of your work. And you??ll discover why playing well with others is at least as important as having great technical skills. By internalizing the techniques in this book, you??ll get more software written, be more influential, be happier in your career."
"Large-scale Discriminative Language Model Reranking for Voice Search.  We present a distributed framework for large-scale discriminative language models that can be integrated within a large vocabulary continuous speech recognition (LVCSR) system using lattice rescoring. We intentionally use a weakened acoustic model in a baseline LVCSR system to generate candidate hypotheses for voice-search data; this allows us to utilize large amounts of unsupervised data to train our models. We propose an efficient and scalable MapReduce framework that uses a perceptron-style distributed training strategy to handle these large amounts of data. We report small but significant improvements in recognition accuracies on a standard voice-search data set using our discriminative reranking model. We also provide an analysis of the various parameters of our models including model size, types of features, size of partitions in the MapReduce framework with the help of supporting experiments."
"Google's Hybrid Approach to Research.  In this viewpoint, we describe how we organize computer science research at Google. We focus on how we integrate research and development and discuss the benefits and risks of our approach."
"Look Who I Found: Understanding the Effects of Sharing Curated Friend Groups.  Online social networks like Google+, Twitter, and Facebook allow users to build, organize, and manage their social connections for the purposes of information sharing and consumption. Nonetheless, most social network users still report that building and curating contact groups is a time-consuming burden. To help users overcome the burdens of contact discovery and grouping, Google+ recently launched a new feature known as ""circle sharing"". The feature makes it easy for users to share the benefits of their own contact curation by sharing entire ""circles"" (contact groups) with others. Recipients of a shared circle can adopt the circle as a whole, merge the circle into one of their own circles, or select specific members of the circle to add. In this paper, we investigate the impact that circle-sharing has had on the growth and structure of the Google+ social network. Using a cluster analysis, we identify two natural categories of shared circles, which represent two qualitatively different use cases: circles comprised primarily of celebrities (celebrity circles), and circles comprised of members of a community (community circles). We observe that exposure to circle-sharing accelerates the rate at which a user adds others to his or her circles. More specifically, we notice that circle-sharing has accelerated the ""densification"" rate of community circles, and also that it has disproportionately affected users with few connections, allowing them to find new contacts at a faster rate than would be expected based on accepted models of network growth. Finally, we identify features that can be used to predict which of a user??s circles (s)he is most likely to share, thus demonstrating that it is feasible to suggest to a user which circles to share with friends."
"LatLong: Diagnosing Wide-Area Latency Changes for CDNs.  Minimizing user-perceived latency is crucial for Content Distribution Networks (CDNs) hosting interactive services.  Latency may increase for many reasons, such as interdomain routing changes and the CDN's own load-balancing policies.  CDNs need greater visibility into the causes of latency increases, so they can adapt by directing traffic to different servers or paths. In this paper, we propose techniques for CDNs to diagnose large latency increases, based on passive measurements of performance, traffic, and routing.  Separating the many causes from the effects is challenging.  We propose a decision tree for classifying
latency changes, and determine how to distinguish traffic shifts from increases in latency for existing servers, routers, and paths.  Another challenge is that network operators group related clients to reduce measurement and control overhead, but the clients in a region may use multiple servers and paths during a measurement interval.  We propose metrics that quantify the latency contributions 
across sets of servers and routers.  Analyzing a month of data from Google's CDN, we find that nearly 1% of the
daily latency changes increase delay by more than 100 msec.  More than 40% of these increases coincide with interdomain routing changes, and more than one-third involve a shift in traffic to different servers.  This is the first work to diagnose latency problems in a large, operational CDN from purely passive measurements.  Through case studies of individual events, we identify research challenges for measuring and managing wide-area latency for CDNs."
"A Feature-Rich Constituent Context Model for Grammar Induction.  We present LLCCM, a log-linear variant of the constituent context model (CCM) of grammar induction. LLCCM retains the simplicity of the original CCM but extends robustly to long sentences. On sentences of up to length 40, LLCCM outperforms CCM by 13.9% brack- eting F1 and outperforms a right-branching baseline in regimes where CCM does not."
"Building Useful Program Analysis Tools Using an Extensible Java Compiler.  Large software companies need customized tools to manage their source code. These tools are often built in an ad-hoc fashion, using brittle technologies such as regular expressions and home-grown parsers. Changes in the language
cause the tools to break. More importantly, these ad-hoc tools often do not support uncommon-but-valid code code patterns. We report our experiences building source-code analysis tools at Google on top of a third-party, open-source, extensible compiler. We describe three tools in use on our Java codebase. The first, Strict Java Dependencies, enforces our dependency policy in order to reduce JAR file sizes and testing load. The second, error-prone, adds new error checks to the compilation process and automates repair of those errors at a whole-codebase scale. The third, Thindex, reduces the indexing burden for a Java IDE so that it can support Google-sized projects."
"A Systematic Comparison of Phrase Table Pruning Techniques.  When trained on very large parallel corpora, the phrase table component of a machine translation system grows to consume vast computational resources. In this paper, we introduce a novel pruning criterion that places phrase table pruning on a sound theoretical foundation. Systematic experiments on four language pairs under various data conditions show that our principled approach is superior to existing ad hoc pruning methods."
"Controlling Complexity in Part-of-Speech Induction.  We consider the problem of fully unsupervised learning of grammatical (part-of-speech) categories from unlabeled text. The standard maximum-likelihood hidden Markov model for this task performs poorly, because of its weak inductive bias and large model capacity. We address this problem by refining the model and modifying the learning objective to control its capacity via para- metric and non-parametric constraints. Our approach enforces word-category association sparsity, adds morphological and orthographic features, and eliminates hard-to-estimate parameters for rare words. We develop an efficient learning algorithm that is not much more computationally intensive than standard training. We also provide an open-source implementation of the algorithm. Our experiments on five diverse languages (Bulgarian, Danish, English, Portuguese, Spanish) achieve significant improvements compared with previous methods for the same task."
"CDE: A Tool For Creating Portable Experimental Software Packages.  One technical barrier to reproducible computational science is that it's hard to distribute scientific code in a form that other researchers can easily execute on their own computers. To help eliminate this barrier, the CDE tool packages all software dependencies required to rerun Linux-based computational experiments on other computers."
"The role of visual complexity and prototypicality regarding first impression of websites: Working towards understanding aesthetic judgments.  This paper experimentally investigates the role of visual complexity (VC) and pro- totypicality (PT) as design factors of websites, shaping users?? first impressions by means of two studies. In the first study, 119 screenshots of real websites varying in VC (low vs. medium vs. high) and PT (low vs. high) were rated on perceived aes- thetics. Screenshot presentation time was varied as a between-subject factor (50 ms vs. 500 ms vs. 1000 ms). Results reveal that VC and PT affect participants?? aesthet- ics ratings within the first 50 ms of exposure. In the second study presentation times were shortened to 17, 33 and 50ms. Results suggest that VC and PT affect aesthetic perception even within 17ms, though the effect of PT is less pronounced than the one of VC. With increasing presentation time the effect of PT becomes as influential as the VC effect. This supports the reasoning of the information-processing stage model of aesthetic processing (Leder et al., 2004), where VC is processed at an earlier stage than PT. Overall, websites with low VC and high PT were perceived as highly appealing."
"Norovirus Disease Surveillance Using Google Internet Query Share Data.  Google Internet query share (IQS) data for gastroenteritis-related search terms correlated strongly with contemporaneous national (R2 = 0.70) and regional (R2 = 0.74) norovirus surveillance data in the United States. IQS data may facilitate rapid identification of norovirus season onset, elevated peak activity, and potential emergence of novel strains."
"Mathematics at Google.  There is a wide variety of Mathematics used at Google. For example Linear Algebra in the PageRank algorithm, used to rank web pages in search results. Or Game Theory, used in ad auctions, or Graph Theory in Google Maps. At Google there are literally dozens of products which use interesting Mathematics. These are not just research prototypes, but real Google products; in which Mathematics play a crucial role. In this presentation, I introduce several applications of Mathematics at Google. I begin with a detailed explanation of search on the web and PageRank. Then I show a dozen examples of Google products and the corresponding Mathematics that are used. The presentation has an extensive list of links and references. And it's available in English and Spanish."
"Incremental Clicks: The Impact of Search Advertising.  In this research, the authors examined how the number of organic clicks changed when search ads were present and when search ad campaigns were turned off. The authors developed a statistical model to estimate the fraction of total clicks that could be attributed to search advertising. A meta-analysis of several hundred of these studies revealed that more than 89 percent of the ads clicks were incremental, in the sense that those visits to the advertiser's site would not have occurred without the ad campaigns."
"Web-Scale Multi-Task Feature Selection for Behavioral Targeting.  A typical behavioral targeting system optimizing purchase
activities, called conversions, faces two main challenges: the web-scale amounts of user histories to process on a daily basis, and the relative sparsity of conversions. In this paper, we try to address these challenges through feature selection. We formulate a multi-task (or group) feature-selection problem among a set of related tasks (sharing a common set of features), namely advertising campaigns. We apply a group-sparse penalty consisting of a combination of an <code>1 and</code>2 penalty and an associated fast optimization algorithm for distributed parameter estimation. Our algorithm relies on a variant of the well known Fast Iterative Thresholding Algorithm (FISTA), a closed-form solution for mixed norm programming and a distributed subgradient oracle. To eciently handle web-scale user histories, we present a distributed inference algorithm for the problem that scales to billions of instances and millions of attributes. We show the superiority of our algorithm in terms of both sparsity and ROC performance over baseline feature selection methods (both single-task L1-regularization and multi-task mutual-information gain)."
"Measuring Ad Effectiveness Using Geo Experiments.  Advertisers have a fundamental need to quantify the effectiveness of their advertising. For search ad spend, this information provides a basis for formulating strategies related to bidding, budgeting, and campaign design. One approach that Google has successfully employed to measure advertising effectiveness is geo experiments. In these experiments, non-overlapping geographic regions are randomly assigned to a control or treatment condition, and each region realizes its assigned condition through the use of geo-targeted advertising. This paper describes the application of geo experiments and demon- strates that they are conceptually simple, have a systematic and effective design process, and provide results that are easy to interpret."
"Periodic Measurement of Advertising Effectiveness Using Multiple-Test-Period Geo Experiments.  In a previous paper [6] we described the application of geo experiments to the measurement of advertising effectiveness. One reason this method of measurement is attractive is that it provides the rigor of a randomized experiment. However, related decisions, such as where and how to spend advertising budget, are not static. To address this issue, we extend this methodology to provide periodic (ongoing) measurement of ad effectiveness. In this approach, the test and control assignments of each geographic region rotate across multiple test periods, and these rotations provide the opportunity to generate a sequence of measurements of campaign effectiveness. The data across test periods can also be pooled to create a single aggregate measurement of campaign effectiveness. These sequential and pooled measurements have smaller confidence intervals than measurements from a series of geo experiments with a single test period. Alternatively, the same confidence interval can be achieved with a reduced magnitude and/or duration of ad spend change, thereby lowering the cost of measurement. The net result is a better method for periodic and isolated measurement of ad effectiveness."
"Origin-Bound Certificates: A Fresh Approach to Strong Client Authentication for the Web.  Client authentication on the web has remained in the internet-equivalent of the stone ages for the last two decades. Instead of adopting modern public-key-based authentication mechanisms, we seem to be stuck with passwords and cookies. In this paper, we propose to break this stalemate by presenting a fresh approach to public-key-based client authentication on the web. We describe a simple TLS extension that allows clients to establish strong authenti- cated channels with servers and to bind existing authen- tication tokens like HTTP cookies to such channels. This allows much of the existing infrastructure of the web to remain unchanged, while at the same time strengthening client authentication considerably against a wide range of attacks. We implemented our system in Google Chrome and Google??s web serving infrastructure, and provide a per- formance evaluation of this implementation."
"Protecting Browsers from Extension Vulnerabilities.  Browser extensions are remarkably popular, with one in three Firefox users running at least one extension. Although well-intentioned, extension developers are often not security experts and write buggy code that can be exploited by malicious web site operators. In the Firefox extension system, these exploits are dangerous because extensions run with the user's full privileges and can read and write arbitrary files and launch new processes. In this paper, we analyze 25 popular Firefox extensions and find that 88% of these extensions need less than the full set of available privileges. Additionally, we find that 76% of these extensions use unnecessarily powerful APIs, making it difficult to reduce their privileges. We propose a new browser extension system that improves security by using least privilege, privilege separation, and strong isolation. Our system limits the misdeeds an attacker can perform through an extension vulnerability. Our design has been adopted as the Google Chrome extension system."
"Real-Time Human Pose Tracking from Range Data.  Tracking human pose in real-time is a difficult problem with many interesting applications. Existing solutions suffer from a variety of problems, especially when confronted with unusual human poses. In this paper, we derive an algorithm for tracking human pose in real-time from depth sequences based on MAP inference in a probabilistic temporal model. The key idea is to extend the iterative closest points (ICP) objective by modeling the constraint that the observed subject cannot enter free space, the area of space in front of the true range measurements. Our primary contribution is an extension to the articulated ICP algorithm that can efficiently enforce this constraint. Our experiments show that including this term improves tracking accuracy significantly. The resulting filter runs at 125 frames per second using a single desktop CPU core. We provide extensive experimental results on challenging real-world data, which show that the algorithm outperforms the previous state-of the-art trackers both in computational efficiency and accuracy."
"Silicon Photonics for Optical Access Networks.  We highlight promising developments and directions in silicon photonics for realizing cost effective WDM-PON: photonic integration for integrated WDM transceivers at the OLT and widely tunable laser technologies for achieving a high performance, colorless ONU."
"The Dangers of Composing Anonymous Channels.  We present traffic analyses of two anonymous communications schemes that build on the classic Crowds/Hordes protocols. The AJSS10 [1] scheme combines multiple Crowds-like forward channels with a Hordes reply channel in an attempt to offer robustness in a mobile environment. We show that the resulting scheme fails to guarantee the claimed k-anonymity, and is in fact more vulnerable to malicious peers than Hordes, while suffering from higher latency. Similarly, the RWS11 [15] scheme invokes multiple instances of Crowds to provide receiver anonymity. We demonstrate that the sender anonymity of the scheme is susceptible to a variant of the predecessor attack [21], while receiver anonymity is fully compromised with an active attack. We conclude that the heuristic security claims of AJSS10 and RWS11 do not hold, and argue that composition of multiple anonymity channels can in fact weaken overall security. In contrast, we provide a rigorous security analysis of Hordes under the same threat model, and reflect on design principles for future anonymous channels to make them amenable to such security analysis."
"Bridging communications and the physical world.  Sense Everything, Control Everything (SECE) is an event-driven system that lets nontechnical users create services that combine communication, location, social networks, presence, calendaring, and physical devices such as sensors and actuators. SECE combines information from multiple sources to personalize services and adapt them to changes in the user's context and preferences. Events trigger associated actions, which can control email delivery, change how phone calls are handled, update the user's social network status, and set the state of actuators such as lights, thermostats, and electrical appliances."
"Reconstructing the World's Museums.  Photorealistic maps are a useful navigational guide for large indoor environments, such as museums and businesses. However, it is impossible to acquire photographs covering a large indoor environment from aerial viewpoints. This paper presents a 3D reconstruction and visualization system to automatically produce clean and well-regularized texture-mapped 3D models for large indoor scenes, from ground-level photographs and 3D laser points. The key component is a new algorithm called ""Inverse CSG"" for reconstructing a scene in a Constructive Solid Geometry (CSG) representation consisting of volumetric primitives, which imposes powerful regularization constraints to exploit structural regularities. We also propose several techniques to adjust the 3D model to make it suitable for rendering the 3D maps from aerial viewpoints. The visualization system enables users to easily browse a large scale indoor environment from a bird's-eye view, locate specific room interiors, fly into a place of interest, view immersive ground-level panorama views, and zoom out again, all with seamless 3D transitions. We demonstrate our system on various museums, including the Metropolitan Museum of Art in New York City -- one of the largest art galleries in the world."
"Managing Distributed UPS Energy for Effective Power Capping in Data Centers.  Power over-subscription can reduce costs for modern data centers. However, designing the power infrastructure for a lower operating power point than the aggregated peak power of all servers requires dynamic techniques to avoid high peak power costs and, even worse, tripping circuit breakers. This work presents an architecture for distributed per-server UPSs that stores energy during low activity periods and uses this energy during power spikes. This work leverages the distributed nature of the UPS batteries and develops policies that prolong the duration of their usage. The specific approach shaves 19.4% of the peak power for modern servers, at no cost in performance, allowing the installation of 24% more servers within the same power budget. More servers amortize infrastructure costs better and, hence, reduce total cost of ownership per server by 6.3%."
"Usage Patterns in an Urban WiFi Network.  While WiFi was initially designed as a local-area access network, mesh networking technologies have led to increasingly expansive deployments of WiFi networks. In urban environments, the WiFi mesh frequently supplements a number of existing access technologies, including wired broadband networks, 3G cellular, and commercial WiFi hotspots. It is an open question what role citywide WiFi deployments play in the increasingly diverse access network spectrum. We study the usage of the Google WiFi network deployed in Mountain View, CA, and find that usage naturally falls into three classes based almost entirely on client device type, which we divide into traditional laptop users, fixed-location access devices, and PDA-like smartphone devices. Moreover, each of these classes of use has significant geographic locality, following the distribution of residential, commercial, and transportation areas of the city. When comparing the network usage of each device class, we find a diverse set of mobility patterns that map well to the archetypal use cases for traditional access technologies. To help place our results in context, we also provide key performance measurements of the mesh backbone and, where possible, compare them to those of previously studied urban mesh networks."
"Life Estimation of Pressurized-Air Solar-Thermal Receiver Tubes.  The operational conditions of the solar thermal receiver for a Brayton-cycle engine are challenging, and lack a large body of operational data unlike steam plants. We explore the receiver??s fundamental element, a pressurized tube in time varying solar flux for a series of 30 year service missions based on hypothetical power plant designs. We developed and compared two estimation methods to predict the receiver tube lifetime based on available creep life and fatigue data for alloy 617. We show that the choice of inelastic strain model and the level of conservatism applied through design rules will vary the lifetime predictions by orders of magnitude. Based on current data and methods, a turbine inlet temperature of 850 C is a necessary 30-year-life-design condition for our receiver. We also showed that even though the time at operating temperature is about three times longer for fossil fuel powered (steady) operation, the damage is always lower than cyclic operation using solar power ."
"Algorithmic Thermodynamics.  Algorithmic entropy can be seen as a special case of entropy as studied in statistical mechanics. This viewpoint allows us to apply many techniques developed for use in thermodynamics to the subject of algorithmic information theory. In particular, suppose we fix a universal prefix-free Turing machine and let X be the set of programs that halt for this machine. Then we can regard X as a set of 'microstates', and treat any function on X as an 'observable'. For any collection of observables, we can study the Gibbs ensemble that maximizes entropy subject to constraints on expected values of these observables. We illustrate this by taking the log runtime, length, and output of a program as observables analogous to the energy E, volume V and number of molecules N in a container of gas. The conjugate variables of these observables allow us to define quantities which we call the 'algorithmic temperature' T, 'algorithmic pressure' P and algorithmic potential' mu, since they are analogous to the temperature, pressure and chemical potential. We derive an analogue of the fundamental thermodynamic relation dE = T dS - P d V + mu dN, and use it to study thermodynamic cycles analogous to those for heat engines. We also investigate the values of T, P and mu for which the partition function converges. At some points on the boundary of this domain of convergence, the partition function becomes uncomputable. Indeed, at these points the partition function itself has nontrivial algorithmic entropy."
"Assessing a New Advertising Effect: Measurement of the Impact of Television Commercials on Internet Search Queries.  Most Americans today use both television and the Internet on a daily basis, and studies have shown that many are frequently online or in proximity of a computer while they are watching television. One result of these multi-platform media use patterns is a new television advertising effect: Today??s consumer can easily obtain more information on an advertised product by searching for more information on the Web. This article demonstrates the measurement of such an effect by introducing a new metric??a measure of changes in Google search queries??that can show how TV commercials or sponsorships can trigger Internet searches by consumers. We believe this metric is a valuable addition to the researcher??s toolkit for assessing advertising effects and regions of interest as it measures an actual behavioral advertising response."
"A Fault Detection and Protection Scheme for Three-Level DC???DC Converters Based on Monitoring Flying Capacitor Voltage.  Fault detection and protection is an important design aspect for any power converter, especially in high-power high-voltage applications, where cost of failure can be high. The three-level dc-dc converter and its varied derivatives are attractive topologies in high-voltage high-power converter applications. The protection method can not only prevent the system failure against unbalanced voltage stresses on the switches, but also provide a remedy for the system as faults occur and save the remaining components. The three-level converter is subject to voltage unbalance in certain abnormal conditions, which can result in switch overvoltage and system failure. The reasons for the unbalanced voltage stresses are fully investigated and categorized. The solutions to each abnormal condition are introduced. In addition to the voltage unbalance, the three-level converters can be protected against multiple faults by the proposed protection method through monitoring the flying capacitor voltage. Phenomena associated with each fault are thoroughly analyzed and summarized. The protection circuit is simple and can be easily implemented, while it can effectively protect the three-level converters and its derivatives, which has been verified by the experiment with a three-level parallel resonant converter."
"CrowdForge: Crowdsourcing Complex Work.  Micro-task markets such as Amazon??s Mechanical Turk represent a new paradigm for accomplishing work, in which employers can tap into a large population of workers around the globe to accomplish tasks in a fraction of the time and money of more traditional methods. However, such markets have been primarily used for simple, independent tasks, such as labeling an image or judging the relevance of a search result. Here we present a general purpose framework for accomplishing complex and interdependent tasks using micro-task markets. We describe our framework, a web-based prototype, and case studies on article writing, decision making, and science journalism that demonstrate the benefits and limitations of the approach."
"The multi-iterative closest point tracker: An online algorithm for tracking multiple interacting targets.  We describe and evaluate a greedy detection-based algorithm for tracking a variable number of dynamic targets online. The algorithm leverages the well-known iterative closest point (ICP) algorithm for aligning target models with target detections. The approach differs from trackers that seek globally optimal solutions because it treats the problem as a set of individual tracking problems. The method works for multiple targets by sequentially matching models to detections, and then removing detections from further consideration once models have been matched to them. This allows targets to pass close to one another with reduced risks of tracking failure due to ??hijacking,'' or track merging. There has been significant previous work in this area, but we believe our approach addresses a number of tracking problems simultaneously that have only been addressed separately before. The algorithm is evaluated using four to eight laser range finders in three settings: quantitatively for a basketball game with 10 people and a 25-person social behavior experiment, and qualitatively for a full-scale soccer game. We also provide qualitative results using video to track ants in a captive habitat. During all the experiments, agents enter and leave the scene, so the number of targets to track varies with time. With eight laser range finders running, the system can locate and track targets at sensor frame rate 37.5 Hz on commodity computing hardware. Our evaluation shows that the tracking system correctly detects each track over 98% of the time."
"Data-driven network connectivity.  Routing on the Internet combines data plane mechanisms for forwarding traffic with control plane protocols for guaranteeing connectivity and optimizing routes (e.g., shortest-paths and load distribution). We propose data-driven connectivity (DDC), a new routing approach that achieves the fundamental connectivity guarantees in the data plane rather than the control plane, while keeping the more complex requirements of route optimization in the control plane. DDC enables faster recovery from failures and easier implementation of control plane optimization."
"Spectral Intersections for Non-Stationary Signal Separation.  We describe a new method for non-stationary noise suppression that is simple to implement yet has performance rivaling far more complex algorithms. 
Spectral Intersections is a model based MMSE signal separation method that uses a new simple approximation to the observation likelihood. Furthermore, Spectral Intersections uses an efficient approximation to the expectation integral of the MMSE estimate that could be described as unscented importance sampling.
We apply the new method to the task of separating speech mixed with music. We report results on the Google Voice Search task where the new method provides a 7% relative reduction in WER at 10dB SNR. Interestingly, the new method provides considerably greater reduction in average WER than the MAX method and approaches the performance of the more complex Algonquin algorithm."
"Time, topic and trawl: stories about how we reach our past.  Legacy web tools attempt to build on information that uses have when they originally conduct web research. In contrast, we examine the information that they have at the time when they attempt to recreate their past. We interviewed 11 non-expert users twice a week for eight weeks in their own physical and computational environments. We used both Google web histories and the prototype Research Trails system as prompts to probe how the participants viewed their past web experiences and how they reconstructed them. The Research Trails system lets users utilize information about both time and topic to help themselves remember and resume everyday research tasks. Based on these observations, a model of users' perceived past web activities informed the iterative refinement of the Research Trails system. The user may see a past action as belonging to multiple categories at the same time or as in different categories at different time"
"Accurate Online Power Estimation and Automatic Battery Behavior Based Power Model Generation for Smartphones.  This paper describes PowerBooter, an automated power
model construction technique that uses built-in battery voltage sensors and knowledge of battery discharge behavior to monitor power consumption while explicitly controlling the power management and activity states of individual components. It requires no external measurement equipment.
We also describe PowerTutor, a component power management and activity state introspection based tool that uses
the model generated by PowerBooter for online power estimation. PowerBooter is intended to make it quick and easy for application developers and end users to generate power models for new smartphone variants, which each have different power consumption properties and therefore require different power models. PowerTutor is intended to ease the
design and selection of power efficient software for embedded systems. Combined, PowerBooter and PowerTutor have
the goal of opening power modeling and analysis for more
smartphone variants and their users."
"Portable and Performant Userspace SCTP Stack.  One of only two new transport protocols introduced in the last 30 years is the Stream Control Transmission Protocol (SCTP). SCTP enables capabilities like additional throughput and fault tolerance for multihomed hosts. An SCTP implementation is included with the Linux kernel and another implementation called sctplib functions successfully in userspace on several platforms but unfortunately neither of these implementations have all of the latest features nor do they perform as well as the FreeBSD kernel implementation of SCTP. We were motivated to produce a portable implementation of the FreeBSD kernel SCTP stack that operates in userspace of any system because of both our desires to obtain a higher performance SCTP stack for Linux as well as to exploit recent developments in hardware virtualization and transport protocol onloading. Unlike any other userspace transport implementation for TCP or SCTP, our userspace SCTP stack simultaneously achieves similar throughput and latency as the Linux kernel TCP stack, without compromising on any of the transport's features as well as maintaining true portability across multiple operating systems and devices. We create a callback API and implement a threshold to control its usage; our userspace SCTP stack with these optimizations obtains higher throughput than the Linux kernel implementation of SCTP. We describe our userspace SCTP stack's design and demonstrate how it gives similar throughput and latency on Linux as the kernel TCP implementation, with the benefits of the new features of SCTP."
"A rational deconstruction of Landin's SECD machine with the J operator.  Landin's SECD machine was the first abstract machine for applicative expressions, ie, functional programs. Landin's J operator was the first control operator for functional languages, and was specified by an extension of the SECD machine. We present a family of evaluation functions corresponding to this extension of the SECD machine, using a series of elementary transformations (transformation into continu-ation-passing style (CPS) and defunctionalization, chiefly) and their left inverses (transformation into direct style and refunctionalization). To this end, we modernize the SECD machine into a bisimilar one that operates in lockstep with the original one but that (1) does not use a data stack and (2) uses the caller-save rather than the callee-save convention for environments. We also identify that the dump component of the SECD machine is managed in a callee-save way. The caller-save counterpart of the modernized SECD machine precisely corresponds to Thielecke's double-barrelled continuations and to Felleisen's encoding of J in terms of call/cc. We then variously characterize the J operator in terms of CPS and in terms of delimited-control operators in the CPS hierarchy. As a byproduct, we also present several reduction semantics for applicative expressions with the J operator, based on Curien's original calculus of explicit substitutions. These reduction semantics mechanically correspond to the modernized versions of the SECD machine and to the best of our knowledge, they provide the first syntactic theories of applicative expressions with the J operator. The present work is concluded by a motivated wish to see Landin's name added to the list of co-discoverers of continuations. Methodologically, however, it mainly illustrates the value of Reynolds's defunctionalization and of refunctionalization as well as the expressive power of the CPS hierarchy (1) to account for the first control operator and the first abstract machine for functional languages and (2) to connect them to their successors. Our work also illustrates the value of Danvy and Nielsen's refocusing technique to connect environment-based abstract machines and syntactic theories in the form of reduction semantics for calculi of explicit substitutions."
"Optimistic Scheduling with Geographically Replicated Services in the Cloud Environment (COLOR).  This paper proposes a system model that unifies different optimistic algorithms designed for deploying geographically replicated services in a cloud environment. The proposed model thereby enables a generalized solution (COLOR) by which well-specified safety and timeliness guarantees are achievable in conjunction with tunable performance requirements. The proposed solution explicitly takes advantage of the unique client-cloud interface in specifying how the level of consistency violation may be bounded, for instance using probabilistic rollbacks or restarts as parameters. The solution differs from traditional Eventual Consistency models in that inconsistency is solved concurrently with online client-cloud interactions over strongly connected networks. We believe that such an approach will bring clarity to the role and limitations of the ever-popular Eventual Consistency model in cloud services."
"Video Description Length Guided Constant Quality Video Coding with Bitrate Constraint.  In this paper, we propose a new video encoding strategy - Video description length guided Constant Quality video coding with Bitrate Constraint (V-CQBC), for large scale video transcoding systems of video charing websites with varying unknown video contents. It provides smooth quality and saves bitrate and computation for transcoding millions of videos in both real time and batch mode. The new encoding strategy is based on the average bitrate-quality regression model and adapt to the encoded videos. Furthermore, three types of video description length (VDL), describing the video overall, spatial and temporal content complexity, are proposed to guide video coding. Experimental results show that the proposed coding strategy with saved computation could achieve better or similar RD performance than other coding strategies."
"The Shoebox and the Safe: When Once-Personal Information Changes Hands.  This paper presents several examples where one user??s personal information is accessed by another, without the consent of the owner, or without the capability of the owner to consent to such sharing. While intentional sharing of information at home as well as at work has been studied in detail, there is extremely limited understanding about the practices, dimensions and models of unintentional sharing. Laws and policies that were developed with paper and other nondigital archives in mind are being found to be inadequate for addressing the challenges that digital personal information brings. Worse, those laws are being enforced in inconsistent ways, prompting lawsuits. Posthumously shared information brings up questions that have not been addressed before. This paper starts by noting examples of posthumous sharing and sharing without consent, proposes models and dimensions for understanding it, and concludes by proposing research questions that need to be addressed by the wider PIM community."
"Embedded Voxel Colouring with Adaptive Threshold Selection Using Globally Minimal Surfaces.  Image-based 3D reconstruction remains a competitive field of research as state-of-the-art algorithms continue to improve. This paper presents a voxel-based algorithm that adapts the earliest space-carving methods and utilises a minimal surface technique to obtain a cleaner result. Embedded Voxel Colouring is built in two stages: (a) progressive voxel carving is used to build a volume of embedded surfaces and (b) the volume is processed to obtain a surface that maximises photo-consistency data in the volume. This algorithm combines the strengths of classical carving techniques with those of minimal surface approaches. We require only a single pass through the voxel volume, this significantly reduces computation time and is the key to the speed of our approach. We also specify three requirements for volumetric reconstruction: monotonic carving order, causality of carving and water-tightness. Experimental results are presented that demonstrate the strengths of this approach."
"Google's C/C++ toolchain for smart handheld devices.  Smart handheld devices are ubiquitous today and software plays an important role on them. Therefore a compiler and related tools can improve devices by generating efficient, compact and secure code. In this paper, we share our experience of applying various compilation techniques at Google to improve software running on smart handheld devices, using our mobile platforms as examples. At Google we use the GNU toolchain for generating code on different platforms and for conducting compiler research and development. We have developed new techniques, added features and functionality in the GNU tools. Some of these results are now used for smart handheld devices."
"On inter-deriving small-step and big-step semantics: A case study for storeless call-by-need evaluation.  Starting from the standard call-by-need reduction for the ??-calculus that is common to Ariola, Felleisen, Maraist, Odersky, and Wadler, we inter-derive a series of hygienic semantic artifacts: a reduction-free storeless abstract machine, a continuation-passing evaluation function, and what appears to be the first heapless natural semantics for call-by-need evaluation. Furthermore we observe that the evaluation function implementing this natural semantics is in defunctionalized form. The refunctionalized counterpart of this evaluation function implements an extended direct semantics in the sense of Cartwright and Felleisen. Overall, the semantic artifacts presented here are simpler than many other such artifacts that have been independently worked out, and which require ingenuity, skill, and independent soundness proofs on a case-by-case basis. They are also simpler to inter-derive because the inter-derivational tools (e.g., refocusing and defunctionalization) already exist."
"Unsupervised Learning for Graph Matching.  Graph matching is an essential problem in computer vision that has been successfully applied to 2D and 3D feature matching and object recognition.  Despite its importance, little has been published on learning the parameters that control graph matching, even though learning has been shown to be vital for improving the matching rate.  In this paper, we show how to perform parameter learning in an unsupervised fashion, that is when no correct correspondences between graphs are given during training.  Our experiments reveal that unsupervised learning compares favorably to the supervised case, both in terms of efficiency and quality, while avoiding the tedious manual labeling of ground truth correspondences.  We verify experimentally that our learning method can improve the performance of several state-of-the-art matching algorithms.  We also show that a similar method can be successfully applied to parameter learning for graphical models and demonstrate its effectiveness empirically."
"Feature Seeding for Action Recognition.  Progress in action recognition has been in large part due to advances in the features that drive learning-based methods.  However, the relative sparsity of training data and the risk of overfitting have made it difficult to directly search for good features.  In this paper, we suggest using synthetic data to search for robust features that can more easily take advantage of limited data, rather than using the synthetic data directly as a substitute for real data.  We demonstrate that the features discovered by our selection method, which we call seeding, improve performance on an action classification task on real data, even though the synthetic data from which our features are seeded differs significantly from the real data, both in terms of appearance and the set of action classes."
"Online Matching with Stochastic Rewards.  The online matching problem has received significant attention in recent years because of its connections to allocation problems in Internet advertising, crowd-sourcing, etc. In these real-world applications, the typical goal is not to maximize the number of allocations, rather it is to maximize the number of successful allocations, where success of an allocation is governed by a stochastic process which follows the allocation. To address such applications, we propose and study the online matching problem with stochastic rewards (called the Online Stochastic Matching problem) in this paper. Our problem also has close connections to the existing literature on stochastic packing problems, in fact, our work initiates the study of online stochastic packing problems. We give a deterministic algorithm for the Online Stochastic Matching problem whose competitive ratio converges to (approximately) 0.567 for uniform and vanishing probabilities. We also give a randomized algorithm which outperforms the deterministic algorithm for higher probabilities. Finally, we complement our algorithms by giving an upper bound on the competitive ratio of any algorithm for this problem. This result shows that the best achievable competitive ratio for the Online Stochastic Matching problem is provably worse than that for the (non-stochastic) online matching problem."
"Resource-bounded multicore emulation using Beefarm.  In this article, we present the Beefarm infrastructure for FPGA-based multiprocessor emulation, a popular research topic of the last few years both in FPGA and computer architecture communities. We explain how we modify and extend a MIPS-based open-source soft core, we discuss various design tradeoffs to make efficient use of the bounded resources available on chip and we demonstrate superior scalability compared to traditional software instruction set simulators through experimental results running Software Transactional Memory (STM) benchmarks. Based on our experience, we comment on the pros and cons and the future trends of using hardware-based emulation for multicore research."
"Language Modeling for Automatic Speech Recognition Meets the Web: Google Search by Voice.  A critical component of a speech recognition system targeting web search is the language model. The talk presents an empirical exploration of the google.com query stream with the end goal of high quality statistical language modeling for mobile voice search. Our experiments show that after text normalization the query
stream is not as ``wild'' as it seems at first sight. One can achieve out-of-vocabulary rates below 1% using a one million word vocabulary, and excellent n-gram hit ratios of 77/88% even at high orders such as n=5/4, respectively.
Using large scale, distributed language models can improve performance significantly---up to 10\% relative reductions in word-error-rate over conventional models used in speech recognition. We also find that the query stream is non-stationary, which means that adding more past training data beyond a certain point provides diminishing returns, and may even degrade performance slightly. Perhaps less surprisingly, we have shown that locale matters significantly for English query data across USA, Great Britain and Australia. In an attempt to leverage the speech data in voice search logs, we successfully build large-scale discriminative N-gram language models and derive small but significant gains in recognition performance."
"LIL: CLOS reaches higher-order, sheds identity, and has a transformative experience.  LIL, the Lisp Interface Library, is a data structure library based on Interface-Passing Style. This programming style was designed to allow for parametric polymorphism (abstracting over types, classes, functions, data) as well as ad-hoc polymorphism (incremental development with inheritance and mixins). It consists in isolating algorithmic information into first-class interfaces, explicitly passed around as arguments dispatched upon by generic functions. As compared to traditional objects, these interfaces typically lack identity and state, while they manipulate data structures without intrinsic behavior. This style makes it just as easy to use pure functional persistent data structures without identity or state as to use stateful imperative ephemeral data structures. Judicious Lisp macros allow developers to avoid boilerplate and to abstract away interface objects to expose classic-looking Lisp APIs. Using on a very simple linear type system to model the side-effects of methods, it is even possible to transform pure interfaces into stateful interfaces or the other way around, or to transform a stateful interface into a traditional object-oriented API."
A practical comparison of the bivariate probit and linear IV estimators.  This paper compares asymptotic and finite sample properties of linear IV and bivariate probit in models with an endogenous binary treatment and binary outcome. The results provide guidance on the choice of model specification and help to explain large differences in the estimates depending on the specification chosen.
"Managing Global UX Teams.  In this interactive workshop, a group of experts from industry will discuss emerging issues and unique challenges related to managing global user experience teams, and how these differ from other disciplines such as marketing, sales, engineering etc."
"E Unum Pluribus - Google Network Filtering Management.  Network filtering can be a very difficult challenge in large, complex and sprawling networks.  Through the use of internally developed software, Google has automated and simplified many of the difficult tasks and provided the capability to easily audit and validate its filters.  This talk will discuss our efforts in this area and release some of these tools to the community."
"A Guided Tour of Datacenter Networking.  The  magic of the cloud is that it is always on and always available from anywhere. Users have come to expect that services are there when they need them. A data center (or warehouse-scale computer) is the nexus from which all the services flow. It is often housed in a nondescript warehouse-sized building bearing no indication of what lies inside. Amidst the whirring fans and refrigerator-sized computer racks is a tapestry of electrical cables and fiber optics weaving everything together??the data-center network. This 
article provides a ??guided tour?? through the principles and central ideas surrounding the network at the heart of a data center??the modern-day loom that weaves the digital fabric of the Internet."
"Understanding Visualization: A Formal Approach using Category Theory and Semiotics.  This article combines the vocabulary of semiotics and category theory to provide a formal analysis of visualization. It shows how familiar processes of visualization fit the semiotic frameworks of both Saussure and Peirce, and extends these structures using the tools of category theory to provide a general framework for understanding visualization in practice, including: relationships between systems, data collected from those systems, renderings of those data in the form of representations, the reading of those representations to create visualizations, and the use of those visualizations to create knowledge and understanding of the system under inspection. The resulting framework is validated by demonstrating how familiar information visualization concepts (such as literalness, sensitivity, redundancy, ambiguity, generalizability, and chart junk) arise naturally from it and can be defined formally and precisely. This article generalizes previous work on the formal characterization of visualization by, inter alia, Ziemkiewicz and Kosara and allows us to formally distinguish properties of the visualization process that previous work does not."
"Backtracking Events as Indicators of Usability Problems in Creation-Oriented Applications.  A diversity of user goals and strategies make creation-oriented applications such as word processors or photo-editors difficult to comprehensively test. Evaluating such applications requires testing a large pool of participants to capture the diversity of experience, but traditional usability testing can be prohibitively expensive. To address this problem, this article contributes a new usability evaluation method called backtracking analysis, designed to automate the process of detecting and characterizing usability problems in creation-oriented applications. The key insight is that interaction breakdowns in creation-oriented applications often manifest themselves in backtracking operations that can be automatically logged (e.g., undo and erase operations). Backtracking analysis synchronizes these events to contextual data such as screen capture video, helping the evaluator to characterize specific usability problems. The results from three experiments demonstrate that backtracking events can be effective indicators of usability problems in creation-oriented applications, and can yield a cost-effective alternative to traditional laboratory usability testing."
"The Incremental Reach and Cost Efficiency of Online Video Ads over TV Ads.  As people spend more time online, an increasing number of brand marketers are including video ads in their advertising campaigns. These advertisers would like to know the incremental reach and cost efficiency of their video and display ads compared to their TV ads. In this paper, we measure the incremental reach to a target demographic and estimate the cost per incremental reach point of YouTube (YT) and the Google Display Network (GDN) compared to TV ad campaigns. We consider two media planning scenarios: what it would have cost for the TV ad campaign to have delivered the equivalent of the online incremental reach, and what saving could have been achieved by having spent less on TV ads and complementing them with online ads for a given reach goal."
"Clinching Auctions with Online Supply.  Auctions for perishable goods such as internet ad inventory need to make real-time allocation and pricing decisions as the supply of the good arrives in an online manner, without knowing the entire supply in advance. These allocation and pricing decisions get complicated when buyers have some global constraints. In this work, we consider a multi-unit model where buyers have global {\em budget} constraints, and the supply arrives in an online manner. Our main contribution is to show that for this setting there is an individually-rational, incentive-compatible and Pareto-optimal auction that allocates these units and calculates prices on the fly, without knowledge of the total supply. We do so by showing that the Adaptive Clinching Auction satisfies a {\em supply-monotonicity} property. 
We also analyze and discuss, using examples, how the insights gained by the allocation and payment rule can be applied to design better ad allocation heuristics in practice. Finally, while our main technical result concerns multi-unit supply, we propose a formal model of online supply that captures scenarios beyond multi-unit supply and has applications to sponsored search. We conjecture that our results for multi-unit auctions can be extended to these more general models."
"Processing a Trillion Cells per Mouse Click.  Column-oriented database systems have been a real game changer for the industry in recent years. Highly tuned and performant systems have evolved that provide users with the possibility of answering ad hoc queries over large datasets in an interactive manner.
In this paper we present the column-oriented datastore developed as one of the central components of PowerDrill. It combines the advantages of columnar data layout with other known techniques (such as using composite range partitions) and extensive algorithmic engineering on key data structures. The main goal of the latter being to reduce the main memory footprint and to increase the efficiency in processing typical user queries. In this combination we achieve large speed-ups. These enable a highly interactive Web UI where it is common that a single mouse click leads to processing a trillion values in the underlying dataset."
"Focused Marix Factorization for Audience Selection in Display Advertising.  Audience selection is a key problem in display advertising systems in which we need to select a list of users who are interested (i.e., most likely to buy) in an advertising campaign. The users?? past feedback on this campaign can be leveraged to construct such a list using collaborative filtering techniques such as matrix factorization. However, the user-campaign interaction is typically extremely sparse, hence the conventional matrix factorization does not perform well. Moreover, simply combining the users feedback from all campaigns does not address this since it dilutes the focus on target campaign in consideration. To resolve these issues, we propose a novel focused matrix factorization model (FMF) which learns users?? preferences towards the specific campaign products, while also exploiting the information about related products. We exploit the product taxonomy to discover related campaigns, and design models to discriminate between the users?? interest towards campaign products and non-campaign products. We develop a parallel multi-core implementation of the FMF model and evaluate its performance over a real-world advertising dataset spanning more than a million products. Our experiments demonstrate the benefits of using our models over existing approaches."
"Latent Factor Models with Additive Hierarchically-smoothed User Preferences.  Items in recommender systems are usually associated with annotated attributes such as brand and price for products; agency for news articles, etc. These attributes are highly informative and must be exploited for accurate recommendation. While learning a user preference model over these attributes can result in an interpretable recommender system and can hands the cold start problem, it suffers from two major drawbacks: data sparsity and the inability to model random effects. On the other hand, latent-factor collaborative filtering models have shown great promise in recommender systems; however, its performance on rare items is poor. In this paper we propose a novel model LFUM, which provides the advantages of both of the above models. We learn user preferences (over the attributes) using a personalized Bayesian hierarchical model that uses a combination (additive model) of a globally learned preference model along with user-specific preferences. To combat  we smooth these preferences over the item-taxonomy  an efficient forward-filtering and backward-smoothing  algorithm. Our inference algorithms can handle both  attributes (e.g., item brands) and continuous attributes (e.g.,  prices). We combine the user preferences with the latent- models and train the resulting collaborative filtering system end- using the successful BPR ranking algorithm. In our  experimental analysis, we show that our proposed model  several commonly used baselines and we carry out an ablation study showing the benefits of each component of our model."
"Large Scale Language Modeling in Automatic Speech Recognition.  Large language models have been proven quite beneficial for a variety of automatic speech recognition tasks in Google. We summarize results on Voice Search and a few YouTube speech transcription tasks to highlight the impact that one can expect from increasing both the amount of training data, and the size of the language model estimated from such data. Depending on the task, availability and amount of training data used, language model size and amount of work and care put into integrating them in the lattice rescoring step we observe reductions in word error rate between 6% and 10% relative, for systems on a wide range of operating points between 17% and 52% word error rate."
"Minimizing weighted flowtime on capacitated machines.  It is well-known that SRPT is optimal for minimizing
flow time on machines that run one job at a time.
However, running one job at a time is a big under-
utilization for modern systems where sharing, simultane-
ous execution, and virtualization-enabled consolidation
are a common trend to boost utilization. Such machines,
used in modern large data centers and clouds, are
powerful enough to run multiple jobs/VMs at a time
subject to overall CPU, memory, network, and disk
capacity constraints. Motivated by this prominent trend and need, in this
work, we give the first scheduling algorithms to minimize
weighted flow time on such capacitated machines. To
capture the difficulty of the problem, we show that
without resource augmentation, no online algorithm can
achieve a bounded competitive ratio. We then investigate
algorithms with a small resource augmentation in speed
and/or capacity. Our first result is a simple (2 + ??)-
capacity O(1/??)-competitive greedy algorithm. Using
only speed augmentation, we then obtain a 1.75-speed
O(1)-competitive algorithm. Our main technical result
is a near-optimal (1 + ??)-speed, (1 + ??)-capacity O(1/??3 )-
competitive algorithm using a novel combination of
knapsacks, densities, job classification into categories,
and potential function methods. We show that our
results also extend to the multiple unrelated capacitated
machines setting."
"The Future of Computing Performance: Game Over or Next Level?.  The end of dramatic exponential growth in single-processor performance marks the end of the dominance of the single microprocessor in computing. The era of sequential computing must give way to a new era in which parallelism is at the forefront. Although important scientific and engineering challenges lie ahead, this is an opportune time for innovation in programming systems and computing architectures. We have already begun to see diversity in computer designs to optimize for such considerations as power and throughput. The next generation of discoveries is likely to require advances at both the hardware and software levels of computing systems. There is no guarantee that we can make parallel computing as common and easy to use as yesterday's sequential single-processor computer systems, but unless we aggressively pursue efforts suggested by the recommendations in this book, it will be ""game over"" for growth in computing performance. If parallel programming and related software efforts fail to become widespread, the development of exciting new applications that drive the computer industry will stall; if such innovation stalls, many other parts of the economy will follow suit. The Future of Computing Performance describes the factors that have led to the future limitations on growth for single processors that are based on complementary metal oxide semiconductor (CMOS) technology. It explores challenges inherent in parallel computing and architecture, including ever-increasing power consumption and the escalated requirements for heat dissipation. The book delineates a research, practice, and education agenda to help overcome these challenges. The Future of Computing Performance will guide researchers, manufacturers, and information technology professionals in the right direction for sustainable growth in computer performance, so that we may all enjoy the next level of benefits to society."
"Programming Interviews Exposed.  Landing a great programming job isn't a matter of luck; it's a matter of being prepared for the unique challenges of the technical job search. Programming interviews require a different set of skills than day-to-day programming, so even expert programmers often struggle if they don't know what to expect. This thoroughly revised and expanded third edition teaches you the skills you need to apply your programming expertise to the types of problems most frequently encountered in interviews at top tech companies today. Step-by-step solutions to an extensive set of sample interview questions simulate the interview experience to hone the skills you've learned. After you've worked through this book, you'll approach your interviews with confidence, knowing you can solve any problem that stands between you and the job you really want."
"No-Regret Algorithms for Unconstrained Online Convex Optimization.  Some of the most compelling applications of online convex
optimization, including online prediction and classification, are
unconstrained: the natural feasible set is R^n.  Existing
algorithms fail to achieve sub-linear regret in this setting unless
constraints on the comparator point x<em> are known in advance.  We
present algorithms that, without such prior knowledge, offer
near-optimal regret bounds with respect to any choice of
x</em>.  In particular, regret with respect to x* = 0 is
constant.  We then prove lower bounds showing that our
guarantees are near-optimal in this setting."
"Large Scale Distributed Deep Networks.  Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm."
"MEASURING NOISE CORRELATION FOR IMPROVED VIDEO DENOISING.  The vast majority of previous work in noise reduction for visual media has assumed uncorrelated, white, noise sources. In practice this is almost always violated by real media. Film grain noise is never white, and this paper highlights that the same applies to almost all consumer video content. We therefore present an algorithm for measuring the spatial and temporal spectral density of noise in archived video content, be it consumer digital camera or film orginated. As an example of how this information can be used for video denoising, the spectral density is then used for spatio-temporal noise reduction in the Fourier frequency domain. Results show improved performance for noise reduction in an easily pipelined system."
"Reading, Laughing, and Connecting with Young Children.  In this chapter, we report on three projects that focus on storybook reading as a way to improve distance communication with very young children. ??Connected Reading?? builds on the insight that communication technologies for families with young children need to focus on play rather than conversations, and that having a shared activity can help structure this play. Our prototypes span a range of embodiments, from mobile video conferencing with physical books, to eBooks, and finally to video conferencing enhanced with depth camera technology. Our findings suggest guidelines to improve family communication with young children."
"Latent Collaborative Retrieval.  Retrieval tasks typically require a ranking of items given a query. Collaborative filtering tasks, on the other hand, learn models comparing users with items. In this paper we study the joint problem of recommending items to a user with respect to a given query, which is a surprisingly common task. This setup differs from the standard collaborative filtering one in that we are given a query ?? user ?? item tensor for training instead of the more traditional user ?? item matrix. Compared to document retrieval we do have a query, but we may or may not have content features (we will consider both cases) and we can also take account of the user??s profile. We introduce a factorized model for this new task that optimizes the top ranked items returned for the given query and user. We report empirical results where it outperforms several baselines."
"An Integrated Framework for Spatio-Temporal-Textual Search and Mining.  This paper presents an integrated framework for Spatio-Temporal-Textual (STT) information retrieval and knowledge discovery system. The proposed ensemble framework contains an efficient STT search engine with multiple indexing, ranking and scoring schemes, an effective STT pattern miner with Spatio-Temporal (ST) analytics, and novel STT topic modeling. Specifically, we design an effective prediction prototype with a third-order linear regression model, and present an innovative STT topic modeling relevance ranker to score documents based on inherent STT features under topical space. We demonstrate the framework with a crime dataset from the Washington, DC area from 2006 to 2010 and a global terrorism dataset from 2004 to 2010."
"Perspective Probe.  This case study describes a variation of cultural, 
technology, and other probes, called a ??perspective 
probe.?? The perspective probe consisted of multiple 
activities that participants completed on their own and 
then discussed with the researcher. The participant??s 
responses to the individual activities added up to their 
whole perspective. The probe??s activities helped guide 
the conversation around a sensitive topic instead of 
asking directly about it. 
This paper illustrates how the perspective probe 
methodology was used to gather information for Google 
Finance. The focus is on the method rather than the 
particular findings from the study. The perspective 
probe methodology was useful in getting rich data from 
participants and building a holistic understanding of the 
participant??s perspective on a difficult topic, in this case 
money and investing."
"Empowering Online Advertisements by Empowering Viewers with the Right to Choose.  In 2010, YouTube introduced TrueView in-stream advertising??online video advertisements that allowed the user to skip directly to the desired video content after five seconds of 
viewing. Google sought to compare these ??skippable?? in-stream advertisements to the conventional (non-skippable) in-stream video advertising formats, using a new advertising 
effectiveness metric based on the propensity to search for terms related to advertising content. Google??s findings indicated that skippable video advertisements may be as 
effective on a per-impression basis as traditional video advertisements. In addition, data from randomized experiments showed a strong implied viewer preference for the skippable advertisements. Taken together, these results suggest that formats like TrueView in-stream advertisements can improve the viewing experience for users without sacrificing advertising value for advertisers or content owners."
"Online Python Tutor: Embeddable Web-Based Program Visualization for CS Education.  This paper presents Online Python Tutor, a web-based program visualization tool for Python, which is becoming a popular language for teaching introductory CS courses. Using this tool, teachers and students can write Python programs directly in the web browser (without installing any plugins), step forwards and backwards through execution to view the run-time state of data structures, and share their program visualizations on the web. In the past three years, over 200,000 people have used Online Python Tutor to visualize their programs. In addition, instructors in a dozen universities such as UC Berkeley, MIT, the University of Washington, and the University of Waterloo have used it in their CS1 courses. Finally, Online Python Tutor visualizations have been embedded within three web-based digital Python textbook projects, which collectively attract around 16,000 viewers per month and are being used in at least 25 universities. Online Python Tutor is free and open source software, available at pythontutor.com"
"Magda: A New Language for Modularity.  We introduce Magda, a modularity-oriented programming language.
The language features lightweight mixins as units of code reuse, modular initial-
ization protocols, and a hygienic approach to identiers. In particular, Magda's
modularity guarantees that client code of a library written in Magda will never
break as a consequence of any addition of members to the library's mixins."
"Building Rome in a day.  We present a system that can reconstruct 3D geometry from large, unorganized collections of photographs such as those found by searching for a given city (e.g., Rome) on Internet photo-sharing sites. Our system is built on a set of new, distributed computer vision algorithms for image matching and 3D reconstruction, designed to maximize parallelism at each stage of the pipeline and to scale gracefully with both the size of the problem and the amount of available computation. Our experimental results demonstrate that it is now possible to reconstruct city-scale image collections with more than a hundred thousand images in less than a day."
"A QCQP Approach to Triangulation.  Triangulation of a three-dimensional point from n &gt;=2 two-dimensional images can be formulated as a quadratically constrained quadratic program. We propose an algorithm to extract candidate solutions to this problem from its semidefinite programming relaxations. We then describe a sucient condition and a polynomial time test for certifying when such a solution is optimal. This test has no false positives. Experiments indicate that false negatives are rare, and the algorithm has excellent performance in practice. We explain this phenomenon in terms of the geometry of the triangulation problem."
"Towards A Unified Modeling and Verification of Network and System Security Configuration.  Systems and networks access control configuration are usually analyzed independently although they are logically combined to define the the end-to-end security property. While systems and applications security policies define access control based on user identity or group, request type and the requested resource, network security policies uses flow information such as host and service addresses for source and destination to define access control. Therefore, both network and systems access control have to be configured consistently in order enforce end-to-end security policies. Many previous research attempt to verify either side separately, but it does not provide a unified approach to automatically validate the logical consistency between both of them. Thus, using existing techniques requires error-prone manual and ad-hoc analysis to validate this link.<br/>In this paper, we introduce a cross-layer modeling and verification system that can analyzes the configurations and policies across both application and network components as a single unit. It combines policies from different devices as firewalls, NAT, routers and IPSec gateways as well as basic RBAC-based policies of higher service layers. This will allow analyzing, for example, firewall polices in the context of application access control and vice versa. Thus, by incorporating policies across the network and over multiple layers, we provide a true end-to-end configuration verification tool. Our model represents the system as a state machine where packet header, service request and location determine the state and transitions that conform with the configurations, device operations, and packet values are established. We encode the model as Boolean functions using binary decision diagrams (BDDs). We used an extended version of computational tree logic (CTL) to provide more useful operators and then use it with symbolic model checking to prove or find counter examples to needed properties. The tool is implemented and we gave special consideration to efficiency and scalability. Our extensive evaluation study shows acceptable computation and space requirements with large number of nodes and configuration sizes."
"On Using Nearly-Independent Feature Families for High Precision and Confidence.  Often we require classification at a very high precision level, such
as 99%. We report that when very different sources of evidence such as
text, audio, and video features are available, combining the outputs
of base classifiers trained on each feature type separately, aka late
fusion, can substantially increase the recall of the combination at
high precisions, compared to the performance of a single classifier
trained on all the feature types i.e., early fusion, or compared to
the individual base classifiers. We show how the probability of a
joint false-positive mistake can be upper bounded by the product of
individual probabilities of conditional false-positive mistakes, by
identifying a simple key criterion that needs to hold. This provides
an explanation for the high precision phenomenon, and motivates
referring to such feature families as (nearly) independent. We assess
the relevant factors for achieving high precision empirically, and
explore combination techniques informed by the analysis. We compare a
number of early and late fusion methods, and observe that classifier
combination via late fusion can more than double the recall at high
precision."
"??N the Network????? Using Internet Resources for Predicting Cell Phone Number Status.  Despite higher hit rates for cell phone samples, inefficiencies in processing calls to these numbers relative to landline numbers continue to be documented in the U.S. literature. In this study, we propose one method for using cell phone provider information and Internet resources for validating number status. Specifically, we describe how we used ????in network???? options available from three major providers?? web sites to determine the validity of cell phone numbers. We tested differences in working number rates (WNRs) among valid and nonvalid numbers against a normal processing control group and determined that the WNR among valid numbers was approximately 14 percentage points higher than the WNR of the comparison group. This process also shows promise in reducing the effort required to determine working status and may provide a basis for developing screening tools for cell phones that capitalize on resources that are unique to this technology."
"Italy.  This chapter highlights the current Italian situation about telephone surveys.
<br/>
Table of contents:
<br/>"
"Do You Know Which Device Your Respondent Has Used to Take Your Online Survey?.  The type of devices that can be used to go online is becoming more varied. Users access the internet through traditional desktops and laptops, as well as netbooks, tablets, videogame consoles, mobile phones and ebook readers. Because many online surveys are designed to be taken on a standard desktop or laptop screen, it is important to monitor from which device your online sample is taking the survey, and to consider the consequences the device might have for visual design impact and survey estimates. A survey designed to be taken on a desktop does not necessarily or automatically look the same when taken from netbooks, smartphones and other devices.
This article will present a description of some tools to collect paradata that allow us to understand from which device the online survey is accessed, along with an initial suggestion for best practices."
"Systematic Software Testing: The Korat Approach.  At ISSTA 2002, the three authors (then Ph.D. students) published the paper Korat: Automated Testing Based on Java Predicates"", which won one of the first ACM SIGSOFT Distinguished paper awards. In 2012, the paper won the ACM SIGSOFT Impact Paper Award. The authors briefly recount the motivation behind the Korat research, the ideas presented in the original paper, and some work it inspired."
"Adolescent search roles.  In this article, we present an in-home observation and in-context research study investigating how 38 adolescents aged 14-17 search on the Internet. We present the search trends adolescents display and develop a framework of search roles that these trends help define. We compare these trends and roles to similar trends and roles found in prior work with children ages 7, 9, and 11.  We use these comparisons to make recommendations to adult stakeholders such as researchers, designers, and information literacy educators about the best ways to design search tools for children and adolescents, as well as how to use the framework of searching roles to find better methods of educating youth searchers. Major findings include the seven roles of adolescent searchers, and evidence that adolescents are social in their computer use, have a greater knowledge of sources than younger children, and that adolescents are less frustrated by searching tasks than younger children."
"HyperLogLog in Practice: Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm.  Cardinality estimation has a wide range of applications and
is of particular importance in database systems. Various
algorithms have been proposed in the past, and the HyperLogLog algorithm is one of them. In this paper, we
present a series of improvements to this algorithm that reduce its memory requirements and significantly increase its
accuracy for an important range of cardinalities. We have
implemented our proposed algorithm for a system at Google
and evaluated it empirically, comparing it to the original
HyperLogLog algorithm. Like HyperLogLog, our improved algorithm parallelizes perfectly and computes the
cardinality estimate in a single pass."
"Distributed Electronic Rights in JavaScript.  Contracts enable mutually suspicious parties to cooperate safely through the exchange of rights. Smart contracts are programs whose behavior enforces the terms of the contract. This paper shows how such contracts can be specified elegantly and executed safely, given an appropriate distributed, secure, persistent, and ubiquitous computational fabric. JavaScript provides the ubiquity but must be significantly extended to deal with the other aspects. The first part of this paper is a progress report on our efforts to turn JavaScript into this fabric. To demonstrate the suitability of this design, we describe an escrow exchange contract implemented in 42 lines of JavaScript code."
"A new approach to the semantics of model diagrams.  Sometimes, a diagram can say more than a thousand lines of code. But, sadly, most of the time, software engineers give up on diagrams after the design phase, and all real work is done in code. The supremacy of code over diagrams would be leveled if diagrams were code. This paper suggests that model and instance diagrams, or, which amounts to the same, class and object diagrams, become first level entities in a suitably expressive programming language, viz., type theory. The proposed semantics of diagrams is compositional and self-describing, i.e., reflexive, or metacircular. Moreover, it is well suited for metamodelling and model driven engineering, as it is possible to prove model transformations correct in type theory. The encoding into type theory has the additional benefit of making diagrams immediately useful, given an implementation of type theory."
"Buildling adaptive dialogue systems via Bayes-adaptive POMDP.  Recent research has shown that effective dialogue management can be achieved through the Partially Observable Markov Decision Process (POMDP) framework. However past research on POMDP-based dialogue systems usually assumed the parameters of the decision process were known a priori. The main contribution of this paper is to present a Bayesian reinforcement learning framework for learning the POMDP parameters online from data, in a decision-theoretic manner. We discuss various approximations and assumptions which can be leveraged to ensure computational tractability, and apply these techniques to learning observation models for several simulated spoken dialogue domains."
"Budget Optimization for Online Campaigns with Positive Carryover Effects.  While it is relatively easy to start an online advertising campaign, proper allocation of the marketing budget is far from trivial. A major challenge faced by the marketers attempting to optimize their campaigns is in the sheer number of variables involved, the many individual decisions they make in fixing or changing these variables, and the nontrivial short and long-term interplay among these variables and decisions.
In this paper, we study interactions among individual advertising decisions using a Markov model of user behavior. We formulate the budget allocation task of an advertiser as a constrained optimal control problem for a Markov Decision Process (MDP). Using the theory of constrained MDPs, a simple LP algorithm yields the optimal solution. Our main result is that, under a reasonable assumption that online advertising has positive carryover effects on the propensity and the form of user interactions with the same advertiser in the future, there is a simple greedy algorithm for the budget allocation with the worst-case running time cubic in the number of model states (potential advertising keywords) and an efficient parallel implementation in a distributed computing framework like MapReduce. Using real-world anonymized datasets from sponsored search advertising campaigns of several advertisers, we evaluate performance of the proposed budget allocation algorithm, and show that the greedy algorithm performs well compared to the optimal LP solution on these datasets and that both show consistent 5-10% improvement in the expected revenue against the optimal baseline algorithm ignoring carryover effects."
"Large Scale Distributed Acoustic Modeling With Back-off N-grams.  The paper revives an older approach to acoustic modeling that borrows from n-gram language modeling in an attempt to scale up both the amount of training data and model size (as measured by the number of parameters in the model), to approximately 100 times larger than current sizes used in automatic speech recognition. In such a data-rich setting, we can expand the phonetic context significantly beyond triphones, as well as increase the number of Gaussian mixture components for the context-dependent states that allow it. We have experimented with contexts that span seven or more context-independent phones, and up to 620 mixture components per state. Dealing with unseen phonetic contexts is accomplished using the familiar back-off technique used in language modeling due to implementation simplicity. The back-off acoustic model is estimated, stored and served using MapReduce distributed computing infrastructure. Speech recognition experiments are carried out in an N-best list rescoring framework for Google Voice Search. Training big models on large amounts of data proves to be an effective way to increase the accuracy of a state-of-the-art automatic speech recognition system. We use 87,000 hours of training data (speech along with transcription) obtained by filtering utterances in Voice Search logs on automatic speech recognition confidence. Models ranging in size between 20--40 million Gaussians are estimated using maximum likelihood training. They achieve relative reductions in word-error-rate of 11% and 6% when combined with first-pass models trained using maximum likelihood, and boosted maximum mutual information, respectively. Increasing the context size beyond five phones (quinphones) does not help."
"Pseudo-likelihood methods for community detection in large sparse networks.  Many algorithms have been proposed for fitting network models with communities but most of them do not scale well to large networks, and often fail on sparse networks. Here we propose a new fast pseudo-likelihood method for fitting the stochastic block model for networks, as well as a variant that allows for an arbitrary degree distribution by conditioning on degrees. We show that the algorithms perform well under a range of settings, including on very sparse networks, and illustrate on the example of a network of political blogs. We also propose spectral clustering with perturbations, a method of independent interest, which works well on sparse networks where regular spectral clustering fails, and use it to provide an initial value for pseudo-likelihood."
"Pay by the Bit: An Information-Theoretic Metric for Collective Human Judgment.  We consider the problem of evaluating the performance of human contributors for tasks involving answering a series of questions, each of which has a single correct answer. The answers may not be known a priori. We assert that the measure of a contributor's judgments is the amount by which having these judgments decreases the entropy of our discovering the answer. This quantity is the pointwise mutual information between the judgments and the answer. The expected value of this metric is the mutual information between the contributor and the answer prior, which can be computed using only the prior and the conditional probabilities of the contributor's judgments given a correct answer, without knowing the answers themselves. We also propose using multivariable information measures, such as conditional mutual information, to measure the interactions between contributors' judgments. These metrics have a variety of applications. They can be used as a basis for contributor performance evaluation and incentives. They can be used to measure the efficiency of the judgment collection process. If the collection process allows assignment of contributors to questions, they can also be used to optimize this scheduling."
"Weakly Supervised Learning of Object Segmentations from Web-Scale Video.  We propose to learn pixel-level segmentations of objects from weakly labeled (tagged) internet videos. Specifically, given a large collection of raw YouTube content, along with potentially noisy tags, our goal is to automatically generate spatiotemporal masks for each object, such as ""dog"", without employing any pre-trained object detectors. We formulate this problem as learning weakly supervised classifiers for a set of independent spatio-temporal segments. The object seeds obtained using segment-level classifiers are further refined using graphcuts to generate high-precision object masks. Our results, obtained by training on a dataset of 20,000 YouTube videos weakly tagged into 15 classes, demonstrate automatic extraction of pixel-level object masks. Evaluated against a ground-truthed subset of 50,000 frames with pixel-level annotations, we confirm that our proposed methods can learn good object masks just by watching YouTube."
"Trustworthy Proxies: Virtualizing Objects with Invariants.  Proxies are a common technique to virtualize objects in object-oriented languages. A proxy is a placeholder object that emulates or wraps another target object. Both the proxy's representation and behavior may differ substantially from that of its target object. In many object-oriented languages, objects may have language-enforced invariants associated with them. For instance, an object may declare immutable fields, which are guaranteed to point to the same value throughout the execution of the program. Clients of an object can blindly rely on these invariants, as they are enforced by the language. In a language with both proxies and objects with invariants, these features interact. Can a proxy emulate or replace a target object purporting to uphold such invariants? If yes, does the client of the proxy need to trust the proxy to uphold these invariants, or are they still enforced by the language? This paper sheds light on these questions in the context of a Javascript-like language, and describes the design of a Proxy API that allows proxies to emulate objects with invariants, yet have these invariants continue to be language-enforced. This design forms the basis of proxies in ECMAScript 6."
"CPI^2: CPU performance isolation for shared compute clusters.  Performance isolation is a key challenge in cloud computing. Unfortunately, Linux has few defenses against performance interference in shared resources such as processor caches and memory buses, so applications in a cloud can experience unpredictable performance caused by other program's behavior. Our solution, CPI2, uses cycles-per-instruction (CPI) data obtained by hardware performance counters to identify problems, select the likely perpetrators, and then optionally throttle them so that the victims can return to their expected behavior. It automatically learns normal and anomalous behaviors by aggregating data from multiple tasks in the same job. We have rolled out CPI2 to all of Google's shared compute clusters. The paper presents the analysis that lead us to that outcome, including both case studies and a large-scale evaluation of its ability to solve real production issues."
"JSWhiz - Static Analysis for JavaScript Memory Leaks.  JavaScript is the dominant language for implementing dynamic web pages in browsers. Even though it is standardized, many browsers implement language and browser bindings in different and incompatible ways. As a result, a plethora of web development frameworks were developed to hide cross-browser issues and to ease development of large web applications. An unwelcome side-effect of these frameworks is that they can introduce memory leaks, despite the fact that JavaScript is garbage collected. Memory bloat is a major issue for web applications, as it affects user perceived latency and may even prevent large web applications from running on devices with limited resources. In this paper we present JSWhiz, an extension to the open-source Closure JavaScript compiler. Based on experiences analyzing memory leaks in Gmail, JSWhiz detects five identified common problem patterns. JSWhiz found a total of 89 memory leaks across Google's Gmail, Docs, Spreadsheets, Books, and Closure itself. It contributed significantly in a recent effort to reduce Gmail memory footprint, which resulted in bloat reduction of 75% at the 99th percentile, and by roughly 50% at the median."
"Generating Precise Dependencies for Large Software.  Intra- and inter-module dependencies can be a significant source of technical debt in the long-term software development, especially for large software with millions of lines of code. This paper designs and implements a precise and scalable tool that extracts code dependencies and their utilization for large C/C++ software projects. The tool extracts both symbol-level and module-level dependencies of a software system and identifies potential underutilized and inconsistent dependencies. Such information points to potential refactoring opportunities and help developers perform large-scale refactoring tasks."
"Human Computation Must Be Reproducible.  Human computation is the technique of performing a computational process by outsourcing some of the difficult-to-automate steps to humans. In the social and behavioral sciences, when using humans as measuring instruments, reproducibility guides the design and evaluation of experiments. We argue that human computation has similar properties, and that the results of human computation must be reproducible, in the least, in order to be informative. We might additionally require the results of human computation to have high validity or high utility, but the results must be reproducible in order to measure the validity or utility to a degree better than chance. Additionally, a focus on reproducibility has implications for design of task and instructions, as well as for the communication of the results. It is humbling how often the initial understanding of the task and guidelines turns out to lack reproducibility. We suggest ensuring, measuring and communicating reproducibility of human computation tasks."
"Autonomous spectrum balancing for digital subscriber lines.  The main performance bottleneck of modern digital subscriber line (DSL) networks is the crosstalk among different lines (i.e., users). By deploying dynamic spectrum management (DSM) techniques and reducing excess crosstalk among users, a network operator can dramatically increase the data rates and service reach of broadband access. However, current DSM algorithms suffer from either substantial suboptimality in typical deployment scenarios or prohibitively high complexity due to centralized computation. This paper develops, analyzes, and simulates a new suite of DSM algorithms for DSL interference-channel models called autonomous spectrum balancing (ASB). The ASB algorithms utilize the concept of a ""reference line,"" which mimics a typical victim line in the interference channel. In ASB, each modem tries to minimize the harm it causes to the reference line under the constraint of achieving its own target data-rate. Since the reference line is based on the statistics of the entire network, rather than any specific knowledge of the binder a modem operates in, ASB can be implemented autonomously without the need for a centralized spectrum management center. ASB has a low complexity and simulations using a realistic simulator show that it achieves large performance gains over existing autonomous algorithms, coming close to the optimal rate region in some typical scenarios. Sufficient conditions for convergence of ASB are also proved."
"A Framework for Benchmarking Entity-Annotation Systems.  In this paper we design and implement a benchmarking framework for fair and exhaustive comparison of entity-annotation systems. The framework is based upon the definition of a set of problems related to the entity-annotation task, a set of measures to evaluate systems performance, and a systematic comparative evaluation involving all publicly available datasets, containing texts of various types such as news, tweets and Web pages. Our framework is easily-extensible with novel entity annotators, datasets and evaluation measures for comparing systems, and it has been released to the public as open source. We use this framework to perform the first extensive comparison among all available entity annotators over all available datasets, and draw many interesting conclusions upon their efficiency and effectiveness. We also draw conclusions between academic versus commercial annotators."
"Multi-Armed Recommendation Bandits for Selecting State Machine Policies for Robotic Systems.  We investigate the problem of selecting a state-machine from a library to control a robot.  We are particularly interested in this problem when evaluating such state machines on a particular robotics task is expensive.  As a motivating example, we consider a problem where a simulated vacuuming robot must select a driving state machine well-suited for a particular (unknown) room layout. By borrowing concepts from collaborative filtering (recommender systems such as Netflix and Amazon.com), we present a multi-armed bandit formulation that incorporates recommendation techniques to efficiently select state machines for individual room layouts. We show that this formulation outperforms the individual approaches (recommendation, multi-armed bandits) as well as the baseline of selecting the `average best' state machine across all rooms."
"Discriminative Segment Annotation in Weakly Labeled Video.  paper tackles the problem of segment annotation in complex Internet videos.  Given a weakly labeled video, we automatically generate spatiotemporal masks for each of the concepts with which it is labeled.  This is a particularly relevant problem in the video domain, as large numbers of YouTube videos are now available, tagged with the visual concepts that they contain. Given such weakly labeled videos, we focus on the problem of spatiotemporal segment classification. We propose a straightforward algorithm, CRANE, that utilizes large amounts of weakly labeled video to rank spatiotemporal segments by the likelihood that they correspond to a given visual concept. We make publicly available segment-level annotations for a subset of the Prest et al. dataset and show convincing results. We also show state-of-the-art results on Hartmann et al.'s more difficult, large-scale object segmentation dataset."
"Spatiotemporal Deformable Part Models for Action Detection.  Deformable part models have achieved impressive performance for object detection, even on difficult image datasets. This paper explores the generalization of deformable part models from 2D images to 3D spatiotemporal volumes to better study their effectiveness for action detection in video. Actions are treated as spatiotemporal patterns and a deformable part model is generated for each action from a collection of examples. For each action model, the most discriminative 3D subvolumes are automatically selected as parts and the spatiotemporal relations between their locations are learned.  By focusing on the most distinctive parts of each action, our models adapt to intra-class variation and show robustness to clutter. Extensive experiments on several video datasets demonstrate the strength of spatiotemporal DPMs for classifying and localizing actions."
"An Overview of Practical Exchange Design.  We consider the problem of designing an online 
exchange. We identify the goals of exchange design, 
and present key techniques for accomplishing these 
goals along with the tradeoffs inherent in the choices."
"Approximation Algorithms for the Directed k-Tour and k-Stroll Problems.  We consider two natural generalizations of the Asymmetric Traveling Salesman problem: the k-Stroll and the k-Tour problems. The input to the k-Stroll problem is a directed n-vertex graph with nonnegative edge lengths, an integer k, as well as two special vertices s and t. The goal is to find a minimum-length s-t walk, containing at least k distinct vertices (including the endpoints s,t). The k-Tour problem can be viewed as a special case of k-Stroll, where s=t. That is, the walk is required to be a tour, containing some pre-specified vertex s. When k=n, the k-Stroll problem becomes equivalent to Asymmetric Traveling Salesman Path, and k-Tour to Asymmetric Traveling Salesman.
Our main result is a polylogarithmic approximation algorithm for the k-Stroll problem. Prior to our work, only bicriteria (O(log2 k),3)-approximation algorithms have been known, producing walks whose length is bounded by 3OPT, while the number of vertices visited is ?(k/log2 k). We also show a simple O(log2 n/loglogn)-approximation algorithm for the k-Tour problem. The best previously known approximation algorithms achieved min(O(log3 k),O(log2 n?logk/loglogn)) approximation in polynomial time, and O(log2 k) approximation in quasipolynomial time."
"Semantic Queries by Example.  With the ever increasing quantities of electronic data, there is a growing need to make sense out of the data. Many advanced database applications are beginning to support this 
need by integrating domain knowledge encoded as ontologies into queries over relational data. However, it is extremely difficult to express queries against graph structured 
  ontology in the relational SQL query language or its extensions. Moreover, semantic queries are usually not precise, especially when data and its related ontology are complicated. Users often only have a vague notion of their information needs and are not able to specify queries precisely. 
  In this paper, we address these challenges by introducing a novel method to support semantic queries in relational 
  databases with ease. Instead of casting ontology into relational form and creating new language constructs to express such queries, we ask the user to provide a small number of examples that satisfy the query she has in mind. Using those examples as seeds, the system infers the exact query automatically, and the user is therefore shielded from the complexity of interfacing with the ontology. Our approach consists of three steps. In the first step, the user provides several examples that satisfy the query. In the second step, we use machine learning techniques to mine the semantics of the query from the given examples and related ontologies. Finally, we apply the query semantics on the data to generate the full query result. We also implement an optional active learning mechanism to find the query semantics accurately and quickly. Our experiments validate the effectiveness of our approach."
"The Tail at Scale.  Systems that respond to user actions very quickly (within 100 milliseconds) feel more fluid and natural to users than those that take longer [Card et al 1991]. Improvements in Internet connectivity and the rise of warehouse-scale computing systems [Barroso &amp; Hoelzle 2009] have enabled Web services that provide fluid responsiveness while consulting multi-terabyte datasets that span thousands of servers. For example, the Google search system now updates query results interactively as the user types, predicting the most likely query based on the prefix typed so far, performing the search, and showing the results within a few tens of milliseconds.  Emerging augmented reality devices such as the Google Glass prototype will need associated Web services with even greater computational needs while guaranteeing seamless interactivity. It is challenging to keep the tail of the latency distribution low for interactive services as the size and complexity of the system scales up or as overall utilization increases. Temporary high latency episodes which are unimportant in moderate size systems may come to dominate overall service performance at large scale. Just as fault-tolerant computing aims to create a reliable whole out of less reliable parts, we suggest that large online services need to create a predictably responsive whole out of less predictable parts. We refer to such systems as latency tail-tolerant, or tail-tolerant for brevity. This article outlines some of the common causes of high latency episodes in large online services and describes techniques that reduce their severity or mitigate their impact in whole system performance. In many cases, tail-tolerant techniques can take advantage of resources already deployed to achieve fault-tolerance, resulting in low additional overheads. We show that these techniques allow system utilization to be driven higher without lengthening the latency tail, avoiding wasteful over-provisioning."
"Optimizing Budget Constrained Spend in Search Advertising.  Search engine ad auctions typically have a significant fraction of advertisers who are budget constrained, i.e., if allowed to participate in every auction that they bid on, they would spend more than their budget. This yields an important problem: selecting the ad auctions in which these advertisers participate, in order to optimize different system objectives such as the return on investment for advertisers, and the quality of ads shown to users. We present a system and algorithms for optimizing such budget constrained spend. The system is designed be deployed in a large search engine, with hundreds of thousands of advertisers, millions of searches per hour, and with the query stream being only partially predictable. We have validated the system design by implementing it in the Google ads serving system and running experiments on live traffic. We have also compared our algorithm to previous work that casts this problem as a large linear programming problem limited to popular queries, and show that our algorithms yield substantially better results."
"Online Graph Edge-Coloring in the Random-Order Arrival Model.  A classic theorem by Vizing asserts that if the maximum degree of a graph is ?, then it is possible to color its edges, in polynomial time, using at most ?+1 colors. However, this algorithm is offline, i.e., it assumes the whole graph is known in advance. A natural question then is how well we can do in the online setting, where the edges of the graph are revealed one by one, and we need to color each edge as soon as it is added to the graph. Online edge coloring has an important application in fast switch scheduling. A natural model is that edges arrive online, but in a random permutation. Even in the random permutation model, the best proven approximation factor for any algorithm is the factor 2 of the simple greedy algorithm (which holds even in the worst-case online model). The algorithm of Aggarwal et al. (FOCS'03) provides a 1+o(1) factor algorithm for the case of very dense multi-graphs, when ?=?(n2), where n is the number of vertices. In this paper, we show that for graphs with ?=?(logn), it is possible to color the graph with (1+ee2?1+o(1))??1.43? colors, with high probability, in the online random-order model. Our algorithm is inspired by a 1.6-approximate distributed offline algorithm of Panconesi and Srinivasan (PODC'92), which we extend by reusing failed colors online. Further, we show how we can extend the algorithm to reuse colors multiple times, which reduces the approximation factor below 1.43. We conjecture that the algorithm becomes nearly optimal (i.e., uses ?+o(?) colors) with O(log(?/logn)) reuses. We reduce the question to proving the non-negativity of a certain recursively defined sequence, which looks true in computer simulations. This non-negativity can be proved explicitly for a small number of reuses, giving improved algorithms: e.g., the algorithm which reuses colors 5 times uses 1.26? colors."
"Design, Implementation and Verification of an eXtensible and Modular Hypervisor Framework.  We present the design, implementation, and verification of XMHF - an eXtensible and Modular Hypervisor Framework. XMHF is designed to achieve three goals - modular extensibility, automated verification, and high performance. XMHF includes a core that provides functionality common to many hypervisor-based security architectures and supports extensions that augment the core with additional security or
functional properties while preserving the fundamental hypervisor security property of memory integrity (i.e., ensuring that the hypervisor's memory is not modified by software running at a lower privilege level). We verify the memory integrity of the XMHF core - 6018 lines of code - using a combination of automated and manual techniques. The model checker CBMC automatically verifies 5208 lines of C code in about 80 seconds using less than 2GB of RAM. We manually audit the remaining 422 lines of C code and 388 lines of assembly language code that are stable and unlikely to change as development proceeds. Our experiments indicate that XMHF's performance is comparable to popular high-performance general-purpose hypervisors for the single guest that it supports."
"Multilingual acoustic models using distributed deep neural networks.  Today??s speech recognition technology is mature enough to be useful
for many practical applications. In this context, it is of paramount
importance to train accurate acoustic models for many languages
within given resource constraints such as data, processing power, and
time. Multilingual training has the potential to solve the data issue
and close the performance gap between resource-rich and resourcescarce
languages. Neural networks lend themselves naturally to parameter
sharing across languages, and distributed implementations
have made it feasible to train large networks. In this paper, we
present experimental results for cross- and multi-lingual network
training of eleven Romance languages on 10k hours of data in total.
The average relative gains over the monolingual baselines are
4%/2% (data-scarce/data-rich languages) for cross- and 7%/2% for
multi-lingual training. However, the additional gain from jointly
training the languages on all data comes at an increased training time
of roughly four weeks, compared to two weeks (monolingual) and
one week (crosslingual)."
"An Empirical study of learning rates in deep neural networks for speech recognition.  Recent deep neural network systems for large vocabulary speech
recognition are trained with minibatch stochastic gradient descent
but use a variety of learning rate scheduling schemes. We investigate
several of these schemes, particularly AdaGrad. Based on our analysis
of its limitations, we propose a new variant ??AdaDec?? that decouples
long-term learning-rate scheduling from per-parameter learning
rate variation. AdaDec was found to result in higher frame accuracies
than other methods. Overall, careful choice of learning rate
schemes leads to faster convergence and lower word error rates"
"Multiframe Deep Neural Networks for Acoustic Modeling.  Deep neural networks have been shown to perform very well
as acoustic models for automatic speech recognition. Compared
to Gaussian mixtures however, they tend to be very
expensive computationally, making them challenging to use
in real-time applications. One key advantage of such neural
networks is their ability to learn from very long observation
windows going up to 400 ms. Given this very long temporal
context, it is tempting to wonder whether one can run neural
networks at a lower frame rate than the typical 10 ms, and
whether there might be computational benefits to doing so.
This paper describes a method of tying the neural network parameters
over time which achieves comparable performance
to the typical frame-synchronous model, while achieving up
to a 4X reduction in the computational cost of the neural network
activations."
"On Rectified Linear Units For Speech Processing.  Deep neural networks have recently become the gold standard
for acoustic modeling in speech recognition systems. The key
computational unit of a deep network is a linear projection
followed by a point-wise non-linearity, which is typically a
logistic function. In this work, we show that we can improve
generalization and make training of deep networks faster and
simpler by substituting the logistic units with rectified linear units. These units are linear when their input is positive and zero otherwise. In a supervised setting, we can successfully train very deep nets from random initialization on a large vocabulary speech recognition task achieving lower word error rates than using a logistic network with the same topology. Similarly in an unsupervised setting, we show how we can learn sparse features that can be useful for discriminative tasks. All our experiments are executed in a distributed environment using several hundred machines and several hundred hours of speech data."
"Ensuring Connectivity via Data Plane Mechanisms.  We typically think of network architectures as having two basic components: a data plane responsible for forwarding packets at line-speed, and a control plane that instantiates the forwarding state the data plane needs. With this separation of concerns, ensuring connectivity is the responsibility of the control plane. However, the control plane typically operates at timescales several orders of magnitude slower than the data plane, which means that failure recovery will always be slow compared to dataplane forwarding rates. In this paper we propose moving the responsibility for connectivity to the data plane. Our design, called Data-Driven Connectivity (DDC) ensures routing connectivity via data plane mechanisms. We believe this new separation of concerns basic connectivity on the data plane, optimal paths on the control plane will allow networks to provide a much higher degree of availability, while still providing flexible routing control."
"Large-Scale Learning with Less RAM via Randomization.  We reduce the memory footprint of popular large-scale online learning methods by projecting our weight vector onto a coarse discrete set using randomized rounding. Compared to standard 32-bit float encodings, this
reduces RAM usage by more than 50% during training and by up to 95% when making predictions from a fixed model, with almost no loss in accuracy. We also show that randomized counting can be used to implement per-coordinate learning rates, improving model quality with little additional RAM. We prove these memory-saving methods achieve regret guarantees similar to their exact variants.  Empirical evaluation confirms excellent performance, dominating standard approaches across memory versus accuracy tradeoffs."
"Fast, Accurate Detection of 100,000 Object Classes on a Single Machine.  Many object detection systems are constrained by the time required to convolve a target image with a bank of filters that code for different aspects of an object's appearance, such as the presence of component parts.  We exploit locality-sensitive hashing to replace the dot-product kernel operator in the convolution with a fixed number of hash-table probes that effectively sample all of the filter responses in time independent of the size of the filter bank.  To show the effectiveness of the technique, we apply it to evaluate 100,000 deformable-part models requiring over a million (part) filters on multiple scales of a target image in less than 20 seconds using a single multi-core processor with 20GB of RAM.  This represents a speed-up of approximately 20,000 times - four orders of magnitude - when compared with performing the convolutions explicitly on the same hardware.  While mean average precision over the full set of 100,000 object classes is around 0.16 due in large part to the challenges in gathering training data and collecting ground truth for so many classes, we achieve a mAP of at least 0.20 on a third of the classes and 0.30 or better on about 20% of the classes."
"Top-k Publish-Subscribe for  Social Annotation of News.  Social content, such as Twitter updates, often have the quickest first-hand reports of news events, as well as numerous commentaries that are indicative of public view of such events. As such, social updates provide a good complement to professionally written news articles. In this paper we consider the problem of automatically annotating news stories with social updates (tweets), at a news website serving high volume of pageviews. The high rate of both the pageviews (millions to billions a day) and of the incoming tweets (more than 100 millions a day) make real-time indexing of tweets ineffective, as this requires an index that is both queried and updated extremely frequently. The rate of tweet updates makes caching techniques almost unusable since the cache would become stale very quickly. We propose a novel architecture where each story is treated as a subscription for tweets relevant to the story's content, and new  algorithms that efficiently match tweets to stories, proactively maintaining the top-k tweets for each story. Such {\em top-k pub-sub} consumes only a small fraction of the resource cost of alternative solutions, and can be applicable to other large scale content-based publish-subscribe problems. We demonstrate the effectiveness of our approach on real-world data: a corpus of news stories from Yahoo! News and a log of Twitter updates."
"Verifying Cloud Services: Present and Future.  As cloud-based services gain popularity in both private and
enterprise domains, cloud consumers are still lacking in tools
to verify that these services work as expected. Such tools
should consider properties such as functional correctness,
service availability, reliability, performance and security guar-
antees. In this paper we survey existing work in these ar-
eas and identify gaps in existing cloud technology in terms
of the verication tools provided to users. We also discuss
challenges and new research directions that can help bridge
these gaps."
"Statistical Parametric Speech Synthesis Using Deep Neural Networks.  Conventional approaches to statistical parametric speech synthesis typically use decision tree-clustered context-dependent hidden Markov models (HMMs) to represent probability densities of speech parameters given texts. Speech parameters are generated from the probability densities to maximize their output probabilities, then a speech waveform is reconstructed from the generated parameters. This approach is reasonably effective but has a couple of limitations, e.g. decision trees are inef?cient to model complex context dependencies. This paper examines an alternative scheme that is based on a deep neural network (DNN). The relationship between input texts and their acoustic realizations is modeled by a DNN. The use of the DNN can address some limitations of the conventional approach. Experimental results show that the DNN-based systems outperformed the HMM-based systems with similar numbers of parameters."
"Data Enriched Linear Regression.  We present a linear regression method for predictions on a small data set making use of a second possibly biased data set that may be much larger. Our method fits linear regressions to the two data sets while penalizing the difference between predictions made by those two models. The resulting algorithm is a shrinkage method similar to those used in small area estimation. We find a Stein-type result for Gaussian responses: when the model has 5 or more coefficients and 10 or more error degrees of freedom, it becomes inadmissible to use only the small data set, no matter how large the bias is. We also present both plug-in and AICc-based methods to tune our penalty parameter. Most of our results use an L2 penalty, but we obtain formulas for L1 penalized estimates when the model is specialized to the location setting. Ordinary Stein shrinkage provides an inadmissibility result for only 3 or more coefficients, but we find that our shrinkage method typically produces much lower squared errors in as few as 5 or 10 dimensions when the bias is small and essentially equivalent squared errors when the bias is large."
"Optical Interconnects for Scale-Out Data Centers.  We review the architecture of modern datacenter networks, as well as their scaling challenges; we then present opportunities and needs for emerging optical technologies to support datacenter scaling."
"A Method for Measuring Online Audiences.  We present a method for measuring the reach and frequency of online ad campaigns by audience attributes. This method uses a combination of data sources, including ad server logs, publisher provided user data (PPD), census data, and a representative online panel. It adjusts for known problems with cookie data and potential non-representative and inaccurate PPD. It generalizes for multiple publishers and for targeting based on the PPD. The method includes the conversion of adjusted cookie counts to unique audience counts. The benefit of our method is that we get both reduced variance from server logs and reduced bias from the panel. Simulation results and a case study are presented."
"Empirical Exploration of Language Modeling for the google.com Query Stream as Applied to Mobile Voice Search.  Mobile is poised to become the predominant platform over which people are accessing the World Wide Web. Recent developments in speech recognition and understanding, backed by high bandwidth coverage and high quality speech signal acquisition on smartphones and tablets are presenting the users with the choice of speaking their web search queries instead of typing them. A critical component of a speech recognition system targeting web search is the language model. The chapter presents an empirical exploration of the google.com query stream with the end goal of high quality statistical language modeling for mobile voice search. Our experiments show that after text normalization the query stream is not as ``wild'' as it seems at first sight. One can achieve out-of-vocabulary rates below 1% using a one million word vocabulary, and excellent n-gram hit ratios of 77/88% even at high orders such as n=5/4, respectively. A more careful analysis shows that a significantly larger vocabulary (approx. 10 million words) may be required to guarantee at most 1% out-of-vocabulary rate for a large percentage (95%) of users. Using large scale, distributed language models can improve performance significantly---up to 10% relative reductions in word-error-rate over conventional models used in speech recognition. We also find that the query stream is non-stationary, which means that adding more past training data beyond a certain point provides diminishing returns, and may even degrade performance slightly. Perhaps less surprisingly, we have shown that locale matters significantly for English query data across USA, Great Britain and Australia. In an attempt to leverage the speech data in voice search logs, we successfully build large-scale discriminative N-gram language models and derive small but significant gains in recognition performance."
"On the k-atomicity-verification problem.  Modern Internet-scale storage systems often provide weak consistency in exchange for better perfor- mance and resilience. An important weak consistency prop- erty is k-atomicity, which bounds the staleness of values returned by read operations. The k-atomicity-verification problem (or k-AV for short) is the problem of deciding whether a given history of operations is k-atomic. The 1-AV problem is equivalent to verifying atomicity/linearizability, a well-known and solved problem. However, for k ? 2, no polynomial-time k-AV algorithm is known.
This paper makes the following contributions towards solving the k-AV problem. First, we present a simple 2- AV algorithm called LBT, which is likely to be efficient (quasilinear) for histories that arise in practice, although it is less efficient (quadratic) in the worst case. Second, we present a more involved 2-AV algorithm called FZF, which runs efficiently (quasilinear) even in the worst case. To our knowledge, these are the first algorithms that solve the 2-AV problem fully. Third, we show that the weighted k-AV problem, a natural extension of the k-AV problem, is NP-complete."
"A taste of Capsicum: practical capabilities for UNIX.  Capsicum is a lightweight operating system (OS) capability and sandbox framework planned for inclusion in 
FreeBSD 9. Capsicum extends, rather than replaces, UNIX 
APIs, providing new kernel primitives (sandboxed capability mode and capabilities) and a userspace sandbox API. 
These tools support decomposition of monolithic UNIX 
applications into compartmentalized logical applications, 
an increasingly common goal that is supported poorly by 
existing OS access control primitives. We demonstrate our 
approach by adapting core FreeBSD utilities and Google"
"Three Controversial Hypotheses Concerning Computation in the Primate Cortex.  We consider three hypotheses concerning the primate neocortex which have influenced computational neuroscience in recent years. Is the mind modular in terms of its being profitably described as a collection of relatively independent functional units? Does the regular structure of the cortex imply a single algorithm at work, operating on many different inputs in parallel? Can the cognitive differences between humans and our closest primate relatives be explained in terms of a scalable cortical architecture? We bring to bear diverse sources of evidence to argue that the answers to each of these questions - with some judicious qualifications - are in the affirmative. In particular, we argue that while our higher cognitive functions may interact in a complicated fashion, many of the component functions operate through well-defined interfaces and, perhaps more important, are built on a neural substrate that scales easily under the control of a modular genetic architecture. Processing in the primary sensory cortices seem amenable to similar algorithmic principles, and, even for those cases where alternative principles are at play, the regular structure of cortex allows the same or greater advantages as the architecture scales. Similar genetic machinery to that used by nature to scale body plans has apparently been applied to scale cortical computations. The resulting replicated computing units can be used to build larger working memory and support deeper recursions needed to qualitatively improve our abilities to handle language, abstraction and social interaction."
"All Smiles : Automatic Photo Enhancement by Facial Expression Analysis.  We propose a framework for automatic enhancement of group photographs by facial expression analysis. We are motivated by the observation that group photographs are seldom perfect. Subjects may have inadvertently closed their eyes, may be looking away, or may not be smiling at that moment. Given a set of photographs of the same group of people, our algorithm uses facial analysis to determine a goodness score for each face instance in those photos. This scoring function is based on classifiers for facial expressions such as smiles and eye-closure, trained over a large set of annotated photos. Given these scores, a best composite for the set is synthesized by (a) selecting the photo with the best overall score, and (b) replacing any low-scoring faces in that photo with high-scoring faces of the same person from other photos, using alignment and seamless composition."
"Fast, Accurate Detection of 100,000 Object Classes on a Single Machine: Technical Supplement.  In the companion paper published in CVPR 2013, we presented a method that can directly use deformable part models (DPMs) trained as in [Felzenszwalb et al CVPR 2008]. After training, HOG based part filters are hashed, and, during inference, counts of hashing collisions summed over all hash bands serve as a proxy for part-filter / sliding-window dot products, i.e., filter responses.  These counts are an approximation and so we take the original HOG-based filters for the top hash counts and calculate the exact dot products for scoring. It is possible to train DPM models not on HOG data but on a hashed WTA [Yagnik et al ICCV 2011] version of this data. The resulting part filters are sparse, real-valued vectors the size of WTA vectors computed from sliding windows.  Given the WTA hash of a window, we exactly recover dot products of the top responses using an extension of locality-sensitive hashing.  In this supplement, we sketch a method for training such WTA-based models."
"Efficient Closed-Form Solution to Generalized Boundary Detection.  Boundary detection is essential for a variety of computer vision tasks such as segmentation and recognition. We propose a unified formulation for boundary detection, with closed-form solution, which is applicable to the localization of different types of boundaries, such as intensity edges and occlusion boundaries from video and RGB-D cameras. Our algorithm simultaneously combines low- and mid-level image representations, in a single eigenvalue problem, and we solve over an infinite set of putative boundary orientations. Moreover, our method achieves state of the art results at a significantly lower computational cost than current methods. We also propose a novel method for soft-segmentation that can be used in conjunction with our boundary detection algorithm and improve its accuracy at a negligible extra computational cost."
"Speech and Natural Language: Where Are We Now And Where Are We Headed?.  <a href=""http://mobilevoiceconference.com/avios.html"">Slides</a> from a presentation on invited panel at the Mobile Voice Conference 2013, San Francisco."
"Recursive Sparse Spatiotemporal Coding.  We present a new approach to learning sparse, spatiotemporal codes in which the number of basis vectors, their orientations, velocities and the size of their receptive fields change over the duration of unsupervised training. The algorithm starts with a relatively small, initial basis with minimal temporal extent. This initial basis is obtained through conventional sparse coding techniques and is expanded over time by recursively constructing a new basis consisting of basis vectors with larger temporal extent that proportionally conserve regions of previously trained weights. These proportionally conserved weights are combined with the result of adjusting newly added weights to represent a greater range of primitive motion features. The size of the current basis is determined probabilistically by sampling from existing basis vectors according to their activation on the training set. The resulting algorithm produces bases consisting of filters that are bandpass, spatially oriented and temporally diverse in terms of their transformations and velocities. The basic methodology borrows inspiration from the layer-by-layer learning of multiple-layer restricted Boltzmann machines developed by Geoff Hinton and his students. Indeed, we can learn multiple-layer sparse codes by training a stack of denoising autoencoders, but we have had greater success using L1 regularized regression in a variation on Olshausen and Field's original SPARSENET. To accelerate learning and focus attention, we apply a space-time interest-point operator that selects for periodic motion. This attentional mechanism enables us to efficiently compute and compactly represent a broad range of interesting motion. We demonstrate the utility of our approach by using it to recognize human activity in video. Our algorithm meets or exceeds the performance of state-of-the-art activity-recognition methods."
"Crowd-Sourced Call Identification and Suppression.  We recommend the creation of a system that allows users to report, to an online database system, the originating telephone number of unwanted solicitations, advertisements or robotically placed calls (henceforth called 'spammers'). We also recommend that users' telephones or external hardware may automatically query the database about the telephone number of an incoming call (before the call is answered, or even before the telephone rings) to determine if the caller has been flagged as a spammer by other users, and optionally block the call or otherwise handle it differently from a non-spam call. The recommended system thereby would provide a means whereby users can make reports of spam calls as well as ask if others have reported a caller as a spammer. While the first few people called would get spammed, after a sufficient number of reports are made, further calls would be blocked. The recommended system would work on most types of telephonic platforms - smartphones, some feature phones, POTS lines, VoIP, PBX, and telephony providers - through the use of software and optional inline hardware. In addition to crowd-sourced blacklisting, we also recommend a means to whitelist specific numbers so that, for example, emergency calls will always go through."
"Large Scale Distributed Acoustic Modeling With Back-off N-grams.  Google Voice Search is an application that provides a data-rich setup for both language and acoustic modeling research. The approach we take revives an older approach to acoustic modeling that borrows from n-gram language modeling in an attempt to scale up both the amount of training data, and the model size (as measured by the number of parameters in the model), to approximately 100 times larger than current sizes used in automatic speech recognition. Speech recognition experiments are carried out in an N-best list rescoring framework for Google Voice Search. We use 87,000 hours of training data (speech along with transcription) obtained by filtering utterances in Voice Search logs on automatic speech recognition confidence. Models ranging in size between 20--40 million Gaussians are estimated using maximum likelihood training. They achieve relative reductions in word-error-rate of 11% and 6% when combined with first-pass models trained using maximum likelihood, and boosted maximum mutual information, respectively. Increasing the context size beyond five phones (quinphones) does not help."
"Improved Approximation Algorithms for (Budgeted) Node-weighted Steiner Problems.  Moss and Rabani [12] study constrained node-weighted Steiner tree problems with two independent weight values associated with each node, namely, cost and prize (or penalty). They give an O(logn)-approximation algorithm for the prize-collecting node-weighted Steiner tree problem (PCST)"
"S-links: Why distributed security policy requires secure introduction.  In this paper we argue that secure introduction via hyperlinks will be essential for distributing security policies on the web. The ""strict transport security"" policy, which makes HTTPS mandatory for a given domain, can already be expressed by links with an https URL. We propose s-links, a set of lightweight HTML extensions to express more complex security policies in links such as key pinning. This is the simplest and most efficient way to secure connections to new domains before persistent security policy can be negotiated directly, requiring no changes to the user experience and aligning trust decisions with the user's mental model. We show how s-links can benefit a variety of proposed protocols and discuss implications for the browser's same-origin policy."
"HTAF: Hybrid Testing Automation Framework to Leverage Local and Global Computing Resources.  In web application development, testing forms an increasingly large portion of software engineering costs due to the growing complexity and short time-to-market of these applications. This paper presents a hybrid testing automation framework (HTAF) that can automate routine works in testing and releasing web software. Using this framework, an individual software engineer can easily describe his routine software engineering tasks and schedule these described tasks by using both his local machine and global cloud computers in an efficient way. This framework is applied to commercial web software development processes. Our industry practice shows four example cases where the hybrid and decentralized architecture of HTAF is helpful at effectively managing both hardware resources and manpower required for testing and releasing web applications."
"Efficient and Accurate Label Propagation on Large Graphs and Label Sets.  Many web-based application areas must infer label distributions starting from a small set of sparse, noisy labels. Examples include searching for, recommending, and advertising against image, audio, and video content. These labeling problems must handle millions of interconnected entities (users, domains, content segments) and thousands of competing labels (interests, tags, recommendations, topics). Previous work has shown that graph-based propagation can be very effective at finding the best label distribution across nodes, starting from partial information and a weighted-connection graph. In their work on video recommendations, Baluja et al. [1] showed high-quality results using Adsorption, a normalized propagation process. An important step in the original formulation of Adsorption was re-normalization of the label vectors associated with each node, between every propagation step. That interleaved normalization forced computation of all label distributions, in synchrony, in order to allow the normalization to be correctly determined. Interleaved normalization also prevented use of standard linear-algebra methods, like stabilized bi-conjugate gradient descent (BiCGStab) and Gaussian elimination. This paper presents a method that replaces the interleaved normalization with a single pre-normalization, done once before the main propagation process starts, allowing use of selective label computation (label slicing) as well as large-matrix-solution methods. As a result, much larger graphs and label sets can be handled than in the original formulation and more accurate solutions can be found in fewer propagation steps. We also report results from using pre-normalized Adsorption in topic labeling for web domains, using label slicing and BiCGStab."
"Does Bug Prediction Support Human Developers? Findings from a Google Case Study.  While many bug prediction algorithms have been developed by academia, they're often only tested and verified in the lab using automated means. We do not have a strong idea about whether such algorithms are useful to guide human developers. We deployed a bug prediction algorithm across Google, and found no identifiable change in developer behavior. Using our experience, we provide several characteristics that bug prediction algorithms need to meet in order to be accepted by human developers and truly change how developers evaluate their code."
"Paradata in Web Surveys.  An important technical distinction regarding the collection of paradata in web surveys is that they can be collected on the server side and/or the client side. In web surveys, paradata is categorized into device-type paradata and questionnaire navigation paradata. Device-type paradata provide information regarding the kind of device used to complete the survey. Questionnaire navigation paradata describe the entire process of filling out the questionnaire. This chapter provides examples of usage for device-type and questionnaire navigation paradata. Another use of paradata pioneered in the early 2000s by Jeavons is adaptive scripting. Adaptive scripting refers to using paradata in real time to change the survey experience for the respondent. The chapter also discusses two main classes of software to collect paradata such as specific paradata software and paradata collection tools embedded in commercial and non-commercial survey platforms. Ethical and communication issues are important considerations in using web survey paradata."
"Web Coverage in the UK and its Potential Impact on General Population Web Surveys.  Mario Callegaro (Google UK) provided some data on internet access in the UK and the digital divide. He concluded that the UK internet access is steadily increasing and is likely to soon reach a level of almost universal coverage. But high coverage does not imply that everyone with access would be capable or willing to take part in web surveys. Furthermore, internet access is becoming mobile (e.g. Smartphone) and respondents are using a wide variety of devices to answer web surveys. Making web surveys"
"Classifying YouTube Channels: a Practical System.  This paper presents a framework for categorizing channels of videos in a thematic taxonomy with high precision and coverage.  The proposed approach consists of three main steps.  First, videos are annotated by semantic entities describing their central topics.  Second, semantic entities are mapped to categories using a combination of classifiers. Last, the categorization of channels is obtained by combining the results of both previous steps. This framework has been deployed on the whole corpus of YouTube, in 8 languages, and used to build several user facing products.  Beyond the description of the framework, this paper gives insight into practical aspects and experience: rationale from product requirements to the choice of the solution, spam filtering, human-based evaluations of the quality of the results, and measured metrics on the live site."
"Ad Click Prediction: a View from the Trenches.  Predicting ad click--through rates (CTR) is a massive-scale learning
  problem that is central to the multi-billion dollar online
  advertising industry.  We present a selection of case studies and
  topics drawn from recent experiments in the setting of a deployed
  CTR prediction system.  These include improvements in the context of
  traditional supervised learning based on an FTRL-Proximal online
  learning algorithm (which has excellent sparsity and convergence
  properties) and the use of per-coordinate learning rates. We also explore some of the challenges that arise in a real-world
  system that may appear at first to be outside the domain of
  traditional machine learning research.  These include useful tricks
  for memory savings, methods for assessing and visualizing
  performance, practical methods for providing confidence estimates
  for predicted probabilities, calibration methods, and methods for
  automated management of features.  Finally, we also detail several
  directions that did not turn out to be beneficial for us, despite
  promising results elsewhere in the literature.  The goal of this
  paper is to highlight the close relationship between theoretical
  advances and practical engineering in this industrial setting, and
  to show the depth of challenges that appear when applying
  traditional machine learning methods in a complex dynamic system."
"Student-t based Robust Spatio-Temporal Prediction.  This paper describes an efficient and effective design of Robust Spatio-Temporal Prediction based on Student??s t distribution, namely, St-RSTP, to provide estimations based on observations over spatio-temporal neighbors. The proposed St-RSTP is more resilient to outliers or other small departures from model assumptions than its ancestor, the Spatio-Temporal Random Effects (STRE) model. STRE is a state-of-the-art statistical model with linear order complexity for large scale processing. However, it assumes Gaussian observations, which has the well-known limitation of non-robustness. In our St-RSTP design, the measurement error follows Student??s t distribution, instead of a traditional Gaussian distribution. This design reduces the influence of outliers, improves prediction quality, and keeps the problem analytically intractable. We propose a novel approximate inference approach, which approximates the model into the form that separates the high dimensional latent variables into groups, and then estimates the posterior distributions of different groups of variables separately in the framework of Expectation Propagation. As a good property, our approximate approach degeneralizes to the standard STRE based prediction, when the degree of freedom of the Student??s t distribution is set to infinite. Extensive experimental evaluations based on both simulation and real-life data sets demonstrated the robustness and the efficiency of our Student-t prediction model. The proposed approach provides critical functionality for stochastic processes on spatio-temporal data."
"HEADY: News headline abstraction through event pattern clustering.  This paper presents HEADY: a novel, ab- stractive approach for headline generation from news collections. From a web-scale corpus of English news, we mine syntactic patterns that a Noisy-OR model generalizes into event descriptions. At inference time, we query the model with the patterns observed in an unseen news collection, identify the event that better captures the gist of the collection and retrieve the most appropriate pattern to generate a headline. HEADY improves over a state-of-the- art open-domain title abstraction method, bridging half of the gap that separates it from extractive methods using human-generated titles in manual evaluations, and performs comparably to human-generated headlines as evaluated with ROUGE."
"Recurrent Neural Networks for Voice Activity Detection.  We present a novel recurrent neural network (RNN) model for voice activity detection. Our multi-layer RNN model, in which nodes compute quadratic polynomials, outperforms a much larger baseline system composed of Gaussian mixture models (GMMs) and a hand-tuned state machine (SM) for temporal smoothing. All parameters of our RNN model are optimized together, so that it properly weights its preference for temporal continuity against the acoustic features in each frame. Our RNN uses one tenth the parameters and outperforms the GMM+SM baseline system by 26% reduction in false alarms, reducing overall speech recognition computation time by 17% while reducing word error rate by 1% relative."
"Language-Independent Discriminative Parsing of Temporal Expressions.  Temporal resolution systems are traditionally tuned to a particular language, requiring significant human effort to translate them to new languages. We present a language independent semantic parser for learning the interpretation of temporal phrases given only a corpus of utterances and the times they reference. We make use of a latent parse that encodes a language-flexible representation of time, and extract rich features over both the parse and associated temporal semantics. The parameters of the model are learned using a weakly supervised bootstrapping approach, without lexical cues or language-specific tuning. We achieve state-of-the-art accuracy on all languages in the TempEval-2 temporal normalization task, reporting a 4% improvement in both English and Spanish accuracy, and to our knowledge the first results for four other languages."
"Perception and Understanding of Social Annotations in Web Search.  As web search increasingly becomes reliant on social signals, it is imperative for us to understand the effect of these signals on users' behavior. There are multiple ways in which social signals can be used in search: (a) to surface and rank important social content; (b) to signal to users which results are more trustworthy and important by placing annotations on search results. We focus on the latter problem of understanding how social annotations affect user behavior. In previous work, through eyetracking research we learned that users do not generally seem to fixate on social annotations when they are placed at the bottom of the search result block, with 11% probability of fixation [22]. A second eyetracking study showed that placing the annotation on top of the snippet block might mitigate this issue [22], but this study was conducted using mock-ups and with expert searchers. In this paper, we describe a study conducted with a new eyetracking mix-method using a live traffic search engine with the suggested design changes on real users using the same experimental procedures. The study comprised of 11 subjects with an average of 18 tasks per subject using an eyetrace-assisted retrospective think-aloud protocol. Using a funnel analysis, we found that users are indeed more likely to notice the annotations with a 60% probability of fixation (if the annotation was in view). Moreover, we found no learning effects across search sessions but found significant differences in query types, with subjects having a lower chance of fixating on annotations for queries in the news category. In the interview portion of the study, users reported interesting ""wow"" moments as well as usefulness in recalling or re-finding content previously shared by oneself or friends. The results not only shed light on how social annotations should be designed in search engines, but also how users make use of social annotations to make decisions about which pages are useful and potentially trustworthy."
"All the news that's fit to read: a study of social annotations for news reading.  As news reading becomes more social, how do different types of annotations affect people's selection of news articles? This paper reports on results from two experiments looking at social annotations in two different news reading contexts. The first experiment simulates a logged-out experience with annotations from strangers, a computer agent, and a branded company. Results indicate that, perhaps unsurprisingly, annotations by strangers have no persuasive effects. However, surprisingly, unknown branded companies still had a persuasive effect. The second experiment simulates a logged-in experience with annotations from friends, finding that friend annotations are both persuasive and improve user satisfaction over their article selections. In post-experiment interviews, we found that this increased satisfaction is due partly because of the context that annotations add. That is, friend annotations both help people decide what to read, and provide social context that improves engagement. Interviews also suggest subtle expertise effects. We discuss implications for design of social annotation systems and suggestions for future research."
"Swipe vs. scroll: web page switching on mobile browsers.  Tabbed web browsing interfaces enable users to multi-task and easily switch between open web pages. However, tabbed browsing is difficult for mobile web browsers due to the limited screen space and the reduced precision of touch. We present an experiment comparing Safari's pages-based switching interface using horizontal swiping gestures with the stacked cards-based switching interface using vertical scrolling gestures, introduced by Chrome. The results of our experiment show that cards-based switching interface allows for faster switching and is less frustrating, with no significant effect on error rates. We generalize these findings, and provide design implications for mobile information spaces."
"Hunting in the Enterprise: Forensic Triage and Incident Response.  In enterprise environments, digital forensic analysis generates data volumes that traditional forensic methods are no longer prepared to handle. Triaging has been proposed as a solution to systematically prioritize the acquisition and analysis of digital evidence. We explore the application of automated triaging processes in such settings, where reliability and customizability are crucial for a successful deployment. We specifically examine the use of GRR Rapid Response (GRR) ?? an advanced open source distributed enterprise forensics system ?? in the triaging stage of common incident response investigations. We show how this system can be leveraged for automated prioritization of evidence across the whole enterprise fleet and describe the implementation details required to obtain sufficient robustness for large scale enterprise deployment. We analyze the performance of the system by simulating several realistic incidents and discuss some of the limitations of distributed agent based systems for enterprise triaging."
"Brief Announcement: Consistency and Complexity Tradeoffs for Highly-Available Multi-Cloud Store.  An emerging
multi-cloud storage paradigm suggests replicating data across multiple cloud storage
services, potentially operated by distinct providers. In this paper, we study the impact of the storage
interfaces and consistency semantics exposed by individual clouds on the complexity of the reliable
multi-cloud storage implementation. Our results establish several inherent space and time tradeoffs
associated with emulating reliable objects over a collection of unreliable storage services with varied
interfaces and consistency guarantees."
"Reducing Web Latency: the Virtue of Gentle Aggression.  To serve users quickly, Web service providers build infrastructure closer to clients and use multi-stage transport connections. Although these changes reduce client-perceived round-trip times, TCP's current mechanisms fundamentally limit latency improvements. We performed a measurement study of a large Web service provider and found that, while connections with no loss complete close to the ideal latency of one round-trip time, TCP's timeout-driven recovery causes transfers with loss to take five times longer on average. In this paper, we present the design of novel loss recovery mechanisms for TCP that judiciously use redundant transmissions to minimize timeout-driven recovery. Proactive, Reactive, and Corrective are three qualitatively different, easily-deployable mechanisms that (1) proactively recover from losses, (2) recover from them as quickly as possible, and (3) reconstruct packets to mask loss. Crucially, the mechanisms are compatible both with middleboxes and with TCP's existing congestion control and loss recovery. Our large-scale experiments on Google's production network that serves billions of flows demonstrate a 23% decrease in the mean and 47% in 99th percentile latency over today's TCP."
"Minimizing change aversion for the Google Drive launch.  Change aversion is a natural response, which technology often exacerbates. Evolutionary changes can be subtle and occur over many generations. But Internet users must sometimes deal with sudden, significant product changes to applications they rely on and identify with. Despite the best intentions of designers and product managers, users often experience anxiety and confusion when faced with a new interface or changed functionality. While some change aversion is often inevitable, it can also be managed and minimized with the right steps. This case study describes how our understanding of change aversion helped minimize negative effects for the transition of the Google Docs List to Google Drive, a product for file storage in the cloud. We describe actions that allowed for a launch with no aversion."
"Efficient Estimation of Word Representations in Vector Space.  We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities."
"packetdrill: Scriptable Network Stack Testing, from Sockets to Packets.  Testing today??s increasingly complex network protocol implementations can be a painstaking process. To help meet this challenge, we developed packetdrill, a portable, open-source scripting tool that enables testing the correctness and performance of entire TCP/UDP/IP network stack implementations, from the system call layer to the hardware network interface, for both IPv4 and IPv6. We describe the design and implementation of the tool, and our experiences using it to execute 657 test cases. The tool was instrumental in our development of three new features for Linux TCP??Early Retransmit, Fast Open, and Loss Probes??and allowed us to find and fix 10 bugs in Linux. Our team uses packetdrill in all phases of the development process for the kernel used in one of the world??s largest Linux installations."
"Photon: Fault-tolerant and Scalable Joining of Continuous Data Streams.  Web-based enterprises process events generated by millions of users interacting with their websites. Rich statistical data distilled from combining such interactions in near real-time generates enormous business value. In this paper, we describe the architecture of Photon, a geographically distributed system for joining multiple continuously flowing streams of data in real-time with high scalability and low latency, where the streams may be unordered or delayed. The system fully tolerates infrastructure degradation and datacenter-level outages without any manual intervention. Photon guarantees that there will be no duplicates in the joined output (at-most-once semantics) at any point in time, that most joinable events will be present in the output in real-time (near-exact semantics), and exactly-once semantics eventually. Photon is deployed within Google Advertising System to join data streams such as web search queries and user clicks on advertisements. It produces joined logs that are used to derive key business metrics, including billing for advertisers. Our production deployment processes millions of events per minute at peak with an average end-to-end latency of less than 10 seconds. We also present challenges and solutions in maintaining large persistent state across geographically distant locations, and highlight the design principles that emerged from our experience."
"Learning Hierarchical Bag of Words Using Naive Bayes Clustering.  Image analysis tasks such as classication, clustering, detection, and retrieval are only as good as the feature representation of the images they use. Much research in computer vision is focused on finding better or semantically richer image representations. Bag of visual Words (BoW) is a representation that has emerged as an eective one for a variety of computer vision tasks. BoW methods traditionally use low level features. We have devised a strategy to use these low level features to create \higher level"" features by making use of the spatial context in images. In this paper, we propose a novel hierarchical feature learning framework that uses a Naive Bayes Clustering algorithm to convert a 2-D symbolic image at one level to a 2-D symbolic image at the next level with richer features. On two popular datasets, Pascal VOC 2007 and Caltech 101, we empirically show that classication accuracy obtained from the hierarchical features computed using our approach is signicantly higher than the traditional SIFT based BoW representation of images even though our image representations are more compact."
"Alice in Warningland: A Large-Scale Field Study of Browser Security Warning Effectiveness.  We empirically assess whether browser security warnings are as ineffective as
suggested by popular opinion and previous literature. We used Mozilla Firefox
and Google Chrome's in-browser telemetry to observe over 25 million warning
impressions in situ. During our field study, users continued through a tenth of Mozilla Firefox's malware and phishing warnings, a quarter of Google Chrome's malware and phishing warnings, and a third of Mozilla Firefox's SSL warnings. This demonstrates that security warnings can be effective in practice; security experts and system architects should not dismiss the goal of communicating security information to end users. We also find that user behavior varies across warnings. In contrast to the other warnings, users continued through 70.2% of Google Chrome's SSL warnings. This indicates that the user experience of a warning can have a significant impact on user behavior. Based on our findings, we make recommendations for warning designers and researchers."
"On the Technology Prospects and Investment Opportunities for Scalable Neuroscience.  Two major initiatives to accelerate research in the brain sciences have focused attention on developing a new generation of scientific instruments for neuroscience. These instruments will be used to record static (structural) and dynamic (behavioral) information at unprecedented spatial and temporal resolution and report out that information in a form suitable for computational analysis. We distinguish between recording ?? taking measurements of individual cells and the extracellular matrix ?? and reporting ?? transcoding, packaging and transmitting the resulting information for subsequent analysis ?? as these represent very different challenges as we scale the relevant technologies to support simultaneously tracking the many neurons that comprise neural circuits of interest. We investigate a diverse set of technologies with the purpose of anticipating their development over the span of the next 10 years and categorizing their impact in terms of short-term [1-2 years], medium-term [2-5 years] and longer-term [5-10 years] deliverables."
"RFC6928 - Increasing TCP's Initial Window.  This document proposes an experiment to increase the permitted TCP initial window (IW) from between 2 and 4 segments, as specified in RFC 3390, to 10 segments with a fallback to the existing recommendation when performance issues are detected. It discusses the motivation behind the increase, the advantages and disadvantages of the higher initial window, and presents results from several large-scale experiments showing that the higher initial window improves the overall performance of many web services without resulting in a congestion collapse. The document closes with a discussion of usage and deployment for further experimental purposes recommended by the IETF TCP Maintenance and Minor Extensions (TCPM) working group."
"Rogue Femtocell Owners: How Mallory Can Monitor My Devices.  Femtocells are small cellular telecommunication base stations that provide improved cellular coverage. These devices provide important improvements in coverage, battery life and throughput, they also present security challenges. We identify a problem which has not been identified in previous studies of femtocell security: rogue owners of femtocells can secretly monitor third-party mobile devices
by using the femtocell's access control features. We present traffic analysis of real femtocell traces are presented and demonstrate the ability to monitor mobile devices through classification of the femtocell's encrypted backhaul traffic. We also consider the femtocell's power usage and status LEDs as other side channels that provide information on the femtocell's operation. We conclude by presenting suitable solutions to overcome this problem."
"Point Representation for Local Optimization: Towards Multi-Dimensional Gray Codes.  In the context of stochastic search, once regions of high performance are found, having the property that small changes in the candidate solution correspond to searching nearby neighborhoods provides the ability to perform effective local optimization. To achieve this, Gray Codes are often employed for encoding ordinal points or discretized real numbers. In this paper, we present a method to label similar and/or close points within arbitrary graphs with small Hamming distances. The resultant point labels can be viewed as an approximate high-dimensional variant of Gray Codes. The labeling procedure is useful for any task in which the solution requires the search algorithm to select a small subset of items out of many. A large number of empirical results using these encodings with a combination of genetic algorithms and hill-climbing are presented."
"Neighborhood Preserving Codes for Assigning Point Labels: Applications to Stochastic Search.  Selecting a good representation of a solution-space is vital to solving any search and optimization problem. In particular, once regions of high performance are found, having the property that small changes in the candidate solution correspond to searching nearby neighborhoods provides the ability to perform effective local optimization. To achieve this, it is common for stochastic search algorithms, such as stochastic hillclimbing, evolutionary algorithms (including genetic algorithms), and simulated annealing, to employ Gray Codes for encoding ordinal points or discretized real numbers. In this paper, we present a novel method to label similar and/or close points within arbitrary graphs with small Hamming distances. The resultant point labels can be seen as an approximate high-dimensional variant of Gray Codes with standard Gray Codes as a subset of the labels found here. The labeling procedure is applicable to any task in which the solution requires the search algorithm to select a small subset of items out of many. Such tasks include vertex selection in graphs, knapsack-constrained item selection, bin packing, prototype selection for machine learning, and numerous scheduling problems, to name a few."
"Incremental Clicks Impact of Mobile Search Advertising.  In this research, we examine how the number of mobile organic clicks changes when advertisers significantly change their mobile ad spend. This continues the line of research of search ads pause by applying it to the mobile platform. We utilize a statistical model to estimate the fraction of clicks that can be attributed to mobile search advertising. A metastudy of 327 advertisers reveals that 88% of ad clicks are incremental, in the sense that the visits to the advertiser??s site would not have occurred without the mobile ad campaigns."
"Cluster forest.  With inspiration from Random Forests (RF) in the context of classification, a new clustering ensemble method---Cluster Forests (CF) is proposed. Geometrically, CF randomly probes a high-dimensional data cloud to obtain ""good local clusterings"" and then aggregates via spectral clustering to obtain cluster assignments for the whole dataset. The search for good local clusterings is guided by a cluster quality measure kappa. CF progressively improves each local clustering in a fashion that resembles the tree growth in RF. Empirical studies on several real-world datasets under two different performance metrics show that CF compares favorably to its competitors. Theoretical analysis reveals that the kappa measure makes it possible to grow the local clustering in a desirable way---it is ""noise-resistant"". A closed-form expression is obtained for the mis-clustering rate of spectral clustering under a perturbation model, which yields new insights into some aspects of spectral clustering."
"Understanding Indoor Scenes using 3D Geometric Phrases.  Visual scene understanding is a difficult problem interleaving object detection, geometric reasoning and scene classification. We present a hierarchical scene model for learning and reasoning about complex indoor scenes which is computationally tractable, can be learned from a reasonable amount of training data, and avoids oversimplification. At the core of this approach is the 3D Geometric Phrase Model which captures the semantic and geometric relationships between objects which frequently co-occur in the same 3D spatial configuration. Experiments show that this model effectively explains scene semantics, geometry and object groupings from a single image, while also improving individual object detections."
"Large-Scale Automated Refactoring Using ClangMR.  Visual scene understanding is a difficult problem interleaving object detection, geometric reasoning and scene classification. We present a hierarchical scene model for learning and reasoning about complex indoor scenes which is computationally tractable, can be learned from a reasonable amount of training data, and avoids oversimplification. At the core of this approach is the 3D Geometric Phrase Model which captures the semantic and geometric relationships between objects which frequently co-occur in the same 3D spatial configuration. Experiments show that this model effectively explains scene semantics, geometry and object groupings from a single image, while also improving individual object detections."
"F1: A Distributed SQL Database That Scales.  F1 is a distributed relational database system built at
Google to support the AdWords business. F1 is a hybrid
database that combines high availability, the scalability of
NoSQL systems like Bigtable, and the consistency and usability of traditional SQL databases. F1 is built on Spanner, which provides synchronous cross-datacenter replication and strong consistency. Synchronous replication implies higher commit latency, but we mitigate that latency
by using a hierarchical schema model with structured data
types and through smart application design. F1 also includes a fully functional distributed SQL query engine and
automatic change tracking and publishing."
"Smoothed marginal distribution constraints for language modeling.  We present an algorithm for re-estimating parameters of backoff n-gram language models so as to preserve given marginal distributions, along the lines of well-known Kneser-Ney smoothing.  Unlike Kneser-Ney, our approach is designed to be applied to any given smoothed backoff model, including models that have already been heavily pruned.  As a result, the algorithm avoids issues observed when pruning Kneser-Ney models (Siivola et al., 2007; Chelba et al., 2010), while retaining the benefits of such marginal distribution constraints.  We present experimental results for heavily pruned backoff n-gram models, and demonstrate perplexity and word error rate reductions when used with various baseline smoothing methods.  An open-source version of the algorithm has been released as part of the OpenGrm ngram library."
"Behavior-Oriented Data Resource Management in Medical Sensing Systems.  Wearable sensing systems have recently enabled a variety of medical monitoring and diagnostic applications in wireless health. The need for multiple sensors and constant monitoring leads these systems to be power hungry and expensive with short operating lifetimes. We introduce a novel methodology that takes advantage of contextual and semantic properties in human behavior to enable efficient design and optimization of such systems from the data and information point of view. This, in turn, directly influences the wireless communication and local processing power consumption. We exploit intrinsic space and temporal correlations between sensor data while considering both user and system contextual behavior. Our goal is to select a small subset of sensors that accurately capture and/or predict all possible signals of a fully instrumented wearable sensing system. Our approach leverages novel modeling, partitioning, and behavioral optimization, which consists of signal characterization, segmentation and time shifting, mutual signal prediction, and a simultaneous minimization composed of subset sensor selection and opportunistic sampling. We demonstrate the effectiveness of the technique on an insole instrumented with 99 pressure sensors placed in each shoe, which cover the bottom of the entire foot, resulting in energy reduction of 72% to 97% for error rates of 5% to 17.5%."
"HERMES: Mobile system for instability analysis and balance assessment.  We introduce Hermes, a lightweight smart shoe and its supporting infrastructure aimed at extending gait and instability analysis and human instability/balance monitoring outside of a laboratory environment. We aimed to create a scientific tool capable of high-level measures, by combining embedded sensing, signal processing and modeling techniques. Hermes monitors walking behavior and uses an instability assessment model to generate quantitative value with episodes of activity identified by physician, researchers or investigators as important. The underlying instability assessment model incorporates variability and correlation of features extracted during ambulation that have been identified by geriatric motion study experts as precursor to instability, balance abnormality and possible fall risk. Hermes provides a mobile, affordable and long-term instability analysis and detection system that is customizable to individual users, and is context-aware, with the capability of being guided by experts. Our experiments demonstrate the feasibility of our model and the complimentary role our system can play by providing long-term monitoring of patients outside a hospital or clinical setting at a reduced cost, with greater user convenience, compliance and inference capabilities that meet the physician's or investigator's needs."
"Joint consideration of energy-efficiency and coverage-preservation in microsensor networks.  This paper presents an energy-efficient and coverage-preserving communication protocol which distributes a uniform energy load to the sensors in a wireless microsensor network. This protocol, called Distance-based Segmentation (DBS), is a cluster-based protocol that divides the entire network into equal-area segments and applies different clustering policies to each segment to (1) reduce total energy dissipation and (2) balance the energy load among the sensors. Therefore, it prolongs the lifetime of the network and improves the sensing coverage. Moreover, the proposed routing protocol does not need any centralized support from a certain node which is at odds with aiming to establish a scalable communication protocol. Results from extensive simulations on two different network configurations show that by lowering the number of wasteful transmissions in the network, the DBS can achieve as much as a 20% reduction in total dissipated energy as compared with current cluster-based protocols. In addition, this protocol is able to distribute energy load more evenly among the sensors in the network. Hence, it yields up to a 66% increase in the useful network lifetime. According to the simulation results, the sensing coverage degradation of the DBS is considerably slower than that of the other cluster-based protocols."
"Spatiotemporal Assignment of Energy Harvesters on a Self-Sustaining Medical Shoe.  We present a new method for spatiotemporal assignment and scheduling of energy harvesters on a medical shoe tasked with measuring gait diagnostics. While prior work exists on the application of dielectric elastomers (DEs) for energy scavenging on shoes, current literature does not address the issues of placement and timing of these harvesters, nor does it address integration into existing sensing systems. We solve these issues and present a self-sustaining medical shoe that harvests energy from human ambulation while simultaneously measuring gait characteristics most relevant to medical diagnosis."
"Semantics-driven sensor configuration for energy reduction in medical sensor networks.  Traditional optimization methods for large multisensory networks often use sensor array reduction and sampling techniques that attempt to reduce energy while retaining full predictability of the raw sensed data. For systems such as medical sensor networks, raw data prediction is unnecessary, rather, only relevant semantics derived from the raw data are essential. We present a new method for sensor fusion, array reduction, and subsampling that reduces both energy and cost through semantics-driven system configuration. Using our method, we reduce the energy requirements of a medical shoe by a factor of 17.9 over the original system configuration while maintaining semantic relevance."
"Energy Optimization in Wireless Medical Systems Using Physiological Behavior.  Wearable sensing systems are becoming widely used for a variety of applications, including sports, entertainment, and military. These systems have recently enabled a variety of medical monitoring and diagnostic applications in Wireless Health. The need for multiple sensors and constant monitoring lead these systems to be power hungry and expensive, with short operating lifetimes. In this paper, we introduce a novel methodology that takes advantage of the influence of human behavior on signal properties and reduces those three metrics from the data size point of view. This, in turn, directly influences the wireless communication and local processing power consumption. We exploit intrinsic space and temporal correlations between sensor data while considering both user and system behavior. Our goal is to select a small subset of sensors to accurately capture and/or predict all possible signals of a fully instrumented wearable sensing system. Our approach leverages novel modeling, partitioning, and behavioral optimization, which consists of signal characterization, segmentation and time shifting, mutual signal prediction, and subset sensor selection. We demonstrate the effectiveness of the technique on an insole instrumented with 99 pressure sensors placed in each shoe, which cover the bottom of the entire foot, resulting in energy reduction of 56% to 96% for error rates of 5% to 17.5%."
"Semantic Multimodal Compression for Wearable sensing Systems.  Wearable sensing systems (WSS's) are emerging as an important class of distributed embedded systems in application domains ranging from medical to military. Such systems can be expensive and power hungry due to their multi sensor implementations that require constant use, yet by nature they demand low-cost and low-power implementations. Semantic multimodal compression (SMC) mitigates these metrics in terms of data size by leveraging the natural tendency of signals in many types of embedded sensing systems to be composed of phases. In our driving example of a medical shoe with an insole lined with pressure sensors, we find that the natural airborne, landing, and take-off segments have sharply different and repetitive properties. SMC models and compresses each segment independently, selecting the best compression scheme for each segment and thus reducing total transmission energy."
"Remote Medical Monitoring Through Vehicular Ad Hoc Network.  Several diseases and medical conditions require constant monitoring of physiological signals and vital signs on daily bases, such as diabetics, hypertension and etc. In order to make these patients capable of living their daily life it is necessary to provide a platform and infrastructure that allows the constant collection of physiological data even when the patient is not inside of the coverage area. The data must be rapidly ""transported"" to care givers or to the designated medical enterprise. The problem is particularly severe in case of emergencies (e.g. natural disasters or hostile attacks) when the communications infrastructure (e.g. cellular telephony, WiFi public access, etc) has failed or is totally congested. In this paper we present an evaluation of of the vehicular ad-hoc networks (VANET) as an alternate method of collecting patient pre-recorded physiological data and at the same time reconfiguring patient medical wearable body vests to select the data specifically requested by the physicians. Another important use of vehicular collection of medical data from body vests is prompted by the need to correlate pedestrian reaction to vehicular traffic hazards such as chemical and noise pollution and traffic congestion. The vehicles collect noise, chemical and traffic samples and can directly correlate with the ""stress level"" of volunteers."
"Rendering Fur in Life of Pi.  We discuss the innovative fur rendering technology that Rhythm &amp; Hues deployed in the photorealistic depiction of the tiger Richard Parker for Ang Lee's Academy-Award-winning feature film ""Life of Pi""."
"Online, Asynchronous Schema Change in F1.  We introduce a protocol for schema evolution in a globally
distributed database management system with shared data,
stateless servers, and no global membership. Our protocol
is asynchronous??it allows different servers in the database
system to transition to a new schema at different times??and
online??all servers can access and update all data during a
schema change. We provide a formal model for determining
the correctness of schema changes under these conditions,
and we demonstrate that many common schema changes can
cause anomalies and database corruption. We avoid these
problems by replacing corruption-causing schema changes
with a sequence of schema changes that is guaranteed to
avoid corrupting the database so long as all servers are no
more than one schema version behind at any time. Finally,
we discuss a practical implementation of our protocol in
F1, the database management system that stores data for
Google AdWords."
"MillWheel: Fault-Tolerant Stream Processing at Internet Scale.  MillWheel is a framework for building low-latency data-processing applications that is widely used at Google. Users specify a directed computation graph and application code for individual nodes, and the system manages persistent state and the continuous flow of records, all within the envelope of the framework's fault-tolerance guarantees. This paper describes MillWheel's programming model as well as its implementation. The case study of a continuous anomaly detector in use at Google serves to motivate how many of MillWheel's features are used. MillWheel's programming model provides a notion of logical time, making it simple to write time-based aggregations. MillWheel was designed from the outset with fault tolerance and scalability in mind. In practice, we find that MillWheel's unique combination of scalability, fault tolerance, and a versatile programming model lends itself to a wide variety of problems at Google."
"Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search.  We propose two solutions for both nearest neigh-
bors and range search problems. For the nearest
neighbors problem, we propose a c-approximate so-
lution for the restricted version of the decision prob-
lem with bounded radius which is then reduced to
the nearest neighbors by a known reduction. For
range searching we propose a scheme that learns
the parameters in a learning stage adopting them
to the case of a set of points with low intrinsic
dimension that are embedded in high dimensional
space (common scenario for image point descrip-
tors). We compare our algorithms to the best known
methods for these problems, i.e. LSH, ANN and
FLANN. We show analytically and experimentally
that we can do better for moderate approximation
factor. In contrast to tree structures, our algorithms
are trivial to parallelize. In the experiments con-
ducted, running on couple of million images, our
algorithms show meaningful speed-ups when com-
pared with the above mentioned methods."
"Overcoming the Lack of Parallel Data in Sentence Compression.  A subset of the described data (10,000 sentence &amp; extracted headlines pairs, with source URL and annotations) is available for <a href=""http://storage.googleapis.com/sentencecomp/compression-data.json"">download</a>."
"Learning Prices for Repeated Auctions with Strategic Buyers.  Inspired by real-time ad exchanges for online display advertising, we consider the problem of inferring a buyer??s value for a good when the buyer is repeatedly interacting with the seller through a posted-price mechanism. We model the buyer as a strategic agent, interested in maximizing her long-term surplus, and are interested in optimizing seller revenue. We show conditions under which the seller cannot hope to gain an advantage by learning the buyer??s value ?? i.e. the buyer can always manipulate the exchange to hide her value. This result is accompanied by a seller algorithm that is able to achieve no-regret when the buyer is unable to incur the short-term costs of such manipulation."
"SAC057 - ICANN SSAC Advisory on Internal Name Certificates.  The SSAC has identified a Certificate Authority (CA) practice that, if widely exploited, could pose a significant risk to the privacy and integrity of secure Internet  communications. This CA practice could impact the new gTLD program. The SSAC thus advises ICANN take immediate steps to mitigate the risks."
"SAC056 - ICANN SSAC Advisory on Impacts of Content Blocking via the Domain Name System.  The use of Domain Name System (DNS) blocking to limit access to resources on
the Internet has become a topic of interest in numerousInternet governance
venues. Several governments around the world, whether by law, treaty, court
order, law enforcement action, or other actions or agreements, have either
implemented DNS blocking or are actively considering doing so. However, due to
the Internet??s architecture, blocking by domain name can be easily bypassed by
end users and is thus likely to be largely ineffective in the long term and fraught
with unanticipated consequences in the near term. In addition, DNS blocking can
present conflicts with the adoption of DNS Security Extensions(DNSSEC) and
could promote balkanization of the Internet into a country-by-country view of the
Internet??s name space."
"Systematic Analysis of Challenge-Driven Improvements in Molecular Prognostic Models for Breast Cancer.  Although molecular prognostics in breast cancer are among the most successful examples of translating genomic analysis to clinical applications, optimal approaches to breast cancer clinical risk prediction remain controversial. The Sage Bionetworks??DREAM Breast Cancer Prognosis Challenge (BCC) is a crowdsourced research study for breast cancer prognostic modeling using genome-scale data. The BCC provided a community of data analysts with a common platform for data access and blinded evaluation of model accuracy in predicting breast cancer survival on the basis of gene expression data, copy number data, and clinical covariates. This approach offered the opportunity to assess whether a crowdsourced community Challenge would generate models of breast cancer prognosis commensurate with or exceeding current best-in-class approaches. The BCC comprised multiple rounds of blinded evaluations on held-out portions of data on 1981 patients, resulting in more than 1400 models submitted as open source code. Participants then retrained their models on the full data set of 1981 samples and submitted up to five models for validation in a newly generated data set of 184 breast cancer patients. Analysis of the BCC results suggests that the best-performing modeling strategy outperformed previously reported methods in blinded evaluations; model performance was consistent across several independent evaluations; and aggregating community-developed models achieved performance on par with the best-performing individual models."
"Real-time communications for the web.  This article provides an overview of the work that W3C and IETF are doing toward defining a framework, protocols, and application programming interfaces that will provide real-time interactive voice, video, and data in web browsers and other applications. The article explains how media and data will flow in a peer-to-peer style directly between two web browsers. This explains the protocols used to transport and secure the encrypted media, traverse NATs and firewalls, negotiate media capabilities, and provide identity for the media."
"Cross Platform Network Access Control.  Discussion of Capirca, an open-sourced multi-platform Network ACL generation system. This talk will discuss the history of Capirca, originating as an internal Google project through its current form and use in the open-source community. Attendees will gain an understand of how to use the system to simplify and improve the efficiency and reliability of network security management. A significant portion of time will also be dedicated to an overview of how the software and libraries work internally, including how to develop new modules and contribute to the open source effort."
"Street View Motion-from-Structure-from-Motion.  We describe a structure-from-motion framework that handles ""generalized"" cameras, such as moving rolling-shutter cameras, and works at an unprecedented scale--billions of images covering millions of linear kilometers of roads--by exploiting a good relative pose prior along vehicle paths. We exhibit a planet-scale, appearance-augmented point cloud constructed with our framework and demonstrate its practical use in correcting the pose of a street-level image collection."
"The Prospect of Inter-Data-Center Optical Networks.  Mega data centers and their interconnection
networks have drawn great attention in recent
years because of the rapid public adoption of
cloud-based services. The unprecedented
amount of data that needs to be communicated
between data centers imposes new requirements
and challenges to inter-data-center optical networks.
In this article, we discuss the traffic
growth trends and capacity demands of Google??s
inter-data-center network, and how they
drive the network architectures and technologies
to scale capacities and operational ease on existing
fiber plants. We extensively review recent
research findings and emerging technologies,
such as digital coherent detection and the flexgrid
dense wavelength-division multiplexed channel
plan, and propose practical implementations,
such as C+L-band transmission, packet and
optical layer integration, and a software-defined
networking enabled network architecture for
both capacity and operational scaling. In addition,
we point out a few critical areas that require
more attention and research to improve efficiency
and flexibility of an inter-data-center optical
network: optical regeneration, data rate mismatch
between Ethernet and optical transport,
and real-time optical performance monitoring."
"A Butterfly Structured Design of The Hybrid Transform Coding Scheme.  The hybrid transform coding scheme that alternates amongst the asymmetric discrete sine transform (ADST) and the discrete cosine transform (DCT) depending on the boundary prediction conditions, is an efficient tool for video and image compression. It optimally exploits the statistical characteristics of prediction residual, thereby achieving significant coding performance gains over the conventional DCT-based approach. A practical concern lies in the intrinsic conflict between transform kernels of ADST and DCT, which prevents a butterfly structured implementation for parallel computing. Hence the hybrid transform coding scheme has to rely on matrix multiplication, which presents a speed-up barrier due to under-utilization of the hardware, especially for larger block sizes. In this work, we devise a novel ADST-like transform whose kernel is consistent with that of DCT, thereby enabling butterfly structured computation flow, while largely retaining the performance advantages of hybrid transform coding scheme in terms of compression efficiency. A prototype implementation of the proposed butterfly structured hybrid transform coding scheme is available in the VP9 codec repository."
"Exploring and enhancing the user experience for TV.  This workshop seeks to help increase the volume and quality of HCI research and innovative practice around user interfaces for television. Internet connectivity is driving a rapid increase in the range and scope of interactive experiences on the TV platform and it represents an exciting new opportunity for developing new HCI practice and methodology, as well as innovative forms of user experience."
"Automatically Discovering Talented Musicians with Acoustic Analysis of YouTube Videos.  Online video presents a great opportunity for up-and-coming singers and artists to be visible to a worldwide audience. However, the sheer quantity of video makes it difficult to discover promising musicians. We present a novel algorithm to automatically identify talented musicians using machine learning and acoustic analysis on a large set of ""home singing"" videos. We describe how candidate musician videos are identified and ranked by singing quality. To this end, we present new audio features specifically designed to directly capture singing quality. We evaluate these vis-a-vis a large set of generic audio features and demonstrate that the proposed features have good predictive performance. We also show that this algorithm performs well when videos are normalized for production quality."
"The Bigger Picture: The Use of Mobile Photos in Shopping.  Mobile phones are becoming, if not already, an integral part of our lives. They have a wide range of applications, such as communication, gaming and commerce. Shopping in particular is a rapidly growing domain. Today, shoppers use their phones to make more informed shopping decisions by researching products and merchants, save money using price comparison, mobile coupons and daily deal apps, even purchase products directly on a mobile device. While mobile commerce and shopping apps are in the spotlight, one area that has received little attention is the role of the native capabilities of a mobile phone, such as the mobile camera, in the shopping process. This paper demonstrates the key role mobile photos play in the shopping process, documenting use cases, practices and pain points, and informing opportunity areas for mobile shopping applications and services."
"RFC 6937 - Proportional Rate Reduction for TCP.  This document describes an experimental Proportional Rate Reduction (PRR) algorithm as an alternative to the widely deployed Fast Recovery and Rate-Halving algorithms. These algorithms determine the amount of data sent by TCP during loss recovery. PRR minimizes excess window adjustments, and the actual window size at the end of recovery will be as close as possible to the ssthresh, as determined by the congestion control algorithm."
"Fast Data Processing with Spark.  Spark is a framework for writing fast, distributed programs. Spark solves similar problems as Hadoop MapReduce does but with a fast in-memory approach and a clean functional style API. With its ability to integrate with Hadoop and inbuilt tools for interactive query analysis (Shark), large-scale graph processing and analysis (Bagel), and real-time analysis (Spark Streaming), it can be interactively used to quickly process and query big data sets. Fast Data Processing with Spark covers how to write distributed map reduce style programs with Spark. The book will guide you through every step required to write effective distributed programs from setting up your cluster and interactively exploring the API, to deploying your job to the cluster, and tuning it for your purposes. Fast Data Processing with Spark covers everything from setting up your Spark cluster in a variety of situations (stand-alone, EC2, and so on), to how to use the interactive shell to write distributed code interactively. From there, we move on to cover how to write and deploy distributed jobs in Java, Scala, and Python. We then examine how to use the interactive shell to quickly prototype distributed programs and explore the Spark API. We also look at how to use Hive with Spark to use a SQL-like query syntax with Shark, as well as manipulating resilient distributed datasets (RDDs)."
"Image Annotation in Presence of Noisy Labels.  Labels associated with social images are valuable source of information for tasks of image annotation, understanding and retrieval. These labels are often found to be noisy, mainly due to the collaborative tagging activities of users. Existing methods on annotation have been developed and verified on noise free labels of images. In this paper, we propose a novel and generic framework that exploits the collective knowledge embedded in noisy label co-occurrence pairs to derive robust annotations. We compare our method with a well-known image annotation algorithm and show its superiority in terms of annotation accuracy on benchmark Corel5K and ESP datasets in presence of noisy labels."
"Compacting Large and Loose Communities.  Detecting compact overlapping communities in large networks is an important pattern recognition problem with applications in many domains. Most community detection algorithms trade-off between community sizes, their compactness and the scalability of finding communities. 
Clique Percolation Method (CPM) and Local Fitness Maximization (LFM) are two prominent and commonly used overlapping community detection methods that scale with large networks. However, significant number of communities found by them are large, noisy, and loose. In this paper, we propose a general algorithm that takes such large and loose communities generated by any method and refines them into compact communities in a systematic fashion. We define a new measure of community-ness based on eigenvector centrality, identify loose communities using this measure and propose an algorithm for partitioning such loose communities into compact communities. We refine the communities found by CPM and LFM using our method and show their effectiveness compared to the original communities in a recommendation engine task."
"Empirical evaluation of 20 web form optimization guidelines.  Mostwebsitesuseinteractiveonlineformsasamain contact point to users. Recently, many publications aim at optimizing web forms. In contrast to former research that focused at the evaluation of single guidelines, the present study shows in a controlled lab experiment with n=23 participants the combined effectiveness of 20 guidelines on real company web forms. Results indicate that optimized web forms lead to faster completion times, less form submission trials, fewer eye fixations and higher user satisfaction in comparison to the original forms."
"Combining compile-time and run-time instrumentation for testing tools.  Dynamic program analysis and testing tools typically require inserting extra instrumentation code into the program to test. The inserted instrumentation then gathers data about the program execution and hands it off to the analysis algorithm. Various analysis algorithms can be used to perform CPU profiling, processor cache simulation, memory error detection, data race detection, etc. Usually the instrumentation is done either at run time or atcompile time ?? called dynamic instrumentation and compiler instrumentation, respectively. However, each of these methods has to make a compromise between performance and versatil-ity when used in industry software development. This paper presents a combined approach to instrumentationwhich takes the best of the two worlds ?? the low run-time overhead and unique features of compile-time instrumentation and the flexibility of dynamic instrumentation. Wepresent modifications of two testing tools that benefit from thisapproach: AddressSanitizer and MemorySanitizer. We propose benchmarks to compare different instrumentation frameworks in conditions specific to hybrid instrumenta-tion. We discuss the changes we made to one of the state-of-the-art instrumentation frameworks to significantly improve the performance of hybrid tools."
"Accuracy of Contemporary Parametric Software  Estimation Models: A Comparative Analysis.  Predicting the effort, duration and cost required to develop and maintain a software system is crucial in IT project management.  Although an accurate estimation is invaluable for the success of an IT development project, it often proves difficult to attain. This paper presents an empirical evaluation of four parametric software estimation models, namely COCOMO II, SEER-SEM, SLIM, and TruePlanning, in terms of their project effort and duration prediction accuracy. Using real project data from 51 software development projects, we evaluated the capabilities of the models by comparing the predictions with the actual effort and duration values. The study showed that the estimation capabilities of the models investigated are on a par in accuracy, while there is still significant room for improvement in order to better address the prediction challenges faced in practice."
"Harmonizing classes, functions, tuples, and type parameters in Virgil III.  Languages are becoming increasingly multi-paradigm. Subtype polymorphism in statically-typed object-oriented languages is being supplemented with parametric polymorphism in the form of generics. Features like first-class functions and lambdas are appearing everywhere. Yet existing languages like Java, C#, C++, D, and Scala seem to accrete ever more complexity when they reach beyond their original paradigm into another; inevitably older features have some rough edges that lead to nonuniformity and pitfalls. Given a fresh start, a new language designer is faced with a daunting array of potential features. Where to start? What is important to get right first, and what can be added later? What features must work together, and what features are orthogonal? We report on our experience with Virgil III, a practical language with a careful balance of classes, functions, tuples and type parameters. Virgil intentionally lacks many advanced features, yet we find its core feature set enables new species of design patterns that bridge multiple paradigms and emulate features not directly supported such as interfaces, abstract data types, ad hoc polymorphism, and variant types. Surprisingly, we find variance for function types and tuple types often replaces the need for other kinds of type variance when libraries are designed in a more functional style."
"Direct construction of compact context-dependency transducers from data.  This paper describes a new method for building compact context-dependency transducers for finite-state transducer-based ASR decoders. Instead of the conventional phonetic decision tree growing followed by FST compilation, this approach incorporates the phonetic context splitting directly into the transducer construction. The objective function of the split optimization is augmented with a regularization term that measures the number of transducer states introduced by a split. We give results on a large spoken-query task for various n-phone orders and other phonetic features that show this method can greatly reduce the size of the resulting context-dependency transducer with no significant impact on recognition accuracy. This permits using context sizes and features that might otherwise be unmanageable."
"Making programs forget: Enforcing Lifetime for Sensitive Data.  This paper introduces guaranteed data lifetime, a novel system property ensuring that sensitive data cannot be retrieved from a system beyond a specified time. The trivial way to achieve this is to ""reboot""; however, this is disruptive from the user's perspective, and may not even eliminate disk copies. We discuss an alternate approach based on state re-incarnation where data expiry is completely transparent to the user, and can be used even if the system is not designed a priori to provide the property."
"2nd international workshop on user evaluations for software engineering researchers (USER).  We have met many software engineering researchers who would like to evaluate a tool or system they developed with real users, but do not know how to begin. In this second iteration of the USER workshop, attendees will collaboratively design, develop, and pilot plans for conducting user evaluations of their own tools and/or software engineering research projects. Attendees will gain practical experience with various user evaluation methods through scaffolded group exercises, panel discussions, and mentoring by a panel of user-focused software engineering researchers. Together, we will establish a community of likeminded researchers and developers to help one another improve our research and practice through user evaluation."
"RealBrush: Painting with Examples of Physical Media.  Conventional digital painting systems rely on procedural rules and physical simulation to render paint strokes. We present an interactive, data-driven painting system that uses scanned images of real natural media to synthesize both new strokes and complex stroke interactions, obviating the need for physical simulation. First, users capture images of real media, including examples of isolated strokes, pairs of overlapping strokes, and smudged strokes. Online, the user inputs a new stroke path, and our system synthesizes its 2D texture appearance with optional smearing or smudging when strokes overlap. We demonstrate high-fidelity paintings that closely resemble the captured media style, and also quantitatively evaluate our synthesis quality via user studies."
"Learning Part-based Templates from Large Collections of 3D Shapes.  As large repositories of 3D shape collections continue to grow, understanding the data, especially encoding the inter-model similarity and their variations, is of central importance. For example, many data-driven approaches now rely on access to semantic segmentation information, accurate inter-model point-to-point correspondence, and deformation models that characterize the model collections. Existing approaches, however, are either supervised requiring manual labeling; or employ super-linear matching algorithms and thus are unsuited for analyzing large collections spanning many thousands of models. We propose an automatic algorithm that starts with an initial template model and then jointly optimizes for part segmentation, point-to-point surface correspondence, and a compact deformation model to best explain the input model collection. As output, the algorithm produces a set of probabilistic part-based templates that groups the original models into clusters of models capturing their styles and variations. We evaluate our algorithm on several standard datasets and demonstrate its scalability by analyzing much larger collections of up to thousands of shapes."
"Optimal Hashing Schemes for Entity Matching.  In this paper, we consider the problem of devising blocking schemes for entity matching. There is a lot of work on blocking techniques for supporting various kinds of predicates, e.g. exact matches, fuzzy string-similarity matches, and spatial matches. However, given a complex entity matching function in the form of a Boolean expression over several such predicates, we show that it is an important and non-trivial problem to combine the individual blocking techniques into an efficient blocking scheme for the entity matching function, a problem that has not been studied previously. In this paper, we make fundamental contributions to this problem. We consider an abstraction for modeling complex entity matching functions as well as blocking schemes. We present several results of theoretical and practical interest for the problem. We show that in general, the problem of computing the optimal blocking strategy is NP-hard in the size of the DNF formula describing the matching function. We also present several algorithms for computing the exact optimal strategies (with exponential complexity, but often feasible in practice) as well as fast approximation algorithms. We experimentally demonstrate over commercially used rule-based matching systems over real datasets at Yahoo!, as well as synthetic datasets, that our blocking strategies can be an order of magnitude faster than the baseline methods, and our algorithms can efficiently find good blocking strategies."
"Fastfood - Approximating Kernel Expansions in Loglinear Time.  Fast nonlinear function classes are crucial for nonparametric estimation, such as in kernel methods. This paper proposes an improvement to random kitchen sinks that offers significantly faster computation in log-linear time without sacrificing accuracy. Furthermore, we show how one may adjust the regularization properties of the kernel simply by changing the spectral distribution of the projection matrix. We provide experimental results which show that even for for moderately small problems we already achieve two orders of magnitude faster computation and three orders of magnitude lower memory footprint."
"Scalable all-pairs similarity search in metric spaces.  Given a set of entities, the all-pairs similarity search aims at identifying all pairs of entities that have similarity greater than (or distance smaller than) some user-defined threshold. In this article, we propose a parallel framework for solving this problem in metric spaces. Novel elements of our solution include: i) flexible support for multiple metrics of interest; ii) an autonomic approach to partition the input dataset with minimal redundancy to achieve good load-balance in the presence of limited computing resources; iii) an on-the- fly lossless compression strategy to reduce both the running time and the final output size. We validate the utility, scalability and the effectiveness of the approach on hundreds of machines using real and synthetic datasets."
"Permutation Indexing: Fast Approximate Retrieval from Large Corpora.  Inverted indexing is a ubiquitous technique used in retrieval systems including web search. Despite its popularity, it has a drawback - query retrieval time is highly variable and grows with the corpus size. In this work we propose an alternative technique,  permutation indexing, where retrieval cost is strictly bounded and has only logarithmic dependence on the corpus size. Our approach is based on two novel techniques:"
"Entrepreneurial Innovation at Google.  Large organizations have enormous innovation potential at their disposal. However, the innovation actually realized in successful products and services is usually only a small fraction of that potential. The amount and type of innovation a company achieves are directly related to the way it approaches, fosters, selects, and funds innovation efforts. To maximize innovation and avoid the dilemmas that mature companies face, Google complements the time-proven model of topdown innovation with its own brand of entrepreneurial innovation."
"Adversary Lower Bound for the k-sum Problem.  We prove a tight quantum query lower bound ??(n^(k/(k+1))) for the problem of deciding whether there exist k numbers among n that sum up to a prescribed number, provided that the alphabet size is sufficiently large."
"Quantum query complexity of state conversion.  State conversion generalizes query complexity to the problem of converting between two input-dependent quantum states by making queries to the input. We characterize the complexity of this problem by introducing a natural information-theoretic norm that extends the Schur product operator norm. The complexity of converting between two systems of states is given by the distance between them, as measured by this norm. In the special case of function evaluation, the norm is closely related to the general adversary bound, a semi-definite program that lower-bounds the number of input queries needed by a quantum algorithm to evaluate a function. We thus obtain that the general adversary bound characterizes the quantum query complexity of any function whatsoever. This generalizes and simplifies the proof of the same result in the case of boolean input and output. Also in the case of function evaluation, we show that our norm satisfies a remarkable composition property, implying that the quantum query complexity of the composition of two functions is at most the product of the query complexities of the functions, up to a constant. Finally, our result implies that discrete and continuous-time query models are equivalent in the bounded-error setting, even for the general state-conversion problem."
"Web Workers Multithreaded Programs in JavaScript.  Web apps would run much better if heavy calculations could be performed in the background, rather than compete with the user interface. With this book, you??ll learn how to use Web Workers to run computationally intensive JavaScript code in a thread parallel to the UI. Yes, multi-threaded programing is complicated, but Web Workers provide a simple API that helps you be productive without the complex algorithms. If you have an intermediate to advanced understanding of JavaScript?? especially event handling and callbacks??you??re ready to tackle Web Workers with the tools in this example-driven guide."
"SPI-SNOOPER: a hardware-software approach for transparent network monitoring in wireless sensor networks.  The lack of post-deployment visibility into system operation is one of the major challenges in ensuring reliable operation of remotely deployed embedded systems such as wireless sensor nodes. Over the years, many software-based solutions (in the form of debugging tools and protocols) have been proposed for in-situ system monitoring. However, all of them share the trait that the monitoring functionality is implemented as software executing on the same embedded processor that the main application executes on. This is a poor design choice from a reliability perspective. This paper makes the case for a joint hardware-software solution to this problem and advocates the use of a dedicated reliability co-processor that is tasked with monitoring the operation of the embedded system. As an embodiment of this design principle, this paper presents Spi-Snooper, a co-processor augmented hardware platform specifically designed for network monitoring. Spi-Snooper is completely cross-compatible with the Telos wireless sensor nodes from an operational standpoint and is based on a novel hardware architecture that enables transparent snooping of the communication bus between the main processor and the radio of the wireless embedded system. The accompanying software architecture provides a powerful tool for monitoring, logging, and even controlling all the communication that takes place between the main processor and the radio. We present a rigorous evaluation of our prototype and demonstrate its utility using a variety of usage scenarios."
"Transfer Learning In MIR: Sharing Learned Latent Representations For Music Audio Classification And Similarity.  This paper discusses the concept of transfer learning and its potential applications to MIR tasks such as music audio classification and similarity. In a traditional supervised machine learning setting, a system can only use labeled data from a single dataset to solve a given task. The labels associated with the dataset define the nature of the task to solve. A key advantage of transfer learning is in leveraging knowledge from related tasks to improve performance on a given target task. One way to transfer knowledge is to learn a shared latent representation across related tasks. This method has shown to be beneficial in many domains of machine learning, but has yet to be explored in MIR. Many MIR datasets for audio classification present a semantic overlap in their labels. Furthermore, these datasets often contain relatively few songs. Thus, there is a strong case for exploring methods to share knowledge between these datasets towards a more general and robust understanding of high level musical concepts such as genre and similarity. Our results show that shared representations can improve classification accuracy. We also show how transfer learning can improve performance for music similarity."
"ReFr: An Open-Source Reranker Framework.  ReFr (<a href=""http://refr.googlecode.com/"">http://refr.googlecode.com</a>) is a software architecture for specifying, training and using reranking models, which take the n-best output of some existing system and produce new scores for each of the n hypotheses that potentially induce a different ranking, ideally yielding better results than the original system. The Reranker Framework has some special support for building discriminative language models, but can be applied to any reranking problem. The framework is designed with parallelism and scalability in mind, being able to run on any Hadoop cluster out of the box. While extremely ef???cient, ReFr is also quite ???exible, allowing researchers to explore a wide variety of features and learning methods. ReFr has been used for building state-of-the-art discriminative LM??s for both speech recognition and machine translation systems."
"Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization.  We present a framework for cross-lingual transfer of sequence information from a resource-rich source language to a resource-impoverished target language that incorporates soft constraints via posterior regularization. To this end, we use automatically word aligned bitext between the source and target language pair, and learn a discriminative conditional random field model on the target side. Our posterior regularization constraints are derived from simple intuitions about the task at hand and from cross-lingual alignment information. We show improvements over strong baselines for two tasks: part-of-speech tagging and named-entity segmentation."
Deep Learning in Speech Synthesis.  Deep learning has been a hot research topic in various machine learning related areas including general object recognition and automatic speech recognition.  This talk will present recent applications of deep learning to statistical parametric speech synthesis and contrast the deep learning-based approaches to the existing hidden Markov model-based one.
"Write here, write now!: an experimental study of group maintenance in collaborative writing.  Writing documents together using collaborative editing tools has become extremely common with the widespread availability of tools such as Google Docs. The design of such tools, rooted in early CSCW research, has historically been focused on providing awareness of the presence and activities of one's collaborators. Evidence from a recent qualitative study, however, suggests that people are also concerned about how their behaviors -- and they themselves -- will be perceived by others; and take steps to mitigate possible negative perceptions. We present an experimental study of dyads composing documents together, focusing in particular on group maintenance, impression management and relationship-focused behavior. Results suggest that communication is positively related to social relations, but only for synchronous writing in a shared space; the reverse can be true in asynchronous commenting and editing"
"Joint Noise Level Estimation from Personal Photo Collections.  Personal photo albums are heavily biased towards faces of people, but most state-of-the-art algorithms for image denoising and noise estimation do not exploit facial information. We propose a novel technique for jointly estimating noise levels of all face images in a photo collection. Photos in a personal album are likely to contain several faces of the same people. While some of these photos would be clean and high quality, others may be corrupted by noise.  Our key idea is to estimate noise levels by comparing multiple images of the same content that differ predominantly in their noise content. Specifically, we compare geometrically and photometrically aligned face images of the same person. Our estimation algorithm is based on a probabilistic formulation that seeks to maximize the joint probability of estimated noise levels across all images. We propose an approximate solution that decomposes this joint maximization into a two-stage optimization. The first stage determines the relative noise between pairs of images by pooling estimates from corresponding patch pairs in a probabilistic fashion. The second stage then jointly optimizes for all absolute noise parameters by conditioning them upon relative noise levels, which allows for a pairwise factorization of the probability distribution. We evaluate our noise estimation method using quantitative experiments to measure accuracy on synthetic data. Additionally, we employ the estimated noise levels for automatic denoising using ""BM3D"", and evaluate the quality of denoising on real-world photos through a user study."
"Sok: The Evolution of Sybil Defense via Social Networks.  Sybil attacks in which an adversary forges a potentially unbounded number of identities are a danger to distributed systems and online social networks. The goal of sybil defense is to accurately identify sybil identities. This paper surveys the evolution of sybil defense protocols that leverage the structural properties of the social graph underlying a distributed system to identify sybil identities. We make two main contributions. First, we clarify the deep connection between sybil defense and the theory of random walks. This leads us to identify a community detection algorithm that, for the first time, offers provable guarantees in the context of sybil defense. Second, we advocate a new goal for sybil defense that addresses the more limited, but practically useful, goal of securely white-listing a local region of the graph."
"A Local Algorithm for Finding Well-Connected Clusters.  Motivated by applications of large-scale graph clustering, we study random-walk-based LOCAL algorithms whose running times depend only on the size of the output cluster, rather than the entire graph. In particular, we develop a method with better theoretical guarantee compared to all previous work, both in terms of the clustering accuracy and the conductance of the output set. We also prove that our analysis is tight, and perform empirical evaluation to support our theory on both synthetic and real data. More specifically, our method outperforms prior work when the cluster is WELL-CONNECTED. In fact, the better it is well-connected inside, the more significant improvement we can obtain. Our results shed light on why in practice some random-walk-based algorithms perform better than its previous theory, and help guide future research about local clustering."
"The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines, Second Edition.  As computation continues to move into the cloud, the computing platform of interest no longer resembles a pizza box or a refrigerator, but a warehouse full of computers. These new large datacenters are quite different from traditional hosting facilities of earlier times and cannot be viewed simply as a collection of co-located servers. Large portions of the hardware and software resources in these facilities must work in concert to efficiently deliver good levels of Internet service performance, something that can only be achieved by a holistic approach to their design and deployment. In other words, we must treat the datacenter itself as one massive warehouse-scale computer (WSC). We describe the architecture of WSCs, the main factors influencing their design, operation, and cost structure, and the characteristics of their software base. We hope it will be useful to architects and programmers of today??s WSCs, as well as those of future many-core platforms which may one day implement the equivalent of today??s WSCs on a single board. Notes for the Second Edition After nearly four years of substantial academic and industrial developments in warehouse-scale computing, we are delighted to present our first major update to this lecture. The increased popularity of public clouds has made WSC software techniques relevant to a larger pool of programmers since our first edition. Therefore, we expanded Chapter 2 to reflect our better understanding of WSC software systems and the toolbox of software techniques for WSC programming. In Chapter 3, we added to our coverage of the evolving landscape of wimpy vs. brawny server trade-offs, and we now present an overview of WSC interconnects and storage systems that was promised but lacking in the original edition. Thanks largely to the help of our new co-author, Google Distinguished Engineer Jimmy Clidaras, the material on facility mechanical and power distribution design has been updated and greatly extended (see Chapters 4 and 5). Chapters 6 and 7 have also been revamped significantly. We hope this revised edition continues to meet the needs of educators and professionals in this area."
"Tablets use in emerging markets: an exploration..  Tablet sales are growing worldwide and changing the landscape of personal computing. This is true across mature markets as well as emerging ones. However, little research has been done on the influence of tablets in the emerging markets. This paper presents insights gained during an exploratory study on the use of tablets in four cities: Sao Paulo, Mexico City, Jakarta and Bangalore. The results uncover similarities and differences in the use of tablets in mature markets versus emerging markets and identify implications for design across markets."
Handling Packet Loss in WebRTC.  WebRTC is an open-source real-time interactive audio and video communication framework. This paper discusses some of the mechanisms utilized in WebRTC to handle packet losses in the video communication path. Various system details are discussed and an adaptive hybrid NACK/FEC method with temporal layers is presented. Results are shown to quantify how the method controls the quality trade-offs for real-time video communication.
"Triple Wollaston-prism complete-Stokes imaging polarimeter.  Imaging polarimetry is emerging as a powerful tool for remote sensing in space science, Earth science, biology, defense, national security, and industry. Polarimetry provides complementary information about a scene in the visible and infrared wavelengths. For example, surface texture, material composition, and molecular structure will affect the polarization state of reflected, scattered, or emitted light. We demonstrate an imaging polarimeter design that uses three Wollaston prisms, addressing several technical challenges associated with moving remote-sensing platforms. This compact design has no moving polarization elements and separates the polarization components in the pupil (or Fourier) plane, analogous to the way a grating spectrometer works. In addition, this concept enables simultaneous characterization of unpolarized, linear, and circular components of optical polarization. The results from a visible-wavelength prototype of this imaging polarimeter are presented, demonstrating remote sensitivity to material properties. This work enables new remote sensing capabilities and provides a viable design concept for extensions into infrared wavelengths."
"3DNN: Viewpoint Invariant 3D Geometry Matching for Scene Understanding.  We present a new algorithm 3DNN (3D Nearest-Neighbor), which is capable of matching an image with 3D data, independently of the viewpoint from which the image was captured. By leveraging rich annotations associated with each image, our algorithm can automatically produce precise and detailed 3D models of a scene from a single image. Moreover, we can transfer information across images to accurately label and segment objects in a scene. The true benefit of 3DNN compared to a traditional 2D nearest-neighbor approach is that by generalizing across viewpoints, we free ourselves from the need to have training examples captured from all possible viewpoints.  Thus, we are able to achieve comparable results using orders of magnitude less data, and recognize objects from never-before-seen viewpoints. In this work, we describe the 3DNN algorithm and rigorously evaluate its performance for the tasks of geometry estimation and object detection/segmentation. By decoupling the viewpoint and the geometry of an image, we develop a scene matching approach which is truly 100% viewpoint invariant, yielding state-of-the-art performance on challenging data."
"Bayesian Touch - A Statistic Criterion of Target Selection with Finger Touch.  To improve the accuracy of target selection for finger touch, we conceptualize finger touch input as an uncertain process, and derive a statistical target selection riterion, Bayesian Touch Criterion, from combining the basic Bayes?? rule of probability with the generalized dual Gaussian distribution hypothesis of finger touch. Bayesian Touch Criterion states that the selected target is the candidate with the shortest Bayesian Touch Distance to the touch point, which is computed from the touch point to target center distance and the size of the target. We give the derivation of the Bayesian touch criterion and its empirical
evaluation with two experiments. The results show for 2D circular target selection, Bayesian Touch Criterion is
significantly more accurate than the commonly used Visual Boundary Criterion (i.e., a target is selected if and only if the touch point falls within its boundary) and its two variants."
"FFitts Law: Modeling Finger Touch with Fitts?? Law.  Fitts?? law has proven to be a strong predictor of pointing performance under a wide range of conditions. However, it has been insufficient in modeling small-target acquisition with finger-touch based input on screens. We propose a dual-distribution hypothesis to interpret the distribution of the endpoints in finger touch input. We hypothesize the movement endpoint distribution as a sum of two independent normal distributions. One distribution reflects the relative precision governed by the speed-accuracy tradeoff rule in the human motor system, and the other captures the absolute precision of finger touch independent of the speed-accuracy tradeoff effect. Based on this hypothesis, we derived the FFitts model??an expansion of Fitts?? law for finger touch input. We present three experiments in 1D target acquisition, 2D target acquisition and touchscreen keyboard typing tasks respectively. The results showed that FFitts law is more accurate than Fitts?? law in modeling finger input on touchscreens. At 0.91 or a greater R2 value, FFitts?? index of difficulty is able to account for significantly more variance than conventional Fitts?? index of difficulty based on either a nominal target width or an effective target width in all the three experiments."
"Octopus: Evaluating Touchscreen Keyboard Correction and Recognition Algorithms via ??Remulation??.  The time and labor demanded by a typical laboratory-based keyboard evaluation are limiting resources for algorithmic adjustment and optimization. We propose Remulation, a complementary method for evaluating touchscreen keyboard correction and recognition algorithms. It replicates prior user study data through real-time, on-device simulation. To demonstrate remulation, we have developed Octopus, an evaluation tool that enables keyboard developers to efficiently measure and inspect the impact of algorithmic changes without conducting resource-intensive user studies. It can also be used to evaluate third-party keyboards in a ??black box?? fashion, without access to their algorithms or source code. Octopus can evaluate both touch keyboards and word-gesture keyboards. Two empirical examples show that Remulation can efficiently and effectively measure many aspects of touch screen keyboards at both macro and micro levels. Additionally, we contribute two new metrics to measure keyboard accuracy at the word level: the Ratio of Error Reduction (RER) and the Word Score."
"Bimanual gesture keyboard.  Gesture keyboards represent an increasingly popular way to input text on mobile devices today. However, current gesture keyboards are exclusively unimanual. To take advantage of the capability of modern multi-touch screens, we created a novel bimanual gesture text entry system, extending the gesture keyboard paradigm from one finger to multiple fingers. To address the complexity of recognizing bimanual gesture, we designed and implemented two related interaction methods, finger-release and space-required, both based on a new multi-stroke gesture recognition algorithm. A formal experiment showed that bimanual gesture behaviors were easy to learn. They improved comfort and reduced the physical demand relative to unimanual gestures on tablets. The results indicated that these new gesture keyboards were valuable complements to unimanual gesture and regular typing keyboards."
"Rate-Distortion Optimization for Multichannel Audio Compression.  Multichannel audio coding is studied from a rate-distortion theoret- 
ical viewpoint. Two practical coding techniques, both of which are 
based on rate-distortion optimization, are also proposed. The first 
  technique decorrelates a multichannel signal hierarchically using el- 
  ementary unitary transforms. The second method rearranges a mul- 
  tichannel signal into sub-signals and compresses them at optimized 
  bit rates using a conventional codec. Both objective and subjective 
  tests were conducted to illustrate the efficiency of the methods."
Summary of Opus listening test results.  This document describes and examines listening test results obtained for the Opus codec and how they relate to the requirements.
"Source-Side Classifier Preordering for Machine Translation.  We present a simple and novel classifier-based preordering approach.  Unlike existing preordering models, we train feature-rich discriminative classifiers that directly predict the target-side word order.  Our approach combines the strengths of lexical reordering and syntactic preordering models by performing long-distance reorderings using the structure of the parse tree, while utilizing a discriminative model with a rich set of features, including lexical features.  We present extensive experiments on 22 language pairs, including preordering into English from 7 other languages.  We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task.  For languages from different families the improvements often exceed 2 BLEU.  Many of these gains are also significant in human evaluations."
"Rolling Up Random Variables in Data Cubes.  Data cubes, first developed in the context of on-line analytic processing (OLAP) applications for databases, have become increasingly widespread as a means of structuring data aggregations in other contexts. For example, increasing levels of aggregation in a data cube can be used to impose a hierarchical structure---often referred to as roll-ups---on sets of cross-categorized values, producing a summary description that takes advantage of commonalities within the cube categories. In this paper, we describe a novel technique for realizing such
a hierarchical structure in a data cube containing discrete random variables. Using a generalization of an approach due to Chow and Liu, this technique construes roll-ups as parsimonious approximations to the joint distribution of the variables in terms of the aggregation structure of the cube. The technique is illustrated using a real-life application that involves monitoring and reporting anomalies in Web traffic streams over time."
"Where Am I? A Meta-Analysis of Experiments on the Effects of Progress Indicators for Web Surveys.  The use of progress indicators seems to be standard in many online surveys. Researchers include them in surveys in the hope they will help reduce drop-off rates. However, there is no consensus in the literature regarding their effects. In this meta-analysis, we analyzed 32 randomized experiments comparing drop-off rates of an experimental group who completed an online survey in which a progress indicator was shown to drop-off rates of a control group to whom the progress indicator was not shown. In all the studies, a drop-off was defined as a discontinuance of the survey (at any point) after it has begun, resulting in failure to complete the survey. Three types of progress indicators were analyzed: constant, fast-to-slow, and slow-to-fast. Our results show that, overall, using a constant progress indicator does not significantly help reduce drop-offs and that effectiveness of the progress indicator varies depending on the speed of indicator: Fast-to-slow indicators reduced drop-offs, whereas slow-to-fast indicators increased drop-offs. We also found that among the studies in which a small incentive was promised, showing a constant progress indicator increased the drop-off rate. These findings question the common belief that progress indicators help reduce drop-off rates."
"Data Fusion: Resolving Conflicts from Multiple Sources.  Many data management applications, such as setting up Web portals, managing enterprise data, managing community data, and sharing scientific data, require integrating data from multiple sources. Each of these sources provides a set of values and different sources can often provide conflicting values. To present quality data to users, it is critical to resolve conflicts and discover values that reflect the real world; this task is called data fusion. This paper describes a novel approach that finds true values from conflicting information when there are a large number of sources, among which some may copy from others. We present a case study on real-world data showing that the described algorithm can significantly improve accuracy of truth discovery and is scalable when there are a large number of data sources."
"The Optimal Mix of TV and Online Ads to Maximize Reach.  Brand marketers often wonder how they should allocate budget between TV and online ads in order to maximize reach or maintain the same reach at a lower cost. We use probability models based on historical cross media panel data 
to suggest the optimal budget allocation between TV and online ads to maximize reach to the target demographics. We take a historical TV campaign and estimate the reach and GRPs of a hypothetical cross-media campaign if some budget was shifted from TV to online. The models are validated against simulations and historical cross-media campaigns. They are illustrated on one case study to show how an optimized cross-media campaign can obtain a higher reach at the same cost or maintain the same reach at a lower cost than the TV-only campaign."
"Google??s Innovation Factory: Testing, Culture, And Infrastructure.  Google takes quality seriously and is reinventing how software is created, tested, released, and maintained."
"EXPERTS AND THEIR RECORDS.  A market where short-lived customers interact with long-lived experts is considered. Experts privately observe which treatment best serves a customer, but are free to choose more or less profitable treatments. Customers only observe records of experts' past actions. If experts are homogeneous there exists an equilibrium where experts always choose the customer's preferred treatment (play truthfully). Experts are incentivized with the promise of future business: new customers tend to choose experts who performed less profitable treatments in the past. If expert payoffs are private information, experts can never always be truthful. But sufficiently patient experts may be truthful almost always."
"TSum: Fast, Principled Table Summarization..  Given a table where rows correspond to records and columns correspond to attributes, we want to find a small number of patterns that succinctly summarize the dataset. For example, given a set of patient records with several attributes each, how can we find (a) that the ""most representative"" pattern is, say, (male, adult, <em>), followed by (</em>, child, low-cholesterol), etc.? We propose TSum, a method that provides a sequence of patterns ordered by their ""representativeness."" It can decide both which these patterns are, as well as how many are necessary to properly summarize the data. Our main contribution is formulating a general framework, TSum, using compression principles. TSum can easily accommodate different optimization strategies for selecting and refining patterns. The discovered patterns can be used to both represent the data efficiently, as well as interpret it quickly. Extensive experiments demonstrate the effectiveness and intuitiveness of our discovered patterns."
"Game Your Campaign.  In this article Carolyn Wei and David Huffaker, Google User Experience researchers, explore how understanding gaming sociability could help marketers communicate with a growing audience in new ways. From heightening personalization with ""virtual goods"", to avoiding the pitfalls of ""noisy"" game notifications, today's marketers can create a gaming niche that is both relevant and meaningful to a highly engaged user base."
"On Big Data Algorithmics (invited talk).  The extensive use of Big Data has now become common in plethora of technologies and industries. From massive data bases to business intelligence and datamining applications; from search engines to recommendation systems; advancing the state of the art of voice recognition, translation and more. The design, analysis and engineering of Big Data algorithms has multiple flavors, including massive parallelism, streaming algorithms, sketches and synopses, cloud technologies, and more. We will discuss some of these aspects, and reflect on their evolution and on the interplay between the theory and practice of Big Data algorithmics."
"Nowcasting with Google Trends.  Since launching Google Trends we have seen extensive interest in what can be learned from search trends. A plethora of studies have shown how to use search trends data for effective nowcasting in diverse areas such as health, finance, economics, politics and more.
We give an overview of Google Trends and Nowcasting, highlighting some exciting Big Data challenges, including large scale engineering, effective data analysis, and domain specific considerations."
"Nearest Neighbor Search in Google Correlate.  This paper presents the algorithms which power Google Correlate, a tool which finds web search terms whose popularity over time best matches a user-provided time series. Correlate was developed to generalize the query-based modeling techniques pioneered by Google Flu Trends and make them available to end users. Correlate searches across millions of candidate query time series to find the best matches, returning results in less than 200 milliseconds. Its feature set and requirements present unique challenges for Approximate Nearest Neighbor (ANN) search techniques. In this paper, we present Asymmetric Hashing (AH), the technique used by Correlate, and show how it can be adapted to the specific needs of the product. We then develop experiments to test the throughput and recall of Asymmetric Hashing as compared to a brute-force search. For ""full"" search vectors, we achieve a 10x speedup over brute force search while maintaining 97% recall. For search vectors which contain holdout periods, we achieve a 4x speedup over brute force search, also with 97% recall."
"Google Correlate Whitepaper.  Trends in online web search query data have been shown useful in providing models of real world phenomena. However, many of these results rely on the careful choice of queries that prior knowledge suggests should correspond with the phenomenon. Here, we present an online, automated method for query selection that does not require such prior knowledge. Instead, given a temporal or spatial pattern of interest, we determine which queries best mimic the data. These search queries can then serve to build an estimate of the true value of the phenomenon. We present the application of this method to produce accurate models of influenza activity and home refinance rate in the United States. We additionally show that spatial patterns in real world activity and temporal patterns in web search query activity can both surface  interesting and useful correlations."
"Similarity-based Clustering by Left-Stochastic Matrix Factorization.  For similarity-based clustering, we propose modeling the entries of a given similarity matrix as the inner products of the unknown cluster probabilities. To estimate the cluster probabilities from the given similarity matrix, we introduce a left-stochastic non-negative matrix factorization problem. A rotation-based algorithm is proposed for the matrix factorization. Conditions for unique matrix factorizations and clusterings are given, and an error bound is provided. The algorithm is particularly efficient for the case of two clusters, which motivates a hierarchical variant for cases where the number of desired clusters is large. Experiments show that the proposed left-stochastic decomposition clustering model produces relatively high within-cluster similarity on most data sets and can match given class labels, and that the efficient hierarchical variant performs surprisingly well."
"Whole-page optimization and submodular welfare maximization with online bidders.  In the context of online ad serving, display ads may appear on different types of webpages, where each page includes several ad slots and therefore multiple ads can be shown on each page. The set of ads that can be assigned to ad slots of the same page needs to satisfy various prespecified constraints including exclusion constraints, diversity constraints, and the like. Upon arrival of a user, the ad serving system needs to allocate a set of ads to the current webpage respecting these per-page allocation constraints. Previous slot-based settings ignore the important concept of a page and may lead to highly suboptimal results in general. In this article, motivated by these applications in display advertising and inspired by the submodular welfare maximization problem with online bidders, we study a general class of page-based ad allocation problems, present the first (tight) constant-factor approximation algorithms for these problems, and confirm the performance of our algorithms experimentally on real-world datasets. A key technical ingredient of our results is a novel primal-dual analysis for handling free disposal, which updates dual variables using a ??level function?? instead of a single level and unifies with previous analyses of related problems. This new analysis method allows us to handle arbitrarily complicated allocation constraints for each page. Our main result is an algorithm that achieves a 1 &amp;minus frac 1 e &amp;minus o(1)-competitive ratio. Moreover, our experiments on real-world datasets show significant improvements of our page-based algorithms compared to the slot-based algorithms. Finally, we observe that our problem is closely related to the submodular welfare maximization (SWM) problem. In particular, we introduce a variant of the SWM problem with online bidders and show how to solve this problem using our algorithm for whole-page optimization."
"Herschel/PACS Survey of protoplanetary disks in Taurus/Auriga -- Observations of [OI] and [CII], and far infrared continuum.  The Herschel Space Observatory was used to observe ~ 120 pre-main-sequence stars in Taurus as part of the GASPS Open Time Key project. PACS was used to measure the continuum as well as several gas tracers such as [OI] 63 ??m, [OI] 145 ??m, [CII] 158 ??m, OH, H2O and CO. The strongest line seen is [OI] at 63 ??m. We find a clear correlation between the strength of the [OI] 63 ??m line and the 63 ??m continuum for disk sources. In outflow sources, the line emission can be up to 20 times stronger than in disk sources, suggesting that the line emission is dominated by the outflow. The tight correlation seen for disk sources suggests that the emission arises from the inner disk (&lt; 50 AU) and lower surface layers of the disk where the gas and dust are coupled. The [OI] 63 ??m is fainter in transitional stars than in normal Class II disks. Simple SED models indicate that the dust responsible for the continuum emission is colder in these disks, leading to weaker line emission. [CII] 158 ??m emission is only detected in strong outflow sources. The observed line ratios of [OI] 63 ??m to [OI] 145 ??m are in the regime where we are insensitive to the gas-to-dust ratio, neither can we discriminate between shock or PDR emission. We detect no Class III object in [OI] 63 ??m and only three in continuum, at least one of which is a candidate debris disk."
"GOOGLE DISEASE TRENDS:  AN UPDATE.  The purpose of Google Flu Trends (GFT) is to use search keyword trends from Google.com to produce a daily estimate, or nowcast, of the occurrence of flu two weeks in advance of publication of official surveillance data. While not covered in detail in this paper, Google Dengue Trends, launched in June 2011, is a service that uses similar techniques to track Dengue fever. During the 2012 flu season we observed our algorithm overestimating influenza-like illness (ILI). We have concluded that our algorithm for Flu and Dengue were susceptible to heightened media coverage and have since developed several improvements."
"The Intervalgram: An Audio Feature for Large-Scale Cover-Song Recognition.  We present a system for representing the musical content of short pieces of audio using a novel chroma-based representation known as the ??intervalgram??, which is a summary of the local pattern of musical intervals in a segment of music. The intervalgram is based on a chroma representation derived from the temporal profile of the stabilized auditory image [10] and is made locally pitch invariant by means of a ??soft?? pitch transposition to a local reference. Intervalgrams are generated for a piece of music using multiple overlapping windows. These sets of intervalgrams are used as the basis of a system for detection of identical melodic and harmonic progressions in a database of music. Using a dynamic-programming approach for comparisons between a reference and the song database, performance is evaluated on the ??covers80?? dataset [4]. A first test of an intervalgram-based system on this dataset yields a precision at top-1 of 53.8%, with an ROC curve that shows very high precision up to moderate recall, suggesting that the intervalgram is adept at identifying the easier-to-match cover songs in the dataset with high robustness. The intervalgram is designed to support locality-sensitive hashing, such that an index lookup from each single intervalgram feature has a moderate probability of retrieving a match, with few false matches. With this indexing approach, a large reference database can be quickly pruned before more detailed matching, as in previous content-identification systems."
"Scene Aligned Pooling for Complex Video Recognition.  Real-world videos often contain dynamic backgrounds and evolving people activities, especially for those web videos generated by users in unconstrained scenarios. This paper proposes a new visual representation, namely scene aligned pooling, for the task of event recognition in complex videos. Based on the observation that a video clip is often composed with shots of different scenes, the key idea of scene aligned pooling is to decompose any video features into concurrent scene components, and to construct classification models adaptive to different scenes. The experiments on two large scale real-world datasets including the TRECVID Multimedia Event Detection 2011 and the Human Motion Recognition Databases (HMDB) show that our new visual representation can consistently improve various kinds of visual features such as different low-level color and texture features, or middle-level histogram of local descriptors such as SIFT, or space-time interest points, and high level semantic model features, by a significant margin. For example, we improve the-state-of-the-art accuracy on HMDB dataset by 20% in terms of accuracy."
Capirca.  Capirca is an open-sourced cross-platform network security policy compiler developed at Google. It allows the creation and deployment of ACL filters across multiple target platforms based on a single security policy and shared network and service definitions. The software is ideal for both small and large organizations to eliminate common errors while greatly simplifying security policy maintenance.
"Drilling Network Stacks with packetdrill.  Testing and troubleshooting network protocols and stacks can be painstaking. To ease this process, our team built packetdrill, a tool that lets you write precise scripts to test entire network stacks, from the system call layer down to the NIC hardware. packetdrill scripts use a familiar syntax and run in seconds, making them easy to use during development, debugging, and regression testing, and for learning and investigation."
"Bayes and Big Data:  The Consensus Monte Carlo Algorithm.  A useful definition of ``big data'' is data that is too big to
  comfortably process on a single machine, either because of
  processor, memory, or disk bottlenecks.  Graphics processing units
  can alleviate the processor bottleneck, but memory or disk
  bottlenecks can only be eliminated by splitting data across multiple
  machines.  Communication between large numbers of machines is
  expensive (regardless of the amount of data being communicated), so
  there is a need for algorithms that perform distributed approximate
  Bayesian analyses with minimal communication.  Consensus Monte Carlo
  operates by running a separate Monte Carlo algorithm on each
  machine, and then averaging individual Monte Carlo draws across
  machines.  Depending on the model, the resulting draws can be nearly
  indistinguishable from the draws that would have been obtained by
  running a single machine algorithm for a very long time.  Examples
  of consensus Monte Carlo are shown for simple models where
  single-machine solutions are available, for large single-layer
  hierarchical models, and for Bayesian additive regression trees
  (BART)."
"Supervised Learning of Complete Morphological Paradigms.  We describe a supervised approach to predicting the set of all inflected forms of a lexical item. Our system automatically acquires the orthographic transformation rules of morphological paradigms from labeled examples, and then learns the contexts in which those transformations apply using a discriminative sequence model. Because our approach is completely data-driven and the model is trained on examples extracted from Wiktionary, our method can extend to new languages without change. Our end-to-end system is able to predict complete paradigms with 86.1% accuracy and individual inflected forms with 94.9% accuracy, averaged across three languages and two parts of speech."
"Identifying Phrasal Verbs Using Many Bilingual Corpora.  We address the problem of identifying multiword expressions in a language, focusing on English phrasal verbs. Our polyglot ranking approach integrates frequency statistics from translated corpora in 50 different languages. Our experimental evaluation demonstrates that combining statistical evidence from many parallel corpora using a novel ranking-oriented boosting algorithm produces a comprehensive set of English phrasal verbs, achieving performance comparable to a human-curated set."
"Inferring causal impact using Bayesian structural time-series models.  An important problem in econometrics and marketing is to infer the causal impact that a designed market intervention has exerted on an outcome metric over time. In order to allocate a given budget optimally, for example, an advertiser must assess to what extent different campaigns have contributed to an incremental lift in web searches, product installs, or sales. This paper proposes to infer causal impact on the basis of a diffusion-regression state-space model that predicts the counterfactual market response that would have occurred had no intervention taken place. In contrast to classical difference-in-differences schemes, state-space models make it possible to (i) infer the temporal evolution of attributable impact, (ii) incorporate empirical priors on the parameters in a fully Bayesian treatment, and (iii) flexibly accommodate multiple sources of variation, including the time-varying influence of contemporaneous covariates, i.e., synthetic controls. Using a Markov chain Monte Carlo algorithm for model inversion, we illustrate the statistical properties of our approach on synthetic data. We then demonstrate its practical utility by evaluating the effect of an online advertising campaign on search-related site visits. We discuss the strengths and limitations of state-space models in enabling causal attribution in those settings where a randomised experiment is unavailable. The CausalImpact R package provides an implementation of our approach."
"Estimation, Optimization, and Parallelism when Data is Sparse.  We study stochastic optimization problems when the \emph{data} is sparse, which is in a sense dual to current perspectives on high-dimensional statistical learning and optimization.  We highlight both the difficulties---in terms of increased sample complexity that sparse data necessitates---and the potential benefits, in terms of allowing parallelism and asynchrony in the design of algorithms. Concretely, we derive matching upper and lower bounds on the minimax rate for optimization and learning with sparse data, and we exhibit algorithms achieving these rates. We also show how leveraging sparsity leads to (still minimax optimal) parallel and asynchronous algorithms, providing experimental evidence complementing our theoretical results on several medium to large-scale learning tasks."
"Minimax Optimal Algorithms for Unconstrained Linear Optimization.  We design and analyze minimax-optimal algorithms for online linear optimization games where the player's choice is unconstrained.  The player strives to minimize regret, the difference between his loss and the loss of a post-hoc benchmark strategy.  While the standard benchmark is the loss of the best strategy chosen from a bounded comparator set, we consider a very broad range of benchmark functions. The problem is cast as a sequential multi-stage zero-sum game, and we give a thorough analysis of the minimax behavior of the game, providing characterizations for the value of the game, as well as both the player's and the adversary's optimal strategy.  We show how these objects can be computed efficiently under certain circumstances, and by selecting an appropriate benchmark, we construct a novel hedging strategy for an unconstrained betting game."
"Proceedings of the 2013 Conference on the Theory of Information Retrieval.  These proceedings contain the refereed papers, posters and abstracts of keynotes, tutorials and panel discussion presented at the Fourth International Conference on the Theory of Information Retrieval (ICTIR13), held in Copenhagen, Denmark, during September 29-October 2, 2013."
"Response to the reviews on Bargas-Avila et al. (2009) Intranet Satisfaction Questionnaire: Development and Validation of a Questionnaire to Measure User Satisfaction with the Intranet.  This article contains the response to the reviews regarding the development and validation of the Intranet Satisfaction Questionnaire (ISQ), which measures user satisfaction with the Intranet. Where appropriate additional data analysis and interpretation is provided, the data show further evidence for the good validity, reliability and sensitivity of this tool. In addition, we provide a short preview of a follow-up publication and show that the ISQ can differentiate effectively between bad and good Intranets."
"Bespoke infrastructures.  Infrastructure developed within an organization for its own internal use can take many forms. The obvious reason for creating a bespoke solution is that it can be tailored to fit the organization's unique needs. This can offer many advantages: better performance, increased flexibility, and tactical or strategic advantages over the competition. However, such solutions are associated with a steep learning curve for newcomers, maintenance and support costs, and the risk of hijacking by groups with vested interests. Given that investment in bespoke infrastructures is a sunk cost and that these polarize the types of employees that stay in the organization, rational approaches for building an organization's infrastructure include customizing a general-purpose solution or adopting an open-source tool and improving it to address the organization's requirements."
"DeViSE: A Deep Visual-Semantic Embedding Model.  Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources ?? such as text data ?? both to train visual models and to constrain their predictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recognition challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18% across thousands of novel labels never seen by the visual model."
"Training Highly Multi-class Linear Classifiers.  Classification problems with thousands or more classes often have a large variance in the confusability between classes, and we show that the more-confusable classes add more noise to the empirical loss that is minimized during training. We propose an online solution that reduces the effect of highly confusable classes in training the classifier parameters, and focuses the training on pairs of classes that are easier to differentiate at any given time in the training.  We also show that the adagrad method, recently proposed for automatically decreasing step sizes for convex stochastic gradient descent optimization, can also be profitably applied to the nonconvex optimization stochastic gradient descent training of a joint supervised dimensionality reduction and linear classifier. Experiments on ImageNet benchmark datasets and proprietary image recognition problems with 15,000 to 97,000 classes show substantial gains in classification accuracy compared to one-vs-all linear SVMs and Wsabie."
"Scalable, Example-Based Refactorings with Refaster.  We discuss Refaster, a tool that uses normal, compilable before-and-after examples of Java code to specify a Java refactoring.  Refaster has been used successfully by the Java Core Libraries Team at Google to perform a wide variety of refactorings across Google's massive Java codebase.   Our main contribution is that a large class of useful refactorings can be expressed in pure Java, without a specialized DSL, while keeping the tool easily accessible to average Java developers."
"Chale, How Much it Cost to Browse? Results from a Mobile Data Price Transparency Trial in Ghana.  Mobile data usage is on the rise globally. In emerging regions, mobile data is particularly expensive and suffers from the lack of price and data usage transparency needed to make informed decisions about Internet use. To measure and address this problem, we designed SmartBrowse, an Internet proxy system that shows mobile data usage information and provides controls to avoid overspending. In this paper, we discuss the results of a 10-week study with SmartBrowse, involving 299 participants in Ghana. Half the users were given SmartBrowse, and the other half was given a regular Internet experience. Our findings suggest that, compared with the control group, using SmartBrowse led to a significant reduction in Internet credit spend and increased online activity among SmartBrowse users, while providing the same or better mobile Internet user experience. Additionally, SmartBrowse users who were prior mobile data non-users increased their webpage views while spending less money than control users. Our discussion contributes to the understanding of how forward-looking ICTD research in the wild can empower mobile data users, in this case, through increased price transparency."
"One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling.  We propose a new benchmark corpus to be used for measuring progress in statistical language modeling. With almost one billion words of training data, we hope this benchmark will be useful to quickly evaluate novel language modeling techniques, and to compare their contribution when combined with other advanced techniques. We show performance of several well-known types of language models, with the best results achieved with a recurrent neural network based language model. The baseline unpruned Kneser-Ney 5-gram model achieves perplexity 67.6; a combination of techniques leads to 35% reduction in perplexity, or 10% reduction in cross-entropy (bits), over that baseline. The benchmark is available as a code.google.com project at https://code.google.com/p/1-billion-word-language-modeling-benchmark/; besides the scripts needed to rebuild the training/held-out data, it also makes available log-probability values for each word in each of ten held-out data sets, for each of the baseline n-gram models."
"The Google Technical Interview.  A review of the Google technical interviews, intended for students of computer science and related disciplines."
"Strato: A Retargetable Framework for Low-level Inlined Reference Monitors.  Low-level Inlined Reference Monitors (IRM) such as control-flow integrity and software-based fault isolation can foil numerous software attacks. Conventionally, those IRMs are implemented through binary rewriting or transformation on equivalent low-level programs that are tightly coupled with a specific Instruction Set Architecture (ISA). Resulting implementations have poor retargetability to different ISAs. This paper introduces an IRM-implementation framework at a compiler intermediate-representation (IR) level. The IR-level framework enables easy retargetability to different ISAs, but raises the challenge of how to preserve security at the low level, as the compiler backend might invalidate the assumptions at the IR level. We propose a constraint language to encode the assumptions and check whether they still hold after the backend transformations and optimizations. Furthermore, an independent verifier is implemented to validate the security of low-level code. We have implemented the framework inside LLVM to enforce the policy of control-flow integrity and data sandboxing for both reads and writes. Experimental results demonstrate that it incurs modest runtime overhead of 19.90% and 25.34% on SPECint2000 programs for ??86- 32 and ??86-64, respectively."
"Macaroons: Cookies with Contextual Caveats for Decentralized Authorization in the Cloud.  Controlled sharing is fundamental to distributed systems; yet, on the Web, and in the Cloud, sharing is still based on rudimentary mechanisms. More flexible, decentralized cryptographic authorization credentials have not been adopted, largely because their mechanisms have not been incrementally deployable, simple enough, or efficient enough to implement across the relevant systems and devices. This paper introduces macaroons: flexible authorization credentials for Cloud services that support decentralized delegation between principals. Macaroons are based on a construction that uses nested, chained MACs (e.g., HMACs) in a manner that is highly efficient, easy to deploy, and widely applicable. Although macaroons are bearer credentials, like Web cookies, macaroons embed caveats that attenuate and contextually confine when, where, by who, and for what purpose a target service should authorize requests. This paper describes macaroons and motivates their design, compares them to other credential systems, such as cookies and SPKI/SDSI, evaluates and measures a prototype implementation, and discusses practical security and application considerations. In particular, it is considered how macaroons can enable more fine-grained authorization in the Cloud, e.g., by strengthening mechanisms like OAuth2, and a formalization of macaroons is given in authorization logic."
"Cloud-based simulations on Google Exacycle reveal ligand modulation of GPCR activation pathways.  Simulations can provide tremendous insight into the atomistic details of biological mechanisms, but micro- to millisecond timescales are historically only accessible on dedicated supercomputers. We demonstrate that cloud computing is a viable alternative that brings long-timescale processes within reach of a broader community. We used Google's Exacycle cloud-computing platform to simulate two milliseconds of dynamics of a major drug target, the G-protein-coupled receptor ??2AR. Markov state models aggregate independent simulations into a single statistical model that is validated by previous computational and experimental results. Moreover, our models provide an atomistic description of the activation of a G-protein-coupled receptor and reveal multiple activation pathways. Agonists and inverse agonists interact differentially with these pathways, with profound implications for drug design."
"Biperpedia: An Ontology for Search Applications.  Search engines make significant efforts to recognize queries that can be answered by structured data and invest heavily in creating and maintaining high-precision databases. While these databases have a relatively wide coverage of entities, the number of attributes they model (e.g., gdp, capital, anthem) is relatively small. Extending the number of attributes known to the search engine can enable it to more precisely answer queries from the long and heavy tail, extract a broader range of facts from the Web, and recover the semantics of tables on the Web. We describe Biperpedia, an ontology with 1.6M (class, attribute) pairs and 67K distinct attribute names. Biperpedia extracts attributes from the query stream, and then uses the best extractions to seed attribute extraction from text. For every attribute Biperpedia saves a set of synonyms and text patterns in which it appears, thereby enabling it to recognize the attribute in more contexts. In addition to a detailed analysis of the quality of Biperpedia, we show that it can increase the number of Web tables whose semantics we can recover by more than a factor of 4 compared with Freebase."
"Efficient and Accurate Label Propagation on Dynamic Graphs and Label Sets.  Many web-based application areas must infer label distributions starting from a small set of sparse, noisy labels.  Previous work has shown that graph-based propagation can be very effective at finding the best label distribution across nodes, starting from partial information and a weighted-connection graph.  In their work on video recommendations, Baluja et al. showed high-quality results using Adsorption, a normalized propagation process.  An important step in the original formulation of Adsorption was re-normalization of the label vectors associated with each node, between every propagation step.  That interleaved normalization forced computation of all label distributions, in synchrony, in order to allow the normalization to be correctly determined.  Interleaved normalization also prevented use of standard linear-algebra methods, like stabilized bi-conjugate gradient descent (BiCGStab) and Gaussian elimination.  We show how to replace the interleaved normalization with a single pre-normalization, done once before the main propagation process starts, allowing use of selective label computation (label slicing) as well as large-matrix-solution methods.  As a result, much larger graphs and label sets can be handled than in the original formulation and more accurate solutions can be found in fewer propagation steps.  We further extend that work to handle graphs that change and expand over time.  We report results from using pre-normalized Adsorption in topic labeling for web domains, using label slicing and BiCGStab.  We also report results from using incremental updates on changing co-author network data.  Finally, we discuss two options for handling mixed-sign (positive and negative) graphs and labels."
"Learning Query-Specific Distance Functions for Large-Scale Web Image Search.  Current Google image search adopts a hybrid search approach in which a text-based query (e.g., ""Paris landmarks"") is used to retrieve a set of relevant images, which are then refined by the user (e.g., by re-ranking the retrieved images based on similarity to a selected example).  We conjecture that given such hybrid image search engines, learning per-query distance functions over image features can improve the estimation of image similarity.  We proposed scalable solutions to learning query-specific distance functions by 1) adopting a simple large-margin learning framework, 2) using the query-logs of a text-based image search engine to train distance functions used in content-based systems.  We evaluate the feasibility and efficacy of our proposed system through comprehensive human evaluation, and compare the results with the state-of-the-art image distance function used by Google image search."
"Size Matters: Exhaustive Geometric Verification for Image Retrieval.  The overreaching goals in large-scale image retrieval are bigger,
better and cheaper. For systems based on local features we show
how to get both efficient geometric verification of every match
and unprecedented speed for the low sparsity situation. Large-scale systems based on quantized local
features usually process the index one term at a time,
forcing two separate scoring steps: First, a scoring step
to find candidates with enough matches, and then a geometric
verification step where a subset of the candidates are checked. Our method searches through the index a document at a time,
verifying the geometry of every candidate in a single pass.
We study the behavior of several algorithms with respect to index density---a
key element for large-scale databases. In order to further improve the
efficiency we also introduce a new 
new data structure, called the counting min-tree, which outperforms other approaches
when working with low database density, a necessary condition for very large-scale systems. We demonstrate the effectiveness of our approach with a proof of concept
system that can match an
image against a database of more than 90~billion images in just a few seconds."
"JustSpeak: Enabling Universal Voice Control on Android.  In this paper we introduce JustSpeak, a universal voice control solution for non-visual access to the Android operating system. JustSpeak offers two contributions as compared to existing systems. First, it enables system wide voice control on Android that can accommodate any application. JustSpeak constructs the set of available voice commands based on application context; these commands are directly synthesized from on-screen labels and accessibility metadata, and require no further intervention from the application developer. Second, it provides more efficient and natural interaction with support of multiple voice commands in the same utterance. We present the system design of JustSpeak and describe its utility in various use cases. We then discuss the system level supports required by a service like JustSpeak on other platforms. By eliminating the target locating and pointing tasks, JustSpeak can significantly improve experience of graphic interface interaction for blind and motion-impaired users."
"Collaboration in the Cloud at Google.  Through a detailed analysis of logs of activity for
all Google employees, this paper shows how the
Google Docs suite (documents, spreadsheets and
slides) enables and increases collaboration within
Google. In particular, visualization and analysis
of the evolution of Google??s collaboration network show that new employees, have started collaborating more quickly and with more people as usage of Docs has grown. Over the last two years, the percentage of new employees who collaborate on Docs per month has risen from 70% to 90% and the percentage who collaborate with more than two people has doubled from 35% to 70%. Moreover, the culture of collaboration has become more open, with public sharing within Google overtaking private sharing."
"Experimenting At Scale With Google Chrome's SSL Warning.  Web browsers shown HTTPS authentication warnings (i.e., SSL warnings) when the integrity and confidentiality of users' interactions with websites are at risk. Our goal in this work is to decrease the number of users who click through the Google Chrome SSL warning. Prior research showed that the Mozilla Firefox SSL warning has a much lower click-through rate (CTR) than Chrome. We investigate several factors that could be responsible: the use of imagery, extra steps before the user can proceed, and style choices. To test these factors, we ran six experimental SSL warnings in Google Chrome 29 and measured 130,754 impressions."
"Self-evaluation in Advanced Power Searching and Mapping with Google MOOCs.  While there is a large amount of work on creating autograded massive open online courses (MOOCs), some kinds of complex, qualitative exam questions are still beyond the current state of the art. For MOOCs that need to deal with these kinds of questions, it is not possible for a small course staff to grade students?? qualitative work. To test the efficacy of self-evaluation as a method for complex-question evaluation, students in two Google MOOCs have submitted projects and evaluated their own work. For both courses, teaching assistants graded a random sample of papers and compared their grades with self-evaluated student grades. We found that many of the submitted projects were of very high quality, and that a large majority of self-evaluated projects were accurately evaluated, scoring within just a few points of the gold standard grading."
"Student Skill and Goal Achievement in the Mapping with Google MOOC.  Students who registered for the Mapping with Google massive open online course (MOOC) were asked several questions during the registration process to identify prior experience with eleven skills as well as their goals for registering for the course. Students selected goals from a list; they were periodically reminded of these goals during the course. At the end of the course, we compared students?? self report of goal achievement on a post-course survey with behavioral click-stream analysis. In addition, we compared whether possessing skills at the outset of the course or completing course activities had a larger effect on course completion. We discovered that prior skill had no significant predictive value on certification, but students who completed course activities were more likely to earn certificates of completion than peers who did not complete activities."
"??My religious aunt asked why I was trying to sell her viagra??: Experiences with account hijacking.  With so much of our lives digital, online, and not entirely under our control, we risk losing access to our communications, reputation, and data. Recent years have brought a rash of high-profile account compromises, but account hijacking is not limited to high-profile accounts. In this paper, we report results of a survey about people??s experiences with and attitudes toward account hijacking. The problem is widespread; 30% of our 294 participants had an email or social networking account accessed by an unauthorized party. Five themes emerged from our results: (1) compromised accounts are often valuable to victims, (2) attackers are mostly unknown, but sometimes known, to victims, (3) users acknowledge some responsibility for keeping their accounts secure, (4) users?? understanding of important security measures is incomplete, and (5) harm from account hijacking is concrete and emotional. We discuss implications for designing security mechanisms to improve chances for user adoption."
"Deep Mixture Density Networks for Acoustic Modeling in Statistical Parametric Speech Synthesis.  Statistical parametric speech synthesis (SPSS) using deep neural networks (DNNs) has shown its potential to produce naturally-sounding synthesized speech. However, there are limitations in the current implementation of DNN-based acoustic modeling for speech synthesis, such as the unimodal nature of its objective function and its lack of ability to predict variances. To address these limitations, this paper investigates the use of a mixture density output layer. It can estimate full probability density functions over real-valued output features conditioned on the corresponding input features. Experimental results in objective and subjective evaluations show that the use of the mixture density output layer improves the prediction accuracy of acoustic features and the naturalness of the synthesized speech."
"Quizz: Targeted Crowdsourcing with a Billion (Potential) Users.  We describe Quizz, a gamified crowdsourcing system that simultaneously assesses 
the knowledge of users and acquires new knowledge from them. Quizz operates by 
asking users to complete short quizzes on specific topics; as a user answers 
the quiz questions, Quizz estimates the user's competence. To acquire new 
knowledge, Quizz also incorporates questions for which we do not have a known 
answer; the answers given by competent users provide useful signals for
selecting the correct answers for these questions. Quizz actively tries to 
identify knowledgeable users on the Internet by running advertising campaigns, 
effectively leveraging  the targeting capabilities of existing, 
publicly available, ad placement services. Quizz quantifies the contributions 
of the users using information theory and sends feedback to the advertising
system about each user. The feedback allows the ad targeting mechanism to 
further optimize ad placement. Our experiments, which involve over ten thousand users, confirm that we can 
crowdsource knowledge curation for niche and specialized topics, as the 
advertising network can automatically identify users with the desired expertise 
and interest in the given topic. We present controlled experiments that examine 
the effect of various incentive mechanisms, highlighting the need for having 
short-term rewards as goals, which incentivize the users to contribute. 
Finally, our cost-quality analysis indicates that the cost of our approach is 
below that of hiring workers through paid-crowdsourcing platforms, while 
offering the additional advantage of giving access to billions of potential 
users all over the planet, and being able to reach users with specialized 
expertise that is not typically available through existing labor marketplaces."
"Trust, but Verify: Predicting Contribution Quality for Knowledge Base Construction and Curation.  The largest publicly available knowledge repositories, such as Wikipedia and Freebase, owe their existence and growth to volunteer contributors around the globe. While the majority of contributions are correct, errors can still creep in, due to editors' carelessness, misunderstanding of the schema, malice, or even lack of accepted ground truth. If left undetected, inaccuracies often degrade the experience of users and the performance of applications that rely on these knowledge repositories. We present a new method, CQUAL, for automatically predicting the quality of contributions submitted to a knowledge base. Significantly expanding upon previous work, our method holistically exploits a variety of signals, including the user's domains of expertise as reflected in her prior contribution history, and the historical accuracy rates of different types of facts. In a large-scale human evaluation, our method exhibits precision of 91% at 80% recall. Our model verifies whether a contribution is correct immediately after it is submitted, significantly alleviating the need for post-submission human reviewing."
"Knowledge Base Completion via Search-Based Question Answering.  Over the past few years, massive amounts of world knowledge have been accumulated in publicly available knowledge bases, such as Freebase, NELL, and YAGO. Yet despite their seemingly huge size, these knowledge bases are greatly incomplete. For example, over 70% of people included in Freebase have no known place of birth, and 99% have no known ethnicity. In this paper, we propose a way to leverage existing Web-search--based question-answering technology to fill in the gaps in knowledge bases in a targeted way. In particular, for each entity attribute, we learn the best set of queries to ask, such that the answer snippets returned by the search engine are most likely to contain the correct value for that attribute. For example, if we want to find Frank Zappa's mother, we could ask the query ""who is the mother of Frank Zappa"". However, this is likely to return ""The Mothers of Invention"", which was the name of his band. Our system learns that it should (in this case) add disambiguating terms, such as Zappa's place of birth, in order to make it more likely that the search results contain snippets mentioning his mother. Our system also learns how many different queries to ask for each attribute, since in some cases, asking too many can hurt accuracy (by introducing false positives). We discuss how to aggregate candidate answers across multiple queries, ultimately returning probabilistic predictions for possible values for each attribute. Finally, we evaluate our system and show that it is able to extract a large number of facts with high confidence."
"You Are What You Emote: Emotional Facial Expressions Impact Sexual Orientation Judgments.  Emotions convey more than sentiment. We found that gendered emotional expressions modulated sexual orientation judgments from faces, consistent with stereotypes of gay individuals as gender-atypical. This is the first research on accuracy of person perceptions at the intersection of stable (sexual orientation) and fleeting (emotion) person characteristics."
"Learning kernels using local rademacher complexity.  We use the notion of local Rademacher complexity to design new algorithms for learning kernels. Our algorithms thereby benefit from the sharper learning bounds based on that notion which, under certain general conditions, guarantee a faster convergence rate. We devise two new learning kernel algorithms: one based on a convex optimization problem for which we give an efficient solution using existing learning kernel techniques, and another one that can be formulated as a DC-programming problem for which we describe a solution in detail. We also re- port the results of experiments with both algorithms in both binary and multi-class classification tasks."
"On Estimating the Average Degree.  Networks are characterized by nodes and edges. While there has been a spate of recent work on estimating the number of nodes in a network, the edge-estimation question appears to be largely unaddressed. In this work we consider the problem of estimating the average degree of a large network using efficient random sampling, where the number of nodes is not known to the algorithm. We propose a new estimator for
this problem that relies on access to edge samples under a prescribed distribution. Next, we show how to efficiently realize this ideal estimator in a random walk setting. Our estimator has a natural and simple implementation using random walks; we bound its performance in terms of the mixing time of the underlying graph. We then show that our estimators are both provably and practically better than many natural estimators for the problem.  Our work contrasts with existing theoretical work on estimating average degree, which assume a uniform random sample of nodes is available and the number of nodes is known."
"Postmarket Drug Surveillance Without Trial Costs: Discovery of Adverse Drug Reactions Through Large-Scale Analysis of Web Search Queries.  Background: Postmarket drug safety surveillance largely depends on spontaneous reports by patients and healthcare providers, hence less common adverse drug reactions??especially those caused by long-term exposure, multidrug treatments, or specific to special populations??often elude discovery. 
Objective: Here we propose an ultra-low-cost fully automated method for continuous monitoring of adverse drug reactions in single drugs and in combinations thereof, and demonstrate the discovery of heretofore unknown ones.
Materials and Methods: We use aggregated search data of large populations of Internet users to extract information related to drugs and adverse reactions to them, and correlate these data over time. We further extend our method to identify adverse reactions to combinations of drugs.
Results: We validate our method by showing high correlation of our findings with known adverse drug reactions (ADRs). However, while acute, early-onset drug reactions are more likely to be reported to regulatory agencies, we show that less acute, later-onset ones are better captured in Web search queries.<br/>
Conclusions:  Our method is advantageous in identifying previously unknown adverse drug reactions. These ADRs should be considered as candidates for further scrutiny by medical regulatory authorities, e.g., through Phase IV trials."
"The Network Value of Products.  Traditionally, the value of a product has been assessed according to the direct revenues the product creates.
However, products do not exist in isolation but rather influence one another??s sales. Such influence is especially
evident in e-commerce environments, in which products are often presented as a collection of web pages linked
by recommendation hyperlinks, creating a large-scale product network. The authors present a systematic approach
to estimate products?? true value to a firm in such a product network. Their approach, which is in the spirit of the
PageRank algorithm, uses available data from large-scale e-commerce sites and separates a product??s value into
its own intrinsic value, the value it receives from the network, and the value it contributes to the network. The
authors demonstrate their approach using data collected from the product network of books on Amazon.com.
Specifically, they show that the value of low sellers may be underestimated, whereas the value of best sellers may
be overestimated. The authors explore the sources of this discrepancy and discuss the implications for managing
products in the growing environment of product networks."
"Modelling Score Distributions Without Actual Scores.  Score-distribution models are used for various practical purposes in search, for example for results merging and threshold setting. In this paper, the basic ideas of the score-distributional approach to viewing and analyzing the effectiveness of search systems are re-examined. All recent score-distribution modelling work depends on the availability of actual scores generated by systems, and makes assumptions about these scores. Such work is therefore not applicable to systems which do not generate or reveal such scores, or whose scoring/ranking approach violates the assumptions. We demonstrate that it is possible to apply at least some score-distributional ideas without access to real scores, knowing only the rankings produced (together with a single effectiveness metric based on relevance judgments). This new basic insight is illustrated by means of simulation experiments, on a range of TREC runs, some of whose reported scores are clearly unsuitable for existing methods."
"Training Data Selection Based On Context-Dependent State Matching.  In this paper we construct a data set for semi-supervised acoustic model training by selecting spoken utterances from a massive collection of anonymized Google Voice Search utterances. Semi-supervised training usually retains high-confidence utterances which are presumed to have an accurate hypothesized transcript, a necessary condition for successful training. Selecting high confidence utterances can however restrict the diversity of the resulting data set. We propose to introduce a constraint enforcing that the distribution of the context-dependent state symbols obtained by running forced alignment of the hypothesized transcript matches a reference distribution estimated from a curated development set. The quality of the obtained training set is illustrated on large scale Voice Search recognition experiments and outperforms random selection of high-confidence utterances."
"Verified Boot on Chrome OS and How to do it yourself.  Chrome OS uses a first stage read-only firmware and second-stage updatable firmware. The updatable firmware is signed and contains kernel keys and a dm-verify hash, so that the firmware, Linux kernel and root filesystem are all protected against corruption and attack. This system is described and discussed. As part of Google's upstream efforts in U-Boot, a generalized secure boot system has been developed and released with U-Boot 2013.07. This implementation uses the FIT format, which collects together images, such as kernels, device tree, RAM disks. Support is provided for TPMs (Trust Platform Module), RSA-based signing and verificaiton, and hashing with hardware acceleration. This system is also described and discussed, along with the specific steps needed to implement it in your designs."
"Foundational Issues in Touch-Surface Stroke Gesture Design ??? An Integrative Review.  The advent of modern touchscreen devices has unleashed many opportunities and calls for innovative use of stroke gestures as a richer interaction medium. A significant body of knowledge on stroke gesture design is scattered throughout the Human-Computer Interaction research literature. Primarily based on the authors' own decade-long gesture user interface (UI) research which launched the word-gesture keyboard paradigm, Foundational Issues in Touch-Surface Stroke Gesture Design - An Integrative Review synthesizes some of the foundational issues of human motor control complexity, visual and auditory feedback, and memory and learning capacity concerning gesture user interfaces. In the second half of the book a set of gesture UI design principles is derived from the research literature. The book also covers system implementation aspects of gesture UI such as gesture recognition algorithms and design toolkits. Foundational Issues in Touch-Surface Stroke Gesture Design - An Integrative Review is an ideal primer for researchers and graduate students embarking on research in gesture interfaces. It is also an excellent reference for designers and developers who want to leverage insights and lessons learned in the academic research community."
"High-Resolution Global Maps of 21st-Century Forest Cover Change.  Quantification of global forest change has been lacking despite the recognized importance of forest ecosystem services. In this study, Earth observation satellite data were used to map global forest loss (2.3 million square kilometers) and gain (0.8 million square kilometers) from 2000 to 2012 at a spatial resolution of 30 meters. The tropics were the only climate domain to exhibit a trend, with forest loss increasing by 2101 square kilometers per year. Brazil??s well-documented reduction in deforestation was offset by increasing forest loss in Indonesia, Malaysia, Paraguay, Bolivia, Zambia, Angola, and elsewhere. Intensive forestry practiced within subtropical forests resulted in the highest rates of forest change globally. Boreal forest loss due largely to fire and forestry was second to that in the tropics in absolute and proportional terms. These results depict a globally consistent and locally relevant record of forest change."
What devices do data centers need.  We discuss the trend in fiber optic technology developments to fulfill the scaling requirements of datacenter networks.
Instant Google Drive Starter.  This book is a Starter which teaches you how to use Google Drive practically. This book is perfect for people of all skill levels who want to enjoy the benefits of using Google Drive to safely store their files online and in the cloud. It's also great for anyone looking to learn more about cloud computing in general. Readers are expected to have an Internet connection and basic knowledge of using the internet.
"System and method for determining active topics.  A method for determining active topics may include receiving topic information for a document, the information including at least one topic and a weight for each topic, where the topic relates to content of the document, and the weight represents how strongly the topic is associated with the document. User activity information for the document, including a user activity value including at least one of a number of viewers and a number of editors of the document may be received. A topic intensity for each topic may be generated and stored by multiplying the user activity value for the document by the weight of the topic in the document. The topic intensity may be monitored over time. An alert may be generated based on the topic intensity."
"A Discriminative Latent Variable Model for Online Clustering.  This paper presents a latent variable structured prediction model for discriminative supervised clustering of items called the Latent Left-linking Model (L3M). We present an online clustering algorithm for L3M based on a feature-based item similarity function. We provide a learning framework for estimating the similarity function and present a fast stochastic gradient-based learning technique. In our experiments on coreference resolution and document clustering, L3M outperforms several existing online as well as batch supervised clustering techniques."
"Programmers?? Build Errors: A Case Study (at Google).  Building is an integral part of the software development process. However, little is known about the errors occurring in this process. In this paper, we present an empirical study of 26.6 million builds produced during a period of nine months by thousands of developers. We describe the workflow through which those builds are generated, and we analyze failure frequency, error types, and resolution efforts to fix those errors. The results provide insights on how a large organization build process works, and pinpoints errors for which further developer support would be most effective."
"Identifying and Exploiting Windows Kernel Race Conditions via Memory Access Patterns.  The overall security posture of operating systems?? kernels ?? and specifically the Microsoft Windows NT kernel ?? against both local and remote attacks has visibly improved throughout the last decade. In our opinion, this is primarily due to the increasing interest in kernel-mode vulnerabilities by both white and black-hat parties, as they ultimately allow attackers to subvert the currently widespread defense-in-depth technologies implemented on operating system level, such as sandboxing, or other features enabling better management of privileges within the execution environment (e.g. Mandatory Integrity Control ). As a direct outcome, Microsoft has invested considerable resources in both improving the development process with programs like Secure Development Lifecycle, and explicitly hardening the kernel against existing attacks; the latter was particularly characteristic to Windows 8, which introduced more kernel security improvements than any NT-family system thus far[11]. In this paper, we discuss the concept of employing CPU-level operating system instrumentation to identify potential instances of local race conditions in fetching user-mode input data within system call handlers and other user-facing ring-0 code, and how it was successfully implemented in the Bochspwn project. Further in the document, we present a number of generic techniques easing the exploitation of timing bound kernel vulnerabilities and show how these techniques can be employed in practical attacks against three exemplary vulnerabilities discovered by Bochspwn. In the last sections, we conclusively provide some suggestions on related research areas
that haven??t been fully explored and require further development."
"SAC064 - ICANN SSAC Advisory on Search List Processing.  This advisory examines how current operating systems and applications process 
search lists. It outlines the issues related to the current search list behavior, and 
proposes both a strawman to improve search list processing in the long term and 
mitigation options for the Internet Corporation for Assigned Names and Numbers  (ICANN) and the Internet community to consider in the short term. The purpose of  these proposals is to help introduce new generic Top Level Domains (gTLDs) in a secure and stable manner with minimum disruptions to currently deployed systems."
"Coupled and k-Sided Placements: Generalizing Generalized Assignment.  In modern datacenters and cloud computing systems, jobs often require resources distributed across nodes providing a wide variety of services. Motivated by this, we study the Coupled Placement problem, in which we place jobs into computation and storage nodes with capacity constraints, so as to optimize some costs or profits associated with the placement. The coupled placement problem is a natural generalization of the widely-studied generalized assignment problem (GAP), which concerns the placement of jobs into single nodes providing one kind of service. We also study a further generalization, the k-Sided Placement problem, in which we place jobs into k-tuples of nodes, each node in a tuple offering one of k services. For both the coupled and k-sided placement problems, we consider minimization and maximization versions. In the minimization versions (MinCP and MinkSP), the goal is to achieve minimum placement cost, while incurring a minimum blowup in the capacity of the individual nodes. Our first main result is an algorithm for MinkSP that achieves optimal cost while increasing capacities by at most a factor of k + 1, also yielding the first constant-factor approximation for MinCP. In the maximization versions (MaxCP and MaxkSP), the goal is to maximize the total weight of the jobs that are placed under hard capacity constraints. MaxkSP can be expressed as a k-column sparse integer program, and can be approximated to within a factor of O(k) factor using randomized rounding of a linear program relaxation. We consider alternative combinatorial algorithms that are much more efficient in practice. Our second main result is a local search based approximation algorithm that yields a 15-approximation and O(k^3)-approximation for MaxCP and MaxkSP respectively. Finally, we consider an online version of MaxkSP and present algorithms that achieve logarithmic competitive ratio under certain necessary technical assumptions."
"Temporal Synchronization of Multiple Audio Signals.  Given the proliferation of consumer media recording devices, events often give rise to a large number of recordings. These recordings are taken from different spatial positions and do not have reliable timestamp information. In this paper, we present two robust graph-based approaches for synchronizing multiple audio signals. The graphs are constructed atop the over-determined system resulting from pairwise signal comparison using cross-correlation of audio features. The first approach uses a Minimum Spanning Tree (MST) technique, while the second uses Belief Propagation (BP) to solve the system. Both approaches can provide excellent solutions and robustness to pairwise outliers, however the MST approach is much less complex than BP. In addition, an experimental comparison of audio features-based synchronization shows that spectral flatness outperforms the zero-crossing rate and signal energy."
"A Crossing-Sensitive Third-Order Factorization for Dependency Parsing.  Parsers that parametrize over wider scopes are generally more accurate than edge-factored models. For graph-based non-projective parsers, wider factorizations have so far implied large increases in the computational complexity of the parsing problem. This paper introduces a ??crossing-sensitive?? generalization of a third-order factorization that trades off complexity in the model structure (i.e., scoring with features over multiple edges) with complexity in the output structure (i.e., producing crossing edges). Under this model, the optimal 1-Endpoint-Crossing tree can be found in O(n^4) time, matching the asymptotic run-time of both the third-order projective parser and the edge-factored 1-Endpoint-Crossing parser. The crossing-sensitive third-order parser is significantly more accurate than the third-order projective parser under
many experimental settings and significantly less accurate on none."
"Who??s calling? The impact of Caller ID on telephone survey response.  The Gallup Organization conducted a caller ID randomized study with a pre-and postexperimental design to test the impact of different caller ID displays (names) on the response, contact, and cooperation rates for telephone surveys. This research focuses on the impact of caller ID listing on the frequency of final dialing dispositions. The authors find initial evidence for the hypothesis that the caller ID transmission works as a sort of ??condensed survey research organization business card?? that can trigger brand awareness, thus legitimating the survey and diminishing suspicions of collector or telemarketing calls."
"From Interaction to Performance with Public Displays.  Interacting with public displays involves more than what happens between individuals and the system; it also concerns how people experience others around and through those displays. In this paper, we use ??performance?? as an analytical lens for understanding experiences with a public display called rhythIMs and explore how displays shift social interaction through their mediation. By performance, we refer to a situation in which people are on display and orient themselves toward an audience that may be co-located, imagined, or virtual. To understand interaction with public displays, we use two related notions of collectives??audiences and groups??to highlight the ways in which people orient to each other through public displays. Drawing examples from rhythIMs, a public display that shows patterns of instant messaging and physical presence, we demonstrate that there can be multiple, heterogeneous audiences and show how people experience these different types of collectives in various ways. By taking a performance perspective, we are able to understand how audiences that were not physically co-present with participants still influenced participants?? interpretations and interactions with rhythIMs. This extension of the traditional notion of audience illuminates the roles audiences can play in a performance."
"A New Entity Salience Task with Millions of Training Examples.  Although many NLP systems are moving toward entity-based processing, most still identify important phrases using classical keyword-based approaches. To bridge this gap, we introduce the task of entity salience: assigning a relevance score to each entity in a document. We demonstrate how a labeled corpus for the task can be automatically generated from a corpus of documents and accompanying abstracts. We then show how a classifier with features derived from a standard NLP pipeline outperforms a strong baseline by 34%. Finally, we outline initial experiments on further improving accuracy by leveraging background knowledge about the relationships between entities."
"Scalable Object Detection using Deep Neural Networks.  Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number of outputs for each instance. In this work, we propose a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood of containing any object of interest. The model naturally handles a variable number of instances for each class and allows for cross-class generalization at the highest levels of the network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012, while using only the top few predicted locations in each image and a small number of neural network evaluations."
"Wikidata: A Free Collaborative Knowledge Base.  Unnoticed by most of its readers, Wikipedia is currently undergoing dramatic changes, as its sister project Wikidata introduces a new multilingual ??Wikipedia for data?? to manage the factual information of the popular online encyclopedia. With Wikipedia??s data becoming cleaned and integrated in a single location, opportunities arise for many new applications. In this article, we provide an extended overview of Wikidata, including its essential design choices and data model. Based on up-to-date statistics, we discuss the project's development so far and outline interesting application areas for this new resource."
"Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks.  Recognizing arbitrary multi-character text in unconstrained natural photographs is a hard problem. In this paper, we address an equally hard sub-problem in this domain viz. recognizing arbitrary multi-digit numbers from Street View imagery. Traditional approaches to solve this problem typically separate out the localiza- tion, segmentation, and recognition steps. In this paper we propose a unified ap- proach that integrates these three steps via the use of a deep convolutional neu- ral network that operates directly on the image pixels. We employ the DistBe- lief (Dean et al., 2012) implementation of deep neural networks in order to train large, distributed neural networks on high quality images. We find that the per- formance of this approach increases with the depth of the convolutional network, with the best performance occurring in the deepest architecture we trained, with eleven hidden layers. We evaluate this approach on the publicly available SVHN dataset and achieve over 96% accuracy in recognizing complete street numbers. We show that on a per-digit recognition task, we improve upon the state-of-the-art and achieve 97.84% accuracy. We also evaluate this approach on an even more challenging dataset generated from Street View imagery containing several tens of millions of street number annotations and achieve over 90% accuracy. Our evaluations further indicate that at specific operating thresholds, the performance of the proposed system is comparable to that of human operators. To date, our system has helped us extract close to 100 million physical street numbers from Street View imagery worldwide."
"Local Collaborative Ranking.  Personalized recommendation systems are used in a wide variety of applications such as electronic commerce, social networks, web search, and more. Collaborative filtering approaches to recommendation systems typically assume that the rating matrix (e.g., movie ratings by viewers) is low-rank. In this paper, we examine an alternative approach in which the rating matrix is \emph{locally low-rank}. Concretely, we assume that the rating matrix is low-rank within certain neighborhoods of the metric space defined by (user, item) pairs. We combine a recent approach for local low-rank approximation based on the Frobenius norm with a general empirical risk minimization for ranking losses. Our experiments indicate that the combination of a mixture of local low-rank matrices each of which was trained to minimize a ranking loss outperforms many of the currently used state-of-the-art recommendation systems.  Moreover, our method is easy to parallelize, making it a viable approach for large scale real-world rank-based recommendation systems."
"Using Web Co-occurrence Statistics for Improving Image Categorization.  Object recognition and localization are important tasks in computer vision.  The focus of this work is the incorporation of contextual information  in order to improve object recognition and localization. For instance, it is natural to expect not to see an elephant to appear in the middle of an ocean.  We consider a simple approach to encapsulate such common sense knowledge using co-occurrence statistics from web documents. By merely counting the number of times nouns (such as elephants, sharks, oceans, etc.) co-occur in web documents, we obtain a good estimate of expected co-occurrences in visual data. We then cast the problem of combining textual co-occurrence statistics with the predictions of image-based classifiers as an optimization problem. The resulting optimization problem serves as a surrogate for our inference procedure. Albeit the simplicity of the resulting optimization problem, it is effective in improving both recognition and localization accuracy. Concretely, we observe significant improvements in recognition and localization rates for both ImageNet Detection 2012 and Sun 2012 datasets."
"Semantic Frame Identification with Distributed Word Representations.  We present a novel technique for semantic frame
identification using distributed representations of
predicates and their syntactic context; this technique leverages automatic syntactic parses and a generic set of word embeddings. Given labeled data annotated with frame-semantic parses, we learn a model that projects the set of word representations for the syntactic context around a predicate to a low dimensional representation. The latter is used for semantic frame identification; with a standard argument identification method inspired by prior work,
we achieve state-of-the-art results on FrameNet-style frame-semantic analysis. Additionally, we report strong results on PropBank-style semantic role labeling in comparison to prior work."
"Data enrichment for incremental reach estimation.  There is increasing interest in measuring the overlap and/or incremental reach of cross-media campaigns. The direct method is to use a cross-media panel but these are expensive to scale across all media. Typically, the cross-media panel is too small to produce reliable estimates when the interest comes down to subsets of the population. An alternative is to combine information from a small cross-media panel with a larger, cheaper but potentially biased single media panel. In this article, we develop a data enrichment approach specifically for incremental reach estimation. The approach not only integrates information from both panels that takes into account potential panel bias, but borrows strength from modeling conditional dependence of cross-media reaches. We demonstrate the approach with data from six campaigns for estimating YouTube video ad incremental reach over TV. In a simulation directly modeled on the actual data, we find that
data enrichment yields much greater accuracy than one would get by either ignoring the larger panel, or by using it in a data fusion."
"Automated Decomposition of Build Targets.  A (build) target specifies the information that is needed to automatically build a software artifact. This paper focuses on underutilized targets??an important dependency problem that we identified at Google. An underutilized target is one with files not needed by some of its dependents. Underutilized targets result in less modular code, overly large artifacts, slow builds, and unnecessary build and test triggers. To mitigate these problems, programmers decompose underutilized targets into smaller targets. However, manually decomposing a target is tedious and error-prone. Although we prove that finding the best target decomposition is NP-hard, we introduce a greedy algorithm that proposes a decomposition through iterative unification of the strongly connected components of the target. Our tool found that 19,994 of 40,000 Java library targets at Google can be decomposed to at least two targets. The results show that our tool is (1) efficient because it analyzes a target in two minutes on average and (2) effective because for each of 1,010 targets, it would save at least 50% of the total execution time of the tests triggered by the target."
"Price Competition in Online Combinatorial Markets.  We consider a single buyer with a combinatorial preference that would like to purchase related products and services from different vendors, where each vendor supplies exactly one product. We study the general case where subsets of products can be substitutes as well as complementary and analyze the game that is induced on the vendors, where a vendor's strategy is the price that he asks for his product. This model generalizes both Bertrand competition (where vendors are perfect substitutes) and Nash bargaining (where they are perfect complements), and captures a wide variety of scenarios that can appear in complex crowd sourcing or in automatic pricing of related products. 
We study the equilibria of such games and show that a pure efficient equilibrium always exists. In the case of submodular buyer preferences we fully characterize the set of pure Nash equilibria, essentially showing uniqueness. For the even more restricted ""substitutes"" buyer preferences we also prove uniqueness over {\em mixed} equilibria. Finally we begin the exploration of natural generalizations of our setting such as when services have costs, when there are multiple buyers or uncertainty about the the buyer's valuation, and when a single vendor supplies multiple products."
"Zero-Shot Learning by Convex Combination of Semantic Embeddings.  Several recent publications have proposed methods for mapping images into continuous semantic embedding spaces.  In some cases the embedding space is trained jointly with the image transformation. In other cases the semantic embedding space is established by an independent natural language processing task, and then the image transformation into that space is learned in a second stage.  Proponents of these image embedding systems have stressed their advantages over the traditional \nway{} classification framing of image understanding, particularly in terms of the promise for zero-shot learning -- the ability to correctly annotate images of previously unseen object categories.  In this paper, we propose a simple method for constructing an image embedding system from any existing \nway{} image classifier and a semantic word embedding model, which contains the $\n$ class labels in its vocabulary.  Our method maps images into the semantic embedding space via convex combination of the class label embedding vectors, and requires no additional training.  We show that this simple and direct method confers many of and indeed outperforms state of the art methods on the ImageNet zero-shot learning task."
Machine Learning in an Auction Environment.  We consider a model of repeated online auctions in which an ad with an uncertain click-through rate faces a random distribution of competing bids in each auction and there is discounting of payoffs.  We formulate the optimal solution to this explore/exploit problem as a dynamic programming problem and show that efficiency is maximized by making a bid for each advertiser equal to the advertiser's expected value for the advertising opportunity plus a term proportional to the variance in this value divided by the number of impressions the advertiser has received thus far.  We then use this result to illustrate that the value of incorporating active exploration into a machine learning system in an auction environment is exceedingly small.
"A Scalable Gibbs Sampler for Probabilistic Entity Linking.  Entity linking involves labeling phrases in text with their referent entities, such as Wikipedia or Freebase entries. This task is challenging due to the large number of possible entities, in the millions, and heavy-tailed mention ambiguity. We formulate the problem in terms of probabilistic inference within a topic model, where each topic is associated with a Wikipedia article. To deal with the large number of topics we propose a novel efficient Gibbs sampling scheme which can also incorporate side information, such as the Wikipedia graph. This conceptually simple probabilistic approach achieves state-of-the-art performance in entity-linking on the Aida-CoNLL dataset."
"Recognition of Complex Events: Exploiting Temporal Dynamics between Underlying Concepts.  While approaches based on bags of features excel at low-level action classification, they are ill-suited for recognizing complex events in video, where concept-based temporal representations currently dominate.  This paper proposes a novel representation that captures the temporal dynamics of windowed mid-level concept detectors in order to improve complex event recognition.  We first express each video as an ordered vector time series, where each time step consists of the vector formed from the concatenated confidences of the pre-trained concept detectors.  We hypothesize that the dynamics of time series for different instances from the same event class, as captured by simple linear dynamical system (LDS) models, are likely to be similar even if the instances differ in terms of low-level visual features.  We propose a two-part representation composed of fusing: (1) a singular value decomposition of block Hankel matrices (SSID-S) and (2) a harmonic signature (H-S) computed from the corresponding eigen-dynamics matrix.  The proposed method offers several benefits over alternate approaches: our approach is straightforward to implement, directly employs existing concept detectors and can be plugged into linear classification frameworks.  Results on standard datasets such as NIST's TRECVID Multimedia Event Detection task demonstrate the improved accuracy of the proposed method."
"What is me online?  Insights into how users manage digital identity..  This talk provides comprehensive and up-to-date insights about how users manage identity-related aspects online. We gathered 100+ user stories from 4 countries as well as 1000+ survey responses in the US and UK. We will illustrate how people present themselves in profiles, manage devices, as well as set up and share accounts. Furthermore, we will show how users curate different audiences using social networking sites, if and how users selectively disclose information to others and how users perceive and deal with identity conflation situations. Finally, I will discuss some implications for the development of identity- and privacy-related features."
"Modelling Events through Memory-based, Open-IE Patterns for Abstractive Summarization.  Abstractive text summarization of news
requires a way of representing events, such
as a collection of pattern clusters in which
every cluster represents an event (e.g.,
marriage) and every pattern in the cluster
is a way of expressing the event (e.g.,
X married Y, X and Y tied the knot). We
compare three ways of extracting event
patterns: heuristics-based, compression-based
and memory-based. While the former
has been used previously in multi-document
abstraction, the latter two have
never been used for this task. Compared
with the first two techniques, the memory-based
method allows for generating significantly
more grammatical and informative
sentences, at the cost of searching a
vast space of hundreds of millions of parse
trees of known grammatical utterances. To
this end, we introduce a data structure and
a search method that make it possible to
efficiently extrapolate from every sentence
the parse sub-trees that match against any
of the stored utterances."
"On the structure of weakly acyclic games.  The class of weakly acyclic games, which includes potential games and dominance-solvable games, captures many practical application domains. In a weakly acyclic game, from any starting state, there is a sequence of better-response moves that leads to a pure Nash equilibrium; informally, these are games in which natural distributed dynamics, such as better-response dynamics, cannot enter inescapable oscillations. We establish a novel link between such games and the existence of pure Nash equilibria in subgames. Specifically, we show that the existence of a unique pure Nash equilibrium in every subgame implies the weak acyclicity of a game. In contrast, the possible existence of multiple pure Nash equilibria in every subgame is insufficient for weak acyclicity in general; here, we also systematically identify the special cases (in terms of the number of players and strategies) for which this is sufficient to guarantee weak acyclicity."
"Best-response dynamics out of sync: complexity and characterization.  In many computational and economic models of multi-agent interaction, each participant repeatedly ""best-responds"" to the others' actions. Game theory research on the prominent ""best-response dynamics"" model typically relies on the premise that the interaction between agents is somehow synchronized. However, in many real-life settings, e.g., internet protocols and large-scale markets, the interaction between participants is asynchronous. We tackle the following important questions: (1) When are best-response dynamics guaranteed to converge to an equilibrium even under asynchrony? (2) What is the (computational and communication) complexity of verifying guaranteed convergence? We show that, in general, verifying guaranteed convergence is intractable. In fact, our main negative result establishes that this task is undecidable. We exhibit, in contrast, positive results for several environments of interest, including complete, computationally-tractable, characterizations of convergent systems. We discuss the algorithmic implications of our results, which extend beyond best-response dynamics to applications such as asynchronous Boolean circuits."
"How to grow more pairs: suggesting review targets for comparison-friendly review ecosystems.  We consider the algorithmic challenges behind a novel interface that simplifies consumer research of online reviews by surfacing relevant comparable review bundles: reviews for two or more of the items being researched, all generated in similar enough circumstances to provide for easy comparison. This can be reviews by the same reviewer, or by the same demographic category of reviewer, or reviews focusing on the same aspect of the items. But such an interface will work only if the review ecosystem often has comparable review bundles for common research tasks. Here, we develop and evaluate practical algorithms for suggesting additional review targets to reviewers to maximize comparable pair coverage, the fraction of co-researched pairs of items that have both been reviewed by the same reviewer (or more generally are comparable in one of several ways). We show the exact problem and many subcases to be intractable, and give a greedy online, linear-time 2-approximation for a very general setting, and an offline 1.583-approximation for a narrower setting. We evaluate the algorithms on the Google+ Local reviews dataset, yielding more than 10x gain in pair coverage from six months of simulated replacement of existing reviews by suggested reviews. Even allowing for 90% of reviewers ignoring the suggestions, the pair coverage grows more than 2x in the simulation. To explore other parts of the parameter space, we also evaluate the algorithms on synthetic models."
"Allocation Folding Based on Dominance.  Memory management system performance is of increasing importance in today's managed languages.
Two lingering sources of overhead are the direct costs of memory allocations and write barriers.
This paper introduces allocation folding, an optimization technique where the virtual machine automatically folds multiple memory allocation operations in optimized code together into a single, larger allocation group.
An allocation group comprises multiple objects and requires just a single bounds check in a bump-pointer style allocation, rather than a check for each individual object.
More importantly, all objects allocated in a single allocation group are guaranteed to be contiguous after allocation and thus exist in the same generation, which makes it possible to statically remove write barriers for reference stores involving objects in the same allocation group.
Unlike object inlining, object fusing, and object colocation, allocation folding requires no special connectivity or ownership relation between the objects in an allocation group.
We present our analysis algorithm to determine when it is safe to fold allocations together and discuss our implementation in V8, an open-source, production JavaScript virtual machine.
We present performance results for the Octane and Kraken benchmark suites and show that allocation folding is a strong performance improvement, even in the presence of some heap fragmentation.
Additionally, we use four hand-selected benchmarks JPEGEncoder, NBody, Soft3D, and Textwriter where allocation folding has a large impact."
"Reduce and aggregate: similarity ranking in multi-categorical bipartite graphs.  We study the problem of computing similarity rankings in large-scale multi-categorical bipartite graphs, where the two sides of the graph represent actors and items, and the items are partitioned into an arbitrary set of categories. The problem has several real-world applications, including identifying competing advertisers and suggesting related queries in an online advertising system or finding users with similar interests and suggesting content to them. In these settings, we are interested in computing on-the-fly rankings of similar actors, given an actor and an arbitrary subset of categories of interest. Two main challenges arise: First, the bipartite graphs are huge and often lopsided (e.g. the system might receive billions of queries while presenting only millions of advertisers). Second, the sheer number of possible combinations of categories prevents the pre-computation of the results for all of them. We present a novel algorithmic framework that addresses both issues for the computation of several graph-theoretical similarity measures, including # common neighbors, and Personalized PageRank. We show how to tackle the imbalance in the graphs to speed up the computation and provide efficient real-time algorithms for computing rankings for an arbitrary subset of categories. Finally, we show experimentally the accuracy of our approach with real-world data, using both public graphs and a very large dataset from Google AdWords."
"Multiplicative Bidding in Online Advertising.  In this paper, we initiate the study of the multiplicative bidding language adopted by major Internet search companies. In multiplicative bidding, the effective bid on a particular search auction is the product of a base bid and bid adjustments that are dependent on features of the search (for example, the geographic location of the user, or the platform on which the search is conducted). We consider the task faced by the advertiser when setting these bid adjustments, and establish a foundational optimization problem that captures the core difficulty of bidding under this language. We give matching algorithmic and approximation hardness results for this problem; these results are against an information-theoretic bound, and thus have implications on the power of the multiplicative bidding language itself. Inspired by empirical studies of search engine price data, we then codify the relevant restrictions of the problem, and give further algorithmic and hardness results. Our main technical contribution is an O(log n)-approximation for the case of multiplicative prices and monotone values. We also provide empirical validations of our problem restrictions, and test our algorithms on real data against natural benchmarks. Our experiments show that they perform favorably compare with the baseline."
"Designing Unbiased Surveys for HCI Research.  Surveys are a commonly used method within HCI research. While it initially appears easy and inexpensive to conduct surveys, overlooking key considerations in questionnaire design and the survey research process can yield skewed, biased, or entirely invalid survey results. Fortunately decades of academic research and analysis exist on optimizing the validity and reliability of survey data, from which this course will draw. To enable the creation of unbiased surveys, this course demonstrates questionnaire design biases and pitfalls, provides best practices for minimizing these, and reviews different uses of surveys within HCI."
"Photographing information needs: the role of photos in experience sampling method-style research.  The Experience Sampling Method (ESM) enables researchers to capture information about participants' experiences in the moment. Adding an end-of-day retrospective survey also allows participants to elaborate on those experiences. Although the use of photos in retrospective interviews and surveys for memory elicitation is well known, little research has investigated the use of photos in ESM studies. As smartphone adoption increases facilitating ESM studies and making photo sharing easier, researchers need to continuously evaluate the method and investigate the role of photos in such studies. We conducted a large-scale ESM and retrospective survey study via Android smartphones with more than 1,000 US participants, and analyzed participants' photo submissions, including how photo use correlated with participants' data quality and what, if any, value photos added for researchers. Our study sheds light on the role of photos in ESM and retrospective studies that researchers can reference when constructing future study designs."
"Trust, transparency &amp; control in inferred user interest models.  This paper explores the importance of transparency and control to users in the context of inferred user interests. More specifically, we illustrate the association between various levels of control the users have on their inferred interests and users' trust in organizations that provide corresponding content. Our results indicate that users value transparency and control very differently. We segment users in two groups, one who states to not care about their personal interest model and another group that desires some level of control. We found substantial differences in trust impact between segments, depending on actual control option provided."
"Online Panel Research: A Data Quality Perspective.  This edited volume provides new insights into the accuracy and value of online panels for completing surveys
Over the last decade, there has been a major global shift in survey and market research towards data collection, using samples selected from online panels. Yet despite their widespread use, remarkably little is known about the quality of the resulting data. This edited volume is one of the first attempts to carefully examine the quality of the survey data being generated by online samples. It describes some of the best empirically-based research on what has become a very important yet controversial method of collecting data. Online Panel Research presents 19 chapters of previously unpublished work addressing a wide range of topics, including coverage bias, nonresponse, measurement error, adjustment techniques, the relationship between nonresponse and measurement error, impact of smartphone adoption on data collection, Internet rating panels, and operational issues. The datasets used to prepare the analyses reported in the chapters are available on the accompanying website: www.wiley.com/go/online_panel"
"Online panel research: History, concepts, applications and a look at the future.  In this introductory chapter, written by the six editors of this volume, we introduce and attempt to systematize the key concepts used when discussing online panels. The connection between Internet penetration and the evolution of panels is discussed as are the different types of online panels, their composition, and how they are built. Most online panels do not use probability-based methods, but some do and the differences are discussed. The chapter also describes in some detail the process of joining a panel, answering initial profiling questions, and becoming an active panel member. We discuss the most common sampling techniques, highlighting their strengths and limitations, and touch on techniques to increase representativeness when using a non-probability panel. The variety of incentive methods in current use also is described. Panel maintenance is another key issue, since attrition often is substantial and a panel must be constantly refreshed. Online panels can be used to support a wide range of study designs, some cross-sectional or and others longitudinal, where the same sample members are surveyed multiple times on the same topic. We also discuss industry standards and professional association guidelines for conducting research using online panels. The chapter concludes with a look to the future of online panels and more generally online sampling via means other than classic panels."
"A critical review of studies investigating the quality of data obtained with online panels based on probability and nonprobability samples.  his chapter provides an overview of studies comparing the quality of data collected by online survey panels by looking at three criteria: (1) comparisons of point estimates from online panels to high-quality, established population benchmarks; (2) comparisons of the relationship among variables; and (3) the reproducibility of results for online survey panels conducted on probability samples to panels conducted on nonprobability samples. When looking at point estimates, all online survey panels differed to some extent from the population benchmarks. However, the largest comparison studies suggest that point estimates from online panels of nonprobability samples have higher differences as compared to benchmarks than online panels of probability samples. This finding is consistent across time and across studies conducted in different countries. Moreover, post-stratification weighting strategies helped little and in an inconsistent way to reduce such differences for data coming from online panels of nonprobability samples, whereas these strategies did bring estimates from online panels of probability samples consistently closer to the benchmarks. When comparing relationships among variables, it was found that researchers would reach different conclusions when using online panels of nonprobability samples versus panels of probability samples. When looking at reproducibility of results, the limited evidence found suggests that there are no substantial differences in replication and effect size across probability and nonprobability samples for question wording experiments and when comparing students samples to other samples. It is worth noting that in pre-election polls, an area where abundant prior knowledge exists, online panels of nonprobability samples have consistently performed as well and in some cases better than polls based on probability samples in predicting election winners."
"Internet and mobile ratings panels.  This chapter examines how Internet (PC and mobile) ratings panels are constructed, managed, and utilized. We provide an overview of the history and evolution of Internet/mobile ratings panels and examines the methodological challenges associated with creating and maintaining accurate and reliable Internet/mobile ratings panels. The research that has assessed the accuracy and validity of online panel data is critically discussed; as well as research that illustrates the type of scholarly and applied research questions that can be investigated using online ratings panel data. The chapter concludes with a discussion of the future of online ratings panels within the rapidly evolving field of Internet audience measurement."
"Survey Research in HCI.  Surveys, now commonplace on the Internet, allow researchers to make inferences about an entire population by gathering information from a small subset of the larger group. Surveys can gather insights about people??s attitudes, perceptions, intents, habits, awarenesses, experiences, and characteristics, at significant moments both in time and over time. Even though they are easy to administer, there is a wide gap between quick-and-dirty surveys and surveys that are properly planned, constructed, and analyzed."
Scalable Hierarchical Multitask Learning Algorithms for Conversion Optimization in Display Advertising.  Many estimation tasks come in groups and hierarchies of related problems. In this paper we propose a hierarchical model and a scalable algorithm to perform inference for multitask learning. It infers task correlation and subtask structure in a joint sparse setting. Implementation is achieved by a distributed subgradient oracle and the successive application of prox-operators pertaining to groups and sub-groups of variables. We apply this algorithm to conversion optimization in display advertising. Experimental results on over 1TB data for up to 1 billion observations and 1 million attributes show that the algorithm provides significantly better prediction accuracy while simultaneously being efficiently scalable by distributed parameter synchronization.
"Taxonomy Discovery for Personalized Recommendation.  Personalized recommender systems based on latent factor models are widely used to increase sales in e-commerce. Such systems use the past behavior of users to recommend new items that are likely to be of interest to them. However, latent factor model suffer from sparse user-item interaction in online shopping data: for a large portion of items that do not have sufficient purchase records, their latent factors cannot be estimated accurately.
In this paper, we propose a novel approach that automatically discovers the taxonomies from online shopping data and jointly learns a taxonomy-based recommendation system. Out model is non-parametric and can learn the taxonomy structure automatically from the data. Since the taxonomy allows purchase data to be shared between item- s, it effectively improves the accuracy of recommending tail items by sharing strength with the more frequent items. Ex- periments on a large-scale online shopping dataset confirm that our proposed model improves significantly over state-of- the-art latent factor models. Moreover, our model generates high-quality and human readable taxonomies. Finally, us- ing the algorithm-generated taxonomy, our model even out- performs latent factor models based on the human-induced taxonomy, thus alleviating the need for costly manual taxonomy generation."
"Computer-aided quality assurance of an Icelandic pronunciation dictionary.  We propose a model-driven method for ensuring the quality of pronunciation dictionaries. The key ingredient is computing an alignment between letter strings and phoneme strings, a standard technique in pronunciation modeling. The novel aspect of our method is the use of informative, parametric alignment models which are refined iteratively as they are tested against the data. We discuss the use of alignment failures as a signal for detecting and correcting problematic dictionary entries. We illustrate this method using an existing pronunciation dictionary for Icelandic. Our method is completely general and has been applied in the construction of pronunciation dictionaries for commercially deployed speech recognition systems in several languages."
"Intriguing properties of neural networks.  Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. 
First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. 
Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input."
"Unconstrained Online Linear Learning in Hilbert Spaces: Minimax Algorithms and Normal Approximations.  We study algorithms for online linear optimization in Hilbert spaces, focusing on the case where the player is unconstrained.  We develop a novel characterization of a large class of minimax algorithms, recovering, and even improving, several previous results as immediate corollaries.  Moreover, using our tools, we develop an algorithm that provides a regret bound of $O(U \sqrt{T \log( U \sqrt{T} \log^2 T +1)})$, where $U$ is the $L_2$ norm of an arbitrary comparator and both $T$ and $U$ are unknown to the player. This bound is optimal up to $\sqrt{\log \log T}$ terms.  When $T$ is known, we derive an algorithm with an optimal regret bound (up to constant factors).  For both the known and unknown $T$ case, a Normal approximation to the conditional value of the game proves to be the key analysis tool."
"Dividing secrets to secure data outsourcing.  Data outsourcing or database as a service is a new paradigm for data management. The third party service provider hosts databases as a service. These parties provide efficient and cheap data management by obviating the need to purchase expensive hardware and software, deal with software upgrades and hire professionals for administrative and maintenance tasks. However, due to recent governmental legislations, competition among companies and database thefts, companies cannot use database service providers directly. They need secure and privacy preserving data management techniques to be able to use them in practice. Since data is remotely stored in a privacy preserving manner, there are efficiency related problems such as poor query response time. We propose a new framework that provides efficient and scalable query response times by reducing the computation and communication costs. Furthermore, the proposed technique uses several service providers to guarantee the availability of the services while detecting the dishonest or faulty service providers without introducing additional overhead on the query response time. The evaluations demonstrate that our data outsourcing framework is scalable and practical."
"Characterization of Impact of Transient Faults and Detection of Data Corruption Errors in Large-Scale N-Body Programs Using Graphics Processing Units.  In N-body programs, trajectories of simulated particles have chaotic patterns if errors are in the initial conditions 
or occur during some computation steps. It was believed that 
the global properties (e.g., total energy) of simulated particles 
are unlikely to be affected by a small number of such errors. In 
this paper, we present a quantitative analysis of the impact of 
transient faults in GPU devices on a global property of simulated particles. We experimentally show that a single-bit error 
in non-control data can change the final total energy of a large-
scale N-body program with ~2.1% probability. We also find 
that the corrupted total energy values have certain biases (e.g., 
the values are not a normal distribution), which can be used to 
reduce the expected number of re-executions. In this paper, we 
also present a data error detection technique for N-body pro-
grams by utilizing two types of properties that hold in simulated physical models. The presented technique and an existing
redundancy-based technique together cover many data errors
(e.g., &gt;97.5%) with a small performance overhead (e.g., 2.3%)."
"Is Once Enough? On the Extent and Content of Replications in Human-Computer Interaction.  A replication is an attempt to confirm an earlier study's findings. It is often claimed that research in Human-Computer Interaction (HCI) contains too few replications. To investigate this claim we examined four publication outlets (891 papers) and found 3% attempting replication of an earlier result. The replications typically confirmed earlier findings, but treated replication as a confirm/not-confirm decision, rarely analyzing effect sizes or comparing in depth to the replicated paper. When asked, most authors agreed that their studies were replications, but rarely planned them as such. Many non-replication studies could have corroborated earlier work if they had analyzed data differently or used minimal effort to collect extra data. We discuss what these results mean to HCI, including how reporting of studies could be improved and how conferences/journals may change author instructions to get more replications."
"Designing Usable Web Forms -- Empirical Evaluation of Web Form Improvement Guidelines.  This study reports a controlled eye tracking experiment (N = 65) that shows the combined effectiveness of 20 guidelines to improve interactive online forms when applied to forms found on real company websites. Results indicate that improved web forms lead to faster completion times, fewer form submission trials, and fewer eye movements. Data from subjective questionnaires and interviews further show increased user satisfaction. Overall, our findings highlight the importance for web designers to improve their web forms using UX guidelines."
"Social media in public opinion research.  AAPOR announces the release of an important report, Social Media in Public Opinion Research, authored by the Emerging Technologies Task Force. As social media platforms ?? such as Facebook, Twitter, and LinkedIn to name a few ?? expand and proliferate, so does access to users?? thoughts, feelings and actions expressed instantaneously, organically, and often publicly, across these platforms. At question is how researchers and others interested in public opinion derive reliable and valid insights from the data generated by social media users. The report, Social Media in Public Opinion Research, highlights the use of social media as a vehicle for facilitating the survey research process (i.e., questionnaire development, recruitment, locating, etc.) and as a way of potentially supplementing or replacing traditional survey methods (i.e., content analysis of existing data). It offers an initial set of guidelines and considerations for researchers and consumers of social media-based studies, noting the opportunities and challenges in this new area."
"Web Surveys for the  General Population:  How, why and when?.  Cultural and technological change has made the web a possible and even desirable mode for complex social surveys, but the financial challenges faced by the Research Councils and the UK Government has accelerated this shift, creating an urgent need to explore both its potential and hazards for a range of studies. While some progress in carrying out large-scale complex social surveys on the web has been made, there is still no consensus about how this can best be achieved while maintaining population representativeness and preserving data quality. 
To address this problem, the NCRM funded a network of methodological innovation ??Web Surveys for the General Population: How, Why and When??? (also known by its acronym GenPopWeb). A key objective of the network??s activities was to review and synthesise existing knowledge about the use of web-based data collection for general population samples and to identify areas where new research is needed. The network ??Web Surveys for the General Population: Why, How and When??? was supported with funding from the ESRC National Centre for Research Methods under the initiative Networks for Methodological Innovation 2012. We are also grateful to the Institute of Education and the University of Essex for hosting the two main events of the network. 
We would like to thank all of the presenters at the events as well as the participants for their contribution. Particular thanks are due to the UK Core Group for their time, advice and support: 
Bill Blyth, TNS Global 
Mario Callegaro, Google UK 
Ed Dunn &amp; Laura Wilson, ONS 
Rory Fitzgerald, City University London 
Joanna Lake, ESRC 
Carli Lessof &amp; Joel Williams, TNS BMRB 
Nick Moon, GfK NOP 
Patten Smith, Ipsos MORI 
Professor Patrick Sturgis, NCRM 
Joe Twyman &amp; Michael Wagstaff, YouGov UK"
"Towards Energy Proportionality for Large-Scale Latency-Critical Workloads.  Reducing the energy footprint of warehouse-scale computer (WSC) systems is key to their affordability, yet difficult to achieve in practice. The lack of energy proportionality of typical WSC hardware and the fact that important workloads (such as search) require all servers to remain up regardless of traffic intensity renders existing power management techniques ineffective at reducing WSC energy use.
We present PEGASUS, a feedback-based controller that significantly improves the energy proportionality of WSC systems, as demonstrated by a real implementation in a Google search cluster. PEGASUS uses request latency statistics to dynamically adjust server power management limits in a fine-grain manner, running each server just fast enough to meet global service-level latency objectives. In large cluster experiments, PEGASUS reduces power consumption by up to 20%. We also estimate that a distributed version of PEGASUS can nearly double these savings."
"A Database for Measuring Linguistic Information Content..  Which languages convey the most information in a given amount of
  space?  This is a question often asked of linguists, especially by engineers
  who often have some information theoretic measure of ``information''  in
  mind, but rarely define exactly how they would measure that information. The
  question is, in fact remarkably hard to answer, and many linguists consider it
  unanswerable.  But it is a question that seems as if it ought to have an answer. If one had a database of close translations between a set of typologically
  diverse languages, with detailed marking of morphosyntactic and morphosemantic
  features, one could hope to quantify the differences between how these
  different languages convey information. Since no appropriate database exists
  we decided to construct one. The purpose of this paper is to present our
  work on the database, along with some preliminary results. We plan to
  release the dataset once complete."
"Hippocratic Abbreviation Expansion.  Incorrect normalization of text can be particularly damaging for applications
  like text-to-speech synthesis (TTS) or typing auto-correction, where the
  resulting normalization is directly presented to the user, versus feeding
  downstream applications.  In this paper, we focus on abbreviation expansion
  for TTS, which requires a ``do no harm'', high precision approach yielding few
  expansion errors at the cost of leaving relatively many abbreviations
  unexpanded. In the context of a large-scale, real-world TTS scenario, we
  present methods for training classifiers to establish whether a particular
  expansion is apt.  We achieve a large increase in correct abbreviation
  expansion when combined with the baseline text normalization component of the
  TTS system, together with a substantial reduction in incorrect expansions."
"Troubleshooting PON networks effectively with Carrier-grade Ethernet and WDM-PON.  WDM-PONs have recently emerged to provide dedicated
and separated point-to-point wavelengths to individual
Optical Network Units (ONTs). In addition, the recently standardised
Ethernet OAM capabilities under the IEEE 802.1ag
standard and the ITU-T Y.1731 recommendation, together with
state-of-the-art Optical Time-Domain Reflectometry (OTDR)
provide new link-layer and physical tools for the effective troubleshooting
of WDM-PONs. This article proposes an Integrated
Troubleshooting Box (ITB) for the effectively combination of both
physical and link-layer information into an effective and efficient
set of management procedures for WDM-PONs. We show its
applicability in a number of realistic troubleshooting scenarios,
including failure situations involving either the feeder fibre, one
of its branches and even Ethernet links after the ONT."
"Using transparent WDM metro rings to provide an out-of-band control network for OpenFlow in MAN.  OpenFlow is a protocol that enables networks to evolve and change flexibly, by giving a remote controller the 
capability of modifying the behavior of network devices. In an OpenFlow network, each device needs to 
maintain a dedicated and separated connection with a remote controller. All these connections can be described 
as the OpenFlow control network, that is the data network which transports control plane information, and can be 
deployed together with the data infrastructure plane (in-band) or separated (out-of-band), with advantages and 
disadvantages in both cases. The control network is a critical subsystem since the communication with the 
controller must be reliable and ideally should be protected against failures. This paper proposes a novel ring 
architecture to efficiently transport both the data plane and an out-of-band control network."
"Insulin Resistance: Regression and Clustering.  In this paper we try to define insulin resistance (IR) precisely for a group of Chinese women. Our definition deliberately does not depend upon body mass index (BMI) or age, although in other studies, with particular random effects models quite different from models used here, BMI accounts for a large part of the variability in IR. We accomplish our goal through application of Gauss mixture vector quantization (GMVQ), a technique for clustering that was developed for application to lossy data compression. Defining data come from measurements that play major roles in medical practice. A precise statement of what the data are is in Section 1. Their family structures are described in detail. They concern levels of lipids and the results of an oral glucose tolerance test (OGTT). We apply GMVQ to residuals obtained from regressions of outcomes of an OGTT and lipids on functions of age and BMI that are inferred from the data. A bootstrap procedure developed for our family data supplemented by insights from other approaches leads us to believe that two clusters are appropriate for defining IR precisely. One cluster consists of women who are IR, and the other of women who seem not to be. Genes and other features are used to predict cluster membership. We argue that prediction with ????main effects???? is not satisfactory, but prediction that includes interactions may be."
"Auto-Rectification of User Photos.  The image auto rectification project at Google aims to create a pleasanter version of user photos by correcting the small, involuntary camera rotations (roll / pitch/ yaw) that often
occur in non-professional photographs. Our system takes the image closer to the fronto-parallel view by performing an affine rectification on the image that restores parallelism of
lines that are parallel in the fronto-parallel image view. This partially corrects perspective distortions, but falls short of full metric rectification which also restores angles between lines. On the other hand the 2D homography for our rectification can be computed from only two (as opposed to three) estimated vanishing points, allowing us to fire upon many more images. A new RANSAC based approach to vanishing point estimation has been developed. The main strength of our vanishing point detector is that it is line-less, thereby avoiding the hard, binary (line/no-line) upstream decisions that cause traditional algorithm to ignore much supporting evidence and/or admit noisy evidence for vanishing points. A robust RANSAC based technique for detecting horizon lines in an image is also proposed for analyzing correctness of the estimated rectification. We post-multiply our affine rectification homography with a 2D rotation which aligns the closer vanishing point with the image Y axis."
"Applications of Maximum Entropy Rankers to Problems in Spoken Language Processing.  We report on two applications of Maximum Entropy-based ranking models to
problems of relevance to automatic speech recognition and text-to-speech
synthesis. The first is stress prediction in Russian, a language with notoriously
complex morphology and stress rules. The second is the classification of
alphabetic non-standard words, which may be read as words (NATO), as
letter sequences (USA), or as a mixed (mymsn). For this second task
we report results on English, and five other European languages."
"Large-Scale Speaker Identification.  Speaker identification is one of the main tasks in speech processing. In addition to identification accuracy, large-scale applications of speaker identification give rise to another challenge: fast search in the database of speakers. In this paper, we propose a system based on i-vectors, a current approach for speaker identification, and locality sensitive hashing, an algorithm for fast nearest-neighbor search in high dimensions. The connection between the two techniques is the cosine distance: one the one hand, we use the cosine distance to compare i-vectors, on the other hand, locality sensitive hashing allows us to quickly approximate the cosine distance in our retrieval procedure. We evaluate our approach on a realistic data set from YouTube with about 1000 speakers. The results show that our algorithm is approximately one to two orders of magnitude faster than a linear search while maintaining the identification accuracy of an i-vector-based system."
"Improving DNN Speaker Independence with I-vector Inputs.  We propose providing additional utterance-level features as inputs to a deep neural network (DNN) to facilitate speaker, channel and background normalization.  Modifications of the basic algorithm are developed which result in significant reductions in word error rates (WERs). The algorithms are shown to combine well with speaker adaptation by backpropagation, resulting in a 9\% relative WER reduction. We address implementation of the algorithm for a streaming task."
"Automatic Language Identification Using Deep Neural Networks.  This work studies the use of deep neural networks (DNNs) to address automatic language identification (LID). Motivated by their recent success in acoustic modelling, we adapt DNNs to the problem of identifying the language of a given spoken utterance from short-term acoustic features. The proposed approach is compared to state-of-the-art i-vector based acoustic systems on two different datasets: Google 5M LID corpus and NIST LRE 2009. Results show how LID can largely benefit from using DNNs, especially when a large amount of training data is available. We found relative improvements up to 70%, in Cavg, over the baseline system."
"Machine Learning Applications for Data Center Optimization.  The modern data center (DC) is a complex interaction of multiple mechanical, electrical and controls systems. The sheer number of possible operating configurations and nonlinear interdependencies make it difficult to understand and optimize energy efficiency. We develop a neural network framework that learns
from actual operations data to model plant performance and predict PUE within a range of 0.004 +/0.005 (mean absolute error +/- 1 standard deviation), or 0.4% error for a PUE of 1.1. The model has been extensively tested and validated at Google DCs. The results demonstrate that machine learning is an effective way of leveraging existing sensor data to model DC performance and improve energy efficiency."
"Word Embeddings for Speech Recognition.  Speech recognition systems have used the concept of states as a way to decompose words into sub-word units for decades. As the number of such states now reaches the number of words used to train acoustic models, it is interesting to consider approaches that relax the assumption that words are made of states.  We present here an alternative construction, where words are projected into a continuous embedding space where words that sound alike are nearby in the Euclidean sense. We show how embeddings can still allow to score words that were not in the training dictionary.  Initial experiments using a lattice rescoring approach and model combination on a large realistic dataset show improvements in word error rate."
"When the Cloud  Goes Local: The  Global Problem  with Data  Localization.  Ongoing efforts to legally define cloud computing and regulate separate parts of the Internet are unlikely to address underlying concerns about data security and privacy. Data localization initiatives, led primarily by European countries, could actually bring the cloud to the ground and make the Internet less secure."
"Your Reputation Precedes You: History, Reputation, and the Chrome Malware Warning.  Several web browsers, including Google Chrome and Mozilla Firefox, use malware warnings to stop people from visiting infectious websites. However, users can choose to click through (i.e., ignore) these malware warnings. In Google Chrome, users click through a fifth of malware warnings on average. We investigate factors that may contribute to why people ignore such warnings. First, we examine field data to see how browsing history affects click-through rates. We find that users consistently heed warnings about websites that they have not visited before. However, users respond unpredictably to warnings about websites that they have previously visited. On some days, users ignore more than half of warnings about websites they've visited in the past. Next, we present results of an online, survey-based experiment that we ran to gain more insight into the effects of reputation on warning adherence. Participants said that they trusted high-reputation websites more than the warnings; however, their responses suggest that a notable minority of people could be swayed by providing more information. We provide recommendations for warning designers and pose open questions about the design of malware warnings."
"Sequence Discriminative Distributed Training of Long Short-Term Memory Recurrent Neural Networks.  We recently showed that Long Short-Term Memory (LSTM) recurrent neural networks (RNNs) outperform state-of-the-art deep neural networks (DNNs) for large scale acoustic modeling where the models were trained with the cross-entropy (CE) criterion.  It has also been shown that sequence discriminative training of DNNs initially trained with the CE criterion gives significant improvements.
In this paper, we investigate sequence discriminative training of LSTM RNNs in a large scale acoustic modeling task. We train the models in a distributed manner using asynchronous stochastic gradient descent optimization technique. We compare two sequence discriminative criteria -- maximum mutual information and state-level minimum Bayes risk, and we investigate a number of variations of the basic training strategy to better understand issues raised by both the sequential model, and the objective function. We obtain significant gains over the CE trained LSTM RNN model using
sequence discriminative training techniques."
"Designing the Chromecast Out-of-Box Experience.  Chromecast is a small HDMI device that provides users with an easy way to stream online videos and music to the TV. The Chromecast out-of-box experience (OOBE) has been lauded by reviewers and consumers for its 
simplicity and ease of use. Even though setting up Chromecast is pretty simple, we found that without proper guidance there were several ways that users could fail. This case study will present the different challenges the team faced in designing the Chromecast OOBE and the different options the team explored. We will also describe our user research process and how the user experience team worked with engineers and product managers to implement a simple and easy OOBE."
"Web-Scale Job Scheduling.  Web datacenters and clusters can be larger than the world??s
largest supercomputers, and run workloads that are at least as heteroge-
neous and complex as their high-performance computing counterparts.
And yet little is known about the unique job scheduling challenges of
these environments. This article aims to ameliorate this situation. It dis-
cusses the challenges of running web infrastructure and describes several
techniques to address them. It also presents some of the problems that
remain open in the field."
"Characterization and Comparison of Cloud versus Grid Workloads.  A new era of Cloud Computing has emerged, but
the characteristics of Cloud load in data centers is not perfectly
clear. Yet this characterization is critical for the design of novel
Cloud job and resource management systems. In this paper, we
comprehensively characterize the job/task load and host load
in a real-world production data center at Google Inc. We use
a detailed trace of over 25 million tasks across over 12,500
hosts. We study the differences between a Google data center
and other Grid/HPC systems, from the perspective of both work
load (w.r.t. jobs and tasks) and host load (w.r.t. machines). In
particular, we study the job length, job submission frequency,
and the resource utilization of jobs in the different systems,
and also investigate valuable statistics of machine??s maximum
load, queue state and relative usage levels, with different job
priorities and resource attributes. We find that the Google data
center exhibits finer resource allocation with respect to CPU
and memory than that of Grid/HPC systems. Google jobs are
always submitted with much higher frequency and they are
much shorter than Grid jobs. As such, Google host load exhibits
higher variance and noise."
"Hostload prediction in a Google compute cloud with a Bayesian model.  Prediction of host load in Cloud systems is crit-
ical for achieving service-level agreements. However, accurate
prediction of host load in Clouds is extremely challenging
because it fluctuates drastically at small timescales. We design
a prediction method based on Bayes model to predict the mean
load over a long-term time interval, as well as the mean load in
consecutive future time intervals. We identify novel predictive
features of host load that capture the expectation, predictabil-
ity, trends and patterns of host load. We also determine the
most effective combinations of these features for prediction.
We evaluate our method using a detailed one-month trace of a
Google data center with thousands of machines. Experiments
show that the Bayes method achieves high accuracy with a
mean squared error of 0.0014. Moreover, the Bayes method
improves the load prediction accuracy by 5.6-50% compared
to other state-of-the-art methods based on moving averages,
auto-regression, and/or noise filters."
"Perspectives on cloud computing: interviews with five leading scientists from the cloud community.  Cloud computing is currently one of the major topics in dis-
tributed systems, with large numbers of papers being writ-
ten on the topic, with major players in the industry releasing
a range of software platforms offering novel Internet-based
services and, most importantly, evidence of real impact on
end user communities in terms of approaches to provision-
ing software services. Cloud computing though is at a for-
mative stage, with a lot of hype surrounding the area, and
this makes it difficult to see the true contribution and impact
of the topic. Cloud computing is a central topic for the Journal of In-
ternet Services and Applications (JISA) and indeed the most
downloaded paper from the first year of JISA is concerned
with the state-of-the-art and research challenges related to
cloud computing [1]. The Editors-in-Chief, Fabio Kon and
Gordon Blair, therefore felt it was timely to seek clarifica-
tion on the key issues around cloud computing and hence
invited five leading scientists from industrial organizations
central to cloud computing to answer a series of questions
on the topic. The five scientists taking part are:
?? Walfredo Cirne, from Google??s infrastructure group in
California, USA
?? Dejan Milojicic, Senior Researcher and Director of the
Open Cirrus Cloud Computing testbed at HP Labs
?? Raghu Ramakrishnan, Chief Scientist for Search and
Cloud Platforms at Yahoo!
?? Dan Reed, Microsoft??s Corporate Vice President for Tech-
nology Strategy and Policy and Extreme Computing
?? Dilma Silva, researcher at the IBM T.J. Watson Research
Center, in New York"
"Affinity Weighted Embedding.  Supervised linear embedding models like Wsabie (Weston et al., 2011) and supervised semantic indexing (Bai et al., 2010) have proven successful at ranking, recommendation and annotation tasks. However, despite being scalable to large datasets they do not take full advantage of the extra data due to their linear nature, and we believe they typically underfit. We propose a new class of models which aim to provide improved performance while retaining many of the benefits of the existing class of embedding models. Our approach works by reweighting each component of the embedding of features and labels with a potentially nonlinear affinity function. We describe several variants of the family, and show its usefulness on several datasets."
"A Comparison of Six Sample Providers Regarding Online Privacy Benchmarks.  Researchers increasingly utilize online tools to gather insights.
We show how privacy comfort as measured by questionnaires
differs across various survey sample providers. To investigate 
potential differences depending on provider, we fielded a small set 
of privacy-related benchmark questions regarding past experience, 
present and future concerns to six major US survey providers. We 
found substantial differences depending on privacy benchmark 
and provider population, illustrating that privacy-related research 
may yield different insights depending on provider choice."
"An Extension of BLANC to System Mentions.  BLANC is a link-based coreference evaluation metric for measuring the quality of coreference systems on gold mentions. This paper extends the original BLANC (??BLANC-gold?? henceforth) to system mentions, removing the gold mention assumption. The proposed BLANC falls back seamlessly to the original one if system mentions are identical to gold mentions, and it is shown to strongly  correlate with existing metrics on the 2011 and 2012 CoNLL data."
"Scoring Coreference Partitions of Predicted Mentions: A Reference Implementation.  The definitions of two coreference scoring metrics??B3 and CEAF??are underspecified with respect to <em>predicted</em>, as opposed to <em>key</em> (or <em>gold</em>) mentions. Several variations have been proposed that manipulate either, or both, the key and predicted mentions in order to get a one-to-one mapping. On the other hand, the metric BLANC was, until recently, limited to scoring partitions of key mentions. In this paper, we (i) argue that mention manipulation for scoring predicted mentions is unnecessary, and potentially harmful as it could produce unintuitive results; (ii) illustrate the application of all these measures to scoring predicted mentions; (iii) make available an open source, thoroughly-tested reference implementation of the main coreference evaluation measures; and (iv) rescore the results of the CoNLL-2011/2012 shared task systems with this implementation. This will help the community accurately measure and compare new end-to-end coreference resolution algorithms."
"Making ??Push On Green?? a Reality: Issues &amp; Actions Involved in Maintaining a Production Service.  Updating production software is a process that may require dozens, if not hundreds, of steps. These include creating and testing the new code, building new binaries and packages, associating the packages with a versioned release, updating the jobs in production datacenters, possibly modifying database schemata, and testing and verifying the results. There are boxes to check and approvals to seek, and the more automated the process, the easier it becomes. When releases can be made faster, it is possible to release more often, and organizationally, one becomes less afraid to ??release early, release often??. This is the fundamental driving force behind the work described in this paper ?? making rollouts as easy and as automated as possible, so that when a ??green?? condition (defined below) is detected, we can more quickly perform a new rollout. Humans may still be needed somewhere in the loop, but we strive to reduce the purely mechanical toil they need to perform. This paper describes how we, as Site Reliability Engineers working on several different Ads and Commerce services at Google, do this, and shares information on how to enable other organizations to do the same. We define Push On Green and describe the development and deployment of best practices that serve as a foundation for this kind of undertaking. Using a ??sample service?? at Google as an example, we look at the historical development of the mechanization of the rollout process, and discuss the steps taken to further automate it. We then examine the steps remaining, both near and long-term, as we continue to gain experience and advance the process towards full automation. We conclude with a set of concrete recommendations for other groups wishing to implement a Push On Green system that keeps production systems not only up-and-running, but also updated with as little engineer-involvement and user-visible downtime as possible."
"RLint: Reformatting R Code to Follow the Google Style Guide.  RLint (https://code.google.com/p/google-rlint/) both checks and reformats R code to the Google R Style Guide. It warns of violations and optionally produces compliant code. It considers proper spacing, line alignment inside brackets, and other style violations, but like all lint programs does not try to handle all syntax issues. Code that follows a uniform style eases maintenance, modification, and ensuring correctness, especially when multiple programmers are involved.  Thus, RLint is automatically used within Google as part of the peer review process for R code.  We encourage CRAN package authors and other R programmers to use this tool.  A user can run the open-source Python-based program in a Linux, Unix, Mac or Windows machine via a command line."
"Up Next: Retrieval Methods for Large Scale Related Video Suggestion.  The explosive growth in sharing and consumption of the video content on the web creates a unique opportunity for scientific advances in video retrieval, recommendation and discovery. In this paper, we focus on the task of video suggestion, commonly found in many online applications. The current state-of-the-art video suggestion techniques are based on the collaborative filtering analysis, and suggest videos that are likely to be co-viewed with the watched video. In this paper, we propose augmenting the collaborative filtering analysis with the topical representation of the video content to suggest related videos. We propose two novel methods for topical video representation. The first
method uses information retrieval heuristics such as tf-idf, while the second method learns the optimal topical representations based on the implicit user feedback available in the
online scenario. We conduct a large scale live experiment on YouTube traffic, and demonstrate that augmenting collaborative filtering with topical representations significantly improves the quality of the related video suggestions in a live setting, especially for categories with fresh and topically-rich
video content such as news videos. In addition, we show that employing user feedback for learning the optimal topical video representations can increase the user engagement by more than 80% over the standard information retrieval representation, when compared to the collaborative filtering baseline."
"Statistical Parametric Speech Synthesis.  Statistical parametric speech synthesis has grown in popularity over the last years. In this tutorial, its system architecture is outlined, and then basic techniques used in the system, including algorithms for speech parameter generation, are described with simple examples."
"The SMAPH System for Query Entity Recognition and Disambiguation.  The SMAPH system implements a pipeline of four main steps: (1) Fetching ?? it fetches the search results returned by a search engine given the query to be annotated; (2) Spotting ?? search result snippets are parsed to identify candidate mentions for the entities to be annotated. This is done in a novel way by detecting the keywords-in-context by looking at the bold parts of the search snippets; (3) Candidate generation ?? candidate entities are generated in two ways: from the Wikipedia pages occurring in the search results, and from an existing annotator, using the mentions identified in the spotting step as input; (4) Pruning ?? a binary SVM classifier is used to decide which entities to keep/discard in order to generate the final annotation set for the query. The SMAPH system ranked third on the development set and first on the final blind test of the 2014 ERD Challenge short text track."
"Libra: Divide and Conquer to Verify Forwarding Tables in Huge Networks.  Data center networks often have errors in the forwarding tables, causing packets to loop indefinitely, fall into black-holes or simply get dropped before they reach the correct destination. Finding forwarding errors is possible using static analysis, but none of the existing tools scale to a large data center network with thousands of switches and millions of forwarding entries. Worse still, in a large data center network the forwarding state is constantly in flux, which makes it hard to take an accurate snapshot of the state for static analysis. We solve these problems with Libra, a new tool for verifying forwarding tables in very large networks. Libra runs fast because it can exploit the scaling properties of MapReduce. We show how Libra can take an accurate snapshot of the forwarding state 99.9% of the time, and knows when the snapshot cannot be trusted. We show results for Libra analyzing a 10,000 switch"
"Preemptive Policy Experimentation.  We develop a model of experimentation and learning in policymaking when control of power is temporary. We demonstrate how an early office holder who would otherwise not experiment is nonetheless induced to experiment when his hold on power is temporary. This preemptive policy experiment is profitable for the early office holder as it reveals information about the policy mapping to his successor, information that shapes future policy choices. Thus policy choices today can cast a long shadow over future choices purely through information transmission and absent any formal institutional constraints or real state variables. The model we develop utilizes a recent innovation that represents the policy mapping as the realized path of a Brownian motion. We provide a precise characterization of when preemptive experimentation emerges in equilibrium and the form it takes. We apply the model to several well known episodes of policymaking, reinterpreting the policy choices as preemptive experiments."
"Enforcing Forward-Edge Control-Flow Integrity in GCC &amp; LLVM.  Constraining dynamic control transfers is a common technique for mitigating software vulnerabilities.  This defense has been widely and successfully used to protect return addresses and stack data; hence, current attacks instead typically corrupt vtable and function pointers to subvert a forward edge (an indirect jump or call) in the control-flow graph.  Forward edges can be protected using Control-Flow Integrity (CFI) but, to date, CFI implementations have been research prototypes, based on impractical assumptions or ad hoc, heuristic techniques.  To be widely adoptable, CFI mechanisms must be integrated into production compilers and be compatible with software-engineering aspects such as incremental compilation and dynamic libraries. This paper presents implementations of fine-grained, forward-edge CFI enforcement and analysis for GCC and LLVM that meet the above requirements.  An analysis and evaluation of the security, performance, and resource consumption of these mechanisms applied to the SPEC CPU2006 benchmarks and common benchmarks for the Chromium web browser show the practicality of our approach: these fine-grained CFI mechanisms have significantly lower overhead than recent academic CFI prototypes.  Implementing CFI in industrial compiler frameworks has also led to insights into design tradeoffs and practical challenges, such as dynamic loading."
"Would a Privacy Fundamentalist Sell Their DNA for $1000...If Nothing Bad Happened as a Result? The Westin Categories, Behavioral Intentions, and Consequences.  Westin's Privacy Segmentation Index has been widely used to measure privacy attitudes and categorize individuals into three privacy groups: fundamentalists, pragmatists, and unconcerned. Previous research has failed to establish a robust correlation between the Westin categories and actual or intended behaviors. Unexplored however is the connection between the Westin categories and individuals' responses to the consequences of privacy behaviors. We use a survey of 884 Amazon Mechanical Turk participants to investigate the relationship between the Westin Privacy Segmentation Index and attitudes and behavioral intentions for both privacy-sensitive scenarios and privacy-sensitive consequences. Our results indicate a lack of correlation between the Westin categories and consequences. We discuss potential implications of this attitude-consequence gap."
"Sources of Traffic Demand Variability and Use of Monte Carlo for Network Capacity Planning.  When sizing any network capacity, several factors, such as Traffic, Quality of Service (QoS), and Total Cost of Ownership (TCO) are usually taken into account.  Generally, it boils down to a joint minimization of cost and maximization of traffic subject to the constraints of protocol and QoS requirements.  Stochastic nature of network traffic and link saturation queueing issues add uncertainty to the already complex optimization problem.  In this paper, we examine the sources of traffic demand variability and dive into Monte-Carlo methodology as an efficient way for solving these problems.  Other sources of uncertainty in network capacity forecasting are briefly discussed in the Attachment."
"Mesa: Geo-Replicated, Near Real-Time, Scalable Data Warehousing.  Mesa is a highly scalable analytic data warehousing system that stores critical measurement data related to Google's Internet advertising business. Mesa is designed to satisfy a complex and challenging set of user and systems requirements, including near real-time data ingestion and queryability, as well as high availability, reliability, fault tolerance, and scalability for large data and query volumes. Specifically, Mesa handles petabytes of data, processes millions of row updates per second, and serves billions of queries that fetch trillions of rows per day. Mesa is geo-replicated across multiple datacenters and provides consistent and repeatable query answers at low latency, even when an entire datacenter fails. This paper presents the Mesa system and reports the performance and scale that it achieves."
"RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response.  Randomized Aggregatable Privacy-Preserving Ordinal Response, or RAPPOR, is a technology for crowdsourcing statistics from end-user client software, anonymously, with strong privacy guarantees. In short, RAPPORs allow the forest of client data to be studied, without permitting the possibility of looking at individual trees. By applying randomized response in a novel manner, RAPPOR provides the mechanisms for such collection as well as for efficient, high-utility analysis of the collected data. In particular, RAPPOR permits statistics to be collected on the population of client-side strings with strong privacy guarantees for each client, and without linkability of their reports. This paper describes and motivates RAPPOR, details its differential-privacy and utility guarantees, discusses its practical deployment and properties in the face of different attack models, and, finally, gives results of its application to both synthetic and real-world data."
"Scalable K-Means by ranked retrieval.  The k-means clustering algorithm has a long history and a proven practical performance, however it does not scale to clustering millions of data points into thousands of clusters in high dimensional spaces. The main computational bottleneck is the need to recompute the nearest centroid for every data point at every iteration, aprohibitive cost when the number of clusters is large. In this paper we show how to reduce the cost of the k-means algorithm by large factors by adapting ranked retrieval techniques. Using a combination of heuristics, on two real life data sets the wall clock time per iteration is reduced from 445 minutes to less than 4, and from 705 minutes to 1.4, while the clustering quality remains within 0.5% of the k-means quality. The key insight is to invert the process of point-to-centroid assignment by creating an inverted index over all the points and then using the current centroids as queries to this index to decide on cluster membership. In other words, rather than each iteration consisting of ""points picking centroids"", each iteration now consists of ""centroids picking points"". This is much more efficient, but comes at the cost of leaving some points unassigned to any centroid. We show experimentally that the number of such points is low and thus they can be separately assigned once the final centroids are decided. To speed up the computation we sparsify the centroids by pruning low weight features. Finally, to further reduce the running time and the number of unassigned points, we propose a variant of the WAND algorithm that uses the results of the intermediate results of nearest neighbor computations to improve performance."
"Large-Scale Object Classification Using Label Relation Graphs.  . In this paper we study how to perform object classification in
a principled way that exploits the rich structure of real world labels. We
develop a new model that allows encoding of flexible relations between
labels. We introduce Hierarchy and Exclusion (HEX) graphs, a new formalism
that captures semantic relations between any two labels applied
to the same object: mutual exclusion, overlap and subsumption. We then
provide rigorous theoretical analysis that illustrates properties of HEX
graphs such as consistency, equivalence, and computational implications
of the graph structure. Next, we propose a probabilistic classification
model based on HEX graphs and show that it enjoys a number of desirable
properties. Finally, we evaluate our method using a large-scale
benchmark. Empirical results demonstrate that our model can signifi-
cantly improve object classification by exploiting the label relations."
"Corporate learning at scale: Lessons from a large online course at Google.  Google Research recently tested a massive online class model for an internal engineering education program, with machine learning as the topic, that blended theoretical concepts and Google-specific software tool tutorials. The goal of this training was to foster engineering capacity to leverage machine learning tools in future products. The course was delivered both synchronously and asynchronously, and students had the choice between studying independently or participating with a group. Since all students are company employees, unlike most publicly offered MOOCs we can continue to measure the students?? behavioral change long after the course is complete. This paper describes the course, outlines the available data set and presents directions for analysis."
"Deep boosting.  We present a new ensemble learning algorithm, DeepBoost, which can use as base classifiers a hypothesis set containing deep decision trees, or members of other rich or complex families, and succeed in achieving high accuracy without overfitting the data. The key to the success of the algorithm is a capacity-conscious criterion for the selection of the hypotheses. We give new data- dependent learning bounds for convex ensembles expressed in terms of the Rademacher complexities of the sub-families composing the base classifier set, and the mixture weight assigned to each sub-family. Our algorithm directly benefits from these guarantees since it seeks to minimize the corresponding learning bound. We give a full description of our algorithm, including the details of its derivation, and report the results of several experiments showing that its performance compares favorably to that of AdaBoost and Logistic Regression and their L1-regularized variants."
"Low-Overhead Network-on-Chip Support for Location-Oblivious Task Placement.  Many-core processors will have many processing cores with a network-on-chip (NoC) that provides access to shared resources such as main memory and on-chip caches. However, locally-fair arbitration in multi-stage NoC can lead to globally unfair access to shared resources and impact system-level performance depending on where each task is physically placed. In this work, we propose an arbitration to provide equality-of-service (EoS) in the network and provide support for location-oblivious task placement. We propose using probabilistic arbitration combined with distance-based weights to achieve EoS and overcome the limitation of round-robin arbiter. However, the complexity of probabilistic arbitration results in high area and long latency which negatively impacts performance. In order to reduce the hardware complexity, we propose an hybrid arbiter that switches between a simple arbiter at low load and a complex arbiter at high load. The hybrid arbiter is enabled by the observation that arbitration only impacts the overall performance and global fairness at a high load. We evaluate our arbitration scheme with synthetic traffic patterns and GPGPU benchmarks. Our results shows that hybrid arbiter that combines round-robin arbiter with probabilistic distance-based arbitration reduces performance variation as task placement is varied and also improves average IPC."
"Hearing the Shape of the Ising Model with a Programmable Superconducting-Flux Annealer.  Two objects can be distinguished if they have different measurable properties. Thus, distinguishability depends on the Physics of the objects. In considering graphs, we revisit the Ising model as a framework to define physically meaningful spectral invariants. In this context, we introduce a family of refinements of the classical spectrum and consider the quantum partition function. We demonstrate that the energy spectrum of the quantum Ising Hamiltonian is a stronger invariant than the classical one without refinements. For the purpose of implementing the related physical systems, we perform experiments on a programmable annealer with superconducting flux technology. Departing from the paradigm of adiabatic computation, we take advantage of a noisy evolution of the device to generate statistics of low energy states. The graphs considered in the experiments have the same classical partition functions, but different quantum spectra. The data obtained from the annealer distinguish non-isomorphic graphs via information contained in the classical refinements of the functions but not via the differences in the quantum spectra."
"Value of Targeting.  We undertake a formal study of the value of targeting data to an advertiser. As expected, this value is increasing in the utility difference between realizations of the targeting data and the accuracy of the data, and depends on the distribution of competing bids. However, this value may vary non-monotonically with an advertiser??s budget. Similarly, modeling the values as either private or correlated, or allowing other advertisers to also make use of the data, leads to unpredictable changes in the value of data. We address questions related to multiple data sources, show that utility of additional data may be non-monotonic, and provide tradeoffs between the quality and the price of data sources. In a game-theoretic setting, we show that advertisers may be worse off than if the data had not been available at all. We also ask whether a publisher can infer the value an advertiser would place on targeting data from the advertiser??s bidding behavior and illustrate that this is impossible."
"RFC7304 - A Method for Mitigating Namespace Collisions.  This document outlines a possible, but not recommended, method to mitigate the effect of collisions in the DNS namespace by providing a means for end users to disambiguate the conflict."
"Near-Data Processing: Insights from a MICRO-46 Workshop.  The cost of data movement in big-data systems motivates careful examination of near-data processing (NDP) frameworks. The concept of NDP was actively researched in the 1990s, but gained little commercial traction. After a decade-long dormancy, interest in this topic has spiked. A workshop on NDP was organized at MICRO-46 and was well attended. Given the interest, the organizers and keynote speakers have attempted to capture the key insights from the workshop into an article that can be widely disseminated. This article describes the many reasons why NDP is compelling today and identifies key upcoming challenges in realizing the potential of NDP."
"Discriminative pronunciation modeling for dialectal speech recognition.  Speech recognizers are typically trained with data from a standard
dialect and do not generalize to non-standard dialects. Mismatch
mainly occurs in the acoustic realization of words, which is represented
by acoustic models and pronunciation lexicon. Standard techniques for
addressing this mismatch are generative in nature and include acoustic 
model adaptation and expansion of lexicon with pronunciation variants, 
both of which have limited effectiveness. We present a discriminative 
pronunciation model whose parameters are learned jointly with
parameters from the language models. We tease apart the
gains from modeling the transitions of canonical phones, the 
transduction from surface to canonical phones, and the language 
model. We report experiments on African American Vernacular English 
(AAVE) using NPR's StoryCorps corpus. Our models improve the 
performance over the baseline by about 2.1% on AAVE, of which 0.6% 
can be attributed to the pronunciation model. The model learns the most 
relevant phonetic transformations for AAVE speech."
"Visualizing Statistical Mix Effects and Simpson's Paradox.  We discuss how ??mix effects?? can surprise users of visualizations and potentially lead them to incorrect conclusions. This statistical issue (also known as ??omitted variable bias?? or, in extreme cases, as ??Simpson??s paradox??) is widespread and can affect any
visualization in which the quantity of interest is an aggregated value such as a weighted sum or average. Our first contribution is to
document how mix effects can be a serious issue for visualizations, and we analyze how mix effects can cause problems in a variety
of popular visualization techniques, from bar charts to treemaps. Our second contribution is a new technique, the ??comet chart,?? that is meant to ameliorate some of these issues."
"A Language-Based Approach to Secure Quorum Replication.  Quorum replication is an important technique for building distributed systems because it can simultaneously improve both the integrity and availability of computation and storage. Information flow control is a well-known method for enforcing the confidentiality and integrity of information. This paper demonstrates that these two techniques can be integrated to simultaneously enforce all three major security properties: confidentiality, integrity and availability. It presents a security-typed language with explicit language constructs for supporting secure quorum replication. The dependency analysis performed by the type system of the language provides a way to formally verify the end-to-end security assurance of complex replication schemes. We also contribute a new multilevel timestamp mechanism for synchronizing code and data replicas while controlling the side channels such mechanisms introduce."
RFC7342 - Practices for Scaling ARP and Neighbor Discovery (ND) in Large Data Centers.  This memo documents some operational practices that allow ARP and Neighbor Discovery (ND) to scale in data center environments.
"The Power of Smartphones.  If you??re new to power monitoring in the mobile design process, either when building mobile hardware or writing
software-based applications, this article will point you in the right direction, helping you identify what characteristics to consider and what test equipment to use."
"APOSTLE: Longterm Transit Monitoring and Stability Analysis of XO-2b.  The Apache Point Survey of Transit Lightcurves of Exoplanets (APOSTLE) observed 10 transits of XO-2b over a
period of 3 yr. We present measurements that confirm previous estimates of system parameters like the normalized
semi-major axis (a/R), stellar density (??), impact parameter (b), and orbital inclination (iorb). Our errors on system
parameters like a/R and ?? have improved by ???40% compared to previous best ground-based measurements.
Our study of the transit times show no evidence for transit timing variations (TTVs) and we are able to rule out
co-planar companions with masses 0.20 M??? in low order mean motion resonance with XO-2b. We also explored
the stability of the XO-2 system given various orbital configurations of a hypothetical planet near the 2:1 mean
motion resonance. We find that a wide range of orbits (including Earth-mass perturbers) are both dynamically stable
and produce observable TTVs. We find that up to 51% of our stable simulations show TTVs that are smaller than
the typical transit timing errors (???20 s) measured for XO-2b, and hence remain undetectable.
Key"
"Secular Behavior of Exoplanetary Systems: Self-Consistency and Comparisons With The Planet-Planet Scattering Hypothesis.  Planet-planet scattering has been suggested as a mechanism to explain the disproportionate number of planet-planet pairs found to lie on or near an apsidal separatrix, in which one planet's eccentricity periodically drops to near-zero. We present the results of numerical simulations of 2-planet systems having arisen from dynamically unstable 3-planet systems. We show that the distribution of near-separatrix systems arising after an instability is consistent with the observed systems, further strengthening the planet-planet scattering hypothesis.
We also note that many observed systems have been found near their extreme eccentricity values. Such a pattern may suggest a bias in exoplanet observations, as planets should have an equal probability of being discovered at any point in their secular cycle. We test this possibility by numerically integrating known multiplanet systems and determining the relative time each planet spends in a given eccentricity range and then comparing this distribution of eccentricity values to the observational uncertainty. We find that planets tend to spend more time near their minimum and maximum values as they represent turning points in the oscillations. Moreover, the uncertainties for many eccentricities are so large that we cannot make strong statements regarding the possibility that planets are being discovered at their extreme eccentricities too often. However, as uncertainties become smaller and more multiplanet systems are discovered, this potential bias should be revisited."
"Planet-Planet Scattering Leads to Tightly Packed Planetary Systems..  The known extrasolar multiple-planet systems share a surprising dynamical attribute: they cluster just beyond the Hill stability boundary. Here we show that the planet-planet scattering model, which naturally explains the observed exoplanet eccentricity distribution, can reproduce the observed distribution of dynamical configurations. We calculated how each of our scattered systems would appear over an appropriate range of viewing geometries; as Hill stability is weakly dependent on the masses, the mass-inclination degeneracy does not significantly affect our results. We consider a wide range of initial planetary mass distributions and find that some are poor fits to the observed systems. In fact, many of our scattering experiments overproduce systems very close to the stability boundary. The distribution of dynamical configurations of two-planet systems actually may provide better discrimination between scattering models than the distribution of eccentricity. Our results imply that, at least in their inner regions which are weakly affected by gas or planetesimal disks, planetary systems should be ""packed"", with no large gaps between planets."
"Planet-Planet Scattering in Planetesimal Disks.  We study the final architecture of planetary systems that evolve under the combined effects of planet-planet and planetesimal scattering. Using N-body simulations we investigate the dynamics of marginally unstable systems of gas and ice giants both in isolation and when the planets form interior to a planetesimal belt. The unstable isolated systems evolve under planet-planet scattering to yield an eccentricity distribution that matches that observed for extrasolar planets. When planetesimals are included the outcome depends upon the total mass of the planets. For M tot gsim 1 MJ the final eccentricity distribution remains broad, whereas for M tot lsim 1 MJ a combination of divergent orbital evolution and recircularization of scattered planets results in a preponderance of nearly circular final orbits. We also study the fate of marginally stable multiple planet systems in the presence of planetesimal disks, and find that for high planet masses the majority of such systems evolve into resonance. A significant fraction leads to resonant chains that are planetary analogs of Jupiter's Galilean satellites. We predict that a transition from eccentric to near-circular orbits will be observed once extrasolar planet surveys detect sub-Jovian mass planets at orbital radii of a sime 5-10 AU."
"Frame by Frame Language Identification in Short Utterances using Deep Neural Networks.  This work addresses the use of deep neural networks (DNNs) in automatic language identification (LID) focused on short test utterances. Motivated by their recent success in acoustic modelling for speech recognition, we adapt DNNs to the problem of identifying the language in a given utterance from the short-term acoustic features. We show how DNNs are particularly suitable to perform LID in real-time applications, due to their capacity to emit a language identification posterior at each new frame of the test utterance. We then analyse different aspects of the system, such
as the amount of required training data, the number of hidden layers, the relevance of contextual information and
the effect of the test utterance duration. Finally, we propose several methods to combine frame-by-frame posteriors.
Experiments are conducted on two different datasets: the public NIST Language Recognition Evaluation 2009 (3
seconds task) and a much larger corpus (of 5 million utterances) known as Google 5M LID, obtained from different
Google Services. Reported results show relative improvements of DNNs versus the i-vector system of 40% in LRE09
3 second task and 76% in Google 5M LID."
RFC7344 - Automating DNSSEC Delegation Trust Maintenance.  This document describes a method to allow DNS Operators to more easily update DNSSEC Key Signing Keys using the DNS as a communication channel.  The technique described is aimed at delegations in which it is currently hard to move information from the Child to Parent.
Learning Fine-grained Image Similarity with Deep Ranking.  Learning fine-grained image similarity is a challenging task. It needs to capture between-class and within-class image differences. This paper proposes a deep ranking model that employs deep learning techniques to learn similarity metric directly from images. It has higher learning capability than models based on hand-crafted features. A novel multiscale network structure has been developed to describe the images effectively. An efficient triplet sampling algorithm is proposed to learn the model with distributed asynchronized stochastic gradient. Extensive experiments show that the proposed algorithm outperforms models based on hand-crafted visual features and deep classification models.
"ParTes. Test Suite for Parsing Evaluation.  This paper presents ParTes, the first test suite in Spanish and Catalan for parsing qualitative evaluation. This resource is a hierarchical test suite of the representative syntactic structure and argument order phenomena. ParTes proposes a simplification of the qualitative evaluation by contributing to the automatization of this task."
"Autoregressive Product of Multi-frame Predictions Can Improve the Accuracy of Hybrid Models.  We describe a simple but effective way of using multi-frame targets to improve the accuracy of Artificial Neural Network- Hidden Markov Model (ANN-HMM) hybrid systems. In this approach a Deep Neural Network (DNN) is trained to predict the forced-alignment state of multiple frames using a separate softmax unit for each of the frames. This is in contrast to the usual method of training a DNN to predict only the state of the central frame. By itself this is not sufficient to improve accuracy of the system significantly. However, if we average the predic- tions for each frame - from the different contexts it is associated with - we achieve state of the art results on TIMIT using a fully connected Deep Neural Network without convolutional archi- tectures or dropout training. On a 14 hour subset of Wall Street Journal (WSJ) using a context dependent DNN-HMM system it leads to a relative improvement of 6.4% on the dev set (test- dev93) and 9.3% on test set (test-eval92)."
Software Defined Networking at Scale.  Software Defined Networks require Software Defined Operations. Google made great progress in SDN data and control plane. This talk discusses how we are working with the industry to transform the network management plane into a software defined framework.
"SAC063 - SSAC Advisory on DNSSEC Key Rollover in the Root Zone.  There is consensus in the security and domain name system (DNS) communities that the root zone DNS Security Extensions (DNSSEC) system poses unique challenges for standard DNSSEC practices. While there is agreement that an eventual root zone Key-Signing Key (KSK) rollover is inevitable regardless of whether that rollover is caused by a key compromise or other factors, there is no solid consensus in the technical community regarding the frequency of routine, scheduled KSK rollovers. 
In this Advisory the SSAC addresses the following topics:"
"Interpretable groups are definable.  We prove that in an arbitrary o-minimal structure, every interpretable group is definably isomorphic to a definable one. We also prove that every definable group lives in a cartesian product of one-dimensional definable group-intervals (or one-dimensional definable groups). We discuss the general open question of elimination of imaginaries in an o-minimal structure."
"SAC062 - ICANN SSAC Advisory Concerning the Mitigation of Name  Collision Risk.  The term ??name collision?? refers to the situation in which a name that is properly defined in one operational domain or naming scope may appear in another domain (in which it is also syntactically valid), where users, software, or other functions in that domain may misinterpret it as if it correctly belonged there. The circumstances that may cause this can be accidental or malicious.
In the context of Top Level Domains (TLDs), the conflicting 
namespaces are the DNS namespace defined in the root zone as published by the root management partners (ICANN, U.S. Dept. of Commerce National Telecommunications Information Administration (NTIA), and VeriSign) and any privately defined namespace, whether that namespace is defined only for the Domain Name System (DNS) or is also intended to ??work?? for other namespaces such as Active Directory"
"DaMN ??? Discriminative and Mutually Nearest: Exploiting Pairwise Category Proximity for Video Action Recognition.  We propose a method for learning discriminative category-level features and demonstrate state-of-the-art results on large-scale action recognition in video. The key observation is that one-vs-rest classifiers, which are ubiquitously employed for this task, face challenges in separating very similar categories (such as running vs. jogging).  Our proposed method automatically identifies such pairs of categories using a criterion of mutual pairwise proximity in the (kernelized) feature space, using a category-level similarity matrix where each entry corresponds to the one-vs-one SVM margin for pairs of categories.  We then exploit the observation that while splitting such ""Siamese Twin"" categories may be difficult, separating them from the remaining categories in a two-vs-rest framework is not.  This enables us to augment one-vs-rest classifiers with a judicious selection of ""two-vs-rest"" classifier outputs, formed from such discriminative and mutually nearest (DaMN) pairs.  By combining one-vs-rest and two-vs-rest features in a principled probabilistic manner, we achieve state-of-the-art results on the UCF101 and HMDB51 datasets.  More importantly, the same DaMN features, when treated as a mid-level representation also outperform existing methods in knowledge transfer experiments, both cross-dataset from UCF101 to HMDB51 and to new categories with limited training data (one-shot and few-shot learning).  Finally, we study the generality of the proposed approach by applying DaMN to other classification tasks; our experiments show that DaMN outperforms related approaches in direct comparisons, not only on video action recognition but also on their original image dataset tasks."
"Video Object Discovery and Co-segmentation with Extremely Weak Supervision.  Video object co-segmentation refers to the problem of simultaneously segmenting a common category of objects from multiple videos. Most existing
video co-segmentation methods assume that all frames from all videos contain the target objects. Unfortunately, this assumption is rarely true in practice, particularly for large video sets, and existing methods perform poorly when the assumption is violated. Hence, any practical video object co-segmentation algorithm needs to identify the relevant frames containing the target object from all videos, and then co-segment the object only from these relevant frames. We present a spatiotemporal energy minimization formulation for simultaneous video object discovery and co-segmentation across multiple videos. Our formulation incorporates a spatiotemporal auto-context model, which is combined with appearance modeling for superpixel labeling. The superpixel-level labels are propagated to the frame level through a multiple instance boosting algorithm with spatial reasoning (Spatial-MILBoosting), based on which frames containing the video object are identified. Our method only needs to be bootstrapped with the frame-level labels for a few video frames (e.g., usually 1 to 3) to indicate if they contain the target objects or not. Experiments on three datasets validate the efficacy of our proposed method, which compares favorably with the state-of-the-art."
"Concise Bid Optimization Strategies with Multiple Budget Constraints.  A major challenge faced by the marketers attempting to optimize their advertising campaigns is to deal with budget constraints. The problem is even harder in the face of multidimensional budget constraints, particularly in the presence of many decision variables involved, and the interplay among the decision variables through these such constraints. Concise bidding strategies help advertisers deal with this challenge by introducing fewer variables to act on.
In this paper, we study the problem of finding optimal concise bidding strategies for advertising campaigns with multiple budget constraints. Given bid landscapes??i.e., predicted value (e.g., number of clicks) and cost per click for any bid??that are typically provided by ad-serving systems, we optimize the value given budget constraints. In particular, we consider bidding strategies that consist of no more than k different bids for all keywords. For constant k, we provide a PTAS to optimize the profit, whereas for arbitrary k we show how constant-factor approximation can be obtained via a combination of solution enumeration and dependent LP-rounding techniques.
Finally, we evaluate the performance of our algorithms on real datasets in two regimes with 1- and 3-dimensional budget constraint. In the former case where uniform bidding has provable performance guarantee, our algorithm beats the state of the art by an increase of 1% to 6% in the expected number of clicks. This is achieved by only two or three clusters??contrast with the single cluster permitted in uniform bidding. With only three dimensions in the budget constraint (one for total consumption, and another two for enforcing minimal diversity), the gap between the performance of our algorithm and an enhanced version of uniform bidding grows to an average of 5% to 6% (sometimes as high as 9%). Although the details of experiments for the multidimensional budget constraint to the full version of the paper are deferred to the full version of the paper, we report some highlights from the results."
"Distributed Balanced Clustering via Mapping Coresets.  Large-scale clustering of data points in metric spaces is an important problem in mining big data sets. For many applications, we face explicit or implicit size constraints for each cluster which leads to the problem of clustering under capacity constraints or the balanced clustering'' problem. Although the balanced clustering problem has been widely studied, developing a theoretically sound distributed algorithm remains an open problem. In the present paper we develop a general framework based on mapping coresets'' to tackle this issue. For a wide range of clustering objective functions such as k-center, k-median, and k-means, our techniques give distributed algorithms for balanced clustering that match the best known single machine approximation ratios."
"Topology-Driven Trajectory Synthesis with an Example on Retinal Cell Motions.  We design a probabilistic trajectory synthesis algorithm for generating time-varying sequences of geometric configuration data. The algorithm takes a set of observed samples (each may come from a different trajectory) and simulates the dynamic evolution of the patterns in O(n^2 log n) time. To synthesize geometric configurations with indistinct identities, we use the pair correlation function to summarize point distribution, and alpha-shapes to maintain topological shape features based on a fast persistence matching approach. We apply our method to build a computational model for the geometric transformation of the cone mosaic in retinitis pigmentosa --- an inherited and currently untreatable retinal degeneration."
"Bridging Text and Knowledge with Frames.  FrameNet is the current best operational
version of Chuck Fillmore??s Frame Semantics. As FrameNet has evolved over the years, we have been building a series of increasingly ambitious prototype applications that exploit the ideas of frame semantics and FrameNet as a resource. Results from this work suggest that frames are a
natural semantic representation linking issue of textual meaning and world knowledge."
"Long-term SLOs for reclaimed cloud computing resources.  The elasticity promised by cloud computing does not come for free. Providers need to reserve resources to allow users to scale on demand, and cope with workload variations, which results in low utilization. The current response to this low utilization is to re-sell unused resources with no Service Level Objectives (SLOs) for availability. In this paper, we show how to make some of these reclaimable resources more valuable by providing strong, long-term availability SLOs for them. These SLOs are based on forecasts of how many resources will remain unused during multi-month periods, so users can do capacity planning for their long-running services. By using confidence levels for the predictions, we give service providers control over the risk of violating the availability SLOs, and allow them trade increased risk for more resources to make available. We evaluated our approach using 45 months of workload data from 6 production clusters at Google, and show that 6--17% of the resources can be re-offered with a long-term availability of 98.9% or better. A conservative analysis shows that doing so may increase the profitability of selling reclaimed resources by 22--60%."
"ZARATHUSTRA: Extracting WebInject Signatures from Banking Trojans.  Modern trojans are equipped with a functionality, called WebInject, that can be used to silently modify a web page on the infected end host. Given its flexibility, WebInject-based malware is becoming a popular information-stealing mechanism. In addition, the structured and well-organized malware-as-a-service model makes revenue out of customization kits, which in turns leads to high volumes of binary variants. Analysis approaches based on memory carving to extract the decrypted webinject.txt and config.bin files at runtime make the strong assumption that the malware will never change the way such files are handled internally, and therefore are not future proof by design. In addition, developers of sensitive web applications (e.g., online banking) have no tools that they can possibly use to even mitigate the effect of WebInjects. WebInject-based trojans insert client-side code (e.g., HTML, 
JavaScript) while the targeted web pages (e.g., online banking 
website, search engine) are rendered on the browser. This 
additional code will capture sensitive information entered by 
the victim (e.g., one-time passwords) or perform other nefarious 
actions (e.g., click fraud or search engine result poisoning). The 
visible effect of a WebInject is that a web page rendered on 
infected clients differs from the very same page rendered on 
clean machines. We leverage this key observation and propose an 
approach to automatically characterize the WebInject behavior. 
Ultimately, our system can be applied to analyze a sample 
automatically against a set of target websites, without requiring 
any manual action, or to generate fingerprints that are useful to 
determine whether a client is infected. Differently from the state 
of the art, our method works regardless of how the WebInject 
module is implemented and requires no reverse engineering. We implemented and evaluated our approach against live online websites and a dataset of distinct variants of WebInject-based financial trojans. The results show that our approach correctly recognize known variants of WebInject-based malware with negligible false positives. Throughout the paper, we describe some use cases that describe how our method can be applied in practice"
"Going Deeper with Convolutions.  We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation of this architecture, GoogLeNet, a 22 layers deep network, was used to assess its quality in the context of object detection and classification."
"Evaluating job packing in warehouse-scale computing.  One of the key factors in selecting a good scheduling
algorithm is using an appropriate metric for comparing
schedulers. But which metric should be used when evaluating
schedulers for warehouse-scale (cloud) clusters, which have
machines of different types and sizes, heterogeneous workloads
with dependencies and constraints on task placement, and long-running
services that consume a large fraction of the total
resources? Traditional scheduler evaluations that focus on metrics
such as queuing delay, makespan, and running time fail to
capture important behaviors ?? and ones that rely on workload
synthesis and scaling often ignore important factors such as
constraints. This paper explains some of the complexities and
issues in evaluating warehouse scale schedulers, focusing on what
we find to be the single most important aspect in practice: how
well they pack long-running services into a cluster. We describe
and compare four metrics for evaluating the packing efficiency
of schedulers in increasing order of sophistication: aggregate
utilization, hole filling, workload inflation and cluster compaction."
"Discovering Groups of People in Images.  Understanding group activities from images is an important yet challenging task. This is because there is an exponentially large number of semantic and geometrical relationships among individuals that one must model in order to effectively recognize and localize the group activities. Rather than focusing on directly recognizing group activities as most of the previous works do, we advocate the importance of introducing an intermediate representation for modeling groups of humans which we call structure groups. Such groups define the way people spatially interact with each other. People might be facing each other to talk, while others sit on a bench side by side, and some might stand alone. In this paper we contribute a method for identifying and localizing these structured groups in a single image despite their varying viewpoints, number of participants, and occlusions. We propose to learn an ensemble of discriminative interaction patterns to encode the relationships between people in 3D and introduce a novel efficient iterative augmentation algorithm for solving this complex inference problem. A nice byproduct of the inference scheme is an approximate 3D layout estimate of the structured groups in the scene. Finally, we contribute an extremely challenging new dataset that contains images each showing multiple people performing multiple activities. Extensive evaluation confirms our theoretical findings."
"A Survey of Algorithms and Analysis for Adaptive Online Learning.  We present tools for the analysis of Follow-The-Regularized-Leader (FTRL), Dual Averaging, and Mirror Descent algorithms when the regularizer (equivalently, prox-function or learning rate schedule) is chosen adaptively based on the data. Adaptivity can be used to prove regret bounds that hold on every round, and also allows for data-dependent regret bounds as in AdaGrad-style algorithms (e.g., Online Gradient Descent with adaptive per-coordinate learning rates). We present results from a large number of prior works in a unified manner, using a modular and tight analysis that isolates the key arguments in easily re-usable lemmas. This approach strengthens previously known FTRL analysis techniques to produce bounds as tight as those achieved by potential functions or primal-dual analysis. Further, we prove a general and exact equivalence between an arbitrary adaptive Mirror Descent algorithm and a correspond- ing FTRL update, which allows us to analyze any Mirror Descent algorithm in the same framework. The key to bridging the gap between Dual Averaging and Mirror Descent algorithms lies in an analysis of the FTRL-Proximal algorithm family. Our regret bounds are proved in the most general form, holding for arbitrary norms and non-smooth regularizers with time-varying weight."
"Backoff Inspired Features for Maximum Entropy Language Models.  Maximum Entropy (MaxEnt) language models are linear models that are typically regularized via well-known L1 or L2 terms in the likelihood objective, hence avoiding the need for the kinds of backoff or mixture weights used in smoothed n-gram language models using Katz backoff and similar techniques. Even though backoff cost is not required to regularize the model, we investigate the use of backoff features in MaxEnt models, as well as some backoff-inspired variants. These features are shown to improve model quality substantially, as shown in perplexity and word-error rate reductions, even in very large scale training scenarios of tens or hundreds of billions of words and hundreds of millions of features."
"Storing and Querying Tree-Structured Records in Dremel.  In Dremel, data is stored as nested relations. The schema
for a relation is a tree, all of whose nodes are attributes, and whose leaf attributes hold values. We explore filter and aggregate queries that are given in the Dremel dialect of SQL. Complications arise because of repeated attributes, i.e., attributes that are allowed to have more than one value. We focus on the common class of Dremel queries that are processed on column-stored data in a way that results in query processing time that is linear on the size of the relevant data, i.e., data in the columns that participate in the query. We formally define the data model, the query language and the algorithms for query processing in column-stored data. The concepts of repetition context and semi-flattening are introduced here and play a central role in understanding this class of queries and their algorithms."
"TRAM: Optimizing Fine-grained Communication with Topological Routing and Aggregation of Messages.  Fine-grained communication in supercomputing applications often limits performance through high communication overhead and poor utilization of network bandwidth. This paper presents Topological Routing and Aggregation Module (TRAM), a library that optimizes fine-grained communication performance by routing and dynamically combining short messages. TRAM collects units of fine-grained communication from the application and combines them into aggregated messages with a common intermediate destination. It routes these messages along a virtual mesh topology mapped onto the physical topology of the network. TRAM improves network bandwidth utilization and reduces communication overhead. It is particularly effective in optimizing patterns with global communication and large message counts, such as all to-all and many-to-many, as well as sparse, irregular, dynamic or data dependent patterns. We demonstrate how TRAM improves performance through theoretical analysis and experimental verification using benchmarks and scientific applications. We present speedups on petascale systems of 6x for communication benchmarks and up to 4x for applications."
"Connected Components in MapReduce and Beyond.  Computing connected components of a graph lies at the core of many data mining algorithms, and is a fundamental subroutine in graph clustering. This problem is well studied, yet many of the algorithms with good theoretical guarantees perform poorly in practice, especially when faced with graphs with hundreds of billions of edges. In this paper, we design improved algorithms based on traditional MapReduce architecture for large scale data analysis. We also explore the effect of augmenting MapReduce with a distributed hash table (DHT) service. We show that these algorithms have provable theoretical guarantees, and easily outperform previously studied algorithms, sometimes by more than an order of magnitude. In particular, our iterative MapReduce algorithms run 3 to 15 times faster than the best previously studied algorithms, and the MapReduce implementation using a DHT is 10 to 30 times faster than the best previously studied algorithms. These are the fastest algorithms that easily scale to graphs with hundreds of billions of edges."
"GyroPen: Gyroscopes for Pen-input with Mobile Phones.  We present GyroPen, a method for text entry into mobile devices using pen-like
writing interaction reconstructed from standard built-in sensors.  The key idea
is to reconstruct a representation of the trajectory of the phone's corner that
is touching a writing surface from the measurements obtained from the phone's
gyroscopes and accelerometers.  We propose to directly use the angular
trajectory for this reconstruction, which removes the necessity for accurate
absolute 3D position estimation, a task that can be difficult using low-cost
accelerometers.  Recognition is then performed using an off-the-shelf
handwriting recognition system, allowing easy extension to new languages and
scripts.  In a small user study (n=10), the average novice participant was able
to write the first word only 37 seconds after the starting to use GyroPen for
the first time. With some experience, users were able to write at the speed of
3-4s for one English word and with a character error rate of 18%."
"Sui sondaggi politici in Italia.  In this discussion piece, Piergiorgio Corbetta and Mario Callegaro analyse the results of Italian pre-election polls for the European election of May 2014. The paper is in Italian language."
"Dialing Back Abuse on Phone Verified Accounts.  In the past decade the increase of for-profit cybercrime has given rise to an entire underground ecosystem supporting large-scale abuse, a facet of which encompasses the bulk registration of fraudulent accounts. In this paper, we present a 10 month longitudinal study of the underlying technical and financial capabilities of criminals who register phone verified accounts (PVA). To carry out our study, we purchase 4,695 Google PVA as well as acquire a random sample of 300,000 Google PVA through a collaboration with Google. We find that miscreants rampantly abuse free VOIP services to circumvent the intended cost of acquiring phone numbers, in effect undermining phone verification. Combined with short lived phone numbers from India and Indonesia that we suspect are tied to human verification farms, this confluence of factors correlates with a market-wide price drop of 30--40% for Google PVA until Google penalized verifications from frequently abused carriers. We distill our findings into a set of recommendations for any services performing phone verification as well as highlight open challenges related to PVA abuse moving forward."
"The Data on Diversity: It's not just about being fair.  Teams and organizations whose members are heterogeneous in meaningful ways have a higher potential for innovation than teams whose members are homogeneous. However, making such teams effective requires addressing a number of barriers. Social science experiments using quantitative methods show bias, stereotype threat, and methods to combat them. The effectiveness of diverse teams depends on trusting and supportive cultures. Data publication is one of the most important tools to identify and combat identity threat and biased decision making. Despite the challenges, there is hope! There are tools that have been shown to combat bias and identity threat effectively."
"Egocentric Field-of-View Localization Using First-Person Point-of-View Devices.  We present a technique that uses images, videos and sensor data taken from first-person point-of-view devices to perform egocentric field-of-view (FOV) localization. We define egocentric FOV localization as capturing the visual information from a person??s field-of-view in a given environment and transferring this information onto a reference corpus of images and videos of the same space, hence determining what a person is attending to. Our method matches images and video taken from the first-person perspective with the reference corpus and refines the results using the first-person??s head orientation information obtained using the device sensors. We demonstrate single and multi-user egocentric FOV localization in different indoor and outdoor environments with applications in augmented reality, event understanding and studying social interactions."
"Delay-Tolerant Algorithms for Asynchronous Distributed Online Learning.  We analyze new online gradient descent algorithms for distributed systems with large delays between gradient computations and the corresponding updates. Using insights from adaptive gradient methods, we develop algorithms that adapt not only to the sequence of gradients, but also to the precise update delays that occur. We first give an impractical algorithm that achieves a regret bound that precisely quantifies the impact of the delays. We then analyze AdaptiveRevision, an algorithm that is efficiently implementable and achieves comparable guarantees. The key algorithmic technique is appropriately and efficiently revising the learning rate used for previous gradient steps. Experimental results show when the delays grow large (1000 updates or more), our new algorithms perform significantly better than standard adaptive gradient methods."
"Neural Networks and Neuroscience-Inspired Computer Vision.  Brains are, at a fundamental level, biological computing machines. They transform a torrent of complex and ambiguous sensory information into coherent thought and action, allowing an organism to perceive and model its environment, synthesize and make decisions from disparate streams of information, and adapt to a changing environment. Against this backdrop, it is perhaps not surprising that computer science, the science of building artificial computational systems, has long looked to biology for inspiration. However, while the opportunities for cross-pollination between neuroscience and computer science are great, the road to achieving brain-like algorithms has been long and rocky. Here, we review the historical connections between neuroscience and computer science, and we look forward to a new era of potential collaboration, enabled by recent rapid advances in both biologically-inspired computer vision and in experimental neuroscience methods. In particular, we explore where neuroscience-inspired algorithms have succeeded, where they still fail, and we identify areas where deeper connections are likely to be fruitful."
"The atoms of neural computation.  The human cerebral cortex is central to a wide array of cognitive functions, from vision to language, reasoning, decision-making, and motor control. Yet, nearly a century after the neuroanatomical organization of the cortex was first defined, its basic logic remains unknown. One hypothesis is that cortical neurons form a single, massively repeated ??canonical?? circuit, characterized as a kind of a ??nonlinear spatiotemporal filter with adaptive properties?? (1). In this classic view, it was ??assumed that these??properties are identical for all neocortical areas.?? Nearly four decades later, there is still no consensus about whether such a canonical circuit exists, either in terms of its anatomical basis or its function. Likewise, there is little evidence that such uniform architectures can capture the diversity of cortical function in simple mammals, let alone characteristically human processes such as language and abstract thinking (2). Analogous software implementations in artificial intelligence (e.g., deep learning networks) have proven effective in certain pattern classification tasks, such as speech and image recognition, but likewise have made little inroads in areas such as reasoning and natural language understanding. Is the search for a single canonical cortical circuit misguided?"
"Circulant Binary Embedding.  Binary embedding of high-dimensional data requires long codes to preserve the discriminative power of the input space. Traditional binary coding methods often suffer from very high computation and storage costs in such a scenario. To
address this problem, we propose Circulant Binary Embedding (CBE) which generates binary codes by projecting the data with a circulant matrix. The circulant structure enables the use of Fast Fourier Transformation to speed up the computation. Compared to methods that use unstructured matrices, the proposed method improves the time complexity from O(d^2)  to O(dlogd),
and the space complexity from O(d^2) to O(d) where d is the input dimensionality. We also propose a novel time-frequency alternating optimization
to learn data-dependent circulant projections, which alternatively minimizes the objective in original and Fourier domains. We show by extensive experiments that the proposed approach gives much better performance than the state-of-the-art approaches for fixed time, and provides much faster computation with no performance degradation for fixed number of bits."
"Discrete Graph Hashing.  Hashing has emerged as a popular technique for fast nearest neighbor search in gigantic databases. In particular, learning based hashing has received considerable attention due to its appealing storage and search efficiency. However, the performance of most unsupervised learning based hashing methods deteriorates rapidly as the hash code length increases. We argue that the degraded performance is due to inferior optimization procedures used to achieve discrete binary codes. This paper presents a graph-based unsupervised hashing model to preserve the neighborhood structure of massive data in a discrete code space. We cast the graph hashing problem into a discrete optimization framework which directly learns the binary codes. A tractable alternating maximization algorithm is then proposed to explicitly deal with the discrete constraints, yielding high-quality codes to well capture the local neighborhoods. Extensive experiments performed on four large datasets with up to one million samples show that our discrete optimization based graph hashing method obtains superior search accuracy over state-of-the-art unsupervised hashing methods, especially for longer codes."
"Machine Learning: The High Interest Credit Card of Technical Debt.  Hashing has emerged as a popular technique for fast nearest neighbor search in gigantic databases. In particular, learning based hashing has received considerable attention due to its appealing storage and search efficiency. However, the performance of most unsupervised learning based hashing methods deteriorates rapidly as the hash code length increases. We argue that the degraded performance is due to inferior optimization procedures used to achieve discrete binary codes. This paper presents a graph-based unsupervised hashing model to preserve the neighborhood structure of massive data in a discrete code space. We cast the graph hashing problem into a discrete optimization framework which directly learns the binary codes. A tractable alternating maximization algorithm is then proposed to explicitly deal with the discrete constraints, yielding high-quality codes to well capture the local neighborhoods. Extensive experiments performed on four large datasets with up to one million samples show that our discrete optimization based graph hashing method obtains superior search accuracy over state-of-the-art unsupervised hashing methods, especially for longer codes."
"Norming to Performing: Failure Analysis and Deployment Automation of Big Data Software Developed by Highly Iterative Models.  We observe many interesting failure characteristics from Big Data software developed and released using some kinds of highly iterative development models (e.g., agile). ~16% of failures occur due to faults in software deployments (e.g., packaging and pushing to production). Our analysis shows that many such production outages are at least partially due to some human errors rooted in the high frequency and complexity of software deployments. ~51% of the observed human errors (e.g., transcription, education, and communication error types) are avoidable through automation. We thus develop a fault-tolerant automation framework to make it efficient to automate end-to-end software deployment procedures. We apply the framework to two Big Data products. Our case studies show the complexity of the deployment procedures of multi-homed Big Data applications and help us to study the effectiveness of the validation and verification techniques for user-provided automation programs. We analyze the production failures of the two products again after the automation. Our experimental data shows how the automation and the associated procedure improvements reduce the deployment faults and overall failure rate, and improve the feature launch velocity. Automation facilitates more formal, procedure-driven software engineering practices which not only reduce the manual work and human-oriented, avoidable production outages but also help engineers to better understand overall software engineering procedures, making them more auditable, predictable, reliable, and efficient. We discuss two novel metrics to evaluate progress in mitigating human errors and the conditions indicating points to start such transition from owner-driven deployment practice."
"Large Scale Deep Learning.  Keynote at CIKM 2014 conference, Shanghai, China, November, 2014.  Talk also given at Tsinghua University."
"T(ether): Spatially-Aware Handhelds, Gestures and Proprioception for Multi-User 3D Modeling and Animation.  T(ether) is a spatially-aware display system for multi-user, collaborative manipulation and animation of virtual 3D objects. The handheld display acts as a window into virtual reality, providing users with a perspective view of 3D data. T(ether) tracks users' heads, hands, fingers and pinching, in addition to a handheld touch screen, to enable rich interaction with the virtual scene. We introduce gestural interaction techniques that exploit proprioception to adapt the UI based on the hand's position above, behind or on the surface of the display. These spatial interactions use a tangible frame of reference to help users manipulate and animate the model in addition to controlling environment properties. We report on initial user observations from an experiment for 3D modeling, which indicate T(ether)'s potential for embodied viewport control and 3D modeling interactions."
"Physical Telepresence: Shape Capture and Display for Embodied, Computer-mediated Remote Collaboration.  We propose a new approach to Physical Telepresence, based on shared workspaces with the ability to capture and remotely render the shapes of people and objects. In this paper, we describe the concept of shape transmission, and propose interaction techniques to manipulate remote physical objects and physical renderings of shared digital content. We investigate how the representation of user's body parts can be altered to amplify their capabilities for teleoperation. We also describe the details of building and testing prototype Physical Telepresence workspaces based on shape displays. A preliminary evaluation shows how users are able to manipulate remote objects, and we report on our observations of several different manipulation techniques that highlight the expressive nature of our system."
"Sequence to Sequence Learning with Neural Networks.  Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier."
"A Game-Theoretic Analysis of Rank-Order Mechanisms for User-Generated Content.  We investigate the widely-used rank-order mechanism for displaying user-generated content, where contributions are displayed on a webpage in decreasing order of their ratings, in a game-theoretic model where strategic contributors benefit from attention and have a cost to quality. We show that the lowest quality elicited by this rank-order mechanism in any mixed-strategy equilibrium becomes optimal as the available attention diverges. Additionally, these equilibrium qualities are higher, with probability tending to 1 in the limit of diverging attention, than those elicited by a more equitable proportional mechanism which distributes attention in proportion to the positive ratings a contribution receives, but the proportional mechanism elicits a greater number of contributions than the rank-order mechanism."
"Unsupervised Spatial Event Detection in Targeted Domains with Applications to Civil Unrest Modeling.  Twitter has become a popular data source as a surrogate for monitoring and detecting events. Targeted domains such as crime, election, and social unrest require the creation of algorithms capable of detecting events pertinent to these domains. Due to the unstructured language, short-length messages, dynamics, and heterogeneity typical of Twitter data streams, it is technically difficult and labor-intensive to develop and maintain supervised learning systems. We present a novel unsupervised approach for detecting spatial events in targeted domains and illustrate this approach using one specific domain, viz. civil unrest modeling. Given a targeted domain, we propose a dynamic query expansion algorithm to iteratively expand domain-related terms, and generate a tweet homogeneous graph. An anomaly identification method is utilized to detect spatial events over this graph by jointly maximizing local modularity and spatial scan statistics. Extensive experiments conducted in 10 Latin American countries demonstrate the effectiveness of the proposed approach."
"Indoor Scene Understanding with Geometric and Semantic Contexts.  Truly understanding a scene involves integrating information at multiple levels as well as studying the interactions between scene elements. Individual object detectors, layout estimators and scene classifiers are powerful but ultimately confounded by complicated real-world scenes with high variability, different viewpoints and occlusions. We propose a method that can automatically learn the interactions among scene elements and apply them to the holistic understanding of indoor scenes from a single image. This interpretation is performed within a hierarchical interaction model which describes an image by a parse graph, thereby fusing together object detection, layout estimation and scene classification. At the root of the parse graph is the scene type and layout while the leaves are the individual detections of objects. In between is the core of the system, our 3D Geometric Phrases (3DGP). We conduct extensive experimental evaluations on single image 3D scene understanding using both 2D and 3D metrics. The results demonstrate that our model with 3DGPs can provide robust estimation of scene type, 3D space, and 3D objects by leveraging the contextual relationships among the visual elements."
"ACDC-JS: explorative benchmarking of javascript memory management.  We present ACDC-JS, an open-source JavaScript memory management benchmarking tool. ACDC-JS incorporates a heap model based on real web applications and may be configured to expose virtually any relevant performance characteristics of JavaScript memory management systems. ACDC-JS is based on ACDC, a benchmarking tool for C/C++ that models periodic allocation and deallocation behavior (AC) as well as persistent memory (DC). We identify important characteristics of JavaScript mutator behavior and propose a configurable heap model based on typical distributions of these characteristics as foundation for ACDC-JS. We describe heap analyses of 13 real web applications extending existing work on JavaScript behavior analysis. Our experimental results show that ACDC-JS enables performance benchmarking and debugging of state-of-the-art JavaScript virtual machines such as V8 and SpiderMonkey by exposing key aspects of their memory management performance."
"How Developers Use Data Race Detection Tools.  Developers need help with multithreaded programming. We
investigate how two program analysis tools are used by developers
at Google: ThreadSafety, an annotation-based
static data race analysis, and TSan, a dynamic data race detector.
The data was collected by interviewing seven veteran
industry developers at Google, and provides unique insight
into how four different teams use tooling in different ways to
help with multithreaded programming. The result is a collection
of perceived pros and cons of using ThreadSafety
and TSan, as well as general issues with multithreading."
"Estimating reach curves from one data point.  Reach curves arise in advertising and media analysis as they relate the number of content impressions to the number of people who have seen it.  This is especially important for measuring the effectiveness of an ad on TV or websites.  For a mathematical and data-driven analysis, it would be very useful to know the entire reach curve; advertisers, however, often only know its last data point, i.e., the total number of impressions and the total reach.  In this work I present a new method to estimate the entire curve using only this last data point. Furthermore, analytic derivations reveal a surprisingly simple, yet insightful relationship between marginal cost per reach, average cost per impression, and frequency.  Thus, advertisers can estimate the cost of an additional reach point by just knowing their total number of impressions, reach, and cost. A comparison of the proposed one-data point method to two competing regression models on TV reach curve data, shows that the proposed methodology performs only slightly poorer than regression fits to a collection of several points along the curve."
"Position Auctions with Externalities.  This paper presents models for predicted click-through rates in position auctions that take into account the externalities ads shown in other positions may impose on the probability that an ad in a particular position receives a click. We present a general axiomatic methodology for how click probabilities are affected by the qualities of the ads in the other positions, and illustrate that using these axioms will increase revenue as long as higher quality ads tend to be ranked ahead of lower quality ads. We also present appropriate algorithms for selecting the optimal allocation of ads when predicted click-through rates are governed by a natural special case of this axiomatic model of externalities."
"HaTS: Large-scale In-product Measurement of User Attitudes &amp; Experiences with Happiness Tracking Surveys.  With the rise of Web-based applications, it is both important and feasible for human-computer interaction practitioners to measure a product??s user experience. While quantifying user attitudes at a small scale has been heavily studied, in this industry case study, we detail best Happiness Tracking Surveys (HaTS) for collecting attitudinal data at a large scale directly in the product and over time. This method was developed at Google to track attitudes and open-ended feedback over time, and to characterize products?? user bases. This case study of HaTS goes beyond the design of the questionnaire to also suggest best practices for appropriate sampling, invitation techniques, and its data analysis. HaTS has been deployed successfully across dozens of Google??s products to measure progress towards product goals and to inform product decisions; its sensitivity to product changes has been demonstrated widely. We are confident that teams in other organizations will be able to embrace HaTS as well, and, if necessary, adapt it for their unique needs."
"Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability Estimation.  We present a novel family of language model (LM) estimation techniques named Sparse Non-negative Matrix (SNM) estimation. A first set of experiments empirically evaluating it on the One Billion Word Benchmark shows that SNM n-gram LMs perform almost as well as the well-established Kneser-Ney (KN) models. When using skip-gram features the models are able to match the state-of-the-art recurrent neural network (RNN) LMs; combining the two modeling techniques yields the best known result on the benchmark. The computational advantages of SNM over both maximum entropy and RNN LM estimation are probably its main strength, promising an approach that has the same flexibility in combining arbitrary features effectively and yet should scale to very large amounts of data as gracefully as n-gram LMs do."
"Towards better measurement of attention and satisfaction in mobile search.  Web Search has seen two big changes recently: rapid growth in mobile search traffic, and an increasing trend towards providing answer-like results for relatively simple information needs (e.g., [weather today]). Such results display the answer or relevant information on the search page itself without requiring a user to click. While clicks on organic search results have been used extensively to infer result relevance and search satisfaction, clicks on answer-like results are often rare (or meaningless), making it challenging to evaluate answer quality. Together, these call for better measurement and understanding of search satisfaction on mobile devices. In this paper, we studied whether tracking the browser viewport (visible portion of a web page) on mobile phones could enable accurate measurement of user attention at scale, and provide good measurement of search satisfaction in the absence of clicks. Focusing on answer-like results in web search, we designed a lab study to systematically vary answer presence and relevance (to the user's information need), obtained satisfaction ratings from users, and simultaneously recorded eye gaze and viewport data as users performed search tasks. Using this ground truth, we identified increased scrolling past answer and increased time below answer as clear, measurable signals of user dissatisfaction with answers. While the viewport may contain three to four results at any given time, we found strong correlations between gaze duration and viewport duration on a per result basis, and that the average user attention is focused on the top half of the phone screen, suggesting that we may be able to scalably and reliably identify which specific result the user is looking at, from viewport data alone."
"Repeated Contextual Auctions with Strategic Buyers.  Motivated by real-time advertising exchanges, we analyze the problem of pricing inventory in a repeated posted-price auction. We consider both the cases of a truthful and surplus-maximizing buyer, where the former makes decisions myopically on every round, and the latter may strategically react to our algorithm, forgoing short-term surplus in order to trick the algorithm into setting better prices in the future. We further assume a buyer??s valuation of a good is a function of a context vector that describes the good being sold. We give the first algorithm attaining sublinear (O(T^{
2/3})) regret in the contextual setting against a surplus-maximizing
buyer. We also extend this result to repeated second-price auctions with multiple buyers."
"A big data approach to acoustic model training corpus selection.  Deep neural networks (DNNs) have recently become the state
of the art technology in speech recognition systems. In this paper we propose a new approach to constructing large high quality unsupervised sets to train DNN models for large vocabulary speech recognition. The core of our technique consists of two steps. We first redecode speech logged by our production recognizer with a very accurate (and hence too slow for real-time usage) set of speech models to improve the quality of ground truth transcripts used for training alignments. Using confidence scores, transcript length and transcript flattening heuristics designed to cull salient utterances from three decades of speech per language, we then carefully select training data sets consisting of up to 15K hours of speech to be used to train acoustic models without any reliance on manual transcription. We show that this approach yields models with approximately 18K context dependent states that achieve 10% relative improvement in large vocabulary dictation and voice-search systems for Brazilian  Portuguese, French, Italian and Russian languages."
"Moving Targets: Security and Rapid-Release in Firefox.  Software engineering practices strongly affect the security of the code produced. The increasingly popular Rapid Release Cycle (RRC) development methodology and easy network software distribution have enabled rapid feature introduction. RRC's defining characteristic of frequent software revisions would seem to conflict with traditional software engineering wisdom regarding code maturity, reliability and reuse, as well as security. Our investigation of the consequences of rapid release comprises a quantitative, data-driven study of the impact of rapid-release methodology on the security of the Mozilla Firefox browser. We correlate reported vulnerabilities in multiple rapid release versions of Firefox code against those in corresponding extended release versions of the same system; using a common software base with different release cycles eliminates many causes other than RRC for the observables. Surprisingly, the resulting data show that Firefox RRC does not result in higher vulnerability rates and, further, that it is exactly the unfamiliar, newly released software (the ""moving targets"") that requires time to exploit. These provocative results suggest that a rethinking of the consequences of software engineering practices for security may be warranted."
"Context-Dependent Fine-Grained Entity Type Tagging.  Entity type tagging is the task of assigning category labels to each mention of an entity in a document. While standard systems focus on a small set of types, recent work (Ling and Weld, 2012) suggests that using a large fine-grained label set can lead to dramatic improvements in downstream tasks. In the absence of labeled training data, existing fine-grained tagging systems obtain examples automatically, using resolved entities and their types extracted from a knowledge base. However, since the appropriate type often depends on context (e.g. Washington could be tagged either as city or government), this procedure can result in spurious labels, leading to poorer generalization. We propose the task of context-dependent fine type tagging, where the set of acceptable labels for a mention is restricted to only those deducible from the local context (e.g. sentence or document). We introduce new resources for this task: 11,304 mentions annotated with their context-dependent fine types, and we provide baseline experimental results on this data."
"Perfect Reconstructability of Control Flow from Demand Dependence Graphs.  Functional demand-based dependence graphs, such as the Regionalized Value State Dependence Graph, are intermediate representations that only model the flow of data and state with implicit and severely restricted control flow. While suitable for formulation of program transformations, they require algorithms for conversion from and to representations with explicit control flow such as CFG. Existing solutions exhibit structural constraints limiting quality of generated control flow, but we show that this is not intrinsic to RVSDGs. We provide algorithms capable of perfect round-trip conversions, prove their correctness and empirically evaluate their run-time performance and representation overhead."
"Multi-Class Deep Boosting.  We present new ensemble learning algorithms for multi-class classification. Our
algorithms can use as a base classifier set a family of deep decision trees or other rich or complex families and yet benefit from strong generalization guarantees. We give new data-dependent learning bounds for convex ensembles in the multiclass classification setting expressed in terms of the Rademacher complexities of the sub-families composing the base classifier set, and the mixture weight assigned to each sub-family. These bounds are finer than existing ones both thanks to an improved dependency on the number of classes and, more crucially, by virtue of a more favorable complexity term expressed as an average of the Rademacher complexities based on the ensemble??s mixture weights. We introduce and discuss several new multi-class ensemble algorithms benefiting from these guarantees, prove positive results for the H-consistency of several of them, and report the results of experiments showing that their performance compares favorably with that of multi-class versions of AdaBoost and Logistic Regression and their L1-regularized counterparts."
"An optimized template matching approach to intra coding in video/image compression.  The template matching prediction is an established approach to intra-frame coding that makes use of previously coded pixels in the same frame for reference. It compares the previously reconstructed upper and left boundaries in searching from the reference area the best matched block for prediction, and hence eliminates the need of sending additional information to reproduce the same prediction at decoder. In viewing the image signal as an auto-regressive model, this work is premised on the fact that pixels closer to the known block boundary are better predicted than those far apart. It significantly extends the scope of the template matching approach, which is typically followed by a conventional discrete cosine transform (DCT) for the prediction residuals, by employing an asymmetric discrete sine transform (ADST), whose basis functions vanish at the prediction boundary and reach maximum magnitude at far end, to fully exploit statistics of the residual signals. It was experimentally shown that the proposed scheme provides substantial coding performance gains on top of the conventional template matching method over the baseline."
"Efficient Inference and Structured Learning for Semantic Role Labeling.  We present a dynamic programming algorithm for efficient constrained inference in semantic role labeling. The algorithm tractably captures a majority of the structural constraints examined by prior work in this area, which has resorted to either approximate methods or off-the-shelf integer linear programming solvers. In addition, it allows training a globally-normalized log-linear model with respect to constrained conditional likelihood. We show that the dynamic program is several times faster than an off-the-shelf integer linear programming solver, while reaching the same solution. Furthermore, we show that our structured model results in significant improvements over its local counterpart, achieving state-of-the-art results on both PropBank- and FrameNet-annotated corpora."
"Extreme usability: adapting research approaches for agile development.  Abstract Agile development is being adopted by many leading software companies, such as those represented by this panel. Though many instructional resources exist to guide companies through a change to Agile Development, there are few resources available on the subject of Agile development and User Centered Design (UCD). As a result, user experience practitioners have had to develop their own tactics and strategies for maintaining sound UCD practices within their organizations when moving to Agile."
"ESM Versus Logs: Filling in the Gaps.  In the last few years we??ve all heard amazing stories of how ??big data?? can make uncanny predictions about users (e.g., Target knowing when someone is pregnant based on their purchases).  However, there are just as many stories of when analytics gets it wrong (e.g., Google Flu Trends overestimating flu cases in 2013). It shouldn??t be at all surprising when predictions based strictly on click analysis (sometimes with demographic data thrown in) gets it wrong because analytics tells us only the WHAT, not the WHY.  In the case of Google Flu Trends, the massive media coverage influenced people to search on the keywords that previously indicated one was coming down with the flu but now simply meant people were curious about it. Our logs were missing the WHY behind the keywords. At Google, we conduct an annual Experience Sampling Methodology (ESM) study with a large sample of our users recording their needs, context, and experience to capture the WHY we cannot see in our query stream. Over a five day period, participants tell us about their experiences throughout their day and submit photos to explain things words alone cannot capture. Doing this over a several month period every year allows us to monitor changes in user??s subjective and objective behavior for a clearer picture than analytics alone."
"You too can collect ??big data??! How to combine quant and qual data to create a holistic picture of your users..  Statements like ??data is the new oil?? abound.  Companies have become obsessed with the ability to track what customers do and predict what they might do next. As we all know, quantitative data gives us the WHAT while qualitative data (e.g., ethnographic research) gives us the WHY.  We need both to develop a holistic understanding of our target users. EPIC 2013 offered several papers and a salon focused on Big Data demonstrating a great interest in the topic; however, it was clear that many attendees were unsure how to combine their rich (or ??thick,?? as Tricia Wang noted) ethnographic data with large scale quantitative data.  There are many ways to collect this type of data and in this workshop, we will offer one, which does not require a computer science degree: ESM via PACO. Experiential Sampling Methodology (ESM) is a type of longitudinal diary study that allows one to understand a person??s experience in the moment. Using a free, open-source mobile app called the ??PACO (Personal Analytics COmpanion),?? we can conduct large scale ESM studies with users anywhere in the world. These studies can be conducted after ethnographic studies to ascertain how broadly your observations apply to your user population or they can be done in advance to identify insights you want to study in-person, in-depth.  By combining these methodologies, you create a more holistic understanding of your users. In this three-hour workshop, we will introduce attendees to ESM, discuss ways we have used this methodology at Google, help attendees create their own ESM study, and discuss data analysis. The goal is for every attendee to leave the session equipped with the knowledge to design and create their own ESM study, analyze the data, and make actionable recommendations.  Attendance will be capped at 25 attendees to ensure 1:1 attention and good group discussions."
"Pirates of the search results page.  Search malware redirects nearly 100% of infected users' clicks on web search results to unintended websites. Most published research details how web-based malware works and technological interventions to stop it before users ever see it; however, the constant evolution of obfuscation techniques makes it difficult to prevent infection altogether. User interventions in the form of toolbars, dialogs, and user education have seen limited success. Previous research has focused on a prototypical type of malware; a sophisticated program that conceals itself (e.g., surreptitious download onto a host computer) or tries to fool the user by mimicking known, trusted websites (e.g., phishing attacks). The goal of our research is to understand users' experience, understanding of and response to search malware. The present research shows that even when confronted with blatantly unusual search behavior, people are unlikely to attribute blame to malware or to engage in behavior that may remedy the situation."
"UX management: current and future trends.  User Experience (UX) leaders and managers are required to continually adapt to changes in: organizational strategies and re-structuring, resources, technology, economic pressures, and other factors. Simultaneously, more companies are realizing that they need UX expertise to ensure that they are competitive in today's marketplace. This panel is comprised of UX leaders who have created strategies and tactics to succeed both in spite of and with the aid of the past and current trends. The panel will focus on the current trends, what strategies and tactics have and have not worked in addressing these trends, and also discuss which future trends they think will impact UX departments, companies, and the field, and how they are preparing for these future trends. The panel will be of interest to managers, practitioners and those who work closely with these teams, including developers, project managers, market researchers, test managers, and executives."
"Designing more Effective Workshops.  Ethnographic researchers are often more at home in the field than in organizational settings and designers in the open studio. We often see competing internal goals trump insights from effective research-based design proposals, presentations and reports. The Strategic Dialogue workshop prepares participants with tools for organizing collaborative stakeholder 
workshops that help you establish joint ownership of the meaning of research."
"Language Modeling in the Era of Abundant Data.  The talk presents an overview of statistical language modeling as applied to real-word problems: speech recognition, machine translation, spelling correction, soft keyboards to name a few prominent ones. We summarize the most successful estimation techniques, and examine how they fare for applications with abundant data, e.g. voice search. We conclude by highlighting a few open problems: getting an accurate estimate for the entropy of text produced by a very specific source, e.g. query stream); optimally leveraging data that is of different degrees of relevance to a given ""domain""; does a bound on the size of a ""good"" model for a given source exist?"
"The Optical Mouse: Early Biomimetic Embedded Vision.  The 1980 Xerox optical mouse invention, and subsequent product, was a successful deployment of embedded vision, as well as of the Mead??Conway VLSI design methodology that we developed at Xerox PARC in the late 1970s. The design incorporated an interpretation of visual lateral inhibition, essentially mimicking biology to achieve a wide dynamic range, or light-level-independent operation. Conceived in the context of a research group developing VLSI design methodologies, the optical mouse chip represented an approach to self-timed semi-digital design, with the analog image-sensing nodes connecting directly to otherwise digital logic using a switch-network methodology. Using only a few hundred gates and pass transistors in 5-micron nMOS technology, the optical mouse chip tracked the motion of light dots in its field of view, and reported motion with a pair of 2-bit Gray codes for x and y relative position??just like the mechanical mice of the time. Besides the chip, the only other electronic components in the mouse were the LED illuminators."
"Applying a Sunburst Visualization to Summarize User Navigation Sequences.  For many Web-based applications, it's important to be able to analyze the paths users have taken through a site--for example, to understand how they're discovering engaging content. These paths are difficult to summarize visually because of the underlying data's complexity. A Google researcher applied a sunburst visualization to this problem, after simplifying the data into a hierarchical format. The resulting visualization was successful in YouTube and is widely referenced and accessed. The code for the visualization is available as open source."
"Improving SSL Warnings: Comprehension and Adherence.  Browsers warn users when the privacy of an SSL/TLS connection might be at risk. An ideal SSL warning would empower users to make informed decisions and, failing that, guide confused users to safety. Unfortunately, users struggle to understand and often disregard real SSL warnings. We report on the task of designing a new SSL warning, with the goal of improving comprehension and adherence. We designed a new SSL warning based on recommendations from warning literature and tested our proposal with microsurveys and a field experiment. We ultimately failed at our goal of a well-understood warning. However, nearly 30% more total users chose to remain safe after seeing our warning. We attribute this success to opinionated design, which promotes safety with visual cues. Subsequently, our proposal was released as the new Google Chrome SSL warning. We raise questions about warning comprehension advice and recommend that other warning designers use opinionated design."
"Unidirectional Long Short-Term Memory Recurrent Neural Network with Recurrent Output Layer for Low-Latency Speech Synthesis.  Long short-term memory recurrent neural networks (LSTM-RNNs) have been applied to various speech applications including acoustic modeling for statistical parametric speech synthesis. One of the concerns for applying them to text-to-speech applications is its effect on latency. To address this concern, this paper proposes a low-latency, streaming speech synthesis architecture using unidirectional LSTM-RNNs with a recurrent output layer. The use of unidirectional RNN architecture allows frame-synchronous streaming inference of output acoustic features given input linguistic features. The recurrent output layer further encourages smooth transition between acoustic features at consecutive frames. Experimental results in subjective listening tests show that the proposed architecture can synthesize natural sounding speech without requiring utterance-level batch processing."
"Directly Modeling Speech Waveforms by Neural Networks for Statistical Parametric Speech Synthesis.  This paper proposes a novel approach for directly-modeling speech at the waveform level using a neural network. This approach uses the neural network-based statistical parametric speech synthesis framework with a specially designed output layer. As acoustic feature extraction is integrated to acoustic model training, it can overcome the limitations of conventional approaches, such as two-step (feature extraction and acoustic modeling) optimization, use of spectra rather than waveforms as targets, use of overlapping and shifting frames as unit, and fixed decision tree structure. Experimental results show that the proposed approach can directly maximize the likelihood defined at the waveform domain."
"RFC 7413 - TCP Fast Open.  This document describes an experimental TCP mechanism called TCP Fast Open (TFO).  TFO allows data to be carried in the SYN and SYN-ACK packets and consumed by the receiving end during the initial connection handshake, and saves up to one full round-trip time (RTT) compared to the standard TCP, which requires a three-way handshake (3WHS) to complete before data can be exchanged.  However, TFO deviates from the standard TCP semantics, since the data in the SYN could be replayed to an application in some rare circumstances.Applications should not use TFO unless they can tolerate this issue, as detailed in the Applicability section."
"Optimizing Touchscreen Keyboards for Gesture Typing.  Despite its growing popularity, gesture typing suffers from a major problem not present in touch typing: gesture ambiguity on the Qwerty keyboard. By applying rigorous mathematical optimization methods, this paper systematically investigates the optimization space related to the accuracy, speed, and Qwerty similarity of a gesture typing keyboard. Our investigation shows that optimizing the layout for gesture clarity (a metric measuring how unique word gestures are on a keyboard) drastically improves the accuracy of gesture typing. Moreover, if we also accommodate gesture speed, or both gesture speed and Qwerty similarity, we can still reduce error rates by 52% and 37% over Qwerty, respectively. In addition to investigating the optimization space, this work contributes a set of optimized layouts such as GK-D and GK-T that can immediately benefit mobile device users."
"Effects of Language Modeling and its Personalization on Touchscreen Typing Performance.  Modern smartphones correct typing errors and learn userspecific
words (such as proper names). Both techniques are useful, yet little has been published about their technical specifics and concrete benefits. One reason is that typing accuracy is difficult to measure empirically on a large scale. We describe a closed-loop, smart touch keyboard (STK) evaluation system that we have implemented to solve this problem. It includes a principled typing simulator for generating human-like noisy touch input, a simple-yet-effective decoder for reconstructing typed words from such spatial data, a large web-scale background language model (LM), and a method for incorporating LM
personalization. Using the Enron email corpus as a personalization test set, we show for the first time at this scale that a combined spatial/language model reduces word error rate from a pre-model baseline of 38.4% down to 5.7%, and that LM personalization can improve this further to 4.6%."
"Training Deep Neural Networks on Noisy Labels with Bootstrapping.  Current state-of-the-art deep learning systems for visual object recognition and
detection use purely supervised training with regularization such as dropout to
avoid overfitting. The performance depends critically on the amount of labeled
examples, and in current practice the labels are assumed to be unambiguous and
accurate. However, this assumption often does not hold; e.g. in recognition, class
labels may be missing; in detection, objects in the image may not be localized;
and in general, the labeling may be subjective. In this work we propose a generic
way to handle noisy and incomplete labeling by augmenting the prediction objective
with a notion of consistency. We consider a prediction consistent if the
same prediction is made given similar percepts, where the notion of similarity is
between deep network features computed from the input data. In experiments we
demonstrate that our approach yields substantial robustness to label noise on several
datasets. On MNIST handwritten digits, we show that our model is robust to
label corruption. On the Toronto Face Database, we show that our model handles
well the case of subjective labels in emotion recognition, achieving state-of-theart
results, and can also benefit from unlabeled face images with no modification
to our method. On the ILSVRC2014 detection challenge data, we show that our
approach extends to very deep networks, high resolution images and structured
outputs, and results in improved scalable detection."
"Show and tell: A neural image caption generator.  Automatically describing the content of an image is a
fundamental problem in artificial intelligence that connects
computer vision and natural language processing. In this
paper, we present a generative model based on a deep recurrent
architecture that combines recent advances in computer
vision and machine translation and that can be used
to generate natural sentences describing an image. The
model is trained to maximize the likelihood of the target description
sentence given the training image. Experiments
on several datasets show the accuracy of the model and the
fluency of the language it learns solely from image descriptions.
Our model is often quite accurate, which we verify
both qualitatively and quantitatively. For instance, while
the current state-of-the-art BLEU score (the higher the better)
on the Pascal dataset is 25, our approach yields 59, to be compared to
human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art."
"Clinching auctions beyond hard budget constraints.  Constraints on agent's ability to pay play a major role in auction design for any setting where the magnitude of financial transactions is sufficiently large. Those constraints have been traditionally modeled in mechanism design as hard budget, i.e., mechanism is not allowed to charge agents more than a certain amount. Yet, real auction systems (such as Google AdWords) allow more sophisticated constraints on agents' ability to pay, such as average budgets. In this work, we investigate the design of Pareto optimal and incentive compatible auctions for agents with constrained quasi-linear utilities, which captures more realistic models of liquidity constraints that the agents may have. Our result applies to a very general class of allocation constraints known as polymatroidal environments, encompassing many settings of interest such as multi-unit auctions, matching markets, video-on demand and advertisement systems. Our design is based Ausubel's clinching framework. Incentive compatibility and feasibility with respect to ability-to-pay constraints are direct consequences of the clinching framework. Pareto-optimality, on the other hand, is considerably more challenging, since the no-trade condition that characterizes it depends not only on whether agents have their budgets exhausted or not, but also on prices {at} which the goods are allocated. In order to get a handle on those prices, we introduce novel concepts of dropping prices and saturation. These concepts lead to our main structural result which is a characterization of the tight sets in the clinching auction outcome and its relation to dropping prices."
"How Many People Visit YouTube? Imputing Missing Events in Panels With Excess Zeros.  Media-metering panels track TV and online usage of people to analyze viewing behavior. However, panel data is often incomplete due to non-registered devices, non-compliant panelists, or work usage. We thus propose a probabilistic model to impute missing events in data with excess zeros using a negative-binomial hurdle model for the unobserved events and beta-binomial sub-sampling to account for missingness. We then use the presented models to estimate the number of people in Germany who visit YouTube."
"Directed Enzymatic Activation of 1-D DNA Tiles.  The tile assembly model is a Turing universal model of self-assembly where a set of square shaped tiles with programmable sticky sides undergo coordinated self-assembly to form arbitrary shapes, thereby computing arbitrary functions. Activatable tiles are a theoretical extension to the Tile assembly model that enhances its robustness by protecting the sticky sides of tiles until a tile is partially incorporated into a growing assembly. In this article, we experimentally demonstrate a simplified version of the Activatable tile assembly model. In particular, we demonstrate the simultaneous assembly of protected DNA tiles where a set of inert tiles are activated via a DNA polymerase to undergo linear assembly. We then demonstrate stepwise activated assembly where a set of inert tiles are activated sequentially one after another as a result of attachment to a growing 1-D assembly. We hope that these results will pave the way for more sophisticated demonstrations of activated assemblies."
"Automatic Gain Control and Multi-style Training for Robust Small-Footprint Keyword Spotting with Deep Neural Networks.  We explore techniques to improve the robustness of small-footprint keyword spotting models based on deep neural networks (DNNs) in the presence of background noise and in far-field conditions. We find that system performance can be improved significantly, with relative improvements up to 75% in far-field conditions, by employing a combination of multi-style training and a proposed novel formulation of automatic gain control (AGC) that estimates the levels of both speech and background noise. Further, we find that these techniques allow us to achieve competitive performance, even when applied to DNNs with an order of magnitude fewer parameters than our baseline."
"Speech Acoustic Modeling from Raw Multichannel Waveforms.  Standard deep neural network-based acoustic models for automatic speech recognition (ASR) rely on hand-engineered input features, typically log-mel filterbank magnitudes. In this paper, we describe a convolutional neural network - deep neural network (CNN-DNN) acoustic model which takes raw multichannel waveforms as input, i.e. without any preceding feature extraction, and learns a similar feature representation through supervised training. By operating directly in the time domain, the network is able to take advantage of the signal's fine time structure that is discarded when computing filterbank magnitude features. This structure is especially useful when analyzing multichannel inputs, where timing differences between input channels can be used to localize a signal in space. The first convolutional layer of the proposed model naturally learns a filterbank that is selective in both frequency and direction of arrival, i.e. a bank of bandpass beamformers with an auditory-like frequency scale. When trained on data corrupted with noise coming from different spatial locations, the network learns to filter them out by steering nulls in the directions corresponding to the noise sources. Experiments on a simulated multichannel dataset show that the proposed acoustic model outperforms a DNN that uses log-mel filterbank magnitude features under noisy and reverberant conditions."
"Internal Access Controls.  Trust, but verify."
"MemorySanitizer: fast detector of uninitialized memory use in C++.  This paper presents MemorySanitizer, a dynamic tool that detects uses of uninitialized memory in C and C++. The tool is based on compile time instrumentation and relies on bit-precise shadow memory at run-time. Shadow propagation technique is used to avoid false positive reports on copying of uninitialized memory. MemorySanitizer finds bugs at a modest cost of 2.5x in
execution time and 2x in memory usage; the tool has an optional origin tracking mode that provides better reports with moderate extra overhead. The reports with origins are more detailed compared to reports from other similar tools; such reports contain names of local variables and the entire history of the uninitialized memory including intermediate stores. In this paper we share our experience in deploying the tool at a large scale and demonstrate the benefits of compile time instrumentation over dynamic binary instrumentation."
"CQIC: Revisiting Cross-Layer Congestion Control f or Cellular Networks.  With the advent of high-speed cellular access and the overwhelming popularity of smartphones, a large percent of today??s Internet content is being delivered via cellular links. Due to the nature of long-range wireless signal propagation, the capacity of the last hop cellular link can vary by orders of magnitude within a short period of time (e.g., a few seconds). Unfortunately, TCP does not perform well in such fast-changing environments, potentially leading to poor spectrum utilization and high end-to-end packet delay. In this paper we revisit seminal work in cross-layer optimization the context of 4G cellular networks. Specifically, we leverage the rich physical layer information exchanged between base stations (NodeB) and mobile phones (UE) to predict the capacity of the underlying cellular link, and propose CQIC, a cross-layer congestion control design. Experiments on real cellular networks confirm that our capacity estimation method is both accurate and precise. A CQIC sender uses these capacity estimates to adjust its packet sending behavior. Our preliminary evaluation reveals that CQIC improves throughput over TCP by 1.08??2.89
?? for small and medium flows. For large flows, CQIC attains throughput comparable to TCP while reducing the average RTT by 2.38??2.65x."
"DNA Nanorobotics.  This chapter overviews the current state of the emerging discipline of DNA nanorobotics that make use of synthetic DNA to self-assemble operational molecular-scale devices. Recently there have been a series of quite astonishing experimental results??which have taken the technology from a state of intriguing possibilities into demonstrated capabilities of quickly increasing scale and complexity. We first state the challenges in molecular robotics and discuss why DNA as a nanoconstruction material is ideally suited to overcome these. We then review the design and demonstration of a wide range of molecular-scale devices; from DNA nanomachines that change conformation in response to their environment to DNA walkers that can be programmed to walk along predefined paths on nanostructures while carrying cargo or performing computations, to tweezers that can repeatedly switch states. We conclude by listing major challenges in the field along with some possible future directions."
"Tricorder: Building a Program Analysis Ecosystem.  Static analysis tools help developers find bugs, improve code readability, and ensure consistent style across a project. However, these tools can be difficult to smoothly integrate with each other and into the developer workflow, particularly when scaling to large codebases. We present Tricorder, a program analysis platform aimed at building a data-driven ecosystem around program analysis. We present a set of guiding principles for our program analysis tools and a scalable architecture for an analysis platform implementing these principles. We include an empirical, in-situ evaluation of the tool as it is used by developers across Google that shows the usefulness and impact of the platform."
"Meta-DNA: A DNA-Based Approach to Synthetic Biology.  The goal of synthetic biology is to design and assemble synthetic systems that mimic biological systems. One of the most fundamental challenges in synthetic biology is to synthesize artificial biochemical systems, which we will call meta-biochemical systems, that provide the same functionality as biological nucleic acids-enzyme systems, but that use a very limited number of types of molecules. The motivation for developing such synthetic biology systems is to enable a better understanding of the basic processes of natural biology, and also to enable re-engineering and programmability of synthetic versions of biological systems. One of the key aspects of modern nucleic acid biochemistry is its extensive use of protein enzymes that were originally evolved in cells to manipulate nucleic acids, and then later adapted by man for laboratory use. This practice provided powerful tools for manipulating nucleic acids, but also limited the extent of the programmability of the available chemistry for manipulating nucleic acids, since it is very difficult to predictively modify the behavior of protein enzymes. Meta-biochemical systems offer the possible advantage of being far easier to re-engineer and program for desired functionality. The approach taken here is to develop a biochemical system which we call meta-DNA (abbreviated as mDNA),Meta-DNA (mDNA) based entirely on strands of DNA as the only component molecules. Our work leverages prior work on the development of self-assembled DNA nanostructures. Each base of a mDNA Meta-DNA (mDNA)is a DNA nanostructure. Our mDNA bases are paired similar to DNA bases, but have a much larger alphabet of bases, thereby providing increased power of base addressability. Our mDNA bases can be assembled to form flexible linear assemblies (single stranded mDNA) analogous to single stranded DNA, and can be hybridized to form stiff helical structures (duplex mDNA) analogous to double Double strand meta-DNA (dsmDNA) stranded DNA, and also can be denatured back to single stranded mDNA. Our work also leverages the abstract activatable tile model developed by Majumder et al. and prior work on the development of enzyme-free isothermal protocols based on DNA hybridization and sophisticated strand displacement hybridization reactions. We describe various isothermal hybridization reactions that manipulate our mDNA in powerful ways analogous to DNA??DNA reactions and the action of various enzymes on DNA. These operations on mDNA include (i) hybridization of single strand mDNA (ssmDNA)Single strand meta-DNA (ssmDNA) into a double strand mDNA (dsmDNA)Double strand meta-DNA (dsmDNA) and heat denaturation of a dsmDNA Double strand meta-DNA (dsmDNA)into its component ssmDNA Single strand meta-DNA (ssmDNA)(analogous to DNA hybridization and denaturation), (ii) strand displacement of one ssmDNA Single strand meta-DNA (ssmDNA)by another (similar to strand displacement in DNA), (iii) restriction cuts on the backbones of ssmDNA Single strand meta-DNA (ssmDNA)and dsmDNA Double strand meta-DNA (dsmDNA)(similar to the action of restriction enzymes on DNA), (iv) polymerization chain reactions that extend ssmDNA Single strand meta-DNA (ssmDNA)on a template to form a complete dsmDNA Double strand meta-DNA (dsmDNA)(similar to the action of polymerase enzyme on DNA), (v) isothermal denaturation of a dsmDNA Double strand meta-DNA (dsmDNA)into its component ssmDNA Single strand meta-DNA (ssmDNA)(like the activity of helicase enzyme on DNA) and (vi) an isothermal replicator reaction which exponentially amplifies ssmDNA Single strand meta-DNA (ssmDNA)strands (similar to the isothermal PCR reaction). We provide a formal model to describe the required properties and operations of our mDNA, and show that our proposed DNA nanostructures and hybridization reactions provide these properties and functionality."
What It Would Really Take to Reverse Climate Change.  What two Googlers learned from a failed attempt to find the renewable energy source of tomorrow.
"Vocaine the Vocoder and Applications in Speech Synthesis.  Vocoders received renewed attention recently as basic components in speech synthesis applications such as voice transformation, voice conversion and statistical parametric speech synthesis. This paper presents a new vocoder synthesizer, referred to as Vocaine, that features a novel Amplitude Modulated-Frequency Modulated (AM-FM) speech model, a new way to synthesize non-stationary sinusoids using quadratic phase splines and a super fast cosine generator. Extensive evaluations are made against several state-ofthe-art methods in Copy-Synthesis and Text-To-Speech synthesis experiments. Vocaine matches or outperforms STRAIGHT in CopySynthesis experiments and outperforms our baseline real-time optimized Mixed-Excitation vocoder with the same computational cost.
We report that Vocaine considerably improves our statistical TTS synthesizers and that our new statistical parametric synthesizer [1] matched the quality of our mature production Unit-Selection system with uncompressed waveforms."
"R for Marketing Research and Analytics.  This book is a complete introduction to the power of R for marketing research practitioners. The text describes statistical models from a conceptual point of view with a minimal amount of mathematics, presuming only an introductory knowledge of statistics. Hands-on chapters accelerate the learning curve by asking readers to interact with R from the beginning. Core topics include the R language, basic statistics, linear modeling, and data visualization, which is presented throughout as an integral part of analysis. Later chapters cover more advanced topics yet are intended to be approachable for all analysts. These sections examine logistic regression, customer segmentation, hierarchical linear modeling, market basket analysis, structural equation modeling, and conjoint analysis in R. The text uniquely presents Bayesian models with a minimally complex approach, demonstrating and explaining Bayesian methods alongside traditional analyses for analysis of variance, linear models, and metric and choice-based conjoint analysis. With its emphasis on data visualization, model assessment, and development of statistical intuition, this book provides guidance for any analyst looking to develop or improve skills in R for marketing applications."
"Ad Injection at Scale: Assessing Deceptive Advertisement Modifications.  Today, web injection manifests in many forms, but fundamentally occurs when malicious and unwanted actors tamper directly with browser sessions for their own profit. In this work we illuminate the scope and negative impact of one of these forms, ad injection, in which users have ads imposed on them in addition to, or different from, those that websites originally sent them. We develop a multi-staged pipeline that identifies ad injection in the wild and captures its distribution and revenue chains. We find that ad injection has entrenched itself as a cross-browser monetization platform impacting more than 5% of unique daily IP addresses accessing Google??tens of millions of users around the globe. Injected ads arrive on a client??s machine through multiple vectors: our measurements identify 50,870 Chrome extensions and 34,407 Windows binaries, 38% and 17% of which are explicitly malicious. A small number of software developers support the vast majority of these injectors who in turn syndicate from the larger ad ecosystem. We have contacted the Chrome Web Store and the advertisers targeted by ad injectors to alert each of the deceptive practices involved."
"From Dorms to Cubicles: How Recent Graduates Communicate.  In a two-part field study, we studied the communication tool use of 29 college students and 20 recent college graduates. In comparing the two groups?? communication choices, we explored how transitioning from attending college to working full time impacts communication. We discuss how communication changes for recent college graduates in terms of both the content of their conversations, as well as the communication methods they use. We found that convenience plays a major role in the adoption and usage of communication tools, with participants preferring methods that were easily accessible at work, at home and in transit. We identify life changes recent graduates experience as they transition into emerging adulthood: the effect of being on a computer at work all day, changing social circles and scenes, being geographically distant from friends and family, and the desire for a professional persona. We discuss the impact of these changes on communication."
"Computational complexity of time-dependent density functional theory.  Time-dependent density functional theory (TDDFT) is rapidly emerging as a premier method for solving dynamical many-body problems in physics and chemistry. The mathematical foundations of TDDFT are established through the formal existence of a fictitious non-interacting system (known as the Kohn??Sham system), which can reproduce the one-electron reduced probability density of the actual system. We build upon these works and show that on the interior of the domain of existence, the Kohn??Sham system can be efficiently obtained given the time-dependent density. We introduce a V-representability parameter which diverges at the boundary of the existence domain and serves to quantify the numerical difficulty of constructing the Kohn??Sham potential. For bounded values of V-representability, we present a polynomial time quantum algorithm to generate the time-dependent Kohn??Sham potential with controllable error bounds."
"Gender Differences in High School Students?? Decisions to Study Computer Science and Related Fields.  Increasing women??s participation in Computer Science (CS) is a critical workforce and equity concern. The technology industry has committed to reversing negative trends for women in CS, engineering, and related fields. Building on previous research, we surveyed 1,739 high school students and recent college graduates to understand factors influencing decisions to pursue CS-related
college degrees. Results indicate social encouragement, career perception, academic exposure, and self perception are the leading factors for women, while the influence of these factors is different for men. These factors are actionable, and understanding differences in their influence on men and women will inform our approaches to achieving gender parity in tech."
"Fast quantum methods for optimization.  Discrete combinatorial optimization consists in finding the optimal configuration that minimizes a given discrete objective function. An interpretation of such a function as the energy of a classical system allows us to reduce the optimization problem into the preparation of a low-temperature thermal state of the system. Motivated by the quantum annealing method, we present three strategies to prepare the low-temperature state that exploit quantum mechanics in remarkable ways. We focus on implementations without uncontrolled errors induced by the environment. This allows us to rigorously prove a quantum advantage. The first strategy uses a classical-to-quantum mapping, where the equilibrium properties of a classical system in d spatial dimensions can be determined from the ground state properties of a quantum system also in d spatial dimensions. We show how such a ground state can be prepared by means of quantum annealing, including quantum adiabatic evolutions. This mapping also allows us to unveil some fundamental relations between simulated and quantum annealing. The second strategy builds upon the first one and introduces a technique called spectral gap amplification to reduce the time required to prepare the same quantum state adiabatically. If implemented on a quantum device that exploits quantum coherence, this strategy leads to a quadratic improvement in complexity over the well-known bound of the classical simulated annealing method. The third strategy is not purely adiabatic; instead, it exploits diabatic processes between the low-energy states of the corresponding quantum system. For some problems it results in an exponential speedup (in the oracle model) over the best classical algorithms."
"What??s Cookin??? Interpreting Cooking Videos using Text, Speech and Vision.  We present a novel method for aligning a sequence
of instructions to a video of someone
carrying out a task. In particular, we focus
on the cooking domain, where the instructions
correspond to the recipe. Our technique
relies on an HMM to align the recipe steps
to the (automatically generated) speech transcript.
We then refine this alignment using
a state-of-the-art visual food detector, based
on a deep convolutional neural network. We
show that our technique outperforms simpler
techniques based on keyword spotting. It also
enables interesting applications, such as automatically
illustrating recipes with keyframes,
and searching within a video for events of interest."
"Qualitatively Characterizing Neural Network Optimization Problems.  Training neural networks involves solving large-scale non-convex optimization
problems. This task has long been believed to be extremely difficult, with fear of
local minima and other obstacles motivating a variety of schemes to improve optimization,
such as unsupervised pretraining. However, modern neural networks are
able to achieve negligible training error on complex tasks, using only direct training
with stochastic gradient descent. We introduce a simple analysis technique to
look for evidence that such networks are overcoming local optima. We find that,
in fact, on a straight path from initialization to solution, a variety of state of the art
neural networks never encounter any significant obstacles."
"Explaining and Harnessing Adversarial Examples.  Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset."
"Helping You Protect You.  Guest editors M. Angela Sasse and Charles C. Palmer speak with security practitioners (L. Jean Camp, Sunny Consolvo, Markus Jakobsson, and Rick Wash) about what companies are doing to keep customers secure, and what users can do to stay safe."
"Modeling the Lifespan of Discourse Entities  with Application to Coreference Resolution.  A discourse typically involves numerous entities, but few are mentioned more than once. Distinguishing those that die out after just one mention (singleton) from those that lead longer lives (coreferent) would dramatically simplify the hypothesis space for coreference resolution models, leading to increased performance. To realize these gains, we build a classifier for predicting the singleton/coreferent distinction. The model??s feature representations synthesize linguistic insights about the factors affecting discourse entity lifespans (especially negation, modality, and attitude predication) with existing results about the benefits of ??surface?? (part-of-speech and n-gram-based) features for coreference resolution. The model is effective in its own right, and the feature representations help to identify the anchor phrases in bridging anaphora as well. Furthermore, incorporating the model into two very different state-of-the-art coreference resolution systems, one rule-based and the other learning-based, yields significant performance improvements."
"Associating Locations with Healthcare Events.  The disclosed subject matter relates to computer implemented methods for associating locations with healthcare events. In one aspect, a method includes receiving location data from a location-aware client device. The location data includes latitude and longitude information. The method further includes determining, based on the received location data, a routine travel pattern of a user associated with the location-aware client device. The method further includes detecting an anomaly in the routine travel pattern. The method further includes detecting a healthcare event. The healthcare event can be a visit to a healthcare facility and/or a healthcare transaction. The method further includes correlating the anomaly in the routine travel pattern of the user with the healthcare event. The method further includes associating one or more healthcare event locations to the healthcare event based on the correlation."
"Modified Alerts for Off Hour Calendar Events.  A modified alert triggering system modifies alerts associated with calendar events. The system identifies a user??s normal calendar hours. Further, the system determines that a calendar event is scheduled outside of user??s normal calendar hours. Based on determining that the calendar event is scheduled outside of user??s normal calendar hours, the system modifies an alert for the calendar event. For example, the modified alert can be louder than a normal alert, the modified alert can be a different alert tone than the normal alert tone, etc. The system then triggers the modified alert for the calendar event."
"Filtering Media Posts.  A social media filtering system filters social media posts based on criteria specified by a user. The system receives a request from a user to hide social media posts associated with a specified criteria. The specified criteria can be location criteria, event criteria, person criteria, and/or time criteria. Further, the system identifies a set of social media posts on a social media website. Then, the system determines social media posts from the set of social media posts that meet the specified criteria. The system further modifies the set of social media posts by removing or otherwise hiding the social media posts that meet the specified criteria. The system presents the modified set of social media posts to the user."
"Temporal/Spatial Calendar Events and Triggers.  Spatial and/or temporal triggers may be established so that when actuated, one or more notifications such as reminders may be provided to one or more users. These triggers may be established manually, e.g., by a user operating a user interface, automatically, e.g., by scraping calendar and/or email data to ascertain and/or predict various aspects of upcoming appointments such as start times, duration, date, location, and so forth, or a combination of the two. Spatial triggers may be actuated based on a determination that a user is, or will be, at a particular location. Temporal triggers may be actuated at particular points in time, e.g., at the scheduled time of an event or at some predetermined time interval before or after the event. Using one or more triggers, it is possible to provide notifications to a user at some predetermined time interval prior to a scheduled event, so that the user has sufficient time to make appropriate arrangements, such as buying tickets, making a reservation, scheduling a rendezvous with a friend, and so forth. A calendar system may also be interfaced with to manually or automatically establish triggers."
"Context Sensitive Paste.  A paste system can be used to generate hypertext based on determining context of a paste command. The paste system receives a selection of certain text in a document. The paste system then receives a paste command from a user of the paste system. The paste command can include data temporarily stored in memory, e.g., stored in a clipboard. The data can be stored as a result of a past copy command from the user. For example, the data can be a URL of a website. The paste system analyzes the data and determines if the data includes a URL. The system does this by, e.g., recognizing if the data includes ??http?? and/or ??www??. If the system recognizes the data as URL, then the system generates hypertext from a combination of the selection of the text and the data. The user can then click on the generated hypertext and the hypertext system will automatically direct the user to the website identified by the URL."
"Adding Third-Party Authentication to Open edX: A Case Study.  In this document, we describe the third-party authentication system we added to Open edX. With this system, Open edX administrators can allow their users to sign in with a large array of external authentication providers. We outline the features and advantages of the system, describe how it can be extended and customized, and highlight reusable design principles that can be applied to other authentication implementations in online education."
Understanding Sensitivity by Analyzing Anonymity.  The range of topics that users of online services consider sensitive is often broader than what service providers or regulators deem sensitive. A data-driven approach can help providers improve products with features that let users exercise privacy preferences more effectively.
"Yes???no answers versus check-all in self-administered modes. A systematic review and analyses.  When writing questions with dichotomous response options, those administering surveys on the web or on paper can choose from a variety of formats, including a check-all-that-apply or a forced-choice format (e.g. yes-no) in self-administered questionnaires. These two formats have been compared and evaluated in many experimental studies. In this paper, we conduct a systematic review and a few meta-analyses of different aspects of the available research that compares these two formats. We find that endorsement levels increase by a factor of 1.42 when questions are posed in a forced-choice rather than check-all format. However, when comparing across a battery of questions, the rank order of endorsement rates remains the same for both formats. While most authors hypothesise that respondents endorse more alternatives presented in a forced-choice (versus check-all-that-apply) format because they process that format at a deeper cognitive level, we introduce the acquiescence bias hypothesis as an alternative and complementary explanation. Further research is required to identify which format elicits answers closer to the ??true level?? of endorsement, since the few validation studies have proved inconclusive."
"Resolving Discourse-Deictic Pronouns: A Two-Stage Approach to Do It.  Discourse deixis is a linguistic phenomenon in which pronouns have verbal or clausal, rather than nominal, antecedents. Studies have estimated that between 5% and 10% of pronouns in non-conversational data are discourse deictic. However, current coreference resolution systems ignore this phenomenon. This paper presents an automatic system for the detection and resolution of  discourse-deictic pronouns. We introduce a two-step approach that first recognizes instances of discourse-deictic pronouns, and then resolves them to their verbal antecedent. Both components rely on linguistically motivated features. We evaluate the components in isolation and in combination with two state-of-the-art coreference resolvers. Results show that our system outperforms several baselines, including the only comparable discourse deixis system, and leads to small but statistically significant improvements over the full coreference resolution systems. An error analysis lays bare the need for a less strict evaluation of this task."
"Deep Learning for Acoustic Modeling in Parametric Speech Generation: A systematic review of existing techniques and future trends.  Hidden Markov models (HMMs) and Gaussian mixture models (GMMs) are the two most common types of acoustic models used in statistical parametric approaches for generating low-level speech waveforms from high-level symbolic inputs via intermediate acoustic feature sequences. However, these models have their limitations in representing complex, nonlinear relationships between the speech generation inputs and the acoustic features. Inspired by the intrinsically hierarchical process of human speech production and by the successful application of deep neural networks (DNNs) to automatic speech recognition (ASR), deep learning techniques have also been applied successfully to speech generation, as reported in recent literature."
"Using Actors to Implement Sequential Simulations.  This thesis investigates using an approach based on the Actors paradigm for implementing a discrete
event simulation system and comparing the results with more traditional approaches. The goal of this work
is to determine if using Actors for sequential programming is viable. If Actors are viable for this type of
programming, then it follows that they would be usable for general programming. One potential advantage
of using Actors instead of traditional paradigms for general programming would be the elimination of a
distinction between designing for a sequential environment and a concurrent/distributed one. Using Actors
for general programming may also allow for a single implementation that can be deployed on both single core
and multiple core systems.
Most of the existing discussions about the Actors model focus on its strengths in distributed environments
and its ability to scale with the amount of available computing resources. The chosen system for implementation
is intentionally sequential to allow for examination of the behaviour of existing Actors implementations
where managing concurrency complexity is not the primary task. Multiple implementations of the simulation
system were built using different languages (C++, Erlang, and Java) and different paradigms, including
traditional ones and Actors. These different implementations were compared quantitatively, based on their
execution time, memory usage, and code complexity.
The analysis of these comparisons indicates that for certain existing development environments, Erlang/OTP,
following the Actors paradigm, produces a comparable or better implementation than traditional
paradigms. Further research is suggested to solidify the validity of the results presented in this research and
to extend their applicability."
"Large-scale cluster management at Google with Borg.  This thesis investigates using an approach based on the Actors paradigm for implementing a discrete
event simulation system and comparing the results with more traditional approaches. The goal of this work
is to determine if using Actors for sequential programming is viable. If Actors are viable for this type of
programming, then it follows that they would be usable for general programming. One potential advantage
of using Actors instead of traditional paradigms for general programming would be the elimination of a
distinction between designing for a sequential environment and a concurrent/distributed one. Using Actors
for general programming may also allow for a single implementation that can be deployed on both single core
and multiple core systems.
Most of the existing discussions about the Actors model focus on its strengths in distributed environments
and its ability to scale with the amount of available computing resources. The chosen system for implementation
is intentionally sequential to allow for examination of the behaviour of existing Actors implementations
where managing concurrency complexity is not the primary task. Multiple implementations of the simulation
system were built using different languages (C++, Erlang, and Java) and different paradigms, including
traditional ones and Actors. These different implementations were compared quantitatively, based on their
execution time, memory usage, and code complexity.
The analysis of these comparisons indicates that for certain existing development environments, Erlang/OTP,
following the Actors paradigm, produces a comparable or better implementation than traditional
paradigms. Further research is suggested to solidify the validity of the results presented in this research and
to extend their applicability."
"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.  Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters."
"Learning semantic relationships for better action retrieval in images.  Human actions capture a wide variety of interactions
between people and objects. As a result, the set of possible
actions is extremely large and it is difficult to obtain
sufficient training examples for all actions. However, we
could compensate for this sparsity in supervision by leveraging
the rich semantic relationship between different actions.
A single action is often composed of other smaller
actions and is exclusive of certain others. We need a method
which can reason about such relationships and extrapolate
unobserved actions from known actions. Hence, we propose
a novel neural network framework which jointly extracts
the relationship between actions and uses them for
training better action retrieval models. Our model incorporates
linguistic, visual and logical consistency based cues
to effectively identify these relationships. We train and test
our model on a largescale image dataset of human actions.
We show a significant improvement in mean AP compared
to different baseline methods including the HEX-graph approach
from Deng et al. [8]"
"Flywheel: Google's Data Compression Proxy for the Mobile Web.  Mobile devices are increasingly the dominant Internet access technology. Nevertheless, high costs, data caps, and throttling are a source of widespread frustration, and a significant barrier to adoption in emerging markets. This paper presents Flywheel, an HTTP proxy service that extends the life of mobile data plans by compressing responses in-flight between origin servers and client browsers. Flywheel is integrated with the Chrome web browser and reduces the size of proxied web pages by 50% for a median user. We report measurement results from millions of users as well as experience gained during three years of operating and evolving the production service at Google."
"Security Vulnerability in Processor-Interconnect Router Design.  Servers that consist of multiple nodes and sockets are interconnected together with a high-bandwidth, low latency processor interconnect network, such as Intel QPI or AMD Hypertransport technologies. The different nodes exchange packets through routers which communicate with other routers. A key component of a router is the routing table which determines which output port an arriving packet should be forwarded through. However, because of the flexibility (or programmability) of the routing tables, we show that it can result in security vulnerability. We describe the procedures for how the routing tables in a processor-interconnect router can be modified. Based on these modifications, we propose new system attacks in a server, which include both performance attacks by degrading the latency and/or the bandwidth of the processor interconnect as well as a livelock attack that hangs the system. We implement these system on an 8-node AMD server and show how performance can be significantly degraded. Based on this vulnerability, we propose alternative solutions that provide various trade-off in terms of flexibility and cost while minimizing the routing table security vulnerability."
"Multilingual Open Relation Extraction Using Cross-lingual Projection.  Open domain relation extraction systems identify
relation and argument phrases in a sentence
without relying on any underlying
schema. However, current state-of-the-art relation
extraction systems are available only
for English because of their heavy reliance
on linguistic tools such as part-of-speech taggers
and dependency parsers. We present a
cross-lingual annotation projection method for
language independent relation extraction. We
evaluate our method on a manually annotated
test set and present results on three typologically
different languages. We release these
manual annotations and extracted relations in
ten languages from Wikipedia."
"How Many Millennials Visit YouTube? Estimating Unobserved Events From Incomplete Panel Data Conditioned on Demographic Covariates.  Many socio-economic studies rely on panel data as they also provide detailed demographic information about consumers. For example, advertisers use TV and web metering panels to estimate ads effectiveness in selected target demographics. However, panels often record only a fraction of all events due to non-registered devices, technical problems, or work usage. Goerg et al. (2015) present a beta-binomial negative-binomial hurdle (BBNBH) model to impute missing events in count data with excess zeros. In this work, we study empirical properties of the MLE for the BBNBH model, extend it to categorical covariates, introduce a penalized maximum likelihood estimator (MLE) to get accurate
estimates by demographic group, and apply the methodology to a German media panel to learn about demographic patterns in the YouTube viewership."
"Google+ Communities as Plazas and Topic Boards.  Researchers have recently been focusing on understanding online communities in social networks that offer easy access to new audiences. In this work, we conducted a mixed-method study of public Google+ Communities and found two major types evident in both how users talk about them and how they appear to use them: plazas to meet new people, and topic boards to discuss common interests. This reflects two common motivations users cite in describing Communities: ""meeting like minded people"" and ""finding great content"". We characterize these two types of Communities within Google+ using mixed methods including surveys, interviews, and quantitative analytics, and expose differences in user behaviors between them."
"Convolutional, Long Short-Term Memory, Fully Connected Deep Neural Networks.  Both Convolutional Neural Networks (CNNs) and Long Short-Term
Memory (LSTM) have shown improvements over Deep Neural Networks
(DNNs) across a wide variety of speech recognition tasks.
CNNs, LSTMs and DNNs are complementary in their modeling
capabilities, as CNNs are good at reducing frequency variations,
LSTMs are good at temporal modeling, and DNNs are appropriate
for mapping features to a more separable space. In this paper, we
take advantage of the complementarity of CNNs, LSTMs and DNNs
by combining them into one unified architecture. We explore the
proposed architecture, which we call CLDNN, on a variety of large
vocabulary tasks, varying from 200 to 2,000 hours. We find that
the CLDNN provides a 4-6% relative improvement in WER over an
LSTM, the strongest of the three individual models."
"Yedalog: Exploring Knowledge at Scale.  With huge progress on data processing frameworks, human programmers are frequently the bottleneck when analyzing large repositories of data. We introduce Yedalog, a declarative programming language that allows programmers to mix data-parallel pipelines and computation seamlessly in a single language. By contrast, most existing tools for data-parallel computation embed a sublanguage of data-parallel pipelines in a general-purpose language, or vice versa. Yedalog extends Datalog, incorporating not only computational features from logic programming, but also features for working with data structured as nested records. Yedalog programs can run both on a single machine, and distributed across a cluster in batch and interactive modes, allowing programmers to mix different modes of execution easily."
"Easy Does It: More Usable CAPTCHAs.  Websites present users with puzzles called CAPTCHAs to curb abuse caused by computer algorithms masquerading as people. While CAPTCHAs are generally effective at stopping abuse, they might impair website usability if they are not properly designed. In this paper we describe how we designed two new CAPTCHA schemes for Google that focus on maximizing usability. We began by running an evaluation on Amazon Mechanical Turk with over 27,000 respondents to test the us- ability of different feature combinations. Then we studied user preferences using Google??s consumer survey infrastructure. Finally, drawing on the insights gleaned during those studies, we tested our new captcha schemes first on Mechanical Turk and then on a fraction of production traffic. The resulting scheme is now an integral part of our production system and is served to millions of users. Our scheme achieved a 95.3% human accuracy, a 6.7% improvement."
"The End is Nigh: Generic Solving of Text-based CAPTCHAs.  Over the last decade, it has become well-established that a captcha??s ability to withstand automated solving lies in the difficulty of segmenting the image into individual characters. The standard approach to solving captchas automatically has been a sequential process wherein a segmentation algorithm splits the image into segments that contain individual characters, followed by a character recognition step that uses machine learning. While this approach has been effective against particular captcha schemes, its generality is limited by the segmentation step, which is hand-crafted to defeat the distortion at hand. No general algorithm is known for the character collapsing anti-segmentation technique used by most prominent real world captcha schemes. This paper introduces a novel approach to solving captchas in a single step that uses machine learning to attack the segmentation and the recognition problems simultaneously. Performing both operations jointly allows our algorithm to exploit information and context that is not available when they are done sequentially. At the same time, it removes the need for any hand-crafted component, making our approach generalize to new captcha schemes where the previous approach can not. We were able to solve all the real world captcha schemes we evaluated ac- curately enough to consider the scheme insecure in practice, including Yahoo (5.33%) and ReCaptcha (33.34%), without any adjustments to the algorithm or its parameters. Our success against the Baidu (38.68%) and CNN (51.09%) schemes that use occluding lines as well as character collapsing leads us to believe that our approach is able to defeat occluding lines in an equally general manner. The effectiveness and universality of our results suggests that combining segmentation and recognition is the next evolution of captcha solving, and that it supersedes the sequential approach used in earlier works. More generally, our approach raises questions about how to develop sufficiently secure captchas in the future."
"RealPigment: Paint Compositing by Example.  The color of composited pigments in digital painting is generally computed one of two ways: either alpha blending in RGB, or the Kubelka-Munk equation (KM). The former fails to reproduce paint like appearances, while the latter is difficult to use. We present a data-driven pigment model that reproduces arbitrary compositing behavior by interpolating sparse samples in a high dimensional space. The input is an of a color chart, which provides the composition samples. We propose two different prediction algorithms, one doing simple interpolation using radial basis functions (RBF), and another that trains a parametric model based on the KM equation to compute novel values. We show that RBF is able to reproduce arbitrary compositing behaviors, even non-paint-like such as additive blending, while KM compositing is more robust to acquisition noise and can generalize results over a broader range of values."
"Painting with Triangles.  Although vector graphics offer a number of benefits, conventional vector painting programs offer only limited support for the traditional painting metaphor. We propose a new algorithm that translates a user's mouse motion into a triangle mesh representation. This triangle mesh can then be composited onto a canvas containing an existing mesh representation of earlier strokes. This representation allows the algorithm to render solid colors and linear gradients. It also enables painting at any resolution. This paradigm allows artists to create complex, multi-scale drawings with gradients and sharp features while avoiding pixel sampling artifacts."
"IsoMatch: Creating Informative Grid Layouts.  Collections of objects such as images are often presented visually in a grid because it is a compact representation that lends itself well for search and exploration. Most grid layouts are sorted using very basic criteria, such as date or filename. In this work we present a method to arrange collections of objects respecting an arbitrary distance measure. Pairwise distances are preserved as much as possible, while still producing the specific target arrangement which may be a 2D grid, the surface of a sphere, a hierarchy, or any other shape. We show that our method can be used for infographics, collection exploration, summarization, data visualization, and even for solving problems such as where to seat family members at a wedding. We present a fast algorithm that can work on large collections and quantitatively evaluate how well distances are preserved."
"The interacting effects of distributed work arrangements and individual dispositions on willingness to engage in sensemaking behaviors.  Faced with highly competitive and dynamic environments, organizations are increasingly investing in technologies that provide them with new options for structuring work. At the same time, firms are increasingly dependent on employees' willingness and ability to make sense of novel tasks, problems, and rapidly changing situations. Yet, in spite of its importance, the impact of technology-enabled distributed work arrangements on sensemaking behavior is largely unknown. Sensemaking remains something that is perceived by many to be an idiosyncratic behavior that is, at best, loosely related to sociotechnical context and culture. Drawing on previous studies of cognitive dispositions (need for cognition, tendency for decisiveness, intolerance for ambiguity, and close-mindedness) and research on how technology-enabled distributed work arrangements affect interpersonal interaction, we theorize how workgroup geographic distribution interacts with individual cognitive differences to affect employees' willingness to engage in the core sociocognitive activities of sensemaking. Our results show that the consequences of individual tendencies can vary under different work arrangements, suggesting that managers seeking to facilitate sensemaking activities must make careful choices about the composition of distributed work groups, as well as how collaboration technologies can be used to encourage sensemaking behaviors."
"Handcrafted Fraud and Extortion: Manual Account Hijacking in the Wild.  Online accounts are inherently valuable resources---both for the data they contain and the reputation they accrue over time. Unsurprisingly, this value drives criminals to steal, or hijack, such accounts. In this paper we focus on manual account hijacking---account hijacking performed manually by humans instead of botnets. We describe the details of the hijacking workflow: the attack vectors, the exploitation phase, and post-hijacking remediation. Finally we share, as a large online company, which defense strategies we found effective to curb manual hijacking."
"GraphSC: Parallel Secure Computation Made Easy.  We propose introducing modern parallel programming
paradigms to secure computation, enabling their secure
execution on large datasets. To address this challenge, we
present GraphSC, a framework that (i) provides a programming
paradigm that allows non-cryptography experts to write secure
code; (ii) brings parallelism to such secure implementations; and
(iii) meets the needs for obliviousness, thereby not leaking any
private information. Using GraphSC, developers can efficiently
implement an oblivious version of graph-based algorithms (including
sophisticated data mining and machine learning algorithms)
that execute in parallel with minimal communication
overhead. Importantly, our secure version of graph-based algorithms
incurs a small logarithmic overhead in comparison
with the non-secure parallel version. We build GraphSC and
demonstrate, using several algorithms as examples, that secure
computation can be brought into the realm of practicality for big
data analysis. Our secure matrix factorization implementation
can process 1 million ratings in 13 hours, which is a multiple
order-of-magnitude improvement over the only other existing attempt, which requires 3 hours to process 16K ratings."
"A classifier for the latency-CPU behaviors of serving jobs in distributed environments.  End-to-end latency of serving jobs in distributed and shared environments, such as a Cloud, is an important metric for jobs' owners and infrastructure providers. Yet it is notoriously challenging to model precisely, since it is affected by a large collection of unrelated moving pieces, from the software design to the job schedulers strategies. In this work we present a novel approach to modeling latency, by tracking how it varies with CPU usage. We train a classifier to automatically assign the latency behavior of methods in three classes: constant latency regardless of CPU, uncorrelated latency and CPU, and predictable latency as a function of CPU. We use our model on a random sample of serving jobs running on the Google infrastructure. We illustrate unexpected and insightful patterns of latency variations with CPU. The visualization of latency-CPU variations and the corresponding class may be used by both jobs' owners and infrastructure providers, for a variety of applications, such as smarter latency alerting, latency-aware configuration of jobs, and automated detection of changes in behavior, either over time, during pre-release testing, or across data centers."
"Palette-based Photo Recoloring.  Color manipulation is a key process in photo enhancement, and professional image editing suites incorporate an array of tools to support it. Some of these tools are easy to understand but offer a limited range of expressiveness. Other more powerful tools are difficult and time consuming to use, and inscrutable to novices. Researchers have described a variety of more sophisticated methods but these are typically not interactive, which is crucial for creative exploration. This paper introduces a simple, intuitive and interactive tool that allows non-experts to recolor an image colors by editing a color palette. This system is comprised of several components: a GUI that is easy to learn and understand, a new efficient algorithm for creating a color palette from an image, and a new efficient color transfer algorithm that recolors the image based on a user-modified palette. We evaluate our approach via a user study, showing that it is faster and easier to use than two alternatives. It also shows that untrained users can quickly achieve results comparable to those of experts using professional software."
"Why Don't Software Developers Use Static Analysis Tools to Find Bugs?.  Using static analysis tools for automating code inspections can be beneficial for software engineers. Such tools can make finding bugs, or software defects, faster and cheaper than manual inspections. Despite the benefits of using static analysis tools to find bugs, research suggests that these tools are underused. In this paper, we investigate why developers are not widely using static analysis tools and how current tools could potentially be improved. We conducted interviews with 20 developers and found that although all of our participants felt that use is beneficial, false positives and the way in which the warnings are presented, among other things, are barriers to use. We discuss several implications of these results, such as the need for an interactive mechanism to help developers fix defects."
"Secrets, Lies, and Account Recovery: Lessons from the Use of Personal Knowledge Questions at Google.  We examine the first large real-world data set on personal knowledge question's security and memorability from their deployment at Google. Our analysis confirms that secret questions generally offer a security level that is far lower than user-chosen passwords. It turns out to be even lower than proxies such as the real distribution of surnames in the population would indicate. Surprisingly, we found that a significant cause of this insecurity is that users often don't answer truthfully. A user survey we conducted revealed that a significant fraction of users (37%) who admitted to providing fake answers did so in an attempt to make them ""harder to guess"" although on aggregate this behavior had the opposite effect as people ""harden"" their answers in a predictable way. On the usability side, we show that secret answers have surprisingly poor memorability despite the assumption that reliability motivates their continued deployment. From millions of account recovery attempts we observed a significant fraction of users (e.g 40\% of our English-speaking US users) were unable to recall their answers when needed. This is lower than the success rate of alternative recovery mechanisms such as SMS reset codes (over 80%). Comparing question strength and memorability reveals that the questions that are potentially the most secure (e.g what is your first phone number) are also the ones with the worst memorability.
We conclude that it appears next to impossible to find secret questions that are both secure and memorable. Secret questions continue have some use when combined with other signals, but they should not be used alone and best practice should favor more reliable alternatives."
"Cardinal Contests.  Contests are widely used as a means for effort elicitation
in settings ranging from government R&amp;D contests to online
crowdsourcing contests on platforms such as Kaggle, Innocentive, or TopCoder. Such rank-order mechanisms????
where agents' rewards depend only on the relative ranking
of their submissions' qualities????are natural mechanisms for
incentivizing effort when it is easier to obtain ordinal, rather than cardinal, information about agents' outputs, or where absolute measures of quality are unverifiable. An increasing number of online contests, however, rank entries according to some numerical evaluation of their absolute quality????for instance, the performance of an algorithm on a test dataset, or the performance of an intervention in a randomized trial. Can the contest designer incentivize higher effort by making the rewards in an ordinal rank-order mechanism contingent on such cardinal information? We model and analyze cardinal contests, where a principal
running a rank-order tournament has access to an absolute
measure of the qualities of agents' submissions in addition
to their relative rankings, and ask how modifying the
rank-order tournament to incorporate cardinal information
can improve incentives for effort. Our main result is that a simple threshold mechanism????a mechanism that awards the
prize for a rank if and only if the absolute quality of the
agent at that rank exceeds a certain threshold????is optimal
amongst all mixed cardinal-ordinal mechanisms where the
fraction of the j-th prize awarded to the j-th-ranked agent
is any arbitrary non-decreasing function of her submission's quality. Further, the optimal threshold mechanism uses exactly the same threshold for each rank. We study what contest parameters determine the extent of the benefit from incorporating such cardinal information into an ordinal rank-order contest, and investigate the extent of improvement in equilibrium effort via numerical simulations."
"When Does Improved Targeting Increase Revenue?.  In second-price auctions with symmetric bidders, we find that improved targeting via enhanced information disclosure decreases revenue when there are two bidders and increases revenue if there are at least four bidders. With asymmetries, improved targeting increases revenue if the most frequent winner wins less than 30.4% of the time, but can decrease revenue otherwise. We derive analogous results for position auctions. Finally, we show that revenue can vary non-monotonically with the number of bidders who are able to take advantage of improved targeting."
"Heracles: Improving Resource Efficiency at Scale.  User-facing, latency-sensitive services, such as websearch, underutilize their computing resources during daily periods of low traffic. Reusing those resources for other tasks is rarely done in production services since the contention for shared resources can cause latency spikes that violate the service-level objectives of latency-sensitive tasks. The resulting under-utilization hurts both the affordability and energy-efficiency of large-scale datacenters. With technology scaling slowing down, it becomes important to address this opportunity.
We present Heracles, a feedback-based controller that enables the safe colocation of best-effort tasks alongside a latency-critical service. Heracles dynamically manages multiple hardware and software isolation mechanisms, such as CPU, memory, and network isolation, to ensure that the latency-sensitive job meets latency targets while maximizing the resources given to best-effort tasks. We evaluate Heracles using production latency-critical and batch workloads from Google and demonstrate average server utilizations of 90% without latency violations across all the load and colocation scenarios that we evaluated."
"Beyond Short Snippets: Deep Networks for Video Classification.  Convolutional neural networks (CNNs) have been extensively applied for image recognition problems giving state-of-the-art results on recognition, detection, segmentation and retrieval. In this work we propose and evaluate several deep neural network architectures to combine image information across a video over longer time periods than previously attempted. We propose two methods capable of handling full length videos. The first method explores various convolutional temporal feature pooling architectures, examining the various design choices which need to be made when adapting a CNN for this task. The second proposed method explicitly models the video as an ordered sequence of frames. For this purpose we employ a recurrent neural network that uses Long Short-Term Memory (LSTM) cells which are connected to the output of the underlying CNN. Our best networks exhibit significant performance improvements over previously published results on the Sports 1 million dataset (73.1% vs. 60.9%) and the UCF-101 datasets with (88.6% vs. 88.0%) and without additional optical flow information (82.6% vs. 72.8%)."
"Ontological Supervision for Fine Grained Classification of Street View Storefronts.  Modern search engines receive large numbers of business
related, local aware queries. Such queries are best
answered using accurate, up-to-date, business listings, that
contain representations of business categories. Creating
such listings is a challenging task as businesses often
change hands or close down. For businesses with street
side locations one can leverage the abundance of street
level imagery, such as Google Street View, to automate the
process. However, while data is abundant, labeled data is
not; the limiting factor is creation of large scale labeled
training data. In this work, we utilize an ontology of geographical
concepts to automatically propagate business
category information and create a large, multi label, training
dataset for fine grained storefront classification. Our
learner, which is based on the GoogLeNet/Inception Deep
Convolutional Network architecture and classifies 208 categories,
achieves human level accuracy."
"Learning to Extract Local Events from the Web.  The goal of this work is extraction and retrieval of local events
from web pages. Examples of local events include small venue
concerts, theater performances, garage sales, movie screenings,
etc. We collect these events in the form of retrievable
calendar entries that include structured information about
event name, date, time and location. Between existing information extraction techniques and
the availability of information on social media and semantic
web technologies, there are numerous ways to collect commercial,
high-profile events. However, most extraction techniques
require domain-level supervision, which is not attainable at
web scale. Similarly, while the adoption of the semantic web
has grown, there will always be organizations without the
resources or the expertise to add machine-readable annotations
to their pages. Therefore, our approach bootstraps
these explicit annotations to massively scale up local event
extraction. We propose a novel event extraction model that uses distant
supervision to assign scores to individual event fields
(event name, date, time and location) and a structural algorithm
to optimally group these fields into event records. Our
model integrates information from both the entire source
document and its relevant sub-regions, and is highly scalable.
We evaluate our extraction model on all 700 million documents
in a large publicly available web corpus, ClueWeb12.
Using the 217,000 unique explicitly annotated events as
distant supervision, we are able to double recall with 85%
precision and quadruple it with 65% precision, with no additional
human supervision. We also show that our model can
be bootstrapped for a fully supervised approach, which can
further improve the precision by 30%. In addition, we evaluate the geographic coverage of the
extracted events. We find that there is a significant increase
in the geo-diversity of extracted events compared to existing
explicit annotations, while maintaining high precision
levels"
"The Maximal Two-Sided Ideals of Nest Algebras.  We give a necessary and sufficient criterion for an operator in a nest algebra to belong to a proper two-sided ideal of that algebra. Using this result, we describe the strong radical of a nest algebra, and give a general description of the maximal two-sided ideals. This also enables us to provide the final piece in the complete description of epimorphisms of one nest algebra onto another."
"Grammar as a Foreign Language.  We give a necessary and sufficient criterion for an operator in a nest algebra to belong to a proper two-sided ideal of that algebra. Using this result, we describe the strong radical of a nest algebra, and give a general description of the maximal two-sided ideals. This also enables us to provide the final piece in the complete description of epimorphisms of one nest algebra onto another."
"A Linear-Time Transition System for Crossing Interval Trees.  We define a restricted class of non-projective trees that 1) covers many natural language sentences; and 2) can be parsed exactly with
a generalization of the popular arc-eager system for projective trees (Nivre, 2003). Crucially, this generalization only adds constant
overhead in run-time and space keeping the parser??s total run-time linear in the worst case. In empirical experiments, our proposed transition-based parser is more accurate on average than both the arc-eager system or the swap-based system, an unconstrained non-projective transition system with a worst-case quadratic runtime (Nivre, 2009)."
"Designing Surveys for HCI Research.  Online surveys are widely used in human-computer interaction (HCI) to gather feedback and measure satisfaction; at a glance many tools are available and the cost of conducting surveys appears low. However, there is a wide gap between quick-and-dirty surveys, and surveys that are properly planned, constructed, and analyzed. This course examines survey research approaches that meet HCI goals, selecting the appropriate sampling method, questionnaire design best practices, identifying and avoiding common survey biases, and questionnaire evaluation. Attendees will gain an appreciation for the breadth and depth of surveys in HCI, combined with keys to conducting valid, reliable, and impactful survey research themselves."
"Metrics and Design Tool for Building and Evaluating Probability-Based Online Panels.  Probability-based online panels are beginning to replace traditional survey modes for existing established surveys in Europe and the United States. In light of this, current standards for panel response rate calculations are herein reviewed. To populate these panels cost-effectively, more diverse recruitment methods, such as, mail, telephone, and recruitment modules added to existing surveys are being used, either alone or in combinations. This results in panel member cohorts from different modes complicating panel response rate calculations. Also, as a panel ages with inevitable attrition, multiple cohorts result from panel refreshment and growth strategies. Formulas are presented to illustrate how to handle multiple cohorts for panel metrics. Additionally, drawing on relevant metrics used for a panel response rate, we further demonstrate a computational tool to assist planners in building a probability-based panel. This provides a means to estimate the recruitment effort required to build a panel of a predetermined size."
"Improving User Topic Interest Profiles by Behavior Factorization.  Many recommenders aim to provide relevant recommendations to users by building personal topic interest profiles and then using these profiles to find interesting contents for the user. In social media, recommender systems build user profiles by directly combining users' topic interest signals from a wide variety of consumption and publishing behaviors, such as social media posts they authored, commented on, +1'd or liked. Here we propose to separately model users' topical interests that come from these various behavioral signals in order to construct better user profiles. Intuitively, since publishing a post requires more effort, the topic interests coming from publishing signals should be more accurate of a user's central interest than, say, a simple gesture such as a +1. By separating a single user's interest profile into several behavioral profiles, we obtain better and cleaner topic interest signals, as well as enabling topic prediction for different types of behavior, such as topics that the user might +1 or comment on, but might never write a post on that topic. To do this at large scales in Google+, we employed matrix factorization techniques to model each user's behaviors as a separate example entry in the input user-by-topic matrix. Using this technique, which we call ""behavioral factorization"", we implemented and built a topic recommender predicting user's topical interests using their actions within Google+. We experimentally showed that we obtained better and cleaner signals than baseline methods, and are able to more accurately predict topic interests as well as achieve better coverage."
"Swapsies on the Internet: First Steps towards Reasoning about Risk and Trust in an Open World.  Contemporary open systems use components developed by many different parties, linked together dynamically in unforeseen constellations. Code needs to live up to strict security specifications: it has to ensure the correct functioning of its objects when they collaborate with external objects which may be malicious. In this paper we propose specifications that model risk and trust in such open systems. We specify Miller, Van Cutsem, and Tulloh??s escrow exchange example, and discuss the meaning of such a specification. We argue informally that the code satisfies its specification."
"The Performance Cost of Shadow Stacks and Stack Canaries.  Control flow defenses against ROP either use strict, expensive, but strong protection against redirected RET instructions with shadow stacks, or much faster but weaker protections without. In this work we study the inherent overheads of shadow stack schemes. We find that the overhead is roughly 10% for a traditional shadow stack. We then design a new scheme, the parallel shadow stack, and show that its performance cost is significantly less: 3.5%. Our measurements suggest it will not be easy to improve performance on current x86 processors further, due to inherent costs associated with RET and memory load/store instructions. We conclude with a discussion of the design decisions in our shadow stack instrumentation, and possible lighter-weight alternatives."
"Web Survey Methodology.  Web Survey Methodology guides the reader through the past fifteen years of research in web survey methodology. It both provides practical guidance on the latest techniques for collecting valid and reliable data and offers a comprehensive overview of research issues. Core topics from preparation to questionnaire design, recruitment testing to analysis and survey software are all covered in a systematic and insightful way. The reader will be exposed to key concepts and key findings in the literature, covering measurement, non-response, adjustments, paradata, and cost issues. The book also discusses the hottest research topics in survey research today, such as internet panels, virtual interviewing, mobile surveys and the integration with passive measurements, e-social sciences, mixed modes and business intelligence. The book is intended for students, practitioners, and researchers in fields such as survey and market research, psychological research, official statistics and customer satisfaction research. REVIEWS Comprehensive and thoughtful! Those two words beautifully describe this terrific book. Internet surveys will be at the centre of survey research for many decades to come, and this book is a must-read handbook for anyone serious about doing online surveys well or using data from such surveys.  No stone is left unturned - the authors address every essential topic and do so with a remarkable command of the big picture and the subtleties involved. Readers will walk away with a clear understanding of the many challenges inherent in conducting online studies and with an appropriate sense of optimism about the promise of the methodology and how best to implement it.
Jon Krosnick
Frederic O. Glover Professor in Humanities and Social Sciences, Stanford University This is an excellent, academic standard, book that every serious market researcher should own and consult. The authors have compiled an immense amount of useful and well-referenced information about every aspect of web surveys, creating an invaluable resource.
Ray Poynter
Managing Director, The Future Place"
"Diagnosing Automatic Whitelisting for Dynamic Remarketing Ads Using Hybrid ASP.  Hybrid ASP (H-ASP) is an extension of ASP that allows users to combine ASP type rules and numerical algorithms. Dynamic Remarketing Ads is Google??s platform for serving customized ads based on past interactions with a user. In this paper we will describe the use of H-ASP to diagnose failures of the automatic whitelisting system for Dynamic Remarketing Ads. We will show that the diagnosing task is an instance of a computational pattern that we call the Branching Computational Pattern (BCP). We will then describe a Python H-ASP library (H-ASP PL) that allows to perform computations using a BCP, and we will describe a H-ASP PL program that solves the diagnosing problem."
"Compressing Deep Neural Networks using a Rank-Constrained Topology.  We present a general approach to reduce the size of feed-forward deep neural networks (DNNs). We propose a rank-constrained topology, which factors the weights in the input layer of the DNN in terms of a low-rank representation: unlike previous work, our technique is applied at the level of the filters learned at individual hidden layer nodes, and exploits the natural two-dimensional time-frequency structure in the input. These techniques are applied on a small-footprint DNN-based keyword spotting task, where we find that we can reduce model size by 75% relative to the baseline, without any loss in performance. Furthermore, we find that the proposed approach is more effective at improving model performance compared to other popular dimensionality reduction techniques, when evaluated with a comparable number of parameters."
"Scalable, high-quality object detection.  Most high quality object detection approaches use the same scheme: salience-based object proposal methods followed by post-classification using deep convolutional features. In this work, we demonstrate that fully learnt, data driven proposal generation methods can effectively match the accuracy of their hand engineered counterparts, while allowing for very efficient runtime-quality trade-offs. This is achieved by making several key improvements to the MultiBox method [4], among which are an improved neural network architecture, use of contextual features and a new loss function that is robust to missing groundtruth labels. We show that our proposal generation method can closely match the performance of Selective Search [22] at a fraction of the cost. We report new single model state-ofthe-art on the ILSVRC 2014 detection challenge data set, with 0.431 mean average precision when combining both Selective Search and MultiBox proposals with our postclassification model. Finally, our approach allows the training
of single class detectors that can process 50 images per second on a Xeon workstation, using CPU only, rivaling the quality of the current best performing methods."
"Revenue Maximization for Selling Multiple Correlated Items.  We study the problem of selling $n$ items to a single buyer with an additive valuation function. We consider the valuation of the items to be correlated, i.e., desirabilities of the buyer for the items are not drawn independently. Ideally, the goal is to design a mechanism to maximize the revenue. However, it has been shown that a revenue optimal mechanism might be very complicated and as a result inapplicable to real-world auctions. Therefore, our focus is on designing a simple mechanism that achieves a constant fraction of the optimal revenue. 
Babaioff et al. (FOCS'14) propose a simple mechanism that achieves a constant fraction of the optimal revenue for independent setting with a single additive buyer. However, they leave the following problem as an open question: ""Is there a simple, approximately optimal mechanism for a single additive buyer whose value for $n$ items is sampled from a common base-value distribution?"" 
Babaioff et al. show a constant approximation factor of the optimal revenue can be achieved by either selling the items separately or as a whole bundle in the independent setting. We show a similar result for the correlated setting when the desirabilities of the buyer are drawn from a common base-value distribution. It is worth mentioning that the core decomposition lemma which is mainly the heart of the proofs for efficiency of the mechanisms does not hold for correlated settings. Therefore we propose a modified version of this lemma which is applicable to the correlated settings as well. Although we apply this technique to show the proposed mechanism can guarantee a constant fraction of the optimal revenue in a very weak correlation, this method alone can not directly show the efficiency of the mechanism in stronger correlations. Therefore, via a combinatorial approach we reduce the problem to an auction with a weak correlation to which the core decomposition technique is applicable. In addition, we introduce a generalized model of correlation for items and show the proposed mechanism  achieves an $O(\log k)$ approximation factor of the optimal revenue in that setting."
"Gender Differences in Factors Influencing Pursuit of Computer Science and Related Fields.  Increasing women??s participation in computer science is a critical workforce and equity concern. The technology industry has committed to reversing negative trends for women in computer science as well as engineering and information technology ??computing?? fields. Building on previously published research, this paper identifies factors that influence young women??s decisions to pursue computer science-related degrees and the ways in which these factors differ for young men. It is based on a survey of 1,739 high school students and recent college graduates. Results identified encouragement and exposure as the leading factors influencing this critical choice for women, while the influence of these factors is different for men. In particular, the influence of family is found to play a critical role in encouragement and exposure, and outreach efforts should focus on ways to engage parents."
"SAC070 - ICANN SSAC Advisory on the Use of Static TLD / Suffix Lists.  This advisory investigates the security and stability needs surrounding the growing use of public suffix lists on the Internet. For the purposes of this Advisory, a public suffix is defined as ??a domain under which multiple parties that are unaffiliated with the owner of the Public Suffix domain may register subdomains.??
Examples of Public Suffix domains include ""org"", ""co.uk"", ""k12.wa.us"" and ""uk.com"". There is no programmatic way to determine the boundary where a Domain Name System (DNS) label changes stewardship from a public suffix, yet tracking the boundary accurately is critically important for security, privacy, and usability issues in many modern systems and applications, such as web browsers. One method of determining this boundary is by use of public suffix lists (PSLs), which are static files listing the known public suffixes."
"RFC7535 - AS112 Redirection Using DNAME.  AS112 provides a mechanism for handling reverse lookups on IP addresses that are not unique (e.g., RFC 1918 addresses).  This document describes modifications to the deployment and use of AS112 infrastructure that will allow zones to be added and dropped much more easily, using DNAME resource records. This approach makes it possible for any DNS zone administrator to sink traffic relating to parts of the global DNS namespace under their control to the AS112 infrastructure without coordination with the operators of AS112 infrastructure."
"Memento Mori: Dynamic Allocation-site-based Optimizations.  Languages that lack static typing are ubiquitous in the world of mobile and web applications. The rapid rise of larger applications like interactive web GUIs, games, and cryptography presents a new range of implementation challenges for modern virtual machines to close the performance gap between typed and untyped languages. While all languages can benefit from efficient automatic memory management, languages like JavaScript present extra thrill with innocent-looking but difficult features like dynamically-sized arrays, deletable properties, and prototypes. Optimizing such languages requires complex dynamic techniques with more radical object layout strategies such as dynamically evolving representations for arrays. This paper presents a general approach for gathering temporal allocation site feedback that tackles both the general problem of object lifetime estimation and improves optimization of these problematic language features. We introduce a new implementation technique where allocation mementos processed by the garbage collector and runtime system efficiently tie objects back to allocation sites in the program and dynamically estimate object lifetime, representation, and size to inform three optimizations: pretenuring, pretransitioning, and presizing. Unlike previous work on pretenuring, our system utilizes allocation mementos to achieve fully dynamic allocation-site-based pretenuring in a production system. We implement all of our techniques in V8, a high performance virtual machine for JavaScript, and demonstrate solid performance improvements across a range of benchmarks."
"Trends and Lessons from Three Years Fighting Malicious Extensions.  In this work we expose wide-spread efforts by criminals to abuse the Chrome Web Store as a platform for distributing malicious extensions. A central component of our study is the design and implementation of WebEval, the first system that broadly identifies malicious extensions with a concrete, measurable detection rate of 96.5%. Over the last three years we detected 9,523 malicious extensions: nearly 10% of every extension submitted to the store. Despite a short window of operation---we removed 50% of malware within 25 minutes of creation---a handful of under 100 extensions escaped immediate detection and infected over 50 million Chrome users. Our results highlight that the extension abuse ecosystem is drastically different from malicious binaries: miscreants profit from web traffic and user tracking rather than email spam or banking theft."
"Kubernetes - Scheduling the Future at Cloud Scale.  Containers are taking over the world, but they aren??t full VMs and present special challenges to people build web-scale services. They need a lot of orchestration to run efficiently and resiliently. Their execution needs to be scheduled and managed. When they die (and they do), they need to be seamlessly replaced and re-balanced. An introductory mini-book designed to explain Kubernetes to IT managers, CIOs, and the otherwise cloud-curious."
"SpecTrans: Versatile Material Classification for Interaction with Textureless, Specular and Transparent Surfaces.  Surface and object recognition is of significant importance in ubiquitous and wearable computing. While various techniques exist to infer context from material properties and appearance, they are typically neither designed for real-time applications nor for optically complex surfaces that may be specular, textureless, and even transparent. These materials are, however, becoming increasingly relevant in HCI for transparent displays, interactive surfaces, and ubiquitous computing. We present SpecTrans, a new sensing technology for surface classification of exotic materials, such as glass, transparent plastic, and metal. The proposed technique extracts optical features by employing laser and multi-directional, multispectral LED illumination that leverages the material??s optical properties. The sensor hardware is small in size, and the proposed classification method requires significantly lower computational cost than conventional image-based methods, which use texture features or reflectance analysis, thereby
providing real-time performance for ubiquitous computing. Our evaluation of the sensing technique for nine different transparent materials, including air, shows a promising recognition rate of 99.0%. We demonstrate a variety of possible applications using SpecTrans?? capabilities."
"Sparse Non-negative Matrix Language Modeling For Skip-grams.  We present a novel family of language model (LM) estimation
techniques named Sparse Non-negative Matrix (SNM) estimation. A first set of experiments empirically evaluating these techniques on the One Billion Word Benchmark [3] shows that with skip-gram features SNMLMs are able to match the state-of-the art recurrent neural network (RNN) LMs; combining the two modeling techniques yields the best known result on the benchmark. The computational advantages of SNM over both maximum
entropy and RNNLM estimation are probably its main strength, promising an approach that has the same flexibility in combining arbitrary features effectively and yet should scale to very large amounts of data as gracefully as n-gram LMs do."
Pruning Sparse Non-negative Matrix N-gram Language Models.  In this paper we present a pruning algorithm and experimental results for our recently proposed Sparse Non-negative Matrix (SNM) family of language models (LMs). We have uncovered a bug in the experimental setup for SNM pruning; see Errata section for correct results. We also illustrate a method for converting an SNMLM to ARPA back-off format which can be readily used in a single-pass decoder for Automatic Speech Recognition.
"Probabilistic Analysis of Localized DNA Hybridization Circuits.  Molecular devices made of nucleic acids can perform complex information processing tasks at the nanoscale, with potential applications in biofabrication and smart therapeutics. However, limitations in the speed and scalability of such devices in a well-mixed setting can significantly affect their performance. In this paper, we propose designs for localized circuits involving DNA molecules that are arranged on addressable substrates and interact via hybridization reactions. We propose designs for localized elementary logic circuits, which we compose to produce more complex devices, including a circuit for computing the square root of a four bit number. We develop an efficient method for probabilistic model-checking of localized circuits, which we implement within the Visual DSD design tool. We use this method to prove the correctness of our circuits with respect to their functional specifications, and to analyze their performance over a broad range of local rate parameters. Specifically, we analyze the extent to which our localized designs can overcome the limitations of well-mixed circuits, with respect to speed and scalability. To provide an estimate of local rate parameters, we propose a biophysical model of localized hybridization. Finally, we use our analysis to identify constraints in the rate parameters that enable localized circuits to retain their advantages in the presence of unintended interferences between strands."
"How Developers Search for Code: A Case Study.  With the advent of large code repositories and sophisticated search capabilities, code search is increasingly becoming a key software development activity. In this work we shed some light into how developers search for code through a case study performed at Google, using a combination of survey and log-analysis methodologies. Our study provides insights into what developers are doing and trying to learn when performing a search, search scope, query properties, and what a search session under different contexts usually entails. Our results indicate that programmers search for code very frequently, conducting an average of five search sessions with 12 total queries each workday. The search queries are often targeted at a particular code location and programmers are typically looking for code with which they are somewhat familiar. Further, programmers are generally seeking answers to questions about how to use an API, what code does, why something is failing, or where code is located."
"Jupiter Rising: A Decade of Clos Topologies and Centralized Control in Google??s Datacenter Network.  We present our approach for overcoming the cost, operational complexity, and limited scale endemic to datacenter networks a decade ago. Three themes unify the five generations of datacenter networks detailed in this paper. First, multi-stage Clos topologies built from commodity switch silicon can support cost-effective deployment of building-scale networks. Second, much of the general, but complex, decentralized network routing and management protocols supporting arbitrary deployment scenarios were overkill for single-operator, pre-planned datacenter networks. We built a centralized control mechanism based on a global configuration pushed to all datacenter switches. Third, modular hardware design coupled with simple, robust software allowed our design to also support inter-cluster and wide-area networks. Our datacenter networks run at dozens of sites across the planet, scaling in capacity by 100x over ten years to more than 1Pbps of bisection bandwidth."
"BwE: Flexible, Hierarchical Bandwidth Allocation for WAN Distributed Computing.  WAN bandwidth remains a constrained resource that is economically infeasible to substantially overprovision. Hence, it is important to allocate capacity according to service priority and based on the incremental value of additional allocation. For example, it may be the highest priority for one service to receive 10Gb/s of bandwidth but upon reaching such an allocation, incremental priority may drop sharply favoring allocation to other services. Motivated by the observation that individual flows with fixed priority may not be the ideal basis for bandwidth allocation, we present the design and implementation of Bandwidth Enforcer (BwE), a global, hierarchical bandwidth allocation infrastructure. BwE supports: i) service-level bandwidth allocation following prioritized bandwidth functions where a service can represent an arbitrary collection of flows, ii) independent allocation and delegation policies according to user-defined hierarchy, all accounting for a global view of bandwidth and failure conditions, iii) multi-path forwarding common in traffic-engineered networks, and iv) a central administrative point to override (perhaps faulty) policy during exceptional conditions. BwE has delivered more service-efficient bandwidth utilization and simpler management in production for multiple years."
"Condor: Better Topologies through Declarative Design.  The design space for large, multipath datacenter networks is large and complex, and no one design fits all purposes. Network architects must trade off many criteria to design cost-effective, reliable, and maintainable networks, and typically cannot explore much of the design space. We present Condor, our approach to enabling a rapid, efficient design cycle. Condor allows architects to express their requirements as constraints via a Topology Description Language (TDL), rather than having to directly specify network structures. Condor then uses constraint-based synthesis to rapidly generate candidate topologies, which can be analyzed against multiple criteria. We show that TDL supports concise descriptions of topologies such as fat-trees, BCube, and DCell; that we can generate known and novel variants of fat-trees with simple changes to a TDL file; and that we can synthesize large topologies in tens of seconds. We also show that Condor supports the daunting task of designing multi-phase network expansions that can be carried out on live networks."
"TIMELY: RTT-based Congestion Control for the  Datacenter.  Datacenter transports aim to deliver low latency messaging together with high throughput. We show that simple packet delay, measured as round-trip times at hosts, is an effective congestion signal without the need for switch feedback. First, we show that advances in NIC hardware have made RTT measurement possible with microsecond accuracy, and that these RTTs are sufficient to estimate switch queueing. Then we describe how TIMELY can adjust transmission rates using RTT gradients to keep packet latency low while delivering high bandwidth. We implement our design in host software running over NICs with OS-bypass capabilities. We show using experiments with up to hundreds of machines on a Clos network topology that it provides excellent performance: turning on TIMELY for OS-bypass messaging over a fabric with PFC lowers 99 percentile tail latency by 9X while maintaining near line-rate throughput. Our system also outperforms DCTCP running in an optimized kernel, reducing tail latency by 13X. To the best of our knowledge, TIMELY is the first delay-based congestion control protocol for use in the datacenter, and it achieves its results despite having an order of magnitude fewer RTT signals (due to NIC offload) than earlier delay-based schemes such as Vegas."
"Best-Buddies Similarity for Robust Template Matching.  We propose a novel method for template matching in unconstrained environments. Its essence is the Best Buddies Similarity (BBS), a useful, robust, and parameter-free similarity measure between two sets of points. BBS is based on a count of Best Buddies Pairs (BBPs)??pairs of points in which each one is the nearest neighbor of the other. BBS has several key features that make it robust against complex geometric deformations and high levels of outliers, such as those arising from background clutter and occlusions. We study these properties, provide a statistical analysis that justifies them, and demonstrate the consistent success of BBS on a challenging real-world dataset."
"Visual Vibrometry: Estimating Material Properties from Small Motion in Video.  The estimation of material properties is important for scene understanding, with many applications in vision, robotics, and structural engineering. This paper connects fundamentals of vibration mechanics with computer vision techniques in order to infer material properties from small, often imperceptible motion in video. Objects tend to vibrate in a set of preferred modes. The shapes and frequencies of these modes depend on the structure and material properties of an object. Focusing on the case where geometry is known or fixed, we show how information about an object??s modes of vibration can be extracted from video and used to make inferences about that object??s material properties. We demonstrate our approach by estimating material properties for a variety of rods and fabrics by passively observing their motion in high-speed and regular frame-rate video."
"VIP: Finding Important People in Images.  People preserve memories of events such as birthdays, weddings,
or vacations by capturing photos, often depicting
groups of people. Invariably, some individuals in the image
are more important than others given the context of the
event. This paper analyzes the concept of the importance
of individuals in group photographs. We address two specific
questions ?? Given an image, who are the most important
individuals in it? Given multiple images of a person,
which image depicts the person in the most important role?
We introduce a measure of importance of people in images
and investigate the correlation between importance and visual
saliency. We find that not only can we automatically
predict the importance of people from purely visual cues,
incorporating this predicted importance results in signifi-
cant improvement in applications such as im2text (generating
sentences that describe images of groups of people)."
"Understanding Your Users: A Practical Guide to User Research Methods.  This new and completely updated edition is a comprehensive, easy-to-read, ""how-to"" guide on user research methods. You'll learn about many distinct user research methods and also pre- and post-method considerations such as recruiting, facilitating activities or moderating, negotiating with product developments teams/customers, and getting your results incorporated into the product. For each method, you'll understand how to prepare for and conduct the activity, as well as analyze and present the data - all in a practical and hands-on way. Each method presented provides different information about the users and their requirements (e.g., functional requirements, information architecture). The techniques can be used together to form a complete picture of the users' needs or they can be used separately throughout the product development lifecycle to address specific product questions. These techniques have helped product teams understand the value of user experience research by providing insight into how users behave and what they need to be successful. You will find brand new case studies from leaders in industry and academia that demonstrate each method in action. This book has something to offer whether you are new to user experience or a seasoned UX professional. After reading this book, you'll be able to choose the right user research method for your research question and conduct a user research study. Then, you will be able to apply your findings to your own products."
"Pedestrian Detection with a Large-Field-Of-View Deep Network.  Pedestrian detection is of crucial importance to autonomous driving applications. Methods based on deep learning have shown significant improvements in accuracy, which makes them particularly suitable for applications, such as pedestrian detection, where reducing miss rate is very important. Although they are accurate, their runtime has been at best in seconds per image, which makes them not practical for onboard applications. We present here a Large-Field-Of-View (LFOV) deep network for pedestrian detection, that can achieve high accuracy and is designed to make deep networks work faster for detection problems.
The idea of the proposed Large-Field-of-View deep network is to learn to make classification decisions simultaneously and accurately at multiple locations. The LFOV network processes larger image areas at much faster speeds than typical deep networks have been able to do, and can intrinsically reuse computations. Our pedestrian detection solution, which is a
combination of a LFOV network and a standard deep network, works at 280 ms per image on GPU and achieves 35.85 average miss rate on the Caltech Pedestrian Detection Benchmark."
"Real-Time Pedestrian Detection With Deep Network Cascades.  We present a new real-time approach to object detection that exploits the efficiency of cascade classifiers with the accuracy of deep neural networks. Deep networks have been shown to excel at classification tasks, and their ability to operate on raw pixel input without the need to design special features is very appealing.
However, deep nets are notoriously slow at inference time.
In this paper, we propose an approach that cascades deep nets and fast features, that is both extremely fast and extremely accurate. We apply it to the challenging task of pedestrian detection. Our algorithm runs in real-time at 15 frames per second. The resulting approach achieves a 26.2% average miss rate on the Caltech Pedestrian detection benchmark, which is competitive with the very best reported results. It is the first work we are aware of that achieves extremely high accuracy while running in real-time."
"Idest: Learning a Distributed Representation for Event Patterns.  This paper describes IDEST, a new method for
learning paraphrases of event patterns. It is
based on a new neural network architecture
that only relies on the weak supervision signal
that comes from the news published on the
same day and mention the same real-world entities.
It can generalize across extractions from
different dates to produce a robust paraphrase
model for event patterns that can also capture
meaningful representations for rare patterns.
We compare it with two state-of-the-art
systems and show that it can attain comparable
quality when trained on a small dataset.
Its generalization capabilities also allow it to
leverage much more data, leading to substantial
quality improvements."
"Sentence Compression by Deletion with LSTMs.  We present an LSTM approach to
deletion-based sentence compression
where the task is to translate a sentence
into a sequence of zeros and ones, corresponding
to token deletion decisions.
We demonstrate that even the most basic
version of the system, which is given no
syntactic information (no PoS or NE tags,
or dependencies) or desired compression
length, performs surprisingly well: around
30% of the compressions from a large test
set could be regenerated. We compare the
LSTM system with a competitive baseline
which is trained on the same amount of
data but is additionally provided with
all kinds of linguistic features. In an
experiment with human raters the LSTM-based
model outperforms the baseline
achieving 4.5 in readability and 3.8 in
informativeness."
"Conflict-Driven Conditional Termination.  Conflict-driven learning, which is essential to the performance of sat and smt solvers, consists of a procedure that searches for a model of a formula, and refutation procedure for proving that no model exists. This paper shows that conflict-driven learning can improve the precision of a termination analysis based on abstract interpretation. We encode non-termination as satisfiability in a monadic second-order logic and use abstract interpreters to reason about the satisfiability of this formula. Our search procedure combines decisions with reachability analysis to find potentially non-terminating executions and our refutation procedure uses a conditional termination analysis. Our implementation extends the set of conditional termination arguments discovered by an existing termination analyzer."
"Abstract Interpretation as Automated Deduction.  Algorithmic deduction and abstract interpretation are two widely used and successful approaches to implementing program verifiers. A major impediment to combining these approaches is that their mathematical foundations and implementation approaches are fundamentally different. This paper presents a new, logical perspective on abstract interpreters that perform reachability analysis using non-relational domains. We encode reachability of a location in a control-flow graph as satisfiability in a monadic, second-order logic parameterized by a first-order theory. We show that three components of an abstract interpreter, the lattice, transformers and iteration algorithm, represent a first-order, substructural theory, parametric deduction and abduction in that theory, and second-order constraint propagation."
"The Correctness-Security Gap in Compiler Optimization.  There is a significant body of work devoted to testing, verifying, and certifying the correctness of optimizing compilers. The focus of such work is to determine if source code and optimized code have the same functional semantics. In this paper, we introduce the correctness-security gap, which arises when a compiler optimization preserves the functionality of but violates a security guarantee made by source code. We show with concrete code examples that several standard optimizations, which have been formally proved correct, in-habit this correctness-security gap. We analyze this gap and conclude that it arises due to techniques that model the state of the program but not the state of the underlying machine. We propose a broad research programme whose goal is to identify, understand, and mitigate the impact of security errors introduced by compiler optimizations. Our proposal includes research in testing, program analysis, theorem proving, and the development of new, accurate machine models for reasoning about the impact of compiler optimizations on security."
"Cutting the Cord: a Robust Wireless Facilities Network for Data Centers.  Today??s network control and management traffic are limited by
their reliance on existing data networks. Fate sharing in this context
is highly undesirable, since control traffic has very different availability
and traffic delivery requirements. In this paper, we explore
the feasibility of building a dedicated wireless facilities network for
data centers. We propose Angora, a low-latency facilities network
using low-cost, 60GHz beamforming radios that provides robust
paths decoupled from the wired network, and flexibility to adapt to
workloads and network dynamics. We describe our solutions to address
challenges in link coordination, link interference and network
failures. Our testbed measurements and simulation results show
that Angora enables large number of low-latency control paths to
run concurrently, while providing low latency end-to-end message
delivery with high tolerance for radio and rack failures."
"Thwarting Fake OSN Accounts by Predicting their Victims.  Traditional defense mechanisms for fighting against automated fake accounts in online social networks are victim-agnostic. Even though victims of fake accounts play an important role in the viability of subsequent attacks, there is no work on utilizing this insight to improve the status quo. In this position paper, we take the first step and propose to incorporate predictions about victims of unknown fakes into the workflows of existing defense mechanisms. In particular, we investigated how such an integration could lead to more robust fake account defense mechanisms. We also used real world datasets from Facebook and Tuenti to evaluate the feasibility of predicting victims of fake accounts using supervised machine learning."
"The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing.  Unbounded, unordered, global-scale datasets are increasingly
common in day-to-day business (e.g. Web logs, mobile
usage statistics, and sensor networks). At the same time,
consumers of these datasets have evolved sophisticated requirements,
such as event-time ordering and windowing by
features of the data themselves, in addition to an insatiable
hunger for faster answers. Meanwhile, practicality dictates
that one can never fully optimize along all dimensions of correctness,
latency, and cost for these types of input. As a result,
data processing practitioners are left with the quandary
of how to reconcile the tensions between these seemingly
competing propositions, often resulting in disparate implementations
and systems. We propose that a fundamental shift of approach is necessary
to deal with these evolved requirements in modern
data processing. We as a field must stop trying to groom unbounded
datasets into finite pools of information that eventually
become complete, and instead live and breathe under
the assumption that we will never know if or when we have
seen all of our data, only that new data will arrive, old data
may be retracted, and the only way to make this problem
tractable is via principled abstractions that allow the practitioner
the choice of appropriate tradeoffs along the axes of
interest: correctness, latency, and cost. In this paper, we present one such approach, the Dataflow
Model, along with a detailed examination of the semantics
it enables, an overview of the core principles that guided its
design, and a validation of the model itself via the real-world
experiences that led to its development."
"8 Things to Consider when Designing Interactive TV Experiences.  TV viewing is a universal leisure and informational activity but technological advancements have changed the experience drastically in the past decade. Nonetheless, despite all of the technological developments in the TV space, there is still very little guidance for user experience professionals around how best to build and design TV products and interfaces. In this paper, we will present 8 things we have learned to keep in mind when designing interactive TV experiences."
"Inferring the Network Latency Requirements of Cloud Tenants.  Cloud IaaS and PaaS tenants rely on cloud providers to provide network infrastructures that make the appropriate tradeoff between cost and performance. This can include mechanisms to help customers understand the performance requirements of their applications. Previous research (e.g., Proteus and Cicada) has shown how to do this for network-bandwidth demands, but cloud tenants may also need to meet latency objectives, which in turn may depend on reliable limits on network latency, and its variance, within the cloud providers infrastructure. On the other hand, if network latency is sufficient for an application, further decreases in latency might add cost without any benefit. Therefore, both tenant and provider have an interest in knowing what network latency is good enough for a given application. This paper explores several options for a cloud provider to infer a tenants network-latency demands, with varying tradeoffs between requirements for tenant participation, accuracy of inference, and instrumentation overhead. In particular, we explore the feasibility of a hypervisor-only mechanism, which would work without any modifications to tenant code, even in IaaS clouds."
"Flexible Network Bandwidth and Latency Provisioning in the Datacenter.  Predictably sharing the network is critical to achieving high utilization in the datacenter. Past work has focussed on providing bandwidth to endpoints, but often we want to allocate resources among multi-node services. In this paper, we present Parley, which provides service-centric minimum bandwidth guarantees, which can be composed hierarchically. Parley also supports service-centric weighted sharing of bandwidth in excess of these guarantees. Further, we show how to configure these policies so services can get low latencies even at high network load. We evaluate Parley on a multi-tiered oversubscribed network connecting 90 machines, each with a 10Gb/s network interface, and demonstrate that Parley is able to meet its goals."
"Real-Time Grasp Detection Using Convolutional Neural Networks.  We present an accurate, real-time approach to robotic grasp detection based on convolutional neural networks. Our network performs single-stage regression to graspable
bounding boxes without using standard sliding window or region proposal techniques. The model outperforms state-of-
the-art approaches by 14 percentage points and runs at 13
frames per second on a GPU. Our network can simultaneously
perform classification so that in a single step it recognizes the object and finds a good grasp rectangle. A modification to this model predicts multiple grasps per object by using a locally constrained prediction mechanism. The locally constrained model performs significantly better, especially on objects that can be grasped in a variety of ways."
"Computing weak consistency in polynomial time.  The k-atomicity property can be used to describe the consistency of data operations in large distributed storage systems. The weak consistency guarantees offered by such systems are seen as a necessary compromise in view of Brewer's CAP principle. The k-atomicity property requires that every read operation obtains a value that is at most k updates (writes) old, and becomes a useful way to quantify weak consistency if k is treated as a variable that can be computed from a history of operations. Specifically, the value of k quantifies how far the history deviates from Lamport's atomicity property for read/write registers. We address the problem of computing k indirectly by solving the k-atomicity verification problem (k-AV): given a history of read/write operations and a positive integer k, decide whether the history is k-atomic. Gibbons and Korach showed that in general this problem is NP-complete when k = 1, and hence not solvable in polynomial time unless P = NP. In this paper we present two algorithms that solve the k-AV problem for any k &gt;= 2 in special cases. Similarly to known solutions for k = 1 and k = 2, both algorithms assume that all the values written to a given object are distinct. The first algorithm places an additional restriction on the structure of the input history and solves k-AV in O(n^2 + n (k log k) time. The second algorithm does not place any additional restrictions on the input but is efficient only when k is small and when concurrency among write operations is limited. Its time complexity is O(n^2) if both k and our particular measure of write concurrency are bounded by constants."
"A Computational Approach for Obstruction-Free Photography.  We present a unified computational approach for taking photos through reflecting or occluding elements such as windows and fences. Rather than capturing a single image, we instruct the user to take a short image sequence while slightly moving the camera. Differences that often exist in the relative position of the background and the obstructing elements from the camera allow us to separate them based on their motions, and to recover the desired background scene as if the visual obstructions were not there. We show results on controlled experiments and many real and practical scenarios, including shooting through reflections, fences, and raindrop-covered windows."
"Atoms, Bits, and Cells.  Biology is entering the world of data. Data brings great potential for understanding all of us and improving the health of each of us. But making sense of data also brings challenges. To realize the potential, we must build on advances in data science, and learn from the experiences of other fields."
"Dynamic iSCSI at Scale: Remote Paging at Google.  Google is experimenting with remotely mounting jobs?? binaries and data packages. Each package is served as an ext4 filesystem atop an iSCSI-mounted block device fed through dm-multipath and dm-verity. A typical job might run on O(1K) machines, employing a N-to-1 multipath configuration yielding O(10K) short-lived iSCSI sessions. Google thus sees O(100K) iSCSI sessions set up and torn down every minute, a rather atypical iSCSI deployment. We will explore the challenges presented by this deployment at scale."
"Focus on the Long-Term: It's better for Users and Business.  Over the past 10+ years, online companies large and small have adopted widespread A/B testing as a robust data-based method for evaluating potential product improvements. In online experimentation, it is straightforward to measure the short-term effect, i.e., the impact observed during the experiment. However, the short-term effect is not always predictive of the long-term effect, i.e., the final impact once the product has fully launched and users have changed their behavior in response. Thus, the challenge is how to determine the long-term user impact while still being able to make decisions in a timely manner. We tackle that challenge in this paper by first developing experiment methodology for quantifying long-term user learning. We then apply this methodology to ads shown on Google search, more specifically, to determine and quantify the drivers of ads blindness and sightedness, the phenomenon of users changing their inherent propensity to click on or interact with ads. We use these results to create a model that uses metrics measurable in the short-term to predict the long-term. We learn that user satisfaction is paramount: ads blindness and sightedness are driven by the quality of previously viewed or clicked ads, as measured by both ad relevance and landing page quality. Focusing on user satisfaction both ensures happier users but also makes business sense, as our results illustrate. We describe two major applications of our findings: a conceptual change to our search ads auction that further increased the importance of ads quality, and a 50%
reduction of the ad load on Google??s mobile search interface. The results presented in this paper are generalizable in two major ways. First, the methodology may be used to quantify user learning effects and to evaluate online experiments in contexts other than ads. Second, the ads blindness/sightedness results indicate that a focus on user satisfaction could help to reduce the ad load on the internet at large with long-term neutral, or even positive, business impact."
"Crowdsourcing and the Semantic Web: A Research Manifesto.  Our goal with this research manifesto is to define a roadmap to guide the evolution of the new research field that is emerging at the intersection between crowdsourcing and the Semantic Web. We analyze the confluence of these two disciplines by exploring their relationship. First, we focus on how the application of crowdsourcing techniques can enhance the machine-driven execution of Semantic Web tasks. Second, we look at the ways in which machine-processable semantics can benefit the design and management of crowdsourcing projects. As a result, we are able to describe a list of successful or promising scenarios for both perspectives, identify scientific and technological challenges, and compile a set of recommendations to realize these scenarios effectively. This research manifesto is an outcome of the Dagstuhl Seminar 14282: Crowdsourcing and the Semantic Web."
"Full-Chip Simulations, Keys to Success.  As designs continue to grow larger and ever more complex, full-chip simulations remain a critical component of design verification.  These simulations pose a unique set of challenges that require different approaches than those used at the block or sub-chip level.  This paper defines the key goals of full-chip simulations and outlines guiding principles to follow when developing a new environment.  Special attention is paid to architecting for speed, both speed of simulation as well as speed of debug.  Lessons learned over the years along with specific recommendations are presented."
"A Computationally Efficient Algorithm for Learning Topical Collocation Models.  Most existing topic models make the bagof-words assumption that words are generated independently, and so ignore potentially useful information about word order. Previous attempts to use collocations (short sequences of adjacent words) in topic models have either relied on a pipeline approach, restricted attention to bigrams, or resulted in models whose inference does not scale to large corpora. This paper studies how to simultaneously learn both collocations and their topic assignments. We present an efficient reformulation of the Adaptor Grammar-based topical collocation model (AG-colloc) (Johnson, 2010), and develop a point-wise sampling algorithm for posterior inference in this new formulation. We further improve the efficiency of the sampling algorithm by exploiting sparsity and parallelising inference. Experimental results derived in text classification, information retrieval and human evaluation tasks across a range of datasets show that this reformulation scales to hundreds of thousands of documents while maintaining the good performance of the AG-colloc model."
"Semantic Role Labeling with Neural Network Factors.  We present a new method for semantic role labeling in which arguments and semantic roles are jointly embedded in a shared vector space for a given predicate. These embeddings belong to a neural network, whose output represents the potential functions of a graphical model designed for the SRL task. We consider both local and structured learning methods and obtain strong results on standard PropBank and FrameNet corpora with a straightforward product-of-experts model. We further show how the model can learn jointly from PropBank and FrameNet annotations to obtain additional improvements on the smaller FrameNet dataset."
"Acoustic Modeling in Statistical Parametric Speech Synthesis - From HMM to LSTM-RNN.  Statistical parametric speech synthesis (SPSS) combines an acoustic model and a vocoder to render speech given a text. Typically decision tree-clustered context-dependent hidden Markov models (HMMs) are employed as the acoustic model, which represent a relationship between linguistic and acoustic features. Recently, artificial neural network-based acoustic models, such as deep neural networks, mixture density networks, and long short-term memory recurrent neural networks (LSTM-RNNs), showed significant improvements over the HMM-based approach. This paper reviews the progress of acoustic modeling in SPSS from the HMM to the LSTM-RNN."
"A Gaussian Mixture Model Layer Jointly Optimized with Discriminative Features within A Deep Neural Network Architecture.  This article proposes and evaluates a Gaussian Mixture Model
(GMM) represented as the last layer of a Deep Neural Network
(DNN) architecture and jointly optimized with all previous layers
using Asynchronous Stochastic Gradient Descent (ASGD). The resulting ??Deep GMM?? architecture was investigated with special attention
to the following issues: (1) The extent to which joint optimization
improves over separate optimization of the DNN-based
feature extraction layers and the GMM layer; (2) The extent to which
depth (measured in number of layers, for a matched total number
of parameters) helps a deep generative model based on the GMM
layer, compared to a vanilla DNN model; (3) Head-to-head performance
of Deep GMM architectures vs. equivalent DNN architectures
of comparable depth, using the same optimization criterion
(frame-level Cross Entropy (CE)) and optimization method (ASGD);
(4) Expanded possibilities for modeling offered by the Deep GMM
generative model. The proposed Deep GMMs were found to yield
Word Error Rates (WERs) competitive with state-of-the-art DNN
systems, at the cost of pre-training using standard DNNs to initialize
the Deep GMM feature extraction layers. An extension to Deep
Subspace GMMs is described, resulting in additional gains."
"Temporospatial SDN for Aerospace Communications.  This paper describes the development of new methods and software leveraging Software Defined Networking (SDN) technology that has become common in terrestrial networking.  We are using SDN to improve the state-of-the-art in design and operation of aerospace communication networks. SDN enables the implementation of services and applications that control, monitor, and reconfigure the network layer and switching functionality.  SDN provides a software abstraction layer that yields a logically centralized view of the network for control plane services and applications.  Recently, new requirements have led to proposals to extend this concept for Software-Defined Wireless Networks (SDWN), which decouple radio control functions, such as spectrum management, mobility management, and interference management, from the radio data-plane.  By combining these concepts with high-fidelity modeling of predicted mobility patterns and wireless communications models, we can enable SDN applications that optimally and autonomously handle aerospace network operations, including steerable beam control, RF interference mitigation, and network routing updates.  This approach is specifically applicable to new constellation designs for LEO relay networks that include hundreds or thousands of spacecraft, serving millions of users, and exceed the ability of legacy network management tools."
"Product Echo State Networks: Time-Series Computation with Multiplicative Neurons.  Echo state networks (ESN), a type of reservoir computing (RC) architecture, are efficient and accurate artificial neural systems for time series processing and learning. An ESN consists of a core of recurrent neural networks, called a reservoir, with a small number of tunable parameters to generate a high-dimensional representation of an input, and a readout layer which is easily trained using regression to produce a desired output from the reservoir states. Certain computational tasks involve real-time calculation of high-order time correlations, which requires nonlinear transformation either in the reservoir or the readout layer. Traditional ESN employs a reservoir with sigmoid or tanh function neurons. In contrast, some types of biological neurons obey response curves that can be described as a product unit rather than a sum and threshold. Inspired by this class of neurons, we introduce a RC architecture with a reservoir of product nodes for time series computation. We find that the product RC shows many properties of standard ESN such as short-term memory and nonlinear capacity. On standard benchmarks for chaotic prediction tasks, the product RC maintains the performance of a standard nonlinear ESN while being more amenable to mathematical analysis. Our study provides evidence that such networks are powerful in highly nonlinear tasks owing to high-order statistics generated by the recurrent product node reservoir"
"""Not some trumped up beef"": Assessing Credibility of Online Restaurant Reviews.  Online reviews, or electronic word of mouth (eWOM), are an essential source of information for people making decisions about products and services, however they are also susceptible to abuses such as spamming and defamation. Therefore when making decisions, readers must determine if reviews are credible. Yet relatively little research has investigated how people make credibility judgments of online reviews. This paper presents quantitative and qualitative results from a survey of 1,979 respondents, showing that attributes of the reviewer and review content influence credibility ratings. Especially important for judging credibility is the level of detail in the review, whether or not it is balanced in sentiment, and whether the reviewer demonstrates expertise. Our findings contribute to the understanding of how people judge eWOM credibility, and we suggest how eWOM platforms can be designed to coach reviewers to write better reviews and present reviews in a manner that facilitates credibility judgments."
"Bayesian Sampling using Stochastic Gradient Thermostats.  Dynamics-based sampling methods, such as Hybrid Monte Carlo (HMC) and Langevin dynamics (LD), are commonly used to sample target distributions. Recently, such approaches have been combined with stochastic gradient techniques to increase sampling efficiency when dealing with large datasets. An outstanding problem with this approach is that the stochastic gradient introduces an unknown amount of noise which can prevent proper sampling after discretization. To remedy this problem, we show that one can leverage a small number of additional variables to stabilize momentum fluctuations induced by the unknown noise. Our method is inspired by the idea of a thermostat in statistical physics and is justified by a general theory."
"Quantum Simulation of Helium Hydride Cation in a Solid-State Spin Register.  Ab initio computation of molecular properties is one of the most promising applications of quantum computing. While this problem is widely believed to be intractable for classical computers, efficient quantum algorithms exist which have the potential to vastly accelerate research throughput in fields ranging from material science to drug discovery. Using a solid-state quantum register realized in a nitrogen-vacancy (NV) defect in diamond, we compute the bond dissociation curve of the minimal basis helium hydride cation, HeH+. Moreover, we report an energy uncertainty (given our model basis) of the order of 1e??14 hartree, which is 10 orders of magnitude below the desired chemical precision. As NV centers in diamond provide a robust and straightforward platform for quantum information processing, our work provides an important step toward a fully scalable solid-state implementation of a quantum chemistry simulator."
"Construction of Non-Convex Polynomial Loss Functions for Training a Binary Classifier with Quantum Annealing.  Quantum annealing is a heuristic quantum algorithm which exploits quantum resources to minimize an objective function embedded as the energy levels of a programmable physical system. To take advantage of a potential quantum advantage, one needs to be able to map the problem of interest to the native hardware with reasonably low overhead. Because experimental considerations constrain our objective function to take the form of a low degree PUBO (polynomial unconstrained binary optimization), we employ non-convex loss functions which are polynomial functions of the margin. We show that these loss functions are robust to label noise and provide a clear advantage over convex methods. These loss functions may also be useful for classical approaches as they compile to regularized risk expressions which can be evaluated in constant time with respect to the number of training examples."
"Exponentially More Precise Quantum Simulation of Fermions in Second Quantization.  We introduce novel algorithms for the quantum simulation of fermionic systems which are dramatically more efficient than those based on the Lie??Trotter??Suzuki decomposition. We present the first application of a general technique for simulating Hamiltonian evolution using a truncated
Taylor series to obtain logarithmic scaling with the inverse of the desired precision. The key difficulty in applying algorithms for general sparse Hamiltonian simulation to fermionic simulation is that a query, corresponding to computation of an entry of the Hamiltonian, is costly to compute. This means that the gate complexity would be much higher than quantified by the query complexity. We solve this problem with a novel quantum algorithm for on-the-fly computation of integrals that is exponentially faster than classical sampling. While the approaches presented here are readily applicable to a wide class of fermionic models, we focus on quantum chemistry simulation in second quantization, perhaps the most studied application of Hamiltonian simulation. Our central result is an algorithm for simulating an N spin??orbital system that requires O(N^5 t) gates. This approach is exponentially faster in the inverse precision and at least cubically faster in N than all previous approaches to chemistry simulation in the literature."
"RFC7607 - Codification of AS 0 Processing.  This document updates RFC 4271 and proscribes the use of Autonomous System (AS) 0 in the Border Gateway Protocol (BGP) OPEN, AS_PATH, AS4_PATH, AGGREGATOR, and AS4_AGGREGATOR attributes in the BGP UPDATE message."
"Neither Snow Nor Rain Nor MITM ... An Empirical Analysis of Email Delivery Security.  The SMTP protocol is responsible for carrying some of users most intimate communication, but like other Internet protocols, authentication and confidentiality were added only as an afterthought. In this work, we present the first report on global adoption rates of SMTP security extensions, including: STARTTLS, SPF, DKIM, and DMARC. We present data from two perspectives: SMTP server configurations for the Alexa Top Million domains, and over a year of SMTP connections to and from Gmail. We find that the top mail providers (e.g., Gmail, Yahoo, and Outlook) all proactively encrypt and authenticate messages. However, these best practices have yet to reach widespread adoption in a long tail of over 700,000 SMTP servers, of which only 35% successfully configure encryption, and 1.1% specify a DMARC authentication policy. This security patchwork -- paired with SMTP policies that favor failing open to allow gradual deployment -- exposes users to attackers who downgrade TLS connections in favor of cleartext and who falsify MX records to reroute messages. We present evidence of such attacks in the wild, highlighting seven countries where more than 20% of inbound Gmail messages arrive in cleartext due to network attackers."
"??...no one can hack my mind??: Comparing Expert and Non-Expert Security Practices.  The state of advice given to people today on how to stay safe online has plenty of room for improvement. Too many things are asked of them, which may be unrealistic, time consuming, or not really worth the effort. To improve the security advice, our community must find out what practices people use and what recommendations, if messaged well, are likely to bring the highest benefit while being realistic to ask of people. In this paper, we present the results of a study which aims to identify which practices people do that they consider most important at protecting their security online. We compare self-reported security practices of non-experts to those of security experts (i.e., participants who reported having five or more years of experience working in computer security). We report on the results of two online surveys??one with 231 security experts and one with 294 MTurk participants??on what the  practices and attitudes of each group are. Our findings show a discrepancy between the security practices that experts and non-experts report taking. For instance, while experts most frequently report installing software updates, using two-factor authentication and using a password manager to stay safe online, non-experts report using antivirus software, visiting only known websites, and changing passwords frequently."
"Sparse Non-negative Matrix Language Modeling for Geo-annotated Query Session Data.  The paper investigates the impact on query language modeling when using skip-grams within query as well as across queries in a given search session, in conjunction with the geo-annotation available for the query stream data. As modeling tool we use the recently proposed sparse non-negative matrix estimation technique, since it offers the same expressive power as the well-established maximum entropy approach in combining arbitrary context features. Experiments on the google.com query stream show that using session-level and geo-location context we can expect reductions in perplexity of 34% relative over the Kneser Ney N-gram baseline; when evaluating on the `''local'' subset of the query stream, the relative reduction in PPL is 51%---more than a bit. Both sources of context information (geo-location, and previous queries in session) are about equally valuable in building a language model for the query stream."
"The Theory of Variational Hybrid Quantum-Classical Algorithms.  Many quantum algorithms have daunting resource requirements when compared to what is available today. To address this discrepancy, a quantum-classical hybrid optimization scheme known as ""the quantum variational eigensolver"" was developed with the philosophy that even minimal quantum resources could be made useful when used in conjunction with classical routines. In this work we extend the general theory of this algorithm and suggest algorithmic improvements for practical implementations. Specifically, we develop a variational adiabatic ansatz and explore unitary coupled cluster where we establish a connection from second order unitary coupled cluster to universal gate sets through relaxation of exponential splitting. We introduce the concept of quantum variational error suppression that allows some errors to be suppressed naturally in this algorithm on a pre-threshold quantum device. Additionally, we analyze truncation and correlated sampling in Hamiltonian averaging as ways to reduce the cost of this procedure. Finally, we show how the use of modern derivative free optimization techniques can offer dramatic computational savings of up to three orders of magnitude over previously used optimization techniques."
"Measuring User Rated Language Quality: Development and Validation of the User Interface Language Quality Survey (LQS).  Written text plays a special role in user interfaces. Key information in interaction elements and content are mostly conveyed through text. The global context, where software has to run in multiple geographical and cultural regions, requires software developers to translate their interfaces into many different languages. This translation process is prone to errors ?? therefore the question of how language quality can be measured is important. This article presents the development of a questionnaire to measure user interface language quality (LQS). After a first validation of the instrument with 843 participants, a final set of 10 items remained, which was tested again (N=690). The survey showed a high internal consistency (Cronbach's ??) of .82, acceptable discriminatory power coefficients (.34 ?? .47), as well as a moderate average homogeneity of .36. The LQS also showed moderate correlation to UMUX, an established usability metric (convergent validity), and it successfully distinguished high and low language quality (discriminative validity). The application to three different products (YouTube, Google Analytics, Google AdWords) revealed similar key statistics, providing evidence that this survey is product-independent. Meanwhile, the survey has been translated and applied to more than 60 languages."
"Theoretical Foundations for Learning Kernels in Supervised Kernel PCA.  This paper presents a novel learning scenario which combines dimensionality reduction, supervised learning as well as kernel selection. We carefully define the hypothesis class that addresses this setting and provide an analysis of its Rademacher complexity and thereby provide generalization guarantees. The proposed algorithm uses KPCA to reduce the dimensionality of the feature space, i.e. by projecting data onto top eigenvectors of covariance operator in a kernel reproducing space. Moreover, it simultaneously learns a linear combination of base kernel functions, which defines a reproducing space, as well as the parameters of a supervised learning algorithm in order to minimize a regularized empirical loss. The bound on Rademacher complexity of our hypothesis is shown to be logarithmic in the number of base kernels, which encourages practitioners to combine as
many base kernels as possible."
"Scalable Community Discovery from Multi-Faceted Graphs.  A multi-faceted graph defines several facets on a set of nodes. Each facet is a set of edges that represent the relationships between the nodes in a specific context. Mining multi-faceted graphs have several applications, including finding fraudster rings that launch advertising traffic fraud attacks, tracking IP addresses of botnets over time, analyzing interactions on social networks and co-authorship of scientific papers. We propose NeSim, a distributed efficient clustering algorithm that does soft clustering on individual facets. We also propose optimizations to further improve the scalability, the efficiency and the clusters quality. We employ general purpose graph-clustering algorithms in a novel way to discover communities across facets. Due to the qualities of NeSim, we employ it as a backbone in the distributed MuFace algorithm, which discovers multi-faceted communities. We evaluate the proposed algorithms on several real and synthetic datasets, where NeSim is shown to be superior to MCL, JP and AP, the well-established clustering algorithms. We also report the success stories of MuFace in finding advertisement click rings."
"RSSAC002 - RSSAC Advisory on Measurements of the Root Server System.  RSSAC has begun work to determine a list of parameters that define the
desired service trends for the root zone system. These parameters include the measured
latency in the distribution of the root zone, the frequency of the updates, and their size.
With knowledge of these parameters in hand, RSSAC can then seek to produce estimates
of acceptable root zone size dynamics to ensure the overall system works within a set of
parameters. The future work to define these parameters will involve RSSAC working
closely with the root server operators to gather best practice estimates for the size and
update frequency of the root zone.
It must be well understood that the measurements described in this document are a
response to the current awareness, experience, and understanding of the Root Zone
System. As time progresses more, less, or entirely different metrics may be required to
investigate new concerns or defined problem statements."
"RSSAC003 - RSSAC Report on Root Zone TTLs.  Root zone TTLs have not changed since 1999. In this report, the RSSAC Caucus studies
the extent to which the current root zone TTLs are still appropriate for today??s Internet
environment.
Selecting a TTL for a given resource record involves finding the right balance between a
few tradeoffs. Intuitively, shorter TTLs are beneficial for data that changes frequently,
whereas longer TTLs are beneficial for data that is relatively stable. Related to this,
longer TTLs provide robustness in the event of operational failures. All other things
being equal, and assuming software involved in queries and responses follow the DNS
protocol standards, shorter TTLs generally result in higher query rates, and longer TTLs
result in lower query rates."
"Attitudes Toward Vehicle-Based Sensing and Recording.  Vehicles increasingly include features that rely on hi-tech sensors and recording; however, little is known of public attitudes toward such recording. We use two studies, an online survey (n=349) and an interview-based study (n=15), to examine perceptions of vehicle-based sensing and recording. We focus on: 1) how vehicle-based recording and sensing may differ from perceptions of current recording; 2) factors that impact comfort with vehicle-based recording for hypothetical drivers versus bystanders; and 3) perceptions of potential privacy-preserving techniques. We find that vehicle-based recording challenges current mental models of recording awareness. Comfort tends to depend on perceived bene-
fits, which can vary by stakeholder type. Perceived privacy in spaces near cars can also impact comfort and reflect mental models of private spaces as well as the range of potentially sensitive activities people perform in and near cars. Privacy-preserving techniques may increase perceived comfort but
may require addressing trust and usability issues."
"Structural maxent models.  We present a new class of density estimation
models, Structural Maxent models, with feature
functions selected from a union of possibly
very complex sub-families and yet benefiting
from strong learning guarantees. The design
of our models is based on a new principle
supported by uniform convergence bounds and
taking into consideration the complexity of the
different sub-families composing the full set of
features. We prove new data-dependent learning
bounds for our models, expressed in terms of the
Rademacher complexities of these sub-families.
We also prove a duality theorem, which we use
to derive our Structural Maxent algorithm. We
give a full description of our algorithm, including
the details of its derivation, and report the results
of several experiments demonstrating that its performance
improves on that of existing L1-norm
regularized Maxent algorithms. We further similarly
define conditional Structural Maxent models
for multi-class classification problems. These
are conditional probability models also making
use of a union of possibly complex feature subfamilies.
We prove a duality theorem for these
models as well, which reveals their connection
with existing binary and multi-class deep boosting
algorithms."
"Learning with Deep Cascades.  We introduce a broad learning model formed by cascades of predictors, Deep Cascades, that is structured as general decision trees in which leaf predictors or node questions may be members of rich function families. We present new detailed data-dependent theoretical guarantees for learning with Deep Cascades with complex leaf predictors or node question in terms of the Rademacher complexities of the sub-families composing these sets of predictors and the fraction of sample points correctly classified at each leaf. These general guarantees can guide the design of a variety of different algorithms for deep cascade models and we give a detailed description of two such algorithms. Our second algorithm uses as node and leaf classifiers SVM predictors and we report the results of experiments comparing its performance with that of SVM combined with polynomial kernels."
"An optimal online algorithm for retrieving heavily perturbed statistical databases in the low-dimensional querying model.  We give the first O(1/sqrt{T})-error online algorithm for reconstructing noisy statistical databases, where T is the number of (online) sample queries received. The algorithm is optimal up to the poly(log(T)) factor in terms of the error and requires only O(log T) memory. It aims to learn a hidden database-vector w<em> in R^d in order to accurately answer a stream of queries regarding the hidden database, which arrive in an online fashion from some unknown distribution D. We assume the distribution D is defined on the neighborhood of a low-dimensional manifold. The presented algorithm runs in O(dD)-time per query, where d is the dimensionality of the query-space. Contrary to the classical setting, there is no separate training set that is used by the algorithm to learn the database ??- the stream on which the algorithm will be evaluated must also be used to learn the database-vector. The algorithm only has access to a binary oracle O that answers whether a particular linear function of the database-vector plus random noise is larger than a threshold, which is specified by the algorithm. We note that we allow for a significant O(D) amount of noise to be added while other works focused on the low noise o(sqrt{D}) setting. For a stream of T queries our algorithm achieves an average error O(1/sqrt{T}) by filtering out random noise, adapting threshold values given to the oracle based on its previous answers and, as a consequence, recovering with high precision a projection of a database-vector w</em> onto the manifold defining the query-space. Our algorithm may be also applied in the adversarial machine learning context to compromise machine learning engines by heavily exploiting the vulnerabilities of the systems that output only binary signal and in the presence of significant noise."
"Distributed Authorization With Distributed Grammars.  While groups are generally helpful for the definition of authorization policies, their use in distributed systems is not straightforward. This paper describes a design for authorization in distributed systems that treats groups as formal languages. The design supports forms of delegation and negative clauses in authorization policies. It also considers the wish for privacy and efficiency in group-membership checks, and the possibility that group definitions may not all be available and may contain cycles."
"1ML - core and modules united (F-ing first-class modules).  ML is two languages in one: there is the core, with types and expressions, and there are modules, with signatures, structures and functors. Modules form a separate, higher-order functional language on top of the core. There are both practical and technical reasons for this stratification; yet, it creates substantial duplication in syntax and semantics, and it reduces expressiveness. For example, selecting a module cannot be made a dynamic decision. Language extensions allowing modules to be packaged up as first-class values have been proposed and implemented in different variations. However, they remedy expressiveness only to some extent, are syntactically cumbersome, and do not alleviate redundancy. We propose a redesign of ML in which modules are truly first-class values, and core and module layer are unified into one language. In this ""1ML"", functions, functors, and even type constructors are one and the same construct; likewise, no distinction is made between structures, records, or tuples. Or viewed the other way round, everything is just (""a mode of use of"") modules. Yet, 1ML does not require dependent types, and its type structure is expressible in terms of plain System F??, in a minor variation of our F-ing modules approach. We introduce both an explicitly typed version of 1ML, and an extension with Damas/Milner-style implicit quantification. Type inference for this language is not complete, but, we argue, not substantially worse than for Standard ML. An alternative view is that 1ML is a user-friendly surface syntax for System F?? that allows combining term and type abstraction in a more compositional manner than the bare calculus."
"F-ing modules.  ML modules are a powerful language mechanism for decomposing programs into reusable components. Unfortunately, they also have a reputation for being ""complex"" and requiring fancy type theory that is mostly opaque to non-experts. While this reputation is certainly understandable, given the many non-standard methodologies that have been developed in the process of studying modules, we aim here to demonstrate that it is undeserved. To do so, we give a very simple elaboration semantics for a full-featured, higher-order ML-like module language. Our elaboration defines the meaning of module expressions by a straightforward, compositional translation into vanilla System F?? (the higher-order polymorphic ??-calculus), under plain F?? typing environments. We thereby show that ML modules are merely a particular mode of use of System F??. We start out with a module language that supports the usual second-class modules with Standard ML-style generative functors, and includes local module definitions. To demonstrate the versatility of our approach, we further extend the language with the ability to package modules as first-class values ?? a very simple extension, as it turns out ?? and a novel treatment of OCaml-style applicative functors. Unlike previous work combining both generative and applicative functors, we do not require two distinct forms of functor or sealing expressions. Instead, whether a functor is applicative or not depends only on the computational purity of its body ?? in fact, we argue that applicative/generative is rather incidental terminology for what is best understood as pure vs. impure functors. This approach results in a semantics that we feel is simpler and more natural, and moreover prohibits breaches of data abstraction that are possible under earlier semantics for applicative functors. We also revive (in refined form) the long-lost notion of structure sharing from SML'90. Although previous work on module type systems has disparaged structure sharing as type-theoretically questionable, we observe that (1) some variant of it is in fact necessary in order to provide a proper treatment of abstraction in the presence of applicative functors, and (2) it is straightforward to account for using ``phantom types''. Based on this, we can even justify the (previously poorly understood) ""where module"" operator for signatures and the related notion of manifest module specifications. Altogether, we describe a comprehensive, unified, and yet simple semantics of a full-blown module language that ?? with the main exception of cross-module recursion ?? covers almost all interesting features that can be found in either the literature or in practical implementations of ML modules. We prove the language sound and its type checking decidable."
"Mixin' up the ML module system.  ML modules provide hierarchical namespace management, as well as fine-grained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. Mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. We synthesize the complementary advantages of these two mechanisms in a novel module system design we call MixML. A MixML module is like an ML structure in which some of the components are specified but not defined. In other words, it unifies the ML structure and signature languages into one. MixML seamlessly integrates hierarchical composition, translucent ML-style data abstraction, and mixin-style recursive linking. Moreover, the design of MixML is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ML module system (and several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role. We provide a declarative type system for MixML, including two important extensions: higher-order modules, and modules as first-class values. We also present a sound and complete, three-pass type checking algorithm for this system. The operational semantics of MixML is defined by an elaboration translation into an internal core language called LTG ?? namely, a polymorphic lambda calculus with single-assignment references and recursive type generativity ?? which employs a linear type and kind system to track definedness of term and type imports."
"Non-Parametric Parametricity.  Type abstraction and intensional type analysis are features seemingly at odds??type abstraction is intended to guarantee parametricity and representation independence, while type analysis is inherently non-parametric. Recently, however, several researchers have proposed and implemented ??dynamic type generation?? as a way to reconcile these features. The idea is that, when one defines an abstract type, one should also be able to generate at run time a fresh type name, which may be used as a dynamic representative of the abstract type for purposes of type analysis. The question remains: in a language with non-parametric polymorphism, does dynamic type generation provide us with the same kinds of abstraction guarantees that we get from parametric polymorphism? Our goal is to provide a rigorous answer to this question. We define a step-indexed Kripke logical relation for a language with both non-parametric polymorphism (in the form of type-safe cast) and dynamic type generation. Our logical relation enables us to establish parametricity and representation independence results, even in a non-parametric setting, by attaching arbitrary relational interpretations to dynamically-generated type names. In addition, we explore how programs that are provably equivalent in a more traditional parametric logical relation may be ??wrapped?? systematically to produce terms that are related by our non-parametric relation, and vice versa. This leads us to develop a ??polarized?? variant of our logical relation, which enables us to distinguish formally between positive and negative notions of parametricity."
"Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks.  Recurrent Neural Networks can be trained to produce sequences of tokens given
some input, as exemplified by recent results in machine translation and image
captioning. The current approach to training them consists of maximizing the
likelihood of each token in the sequence given the current (recurrent) state
and the previous token. At inference, the unknown previous token is then
replaced by a token generated by the model itself. This discrepancy between
training and inference can yield errors that can accumulate quickly along the
generated sequence.
We propose a curriculum learning strategy to gently change the
training process from a fully guided scheme using the true previous token,
towards a less guided scheme which mostly uses the generated token instead.
Experiments on several sequence prediction tasks show that this approach
yields significant improvements. Moreover, it was used successfully
in our winning entry to the MSCOCO image captioning challenge, 2015."
"In-Memory Performance for Big Data.  When a working set fits into memory, the overhead imposed by the buffer pool renders traditional databases non-competitive with in-memory designs that sacrifice the benefits of a buffer pool. However, despite the large memory available with modern hardware, data skew, shifting workloads, and complex mixed workloads make it difficult to guarantee that a working set will fit in memory. Hence, some recent work has focused on enabling in-memory databases to protect performance when the working data set almost fits in memory. Contrary to those prior efforts, we enable buffer pool designs to match in-memory performance while supporting the ""big data"" workloads that continue to require secondary storage, thus providing the best of both worlds. We introduce here a novel buffer pool design that adapts pointer swizzling for references between system objects (as opposed to application objects), and uses it to practically eliminate buffer pool overheads for memoryresident data. Our implementation and experimental evaluation demonstrate that we achieve graceful performance degradation when the working set grows to exceed the buffer pool size, and graceful improvement when the working set shrinks towards and below the memory and buffer pool sizes."
"Micro-Auction-Based Traffic-Light Control: Responsive, Local Decision Making.  Real-time, responsive optimization of traffic flow
serves to address important practical problems: reducing drivers?? wasted time and improving city-wide efficiency, as well as reducing gas emissions and improving air quality. Much of the current research in traffic-light optimization relies on extending the capabilities of basic traffic lights to either communicate with each other or communicate with vehicles. However, before such capabilities become ubiquitous, opportunities exist to improve traffic lights by being more responsive to current traffic situations within the existing, deployed, infrastructure. In this paper, we use micro-auctions as the organizing principle with which to incorporate local induction loop information; no other outside sources of information are assumed. At every time step in which a phase change is permitted, each light conducts a decentralized, weighted, micro-auction to determine which phase to instantiate next. We test the lights on real-world data collected over a period of several weeks around the Mountain View, California area. In our simulations, the auction mechanisms based only on local sensor data surpass longer-term planning approaches that rely on widely placed sensors and communications."
"Approximating the Effects of Installed Traffic Lights: A Behaviorist Approach Based on Travel Tracks.  Decades of research have been directed towards improving the timing of existing traffic lights. In many parts of the world where this research has been conducted, detailed maps of the streets and the precise locations of the traffic lights are publicly available. Continued timing research has recently been further spurred by the increasing ubiquity of personal cell-phone based GPS systems. Through their use, an enormous amount of travel tracks have been amassed ?? thus providing an easy source of real traffic data. Nonetheless, one fundamental piece of information remains absent that limits the quantification of the benefits of new approaches: the existing traffic light schedules and traffic light response behaviors. Unfortunately, deployed traffic light schedules are often not known. Rarely are they kept in a central database, and even when they are, they are often not easily obtainable. The alternative, manual inspection of a system of multiple traffic lights may be prohibitively expensive and time-consuming for many experimenters. Without the existing light schedules, it is difficult to ascertain the real-improvements that new traffic light algorithms and approaches will have ?? especially on traffic patterns that have not yet been encountered in the collected data. To alleviate this problem, we present an approach to estimating existing traffic light schedules based on collected GPS-travel tracks. We present numerous ways to test the results and comprehensively demonstrate them on both synthetic and real data. One of the many uses, beyond studying the effects of existing lights in previously unencountered traffic flow environments, is to serve as a realistic baseline for light timing and schedule optimization studies."
"Robust Estimation of Reverberation Time Using Polynomial Roots.  This paper further investigates previous findings that coefficients of acoustic responses can be modelled as random polynomials with certain constraints applied. In the case of room impulse responses, the median value of their clustered roots has been shown to be directly related to the reverberation time of the room.
In this paper we examine the frequency dependency of reverberation time and we also demonstrate the method??s robustness to truncation of impulse responses."
"ViSQOL: an objective speech quality model.  This paper presents an objective speech quality model, ViSQOL, the Virtual Speech Quality Objective Listener. It is a
signal-based, full-reference, intrusive metric that models human speech quality perception using a spectro-temporal
measure of similarity between a reference and a test speech signal. The metric has been particularly designed to be
robust for quality issues associated with Voice over IP (VoIP) transmission. This paper describes the algorithm and
compares the quality predictions with the ITU-T standard metrics PESQ and POLQA for common problems in VoIP:
clock drift, associated time warping, and playout delays. The results indicate that ViSQOL and POLQA significantly outperform PESQ, with ViSQOL competing well with POLQA. An extensive benchmarking against PESQ, POLQA, and simpler distance metrics using three speech corpora (NOIZEUS and E4 and the ITU-T P.Sup. 23 database) is also presented. These experiments benchmark the performance for a wide range of quality impairments, including VoIP
degradations, a variety of background noise types, speech enhancement methods, and SNR levels. The results and
subsequent analysis show that both ViSQOL and POLQA have some performance weaknesses and under-predict
perceived quality in certain VoIP conditions. Both have a wider application and robustness to conditions than PESQ or
more trivial distance metrics. ViSQOL is shown to offer a useful alternative to POLQA in predicting speech quality in
VoIP scenarios."
"ViSQOLAudio: An objective audio quality metric for low bitrate codecs.  Streaming services seek to optimise their use of bandwidth across audio and visual channels to maximise the quality of experience for users. This letter evaluates whether objective quality metrics can predict the audio quality for music encoded at low bitrates by comparing objective predictions with results from listener tests. Three objective metrics were benchmarked: PEAQ, POLQA, and VISQOLAudio. The results demonstrate objective metrics designed for speech quality assessment have a strong potential for quality assessment of low bitrate audio codecs."
"Fast Orthogonal Projection Based on Kronecker Product.  We propose a family of structured matrices to speed up orthogonal projections for high-dimensional data commonly seen in computer vision applications. In this, a structured matrix is formed by the Kronecker product of a series of smaller orthogonal matrices. This achieves O(dlogd) computational complexity and O(logd) space complexity for d-dimensional data, a drastic improvement over the standard unstructured projections whose computational and space complexities are both O(d^2). We also introduce an efficient learning procedure for optimizing such matrices in a data dependent fashion. We demonstrate the significant advantages of the proposed approach in solving the approximate nearest neighbor (ANN) image search problem with both binary embedding and quantization. Comprehensive experiments show that the proposed approach can achieve similar or better accuracy as the existing state-of-the-art but with significantly less time and memory."
"An Exploration of Parameter Redundancy in Deep Networks with Circulant Projections.  We explore the redundancy of parameters in deep neural networks by replacing the conventional linear projection in fully-connected layers with the circulant projection. The circulant structure substantially reduces memory footprint and enables the use of the Fast Fourier Transform to speed up the computation. Considering a fully-connected neural network layer with d input nodes, and d output nodes, this method improves the time complexity from O(d^2) to O(dlogd) and space complexity from O(d^2) to O(d). The space savings are particularly important for modern deep convolutional neural network architectures, where fully-connected layers typically contain more than 90% of the network parameters. We further show that the gradient computation and optimization of the circulant projections can be performed very efficiently. Our experiments on three standard datasets show that the proposed approach achieves this significant gain in storage and efficiency with minimal increase in error rate compared to neural networks with unstructured projections."
"Perceived Frequency of Advertising Practices.  In this paper, we introduce a new construct for measuring individuals?? privacy-related beliefs and understandings, namely their perception of the frequency with which information about individuals is gathered and used by others for advertising purposes. We introduce a preliminary instrument for measuring this perception, called the Ad Practice Frequency Perception Scale. We report data from a survey using this instrument, as well as the results of an initial clustering of participants based on this data. Our results, while preliminary, suggest that this construct may have future potential to characterize and segment individuals, and is worthy of further exploration."
"HMM-based script identification for OCR.  While current OCR systems are able to recognize text in
an increasing number of scripts and languages, typically
they still need to be told in advance what those scripts and
languages are. We propose an approach that repurposes
the same HMM-based system used for OCR to the task of
script/language ID, by replacing character labels with script
class labels. We apply it in a multi-pass overall OCR process
which achieves ??universal?? OCR over 54 tested languages
in 18 distinct scripts, over a wide variety of typefaces in
each. For comparison we also consider a brute-force approach,
wherein a singe HMM-based OCR system is trained
to recognize all considered scripts. Results are presented on
a large and diverse evaluation set extracted from book images,
both for script identification accuracy and for overall
OCR accuracy. On this evaluation data, the script ID system
provided a script ID error rate of 1.73% for 18 distinct
scripts. The end-to-end OCR system with the script ID system
achieved a character error rate of 4.05%, an increase of
0.77% over the case where the languages are known a priori."
"Efficient Densest Subgraph Computation in Evolving Graphs.  Densest subgraph computation has emerged as an important primitive in a wide range of data analysis tasks such as community and event detection. Social media such as Facebook and Twitter are highly dynamic with new friendship links and tweets being generated incessantly, calling for efficient algorithms that can handle very large and highly dynamic input data. While either scalable or dynamic algorithms for finding densest subgraphs have been proposed, a viable and satisfactory solution for addressing both the dynamic aspect of the input data and its large size is still missing. We study the densest subgraph problem in the the dynamic graph model, for which we present the first scalable algorithm with provable guarantees. In our model, edges are added adversarially while they are removed uniformly at random from the current graph. We show that at any point in time we are able to maintain a 2(1+??)-approximation of a current densest subgraph, while requiring O(polylog(n+r)) amortized cost per update (with high probability), where r is the total number of update operations executed and n is the maximum number of nodes in the graph. In contrast, a naive algorithm that recomputes a dense subgraph every time the graph changes requires Omega(m) work per update, where m is the number of edges in the current graph. Our theoretical analysis is complemented with an extensive experimental evaluation on large real-world graphs showing that (approximate) densest subgraphs can be maintained efficiently within hundred of microseconds per update."
"Efficient Algorithms for Public-Private Social Networks.  We introduce the public-private model of graphs. In this model, we have a public graph and each node in the public graph has an associated private graph. The motivation for studying this model stems from social networks, where the nodes are the users, the public graph is visible to everyone, and the private graph at each node is visible only to the user at the node. From each node's viewpoint, the graph is just a union of its private graph and the public graph. We consider the problem of efficiently computing various properties of the graphs from each node's point of view, with minimal amount of recomputation on the public graph. To illustrate the richness of our model, we explore two powerful computational paradigms for studying large graphs, namely, sketching and sampling, and focus on some key problems in social networks and show efficient algorithms in the public-private graph model. In the sketching model, we show how to efficiently approximate the neighborhood function, which in turn can be used to approximate various notions of centrality. In the sampling model, we focus on all-pair shortest path distances, node similarities, and correlation clustering."
"Take me to your leader! Online Optimization of Distributed Storage Configurations.  The configuration of a distributed storage system typically
includes, among other parameters, the set of servers and
their roles in the replication protocol. Although mechanisms for changing the configuration at runtime exist, it is usually left to system administrators to manually determine the ??best?? configuration and periodically reconfigure the system, often by trial and error. This paper describes a new workload-driven optimization framework that dynamically determines the optimal configuration at runtime. We focus on optimizing leader and quorum based replication schemes and divide the framework into three optimization tiers, dynamically optimizing different configuration aspects: 1) leader placement, 2) roles of different servers in the replication protocol, and 3) replica locations. We showcase our optimization framework by applying it to a large-scale distributed storage system used internally in Google and demonstrate that most client applications significantly benefit from using our framework, reducing average operation latency by up to 94%."
"Expander via Local Edge Flips.  Designing distributed and scalable algorithms to improve network connectivity is a central topic in peer-to-peer networks. In this paper we focus on the following well-known problem: given an n-node d-regular network for d = ??(log n), we want to design a decentralized, local algorithm that transforms the graph into one that has good connectivity properties (low diameter, expansion, etc.) without affecting the sparsity of the graph. To this end, Mahlmann and Schindelhauer introduced the random ""flip"" transformation, where in each time step, a random pair of vertices that have an edge decide to 'swap a neighbor'. They conjectured that performing O(nd) such flips at random would convert any connected d-regular graph into a d-regular expander graph, with high probability. However, the best known upper bound for the number of steps is roughly O(n17d23), obtained via a delicate Markov chain comparison argument. Our main result is to prove that a natural instantiation of the random flip produces an expander in at most O(n2d2[EQUATION] n) steps, with high probability. Our argument uses a potential-function analysis based on the matrix exponential, together with the recent beautiful results on the higher-order Cheeger inequality of graphs. We also show that our technique can be used to analyze another well-studied random process known as the 'random switch', and show that it produces an expander in O(nd) steps with high probability."
"Poster Paper: Automatic Reconfiguration of Distributed Storage.  The configuration of a distributed storage system with multiple data replicas typically includes the set of servers and their roles in the replication protocol. The configuration can usually be changed manually, but in most cases, system administrators have to determine a good configuration by trial and error. We describe a new workload-driven optimization framework that dynamically determines the optimal configuration at run time. Applying the framework to a large-scale distributed storage system used internally in Google resulted in halving the operation
latency in 17% of the tested databases, and reducing it by more than 90% in some cases."
"Pose Embeddings: A Deep Architecture for Learning to Match Human Poses.  We present a method for learning an embedding that places images of humans in similar poses nearby. This embedding can be used as a direct method of comparing images based on human pose, avoiding potential challenges of estimating body joint positions. Pose embedding learning is formulated under a triplet-based distance criterion. A deep architecture is used to allow learning of a representation capable of making distinctions between different poses. Experiments on human pose matching and retrieval from video data demonstrate the potential of the method."
"Fast Bilateral-Space Stereo for Synthetic Defocus.  Given a stereo pair it is possible to recover a depth map and use that depth to render a synthetically defocused image. Though stereo algorithms are well-studied, rarely are those algorithms considered solely in the context of producing these defocused renderings. In this paper we present a technique for efficiently producing disparity maps using a novel optimization framework in which inference is performed in ""bilateral-space"". Our approach produces higher-quality ""defocus"" results than other stereo algorithms while also being 10-100 times faster than comparable techniques."
"Convolutional Color Constancy.  Color constancy is the problem of inferring the color of the light that illuminated a scene, usually so that the illumination color can be removed. Because this problem is underconstrained, it is often solved by modeling the statistical regularities of the colors of natural objects and illumination. In contrast, in this paper we reformulate the problem of color constancy as a 2D spatial localization task in a log-chrominance space, thereby allowing us to apply techniques from object detection and structured prediction to the color constancy problem. By directly learning how to discriminate between correctly white-balanced images and poorly white-balanced images, our model is able to improve performance on standard benchmarks by nearly 40%."
"Massively Multitask Networks for Drug Discovery.  Massively multitask neural architectures provide a learning framework for drug discovery that synthesizes information from many distinct biological sources. To train these architectures at scale, we gather large amounts of data from public sources to create a dataset of nearly 40 million measurements across more than 200 biological targets. We investigate several aspects of the multitask framework by performing a series of empirical studies and obtain some interesting results: (1) massively multitask networks obtain predictive accuracies significantly better than single-task methods, (2) the predictive power of multitask networks improves as additional tasks and data are added, (3) the total amount of data and the total number of tasks both contribute significantly to multitask improvement, and (4) multitask networks afford limited transferability to tasks not in the training set. Our results underscore the need for greater data sharing and further algorithmic innovation to accelerate the drug discovery process."
"Understanding user behavior at three scales: The AGoogleADay story.  How people behave is the central question for data analytics, and a single  approach to understanding user behavior is often limiting.  The way people play, the ways they interact, the kinds of behaviors they bring to the game, these factors all ultimately drive how our systems perform, and what we can understand about why  users do what they do.  I suggest that looking at user data at three different scales of time and sampling resolution shows us how looking at behavior data at the micro-,  meso-, and macro levels is a superb way to understand what people are doing in our  systems, and why.  Knowing this lets you not just understand what??s going on, but  also how to improve the user experience for the next design cycle"
Research skills matter:  How to teach them.  There??s always been a gap between those who know how to use information resources and those who don??t.  Students who knew the ways to leverage a library for research could consistently do better research than those who couldn??t.  This chapter is about why teaching research skills is a necessary step in the development of students.
"AdAlyze Redux: Post-Click and Post-Conversion Text Feature Attribution for Sponsored Search Ads.  In this paper, we present our ongoing research on an ads quality testing tool that we call AdAlyze Redux. This tool allows advertisers to get individual best practice recommendations based on an expandable set of textual ads features, tailored to exactly the ads in an advertiser's set of accounts. This lets them optimize their ad copies against the common online advertising key performance indicators clickthrough rate and, if available, conversion rate. We choose the Web as the tool's platform and automatically generate the analyses as platform-independent HTML5 slides and full reports."
"Dynamic adjustment of video quality.  A video quality module receives data indicating a visibility status of a tab of a web browser running on a user device. The video quality module determines, based on the data indicating the visibility status of the tab whether the tab of the web browser is currently visible to a user of the user device, the tab of the web browser comprising a streaming media player. If the tab of the web browser is not currently visible to the user, the video quality module decreases a quality of a video component of a streaming media file playing in the streaming media player."
"Plato: A Selective Context Model for Entity Resolution.  We present Plato, a probabilistic model for entity resolution that includes a novel approach for handling noisy or uninformative features,and supplements labeled training data derived from Wikipedia with a very large unlabeled text corpus. Training and inference in the proposed model can easily be distributed across many servers, allowing it to scale to over 10^7 entities. We evaluate Plato on three standard datasets for entity resolution. Our approach achieves the best results to-date on TAC KBP 2011 and is highly competitive on both the CoNLL 2003 and TAC KBP 2012 datasets."
"Machine Learning for Dialog State Tracking: A Review.  Spoken dialog systems help users achieve a task using natural language. Noisy speech recognition and ambiguity in natural language motivate statistical approaches that exploit distributions over the user's goal at every step in the dialog. The task of tracking these distributions, termed Dialog State Tracking, is therefore an essential component of any Spoken dialog system. In recent years, the Dialog State Tracking Challenges have provided a common test-bed and evaluation framework for this task, as well as labeled dialog data. As a result, a variety of machine-learned methods have been successfully applied to Dialog State Tracking. This paper reviews the machine-learning techniques that have been adapted to Dialog State Tracking, and gives an overview of published evaluations. Discriminative machine-learned methods outperform generative and rule-based methods, the previous state-of-the-art."
"Large-scale, sequence-discriminative, joint adaptive training for masking-based robust ASR.  Recently, it was shown that the performance of supervised time-frequency masking based robust automatic speech recognition techniques can be improved by training them jointly with the acoustic model [1]. The system in [1], termed deep neural network based joint adaptive training, used fully-connected feed-forward deep neural networks for estimating time-frequency masks and for acoustic modeling; stacked log mel spectra was used as features and training minimized cross entropy loss. In this work, we extend such jointly trained systems in several ways. First, we use recurrent neural networks based on long short-term memory (LSTM) units ?? this allows the use of unstacked features, simplifying joint optimization. Next, we use a sequence discriminative training criterion for optimizing parameters. Finally, we conduct experiments on large scale data and show that joint adaptive training can provide gains over a strong baseline. Systematic evaluations on noisy voice-search data show relative improvements ranging from 2% at 15 dB to 5.4% at -5 dB over a sequence discriminative, multi-condition trained LSTM acoustic model."
"Next Steps for Value Sensitive Design? A Practitioner's Progress.  Over the last 20 years, value sensitive design (VSD) as a framework and approach based in theory has been widely applied, and also contested, in HCI. In this presentation, I draw on 8 years of practice to show how VSD is a suitable and productive methodological framework for research on social change and social impact. My goal is to promote discussion at HCIC of the connection of theory and practice, and how bridging the gap between theory and practice can, in turn, bridge the gap between academic and industry research."
"Understanding America??s Interested Bystander: A Complicated Relationship with Civic Duty.  Among those who are interested in improving democracy in the United States, a question that often comes up is how to engage the unengaged. To support the broader ecosystem of individuals and institutions working hard to make our civic life more inclusive and meaningful, we sought to contribute to these efforts by undertaking needed and detailed user research about the attitudes and behaviors of average Americans. In particular, this paper outlines a joint qualitative and quantitative study for understanding ??Interested Bystanders,?? or that portion of the population that is paying attention to the world around them, but not regularly voicing their opinions or taking action. These are the findings of this research, conducted by the Google Civic Innovation Team in 2014. As applied research, this work sought to inform the design of civic-related products and services at Google and across the civic technology community more broadly. In reporting what we learned, we also have attempted to share how we learned it, and offer a case study for the use of human-centered research to inform civic interventions."
"Findings from Crisis Field Research in Four Countries.  Focus on the user and all else will follow - that's Google's #1 ""Thing We Know to be True."" In this talk, the Google Crisis Response user research team shares the key findings from field research in four countries. We went to Brazil, Indonesia, Mexico, and the Philippines to interview users in their homes about how they respond to crises that range from floods to landslides. We want to share those stories with you at ICCM so we can all focus on the user."
"The landscape of digital media research: big data, big research, right impact.  Invited keynote"
What you should know about R.  A primer on the industry??s open-source statistical analysis language.
"Supporting Privacy-Conscious App Update Decisions with User Reviews.  Smartphone app updates are critical to user security and privacy. New versions may fix important security bugs, which is why users should usually update their apps. However, occasionally apps turn malicious or radically change features in a way users dislike. Users should not necessarily always update in those circumstances, but current update processes are largely automatic. Therefore, it is important to understand user behaviors around updating apps and help them to make security-conscious choices. We conducted two related studies in this area. First, to understand users' current update decisions, we conducted an online survey of user attitudes toward updates. Based on the survey results, we then designed a notification scheme integrating user reviews, which we tested in a field study. Participants installed an Android app that simulated update notifications, enabling us to collect users' update decisions and reactions. We compared the effectiveness of our review-based update notifications with the permission-based notifications. Compared to notifications with permission descriptions only, we found our review-based update notification was more effective at alerting users of invasive or malicious app updates, especially for less trustworthy apps."
"Understanding and Comparing Smartphone and Tablet Use: Insights from a Large-Scale Diary Study.  In recent years, smartphone and tablet ownership has shown continued growth; however, there is a lack of research thoroughly investigating the use of these devices within the general public. This paper describes a large-scale diary study with U.S. mobile device owners, examining details of smartphone and tablet use. Results provide a comprehensive breakdown of frequent activities and contexts of use, highlighting key differences in smartphone and tablet use. Activities on smartphones were found to be dominated by communication needs, while tablets were frequently used for consumption and entertainment. Both devices were most often used at home, with tablets rarely leaving the home. Within the home, smartphones were used mostly in the bedroom, and tablets in the living room. Both devices were used frequently while doing something else, such as using tablets primarily while watching TV. Conclusions discuss implications for enriching the experience of mobile devices and opportunities for future research."
"Randomized Composable Core-sets for Distributed Submodular Maximization.  An effective technique for solving optimization problems over massive data sets is to partition the data into smaller pieces, solve the problem on each piece and compute a representative solution from it, and finally obtain a solution inside the union of the representative solutions for all pieces. This technique can be captured via the concept of composable core-sets, and has been recently applied to solve diversity maximization problems as well as several clustering problems [7,15,8]. However, for coverage and submodular maximization problems, impossibility bounds are known for this technique [15]. In this paper, we focus on efficient construction of a randomized variant of composable core-sets where the above idea is applied on a random clustering of the data. We employ this technique for the coverage, monotone and non-monotone submodular maximization problems. Our results significantly improve upon the hardness results for non-randomized core-sets, and imply improved results for submodular maximization in a distributed and streaming settings. The effectiveness of this technique has been confirmed empirically for several machine learning applications [22], and our proof provides a theoretical foundation to this idea. In summary, we show that a simple greedy algorithm results in a 1/3-approximate randomized composable core-set for submodular maximization under a cardinality constraint. Our result also extends to non-monotone submodular functions, and leads to the first 2-round MapReduce-based constant-factor approximation algorithm with O(n) total communication complexity for either monotone or non-monotone functions. Finally, using an improved analysis technique and a new algorithm PseudoGreedy, we present an improved 0.545-approximation algorithm for monotone submodular maximization, which is in turn the first MapReduce-based algorithm beating factor 1/2 in a constant number of rounds."
"Distributed Graph Algorithmics: Theory and Practice.  As a fundamental tool in modeling and analyzing social, and information networks, large-scale graph mining is an important component of any tool set for big data analysis. Processing graphs with hundreds of billions of edges is only possible via developing distributed algorithms under distributed graph mining frameworks such as MapReduce, Pregel, Gigraph, and alike. For these distributed algorithms to work well in practice, we need to take into account several metrics such as the number of rounds of computation and the communication complexity of each round. For example, given the popularity and ease-of-use of MapReduce framework, developing practical algorithms with good theoretical guarantees for basic graph algorithms is a problem of great importance. In this tutorial, we first discuss how to design and implement algorithms based on traditional MapReduce architecture. In this regard, we discuss various basic graph theoretic problems such as computing connected components, maximum matching, MST, counting triangle and overlapping or balanced clustering. We discuss a computation model for MapReduce and describe the sampling, filtering, local random walk, and core-set techniques to develop efficient algorithms in this framework. At the end, we explore the possibility of employing other distributed graph processing frameworks. In particular, we study the effect of augmenting MapReduce with a distributed hash table (DHT) service and also discuss the use of a new graph processing framework called ASYMP based on asynchronous message-passing method. In particular, we will show that using ASyMP, one can improve the CPU usage, and achieve significantly improved running time."
"Composable core-sets for diversity and coverage maximization.  In this paper we consider efficient construction of ??composable
core-sets?? for basic diversity and coverage maximization
problems. A core-set for a point-set in a metric space is a
subset of the point-set with the property that an approximate
solution to the whole point-set can be obtained given
the core-set alone. A composable core-set has the property
that for a collection of sets, the approximate solution to the
union of the sets in the collection can be obtained given the
union of the composable core-sets for the point sets in the
collection. Using composable core-sets one can obtain effi-
cient solutions to a wide variety of massive data processing
applications, including nearest neighbor search, streaming
algorithms and map-reduce computation.
Our main results are algorithms for constructing composable
core-sets for several notions of ??diversity objective
functions??, a topic that attracted a significant amount of
research over the last few years. The composable core-sets
we construct are small and accurate: their approximation
factor almost matches that of the best ??off-line?? algorithms
for the relevant optimization problems (up to a constant
factor). Moreover, we also show applications of our results
to diverse nearest neighbor search, streaming algorithms and
map-reduce computation. Finally, we show that for an alternative
notion of diversity maximization based on the maximum
coverage problem small composable core-sets do not
exist."
"Randomized Composable Core-sets for Distributed Submodular Maximization.  An effective technique for solving optimization problems over massive data sets is to partition the data into smaller pieces, solve the problem on each piece and compute a representative solution from it, and finally obtain a solution inside the union of the representative solutions for all pieces. This technique can be captured via the concept of composable core-sets, and has been recently applied to solve diversity maximization problems as well as several clustering problems [7,15,8]. However, for coverage and submodular maximization problems, impossibility bounds are known for this technique [15]. In this paper, we focus on efficient construction of a randomized variant of composable core-sets where the above idea is applied on a random clustering of the data. We employ this technique for the coverage, monotone and non-monotone submodular maximization problems. Our results significantly improve upon the hardness results for non-randomized core-sets, and imply improved results for submodular maximization in a distributed and streaming settings. The effectiveness of this technique has been confirmed empirically for several machine learning applications [22], and our proof provides a theoretical foundation to this idea. In summary, we show that a simple greedy algorithm results in a 1/3-approximate randomized composable core-set for submodular maximization under a cardinality constraint. Our result also extends to non-monotone submodular functions, and leads to the first 2-round MapReduce-based constant-factor approximation algorithm with O(n) total communication complexity for either monotone or non-monotone functions. Finally, using an improved analysis technique and a new algorithm PseudoGreedy, we present an improved 0.545-approximation algorithm for monotone submodular maximization, which is in turn the first MapReduce-based algorithm beating factor 1/2 in a constant number of rounds."
"Robust Price of Anarchy Bounds via LP and Fenchel Duality.  Bounding the price of anarchy (PoA), which quantifies the degradation in the quality of outcomes in a (pure) Nash equilibrium of a game, is one of the fundamental questions in computational game theory. However, for a large class of games, a pure NE may not always exist and hence a natural question to pursue is to quantify the inefficiency for weaker notions of equilibrium such as mixed Nash equilibrium, correlated equilibrium or coarse correlated equilibrium, all of which are known to exist for finite games. Several techniques have been developed for bounding the price of anarchy, yet, only a handful of them are applicable for proving the PoA bounds for general equilibrium concepts. Most notable among such techniques is Roughgarden's elegant smoothness framework, which led to the concept of robust price of anarchy. The term refers to the inefficiency bounds applicable to general equilibrium notions such as coarse correlated equilibrium. In this paper, we develop a new framework based on LP and Fenchel duality for bounding the robust price of anarchy for a large class of games. We use our framework to give the first PoA bounds for temporal routing games on graphs and energy minimization games in machine scheduling. Most notably, we present the first coordination mechanisms with bounded PoA for temporal routing over general graphs, show a related lowerbound result, and an improved bound on the price of stability for this game. Previously, coordination mechanisms with bounded PoA were only known for restricted classes of graphs such as trees or parallel edges. Furthermore, we demonstrate the wide applicability of our framework by giving new proofs of the PoA bounds for three classical games ?? weighted affine congestion games, competitive facility location games and simultaneous second price auctions. Our price anarchy bounds for these games match the ones known in the literature or obtained using the smoothness framework. All our proofs use the following technique: we first show that for a wide class of games, one can formulate the underlying optimization problem as a linear (or convex) program such that the (Fenchel) dual of the relaxation encodes the equilibrium condition. Further, the dual program has a specific structure with variables for players and resources, which can be naturally interpreted as the cost incurred by the players and the congestion of the resource in an equilibrium outcome. This lets us argue that our definition of dual variables satisfy the dual constraints and using the weak duality theorem we establish the PoA bounds."
"Online Allocation with Traffic Spikes: Mixing Adversarial and Stochastic Models.  Motivated by Internet advertising applications, online allocation problems have been studied extensively in various adversarial and stochastic models. While the adversarial arrival models are too pessimistic, many of the stochastic (such as i.i.d or random-order) arrival models do not realistically capture uncertainty in predictions. A significant cause for such uncertainty is the presence of unpredictable traffic spikes, often due to breaking news or similar events. To address this issue, a simultaneous approximation framework has been proposed to develop algorithms that work well both in the adversarial and stochastic models; however, this framework does not enable algorithms that make good use of partially accurate forecasts when making online decisions. In this paper, we propose a robust online stochastic model that captures the nature of traffic spikes in online advertising. In our model, in addition to the stochastic input for which we have good forecasting, an unknown number of impressions arrive that are adversarially chosen.We design algorithms that combine an stochastic algorithm with an online algorithm that adaptively reacts to inaccurate predictions. We provide provable bounds for our new algorithms in this framework. We accompany our positive results with a set of hardness results showing that that our algorithms are not far from optimal in this framework. As a byproduct of our results, we also present improved online algorithms for a slight variant of the simultaneous approximation framework."
"Recent Books and Journals Articles in Public Opinion, Survey Methods, and Survey Statistics. 2015 Update.  Welcome to the 7th edition of this column on recent books and journal articles in the field of public opinion, survey methods, and survey statistics. This year I had the chance to visit the London book fair, so I was able actually to see some of the new books in our field.
This article is an update of the April 2014 article. Like the previous year, the books are organized by topic; this should help the readers to focus on their interests.
It is unlikely to list all new books in the field; I did my best scouting different resources and websites, but I take full responsibility for any omission. The list is also focusing only on books published in English language and available for purchase (as an Ebook or in print) at the time of this review (June 2015). Books are listed based on the relevance to the topic, and no judgment is made in terms of quality of the content. We let the readers do so.
Given our field is becoming more and more interdisciplinary, this year I added a new section called ??big data, social media and other relevant books?? to capture areas that are overlapping more and more with public opinion, survey research, and survey statistics."
Gesture On: Always-On Touch Gestures for Fast Mobile Access from Device Standby Mode.  Contributes a system that overrides the mobile platform kernel behavior to enable touchscreen gesture shortcuts in standby mode. A user can issue a gesture on the touchscreen before the screen is even turned on.
"Weave: Scripting Cross-Device Wearable Interaction.  Provides a set of high-level APIs, based on JavaScript, and integrated tool support for developers to easily distribute UI output and combine user input and sensing events across devices for cross-device interaction."
"Ego-net Community Mining Applied to Friend Suggestion.  In this paper, we present a study of the community structure
of ego-networks??the graphs representing the connections
among the neighbors of a node??for several online social
networks. Toward this goal, we design a new technique to
efficiently build and cluster all the ego-nets of a graph in
parallel (note that even just building the ego-nets efficiently
is challenging on large networks). Our experimental findings
are quite compelling: at a microscopic level it is easy to
detect high quality communities.
Leveraging on this fact we, then, develop new features
for friend suggestion based on co-occurrences of two nodes
in different ego-nets?? communities. Our new features can
be computed efficiently on very large scale graphs by just
analyzing the neighborhood of each node. Furthermore, we
prove formally on a stylized model, and by experimental
analysis that this new similarity measure outperforms the
classic local features employed for friend suggestions"
"Unified and contrasting cuts in multiple graphs: application to medical imaging segmentation.  The analysis of data represented as graphs is common having wide scale applications from social networks to medical imaging. A popular analysis is to cut the graph so that the disjoint subgraphs can represent communities (for social network) or background and foreground cognitive activity (for medical imaging). An emerging setting is when multiple data sets (graphs) exist which opens up the opportunity for many new questions. In this paper we study two such questions: i) For a collection of graphs find a single cut that is good for all the graphs and ii) For two collections of graphs find a single cut that is good for one collection but poor for the other. We show that existing formulations of multiview, consensus and alternative clustering cannot address these questions and instead we provide novel formulations in the spectral clustering framework. We evaluate our approaches on functional magnetic resonance imaging (fMRI) data to address questions such as: ""What common cognitive network does this group of individuals have?"" and ""What are the differences in the cognitive networks for these two groups?"" We obtain useful results without the need for strong domain knowledge."
"Semi-supervised sequence learning.  The analysis of data represented as graphs is common having wide scale applications from social networks to medical imaging. A popular analysis is to cut the graph so that the disjoint subgraphs can represent communities (for social network) or background and foreground cognitive activity (for medical imaging). An emerging setting is when multiple data sets (graphs) exist which opens up the opportunity for many new questions. In this paper we study two such questions: i) For a collection of graphs find a single cut that is good for all the graphs and ii) For two collections of graphs find a single cut that is good for one collection but poor for the other. We show that existing formulations of multiview, consensus and alternative clustering cannot address these questions and instead we provide novel formulations in the spectral clustering framework. We evaluate our approaches on functional magnetic resonance imaging (fMRI) data to address questions such as: ""What common cognitive network does this group of individuals have?"" and ""What are the differences in the cognitive networks for these two groups?"" We obtain useful results without the need for strong domain knowledge."
"Large Vocabulary Automatic Speech Recognition for Children.  Recently, Google launched YouTube Kids, a mobile application for children, that uses a speech recognizer built specifically for recognizing children??s speech. In this paper we present techniques we explored to build such a system. We describe the use of a neural network classifier to identify matched acoustic training data, filtering data for language modeling to reduce the chance of producing offensive results. We also compare long short-term memory (LSTM) recurrent networks to convolutional, LSTM, deep neural networks (CLDNN). We found that a CLDNN acoustic model outperforms an LSTM across a variety of different conditions, but does not specifically model child speech relatively better than adult. Overall, these findings allow us to build a successful, state-of-the-art large vocabulary speech recognizer for both children and adults."
"Acoustic Modelling with CD-CTC-SMBR LSTM RNNS.  This paper describes a series of experiments to extend the application of Context-Dependent (CD) long short-term memory (LSTM) recurrent neural networks (RNNs) trained with Connectionist Temporal Classification (CTC) and sMBR loss. Our experiments, on a noisy, reverberant voice search task, include training with alternative pronunciations and the application to child speech recognition; combination of multiple models, and convolutional input layers. We also investigate the latency of CTC models and show that constraining forward-backward alignment in training can reduce the delay for a real-time streaming speech recognition system. Finally we investigate transferring knowledge from one network to another through alignments"
"Profiling a warehouse-scale computer.  With the increasing prevalence of warehouse-scale (WSC) and cloud computing, understanding the interactions of server applications with the underlying microarchitecture becomes ever more important in order to extract maximum performance out of server hardware. To aid such understanding, this paper presents a detailed microarchitectural analysis of live datacenter jobs, measured on more than 20,000 Google machines over a three year period, and comprising thousands of different applications. We first find that WSC workloads are extremely diverse, breeding the need for architectures that can tolerate application variability without performance loss. However, some patterns emerge, offering opportunities for co-optimization of hardware and software. For example, we identify common building blocks in the lower levels of the software stack. This ""datacenter tax"" can comprise nearly 30% of cycles across jobs running in the fleet, which makes its constituents prime candidates for hardware specialization in future server systems-on-chips. We also uncover opportunities for classic microarchitectural optimizations for server processors, especially in the cache hierarchy. Typical workloads place significant stress on instruction caches and prefer memory latency over bandwidth. They also stall cores often, but compute heavily in bursts. These observations motivate several interesting directions for future warehouse-scale computers."
"Reasoning about Risk and Trust in an Open World.  Contemporary open systems use components developed by different parties, linked together dynamically in unforeseen constellations. Code needs to live up to strict security requirements, and ensure the correct functioning of its objects even when they collaborate with external, potentially malicious, objects. In this paper we propose special specification predicates that model risk and trust in open systems. We specify Miller, Van Cutsem, and Tulloh??s escrow exchange example, and discuss the meaning of such a specification. We propose a novel Hoare logic, based on four-tuples, including an invariant describing properties preserved by the execution of a statement as well as a post-condition describing the state after execution. We model specification and programing languages based on the Hoare logic, prove soundness, and prove the key steps of the Escrow protocol."
"A Subjective Study for the Design of Multi-resolution ABR Video Streams with the VP9 Codec.  Adaptive bitrate (ABR) streaming is one enabling technology for video streaming over modern throughput-varying communication networks. A widely used ABR streaming method is to adapt the video bitrate to channel throughput by dynamically changing the video resolution. Since videos have different rate-quality performances at different resolutions, such ABR strategy can achieve better rate-quality trade-off than single resolution ABR streaming. The key problem for resolution switched ABR is to work out the bitrate appropriate at each resolution. In this paper, we investigate optimal strategies to estimate this bitrate using both quantitative and subjective quality assessment. We use the design of 2K and 4K bitrates as an example of the performance of this strategy. We introduce strategies for selecting an appropriate corpus for subjective assessment and find that at this high resolution there is good agreement between quantitative and subjective analysis."
"Precentile-Based Approach to Forecasting Workload Growth.  When forecasting resource workloads (traffic, CPU load, memory usage, etc.), we often extrapolate from the upper percentiles of data distributions.  This works very well when the resource is far enough from its saturation point.  However, when the resource utilization gets closer to the workload-carrying capacity of the resource, upper percentiles level off (the phenomenon is colloquially known as flat-topping or clipping), leading to underpredictions of future workload and potentially to undersized resources. This paper explains the phenomenon and proposes a new approach that can be used for making useful forecasts of workload when historical data for the forecast are collected from a resource approaching saturation."
"Multinomial Loss on Held-out Data for the Sparse Non-negative Matrix Language Model.  We describe Sparse Non-negative Matrix (SNM) language model estimation using multinomial loss on held-out data. Being able to train on held-out data is important in practical situations where the training data is usually mismatched from the held-out/test data. It is also less constrained than the previous training algorithm using leave-one-out on training data: it allows the use of richer meta-features in the adjustment model, e.g. the diversity counts used by Kneser-Ney smoothing which would be difficult to deal with correctly in leave-one-out training. In experiments on the one billion words language modeling benchmark, we are able to slightly improve on our previous results which use a different loss function, and employ leave-one-out training on a subset of the main training set. Surprisingly, an adjustment model with meta-features that discard all lexical information can perform as well as lexicalized meta-features. We find that fairly small amounts of held-out data (on the order of 30-70 thousand words) are sufficient for training the adjustment model. In a real-life scenario where the training data is a mix of data sources that are imbalanced in size, and of different degrees of relevance to the held-out and test data, taking into account the data source for a given skip-/n-gram feature and combining them for best performance on held-out/test data improves over skip-/n-gram SNM models trained on pooled data by about 8% in the SMT setup, or as much as 15% in the ASR/IME setup. The ability to mix various data sources based on how relevant they are to a mismatched held-out set is probably the most attractive feature of the new estimation method for SNM LM."
"Spherical Random Features for Polynomial Kernels.  Compact explicit feature maps provide a practical framework to scale kernel methods to large-scale learning, but deriving such maps for many types of kernels remains a challenging open problem. Among the commonly used kernels for nonlinear classification are polynomial kernels, for which low approximation error has thus far necessitated explicit feature maps of large dimensionality, especially for higher-order polynomials. Meanwhile, because polynomial kernels are unbounded, they are frequently applied to data that has been normalized to unit l2 norm. The question we address in this work is: if we know a priori that data is normalized, can we devise a more compact map? We show that a putative affirmative answer to this question based on Random Fourier Features is impossible in this setting, and introduce a new approximation paradigm, Spherical Random Fourier (SRF) features, which circumvents these issues and delivers a compact approximation to polynomial kernels for data on the unit sphere. Compared to prior work, SRF features are less rank-deficient, more compact, and achieve better kernel approximation, especially for higher-order polynomials. The resulting predictions have lower variance and typically yield better classification accuracy."
"Advertising on YouTube and TV: A Meta-analysis of Optimal Media-mix Planning.  In this work we investigate under what circumstances a TV campaign should be complemented with online advertising to increase combined reach. First, we use probabilistic models to derive necessary and sufficient conditions. We then test these optimality conditions on empirical findings of a large collection of TV campaigns to answer two important questions: i) which characteristics of a TV campaign make it favorable to shift part of its budget to online advertising?; and ii) if it should shift, how much cost savings and additional reach can advertisers expect? First, we use classification methods such as linear discriminant analysis, logistic regression, and decision trees to decide whether a TV campaign should add online advertising; secondly, we train linear and support vector regression models to predict optimal budget allocation, cost savings, or additional reach. To train these models we use optimization results on roughly 26,000 campaigns. We do not only achieve excellent out-of-sample predictive power, but also obtain simple, interpretable, and actionable rules that improve the understanding of media mix advertising."
"Hierarchical Label Propagation and Discovery for Machine Generated Email.  Machine-generated documents such as email or dynamic web pages are single instantiations of a pre-defined structural template. As such, they can be viewed as a hierarchy of template and document specific content. This hierarchical template representation has several important advantages for document clustering and classification. First, templates capture common topics among the documents, while filtering out the potentially noisy variabilities such as personal information. Second, template representations scale far better than document representations since a single template captures numerous documents. Finally, since templates group together structurally similar documents, they can propagate properties between all the documents that match the template. In this paper, we use these advantages for document classification by formulating an efficient and effective hierarchical label propagation and discovery algorithm. The labels are propagated first over a template graph (constructed based on either term-based or topic-based similarities), and then to the matching documents. We evaluate the performance of the proposed algorithm using a large donated email corpus and show that the resulting template graph is significantly more compact than the corresponding document graph and the hierarchical label propagation is both efficient and effective in increasing the coverage of the baseline document classification algorithm. We demonstrate that the template label propagation achieves more than 91% precision and 93% recall, while increasing the label coverage by more than 11%."
"Federated Optimization: Distributed Optimization Beyond the Datacenter.  We introduce a new and increasingly relevant setting for distributed optimization in machine learning, where the data defining the optimization are distributed (unevenly) over an extremely large number of nodes, but the goal remains to train a high-quality centralized model. We refer to this setting as Federated Optimization. In this setting, communication efficiency is of utmost importance. A motivating example for federated optimization arises when we keep the training data locally on users' mobile devices rather than logging it to a data center for training. Instead, the mobile devices are used as nodes performing computation on their local data in order to update a global model. We suppose that we have an extremely large number of devices in our network, each of which has only a tiny fraction of data available totally; in particular, we expect the number of data points available locally to be much smaller than the number of devices. Additionally, since different users generate data with different patterns, we assume that no device has a representative sample of the overall distribution. We show that existing algorithms are not suitable for this setting, and propose a new algorithm which shows encouraging experimental results. This work also sets a path for future research needed in the context of federated optimization."
"Mantis: Efficient Predictions of Execution Time, Energy Usage, Memory Usage and Network Usage on Smart Mobile Devices.  We present Mantis, a framework for predicting the computational resource consumption (CRC) of Android applications on given inputs accurately, and efficiently. A key insight underlying Mantis is that program codes often contain features that correlate with performance and these features can be automatically computed efficiently. Mantis synergistically combines techniques from program analysis and machine learning. It constructs concise CRC models by choosing from many program execution features only a handful that are most correlated with the program??s CRC metric yet can be evaluated efficiently from the program??s input. We apply program slicing to reduce evaluation time of a feature and automatically generate executable code snippets for efficiently evaluating features. Our evaluation shows that Mantis predicts four CRC metrics of seven Android apps with estimation error in the range of 0-11.1 percent by executing predictor code spending at most 1.3 percent of their execution time on Galaxy Nexus."
Statistical parametric speech synthesis: from HMM to LSTM-RNN.  This talk will present progress of acoustic modeling in statistical parametric speech synthesis from the conventional hidden Markov model HMM to the state-of-the-art long short-term memory recurrent neural network. The details of implementation and applications of statistical parametric speech synthesis are also included.
"RFC7646 -Definition and Use of DNSSEC Negative Trust Anchors.  DNS Security Extensions (DNSSEC) is now entering widespread deployment.  However, domain signing tools and processes are not yet as mature and reliable as those for non-DNSSEC-related domain administration tools and processes.  This document defines Negative Trust Anchors (NTAs), which can be used to mitigate DNSSEC validation failures by disabling DNSSEC validation at specified domains."
"RFC7706 - Decreasing Access Time to Root Servers by Running One on Loopback.  Some DNS recursive resolvers have longer-than-desired round-trip times to the closest DNS root server.  Some DNS recursive resolver operators want to prevent snooping of requests sent to DNS root servers by third parties.  Such resolvers can greatly decrease the round-trip time and prevent observation of requests by running a copy of the full root zone on a loopback address (such as 127.0.0.1). This document shows how to start and maintain such a copy of the root zone that does not pose a threat to other users of the DNS, at the cost of adding some operational fragility for the operator."
"Using Entity Information from a Knowledge Base to Improve Relation Extraction.  Relation extraction is the task of extracting predicate-argument relationships between entities from natural language text. This paper investigates whether background information about entities available in knowledge bases such as FreeBase can be used to improve the accuracy of a state-of-the-art relation extraction system. We describe a simple and effective way of incorporating FreeBase??s notable types into a state-of-the-art relation extraction system (Riedel et al., 2013). Experimental results show that our notable type-based system achieves an average 7.5% weighted MAP score improvement. To understand where the notable type information contributes the most, we perform a series of ablation experiments. Results show that the notable type information improves relation extraction more than NER labels alone across a wide range of entity types and relations."
"RFC7710 - Captive-Portal Identification Using DHCP or Router Advertisements (RAs).  In many environments offering short-term or temporary Internet access
   (such as coffee shops), it is common to start new connections in a
   captive-portal mode.  This highly restricts what the customer can do
   until the customer has authenticated. This document describes a DHCP option (and a Router Advertisement
   (RA) extension) to inform clients that they are behind some sort of
   captive-portal device and that they will need to authenticate to get
   Internet access.  It is not a full solution to address all of the
   issues that clients may have with captive portals; it is designed to
   be used in larger solutions.  The method of authenticating to and
   interacting with the captive portal is out of scope for this
   document."
"Balancing the needs of children and adults in the design of technology for children.  In the design of technology for children, many products hope to encourage ideal behavior. Goals or desired outcomes for children-oriented products, such as learning, exploration or self-expression, are often set by adults (e.g. parents, guardians, teachers). These adult goals are often considered alongside the goals and interests of children, but what happens when these are conflicting? It is common for technology creators to have to make choices that support or prioritize one set of goals over the other. In this workshop, we will be discussing real world case studies, as well as theoretical approaches used by researchers, designers, and academics to design technology for children between the ages of 5 and 14. The expected outcome of the workshop will be a set of principles to consider when balancing the needs of children and adults in the design of technology for children."
"Im2Calories: towards an automated mobile vision food diary.  We present a system which can recognize the contents
of your meal from a single image, and then predict its nutritional
contents, such as calories. The simplest version
assumes that the user is eating at a restaurant for which we
know the menu. In this case, we can collect images offline
to train a multi-label classifier. At run time, we apply the
classifier (running on your phone) to predict which foods
are present in your meal, and we lookup the corresponding
nutritional facts. We apply this method to a new dataset of
images from 23 different restaurants, using a CNN-based
classifier, significantly outperforming previous work. The
more challenging setting works outside of restaurants. In
this case, we need to estimate the size of the foods, as
well as their labels. This requires solving segmentation and
depth / volume estimation from a single image. We present
CNN-based approaches to these problems, with promising
preliminary results."
"Minimum Description Length (MDL) Regularization for Online Learning.  An approach inspired by the Minimum Description Length (MDL) principle is proposed for adaptively selecting features during online learning based on their usefulness in improving the objective. The approach eliminates noisy or useless features from the optimization process, leading to improved loss. Several algorithmic variations on the approach are presented. They are based on using a Bayesian mixture in each of the dimensions of the feature space. By utilizing the MDL principle, the mixture reduces the dimensionality of the feature space to its subspace with the lowest loss. Bounds on the loss, derived, show that the loss for that subspace is essentially achieved. The approach can be tuned for trading off between model size and the loss incurred. Empirical results on large scale real-world systems demonstrate how it improves such tradeoffs. Huge model size reductions can be achieved with no loss in performance relative to standard techniques, while moderate loss improvements (translating to large regret improvements) are achieved with moderate size reductions. The results also demonstrate that overfitting is eliminated by this approach."
"Using Recurrent Neural Networks for Slot Filling in Spoken Language Understanding.  Semantic slot filling is one of the most challenging problems in spoken language understanding (SLU). In this paper, we propose to use recurrent neural networks (RNNs) for this task, and present several novel architectures designed to efficiently model past and future temporal dependencies. Specifically, we implemented and compared several important RNN architectures, including Elman, Jordan, and hybrid variants. To facilitate reproducibility, we implemented these networks with the publicly available Theano neural network toolkit and completed experiments on the well-known airline travel information system (ATIS) benchmark. In addition, we compared the approaches on two custom SLU data sets from the entertainment and movies domains. Our results show that the RNN-based models outperform the conditional random field (CRF) baseline by 2% in absolute error reduction on the ATIS benchmark. We improve the state-of-the-art by 0.5% in the Entertainment domain, and 6.7% for the movies domain."
"Large Scale Business Discovery from Street Level Imagery.  Search with local intent is becoming increasingly useful due to the popularity of the mobile device. The creation and maintenance of accurate listings of local businesses worldwide is time consuming and expensive. In this paper, we propose an approach to automatically discover businesses that are visible on street level imagery. Precise business store-front detection enables accurate geo-location of businesses, and further provides input for business categorization, listing generation, etc. The large variety of business categories in different countries makes this a very challenging problem. Moreover, manual annotation is prohibitive due to the scale of this problem. We propose the use of a MultiBox [4] based approach that takes input image pixels and directly outputs store front bounding boxes. This end-to-end learning approach instead preempts the need for hand modelling either the proposal generation phase or the post-processing phase, leveraging large labelled training datasets. We demonstrate our approach outperforms the state of the art detection techniques with a large margin in terms of performance and run-time efficiency. In the evaluation, we show this approach achieves human accuracy in the low-recall settings. We also provide an end-to-end evaluation of business discovery in the real world."
"Acoustic Modeling for Speech Synthesis: from HMM to RNN.  Statistical parametric speech synthesis (SPSS) combines an acoustic model and a vocoder to render speech given a text. Typically decision tree-clustered context-dependent hidden Markov models (HMMs) are employed as the acoustic model, which represent a relationship between linguistic and acoustic features. There have been attempts to replace the HMMs by alternative acoustic models, which provide trajectory and context modeling. Recently, artificial neural network-based acoustic models, such as deep neural networks, mixture density networks, and recurrent neural networks (RNNs), showed significant improvements over the HMM-based one. This talk reviews the progress of acoustic modeling in SPSS from the HMM to the RNN."
"Personalized Speech Recognition On Mobile Devices.  We describe a large vocabulary speech recognition system that is accurate, has low latency, and yet has a small enough memory and computational footprint to run faster than real-time on a Nexus 5 Android smartphone. We employ a quantized Long Short-Term Memory (LSTM) acoustic model trained with connectionist temporal classification (CTC) to directly predict phoneme targets, and further reduce its memory footprint using an SVD-based compression scheme. Additionally, we minimize our memory footprint by using a single language model for both dictation and voice command domains, constructed using Bayesian interpolation. Finally, in order to properly handle device-specific information, such as proper names and other context-dependent information, we inject vocabulary items into the decoder graph and bias the language model on-the-fly. Our system achieves 13.5% word error rate on an open-ended dictation task, running with a median speed that is seven times faster than real-time."
"On The Compression Of Recurrent Neural Networks With An Application To LVCSR Acoustic Modeling For Embedded Speech Recognition.  We study the problem of compressing recurrent neural networks (RNNs). In particular, we focus on the compression
of RNN acoustic models, which are motivated by the goal
of building compact and accurate speech recognition systems
which can be run efficiently on mobile devices. In this work, we present a technique for general recurrent model compression that jointly compresses both recurrent and non-recurrent inter-layer weight matrices. We find that the proposed technique allows us to reduce the size of our Long Short-Term Memory (LSTM) acoustic model to a third of its original size with negligible loss in accuracy."
"Scaling Distributed Machine Learning with the Parameter Server.  We propose a parameter server framework for distributed
machine learning problems. Both data and workloads
are distributed over worker nodes, while the server nodes
maintain globally shared parameters, represented as dense
or sparse vectors and matrices. The framework manages
asynchronous data communication between nodes, and
supports flexible consistency models, elastic scalability,
and continuous fault tolerance. To demonstrate the scalability of the proposed framework,
we show experimental results on petabytes of real
data with billions of examples and parameters on problems
ranging from Sparse Logistic Regression to Latent
Dirichlet Allocation and Distributed Sketching."
"Large Scale Distributed Semi-Supervised Learning Using Streaming Approximation.  Traditional graph-based semi-supervised learning (SSL) approaches are not suited for massive data and large label scenarios since they scale linearly with the number of edges |E| and distinct labels m. To deal with the large label size problem, recent works propose sketch-based methods to approximate the label distribution per node thereby achieving a space reduction from O(m) to O(log m), under certain conditions. In this paper, we present a novel streaming graph-based SSL approximation that effectively captures the sparsity of the label distribution and further reduces the space complexity per node to O(1). We also provide a distributed version of the algorithm that scales well to large data sizes. Experiments on real-world datasets demonstrate that the new method achieves better performance than existing state-of-the-art algorithms with significant reduction in memory footprint. Finally, we propose a robust graph augmentation strategy using unsupervised deep learning architectures that yields further significant quality gains for SSL in natural language applications."
"??WTH..!?!?? Experiences, reactions, and expectations related to online privacy panic situations.  There are moments in which users might find themselves experiencing feelings of panic with the realization that their privacy or personal information on the Internet might be at risk. We present an exploratory study on common experiences of online privacy-related panic and on users?? reactions to frequently occurring privacy incidents. By using the metaphor of a privacy panic button, we also gather users?? expectations on the type of help that they would like to obtain in such situations. Through user interviews (n = 16) and a survey (n = 549), we identify 18 scenarios of privacy panic situations. We ranked these scenarios according to their frequency of occurrence and to the concerns of users to become victims of these incidents. We explore users?? underlying worries of falling pray for these incidents and other contextual factors common to privacy panic experiences. Based on our findings we present implications for the design of a help system for users experiencing privacy panic situations."
"(Smart) watch your taps: side-channel keystroke inference attacks using smartwatches.  In this paper, we investigate the feasibility of keystroke inference attacks on handheld numeric touchpads by using smartwatch motion sensors as a side-channel. The proposed attack approach employs supervised learning techniques to accurately map the uniqueness in the captured wrist movements to each individual keystroke. Experimental evaluation shows that keystroke inference using smartwatch motion sensors is not only fairly accurate, but also better than similar attacks previously demonstrated using smartphone motion sensors."
"""If You Put All The Pieces Together..."" - Attitudes Towards Data Combination and Sharing Across Services and Companies.  Online services often rely on processing users?? data, which can be either provided directly by the users or combined from other services. Although users are aware of the latter, it is unclear whether they are comfortable with such data combination, whether they view it as beneficial for them, or the extent to which they believe that their privacy is exposed. Through an online survey (N=918) and follow-up interviews (N=14), we show that (1) comfort is highly dependent on the type of data, type of service and on the existence of a direct relationship with a company, (2) users have a highly different opinion about the presence of benefits for them, irrespectively of the context, and (3) users perceive the combination of online data as more identifying than data related to offline and physical behavior (such as location). Finally, we discuss several strategies for companies to improve upon these issues"
"ESOMAR/GRBN Online research guideline.  This ESOMAR/GRBN Online Research Guideline is designed to help researchers address legal, ethical and practical considerations in using new technologies when conducting research online and is an update of guidance issued in 2011. To ensure that it is in line with most recent practice, in addition to other updated sections, this new draft Guideline also contains: New guidance on passive data collection requirements
A new section on incentives, sweepstakes and free prize draws
A new section on sample source and management
An updated section on specific online technologies such as tracking, cloud storage and static and dynamic IDs"
"HEADS: Headline Generation as Sequence Prediction Using an Abstract Feature-Rich Space.  Automatic headline generation is a sub-task of document summarization with many reported applications. In this study we present a sequence-prediction technique for learning how editors title their news stories. The introduced technique models the problem as a discrete
optimization task in a feature-rich space. In this space the global optimum can be found in polynomial time by means of dynamic programming. We train and test our model on an
extensive corpus of financial news, and compare it against a number of baselines by using standard metrics from the document summarization domain, as well as some new ones
proposed in this work. We also assess the readability and informativeness of the generated titles through human evaluation. The obtained results are very appealing and substantiate the soundness of the approach."
"A New Approach to Optimal Code Formatting.  The rfmt code formatter incorporates a new algorithm that optimizes code layout with respect to an intuitive notion of layout cost. This note describes the foundations of the algorithm, and the programming abstractions used to facilitate its use with a variety of languages and code 
layout policies."
"Confidentiality in the Face of Pervasive Surveillance: A Threat Model and Problem Statement.  Since the initial revelations of pervasive surveillance in 2013, several classes of attacks on Internet communications have been discovered. In this document, we develop a threat model that describes these attacks on Internet confidentiality. We assume an attacker that is interested in undetected, indiscriminate eavesdropping. The threat model is based on published, verified attacks."
"Guidelines and Registration Procedures for URI Schemes.  The Uniform Resource Identifier (URI) protocol element and generic syntax is defined by [RFC3986]. Each URI begins with a scheme name, as defined by Section 3.1 of RFC 3986, that refers to a specification for identifiers within that scheme. The URI syntax provides a federated and extensible naming system, where each scheme??s specification can further restrict the syntax and define the semantics of identifiers using that scheme.   This document provides updated guidelines for the definition of new schemes, for consideration by those who are defining, registering, or evaluating those definitions."
"??She??ll just grab any device that??s closer??: A Study of  Everyday Device &amp; Account Sharing in Households.  Many technologies assume a single user will use an account or device. But account and device sharing situations (when 2+ people use a single device or account) may arise during everyday life. We present results from a multiple-methods study of device and account sharing practices among household members and their relations. Among our findings are that device and account sharing was common, and mobile phones were often shared despite being considered ??personal?? devices. Based on our study results, we organize sharing practices into a taxonomy of six sharing types ?? distinct patterns of what, why, and how people shared. We also present two themes that cut across sharing types: that (1) trust in sharees and (2) convenience highly influenced sharing practices. Based on these findings, implications for study and technology design."
"Structured Transforms for Small-footprint Deep Learning.  We consider the task of building compact deep learning pipelines suitable for deployment
on storage and power constrained mobile devices. We propose a uni-
fied framework to learn a broad family of structured parameter matrices that are
characterized by the notion of low displacement rank. Our structured transforms
admit fast function and gradient evaluation, and span a rich range of parameter
sharing configurations whose statistical modeling capacity can be explicitly tuned
along a continuum from structured to unstructured. Experimental results show
that these transforms can significantly accelerate inference and forward/backward
passes during training, and offer superior accuracy-compactness-speed tradeoffs
in comparison to a number of existing techniques. In keyword spotting applications
in mobile speech recognition, our methods are much more effective than
standard linear low-rank bottleneck layers and nearly retain the performance of
state of the art models, while providing more than 3.5-fold compression."
"Software engineering for privacy in-the-large.  There will be an estimated 35 zettabytes (35?? 1021) of digital records worldwide 
by the year 2020. This effectively amounts to privacy management on an ultra-large-scale. In 
this briefing, we discuss the privacy challenges posed by such an ultralarge-scale 
ecosystem-we term this ??Privacy in the Large??. We will contrast existing approaches to 
privacy management, reflect on their strengths and limitations in this regard and outline key 
software engineering research and practice challenges to be addressed in the future."
"Inferring semantic mapping between policies and code: the clue is in the language.  A common misstep in the development of security and privacy solutions is the failure to keep the demands resulting from high-level policies in line with the actual implementation that is supposed to operationalize those policies. This is especially problematic in the domain of social networks, where software typically predates policies and then
evolves alongside its user base and any changes in policies that arise from their interactions with (and the demands that they place on) the system. Our contribution targets this specific problem, drawing together the assurances actually presented to users in the form of policies and
the large codebases with which developers work. We demonstrate that a mapping between policies and code can be inferred from the semantics of the natural language. These semantics manifest not only in the policy statements but also coding conventions. Our technique, implemented in
a tool (CASTOR ), can infer semantic mappings with F1 accuracy of 70% and 78% for two social networks, Diaspora and Friendica respectively ?? as compared with a ground truth mapping established through manual examination of the policies and code."
"The Anatomy of Smartphone Unlocking: A Field Study of Android Lock Screens.  To prevent unauthorized parties from accessing data stored on their smartphones, users have the option of enabling a ""lock screen"" that requires a secret code (e.g., PIN, drawing a pattern, or biometric) to gain access to their devices. We present a detailed analysis of the smartphone locking mechanisms currently available to billions of smartphone users worldwide. Through a month-long field study, we logged events from a panel of users with instrumented smartphones (N=134). We are able to show how existing lock screen mechanisms provide users with distinct tradeoffs between usability (unlocking speed vs. unlocking frequency) and security. We find that PIN users take longer to enter their codes, but commit fewer errors than pattern users, who unlock more frequently and are very prone to errors. Overall, PIN and pattern users spent the same amount of time unlocking their devices on average. Additionally, unlock performance seemed unaffected for users enabling the stealth mode for patterns. Based on our results, we identify areas where device locking mechanisms can be improved to result in fewer human errors - increasing usability - while also maintaining security."
"Keep on Lockin' in the Free World: A Multi-National Comparison of Smartphone Locking.  We present the results of an online survey of smartphone unlocking (N=8,286) that we conducted in eight different countries. The goal was to investigate differences in attitudes towards smartphone unlocking between different national cultures. Our results show that there are indeed significant differences across a range of categories. For instance, participants in Japan considered the data on their smartphones to be much more sensitive than those in other countries, and respondents in Germany were 4.5 times more likely than others to say that protecting data on their smartphones was important. The results of this study shed light on how motivations to use various security mechanisms are likely to differ from country to country."
"Combining landline and mobile phone samples A dual frame approach.  More and more households abandon their landline phones and rely solely on cell phones. This implies a
challenge for survey researchers: since the cell phone only households are not included in the frames for
landline telephone surveys, samples based on these frames are in danger to be seriously biased due to
undercoverage, if respondents who do not have a landline are systematically different from respondents
who have a landline. Thus, strategies for combining samples from different frames need to be developed.
In this paper we give theoretical foundations for a dual frame approach to sampling, explain how samples
can be optimally allocated from these two frames, and describe an empirical application of a survey
conducted in Germany that used a dual frame approach."
"Bayesian Dark Knowledge.  We consider the problem of Bayesian parameter estimation for deep neural networks, which is important in problem settings where we may have little data, and/ or where we need accurate posterior predictive densities, e.g., for applications involving bandits or active learning. One simple approach to this is to use online Monte Carlo methods, such as SGLD (stochastic gradient Langevin dynamics). Unfortunately, such a method needs to store many copies of the parameters (which wastes memory), and needs to make predictions using many versions of the model (which wastes time). 
We describe a method for ""distilling"" a Monte Carlo approximation to the posterior predictive density into a more compact form, namely a single deep neural network. We compare to two very recent approaches to Bayesian neural networks, namely an approach based on expectation propagation [Hernandez-Lobato and Adams, 2015] and an approach based on variational Bayes [Blundell et al., 2015]. Our method performs better than both of these, is much simpler to implement, and uses less computation at test time."
"Communicating Semantics: Reference by Description.  Messages often refer to entities such as people, places and events. Correct identification of the intended reference is an essential part of communication. Lack of shared unique names often complicates entity reference. Shared knowledge can be used to construct uniquely identifying descriptive references for entities with ambiguous names. We introduce a mathematical model for `Reference by Description', derive results on the conditions under which, with high probability, programs can construct unambiguous references to most entities in the domain of discourse and provide empirical validation of these results."
"An estimation-theoretic approach to video denoising.  A novel denoising scheme is proposed to fully exploit the spatio-temporal correlations of the video signal for efficient enhancement. Unlike conventional pixel domain approaches that directly connect motion compensated reference pixels and spatially neighboring pixels to build statistical models for noise filtering, this work first removes spatial correlations by applying transformations to both pixel blocks and performs estimation in the frequency domain. It is premised on the realization that the precise nature of temporal dependencies, which is entirely masked in the pixel domain by the statistics of the dominant low frequency components, emerges after signal decomposition and varies considerably across the spectrum. We derive an optimal non-linear estimator that accounts for both motion compensated reference and the noisy observations to resemble the original video signal per transform coefficient. It departs from other transform domain approaches that employ linear filters over a sizable reference set to reduce the uncertainty due to the random noise term. Instead it jointly exploits this precise statistical property appeared in the transform domain and the noise probability model in an estimation-theoretic framework that works on a compact support region. Experimental results provide evidence for substantial denoising performance improvement."
"End-to-End Text-Dependent Speaker Verification.  In this paper we present a data-driven, integrated approach to speaker verification, which maps a test utterance and a few reference utterances directly to a single score for verification and jointly optimizes the system??s components using the same evaluation protocol and metric as at test time. Such an approach will result in simple and efficient systems, requiring little domain-specific knowledge and making few model assumptions. We implement the idea by formulating the problem as a single neural network architecture, including the estimation of a speaker model on only a few utterances, and evaluate it on our internal ??Ok Google?? benchmark for text-dependent speaker verification. The proposed approach appears to be very effective for big data applications like ours that require highly accurate, easy-to-maintain systems with a small footprint."
"High-Availability at Massive Scale: Building Google??s Data Infrastructure for Ads.  Google??s Ads Data Infrastructure systems run the multi-
billion dollar ads business at Google. High availability and strong consistency are critical for these systems. While most distributed systems
handle machine-level failures well, handling datacenter-level failures is
less common. In our experience, handling datacenter-level failures is critical for running true high availability systems. Most of our systems (e.g.
Photon, F1, Mesa) now support multi-homing as a fundamental design property. Multi-homed systems run live in multiple datacenters all the time, adaptively moving load between datacenters, with the ability to handle outages of any scale completely transparently. This paper focuses primarily on stream processing systems, and describes our general approaches for building high availability multi-homed systems, discusses common challenges and solutions, and shares what we
have learned in building and running these large-scale systems for over ten years."
"Privacy Mediators: Helping IoT Cross the Chasm.  Unease over data privacy will retard consumer acceptance of IoT deployments. The primary source of discomfort is a lack of user control over raw data that is streamed directly from sensors to the cloud. This is a direct consequence of the over-centralization of today??s cloud-based IoT hub designs. We propose a solution that interposes a locally-controlled software component called a privacy mediator on every raw sensor stream. Each mediator is in the same administrative domain as the sensors whose data is being collected, and dynamically enforces the current privacy policies of the owners of the sensors or mobile users within the domain. This solution ne-
cessitates a logical point of presence for mediators within the admin-
istrative boundaries of each organization. Such points of presence are provided by cloudlets, which are small locally-administered data centers at the edge of the Internet that can support code mobility. The use of cloudlet-based mediators aligns well with natural personal and organizational boundaries of trust and responsibility."
"Position Auctions with Dynamic Resizing.  This paper analyzes mechanisms for selling advertising inventory in a position auction in which displaying less than the maximal number of ads means the ads that are shown can be dynamically resized and displayed more prominently. I characterize the optimal mechanism with and without dynamic resizing, and illustrate how the optimal reserve prices in a Vickrey??Clarke??Groves mechanism vary with the amount of dynamic resizing and the number of bidders."
"Directly Modeling Voiced and Unvoiced Components in Speech Waveforms by Neural Networks.  This paper proposes a novel acoustic model based on neural networks for statistical parametric speech synthesis. The neural network outputs parameters of a non-zero mean Gaussian process, which defines a probability density function of a speech waveform given linguistic features. The mean and covariance functions of the Gaussian process represent deterministic (voiced) and stochastic (unvoiced) components of a speech waveform, whereas the previous approach considered the unvoiced component only. Experimental results show that the proposed approach can generate speech waveforms approximating natural speech waveforms."
"Tunable Performance and Consistency Tradeoffs for Geographically Replicated Cloud Services (COLOR).  COLOR (client-oriented layered optimistic replication) is a combination of optimistic and conservative data replication that allows cloud services to be replicated across widely distributed locations without suffering from the latency overhead of strict algorithms, and with quantifiable and controllable tradeoffs between performance and consistency guarantees. The COLOR solution adopts a layered approach to enable optimistic delivery of client messages on top of any existing storage layer that manages the strict replication of the cloud service. When clients may be temporarily exposed to inconsistent states due to replication failures, such inconsistency is made recoverable similar to ""optimistic concurrency control"" for clients that cache the server state. COLOR supports different numeric parameters to trade the strict consistency for better performance to possibly match Eventual Consistency, while the end-to-end consistency is always guaranteed as the storage layer will never deliver any client messages generated from inconsistent states"
"Being an On-Call Engineer: A Google SRE Perspective.  Being on-call is a critical duty that many operations and engineering teams must undertake in order to keep their services reliable and available. However, there are several pitfalls in the organization of on-call rotations and responsibilities that can lead to serious consequences for the services and for the teams if not avoided. We provide the primary tenets of the approach to on-call that Google??s Site Reliability Engineers have developed over years, and explain how that approach has led to reliable services and sustainable workload over time."
"What is the Computational Value of Finite Range Tunneling?.  Quantum annealing (QA) has been proposed as a quantum enhanced optimization heuristic exploiting tunneling. Here, we demonstrate how finite-range tunneling can provide considerable computational advantage. For a crafted problem designed to have tall and narrow energy barriers separating local minima, the D-Wave 2X quantum annealer achieves significant runtime advantages relative to simulated annealing (SA). For instances with 945 variables, this results in a time-to-99%-success-probability that is ~1e8 times faster than SA running on a single processor core. We also compare physical QA with the quantum Monte Carlo algorithm, an algorithm that emulates quantum tunneling on classical processors. We observe a substantial constant overhead against physical QA: D-Wave 2X again runs up to ~ 1e8 times faster than an optimized implementation of the quantum Monte Carlo algorithm on a single core. We note that there exist heuristic classical algorithms that can solve most instances of Chimera structured problems in a time scale comparable to the D-Wave 2X. However, it is well known that such solvers will become ineffective for sufficiently dense connectivity graphs. To investigate whether finite-range tunneling will also confer an advantage for problems of practical interest, we conduct numerical studies on binary optimization problems that cannot yet be represented on quantum hardware. For random instances of the number partitioning problem, we find numerically that algorithms designed to simulate QA scale better than SA. We discuss the implications of these findings for the design of next-generation quantum annealers."
Silicon Photonics Technologies: Gaps Analysis for Datacenter Interconnects.  We give an overview of optical interconnect requirements for large scale datacenters. We then make a comparison between silicon photonics technologies and more traditional options in meeting these requirements.
"Wikipedia Tools for Google Spreadsheets.  In this paper, we introduce the Wikipedia Tools for Google Spreadsheets. Google Spreadsheets is part of a free, Web-based software office suite offered by Google within its Google Docs service. It allows users to create and edit spreadsheets online, while collaborating with other users in realtime. Wikipedia is a free-access, free-content Internet encyclopedia, whose content and data is available, among other means, through an API. With the Wikipedia Tools for Google Spreadsheets, we have created a toolkit that facilitates working with Wikipedia data from within a spreadsheet context. We make these tools available as open-source on GitHub [https://github.com/tomayac/wikipedia-tools-for-google-spreadsheets], released under the permissive Apache 2.0 license."
"From Freebase to Wikidata: The Great Migration.  Collaborative knowledge bases that make their data freely available in a machine-readable form are central for the data strategy of many projects and organizations. The two major collaborative knowledge bases are Wikimedia??s Wikidata and Google??s Freebase. Due to the success of Wikidata, Google decided in 2014 to offer the content of Freebase to the Wikidata community. In this paper, we report on the ongoing transfer efforts and data mapping challenges, and provide an analysis of the effort so far. We describe the Primary Sources Tool, which aims to facilitate this and future data migrations. Throughout the migration, we have gained deep insights into both Wikidata and Freebase, and share and discuss detailed statistics on both knowledge bases."
"Fast k-best Sentence Compression.  A popular approach to sentence compression is to formulate the task as a constrained optimization problem and solve it with integer linear programming (ILP) tools. Unfortunately, dependence on ILP may make the compressor prohibitively slow, and thus approximation techniques have been proposed which are often complex and offer a moderate gain in speed. As an alternative solution, we introduce a novel compression algorithm which generates k-best compressions relying on local deletion decisions. Our algorithm is two orders of magnitude faster than a recent ILP-based method while producing better compressions. Moreover, an extensive evaluation demonstrates that the quality of compressions does not degrade much as we move from single best to top-five results."
"Communication-Efficient Learning of Deep Networks from Decentralized Data.  Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent."
"Label Transition and Selection Pruning and Automatic Decoding Parameter Optimization for Time-Synchronous Viterbi Decoding.  Hidden Markov Model (HMM)-based classifiers have been successfully used for sequential labeling problems such as speech recognition and optical character recognition for decades. They have been especially successful in the domains where the segmentation is not known or difficult to obtain, since, in principle, all possible segmentation points can be taken into account. However, the benefit comes with a non-negligible computational cost. In this paper, we propose simple yet effective new pruning algorithms to speed up decoding with HMM-based classifiers of up to 95% relative over a baseline. As the number of tunable decoding parameters increases, it becomes more difficult to optimize the parameters for each configuration. We also propose a novel technique to estimate the parameters based on a loss value without relying on a grid search."
"Maglev: A Fast and Reliable Software Network Load Balancer.  Maglev is Google??s network load balancer. It is a large distributed software system that runs on commodity Linux servers. Unlike traditional hardware network load balancers,  it does not require a specialized physical rack deployment, and its capacity can be easily adjusted by adding or removing servers. Network routers distribute packets evenly to the Maglev machines via Equal Cost Multipath (ECMP); each Maglev machine then matches the packets to their corresponding services and spreads them evenly to the service endpoints. To accommodate high and ever-increasing traffic, Maglev is specifically optimized for packet processing performance. A single Maglev machine is able to saturate a 10Gbps link with small packets. Maglev is also equipped with consistent hashing and connection tracking features, to minimize the negative impact of unexpected faults and failures on connection-oriented protocols. Maglev has been serving Google's traffic since 2008. It has sustained the rapid global growth of Google services, and it also provides network load balancing for Google Cloud Platform."
"Enhancing Cross-Device Interaction Scripting with Interactive Illustrations.  Cross-device interactions involve input and output on multiple computing devices. Implementing and reasoning about interactions that cover multiple devices with a diversity of form factors and capabilities can be complex. To assist developers in programming cross-device interactions, we created DemoScript, a technique that automatically analyzes a cross-device interaction program while it is being written. DemoScript visually illustrates the step-by-step execution of a selected portion or the entire program with a novel, automatically generated cross-device storyboard visualization. In addition to helping developers understand the behavior of the program, DemoScript also allows developers to revise their program by interactively manipulating the cross-device storyboard. We evaluated DemoScript with 8 professional programmers and found that DemoScript significantly improved development efficiency by helping developers interpret and manage cross-device interaction; it also encourages testing to think through the script in a development process."
"Disks for Data Centers.  Disks form the central element of Cloud-based storage, whose demand far outpaces the considerable rate of innovation in disks. Exponential growth in demand, already in progress for 15+ years, implies that most future disks will be in data centers and thus part of a large collection of disks. We describe the ??collection view?? of disks and how it and the focus on tail latency, driven by live services, place new and different requirements on disks. Beyond defining key metrics for data-center disks, we explore a range of new physical design options and changes to firmware that could improve these metrics. We hope this is the beginning of a new era of ??data center?? disks and a new broad and open discussion about how to evolve disks for data centers.  The ideas presented here provide some guidance and some options, but we believe the best solutions will come from the combined efforts of industry, academia and other large customers."
"Optimal trajectory control for parallel single phase H-bridge inverters.  We describe a novel inverter control method that solves an optimization problem during each switching interval to closely follow a virtual impedance control law.
We report droop behavior over a wide range of applied loads and power sharing among multiple inverters."
"Reusable Components of Semantic Specifications.  Semantic specifications of programming languages typically have poor modularity. This hinders reuse of parts of the semantics of one language when specifying a different language ?? even when the two languages have many constructs in common ?? and evolution of a language may require major reformulation of its semantics. Such drawbacks have discouraged language developers from using formal semantics to document their designs.
In the PLanCompS project, we have developed a component-based approach to semantics. Here, we explain its modularity aspects, and present an illustrative case study: a component-based semantics for Caml Light. We have tested the correctness of the semantics by running programs on an interpreter generated from the semantics, comparing the output with that produced on the standard implementation of the language.
Our approach provides good modularity, facilitates reuse, and should support co-evolution of languages and their formal semantics. It could be particularly useful in connection with domain-specific languages and language-driven software development."
SAC073 - SSAC Comments on Root Zone Key Signing Key Rollover Plan.  SSAC Comments on the Design Teams Draft Report on the Root Zone Key Signing Key Rollover Plan.
"SAC078 - SSAC Advisory on Uses of the Shared Global Domain Name Space.  It is widely known that the Domain Name System (DNS) includes both a set of rules for constructing syntactically valid domain names (the ??domain name space??) and a protocol for associating domain names with data such as IP addresses (??domain name resolution??). It is less widely understood, however, that DNS name resolution coexists with other name resolution systems that also use domain names. In many cases these other name resolution systems deliberately use domain names, rather than some other naming scheme, for compatibility with the widely deployed infrastructure of the DNS. They depend on the ability of DNS name resolution protocols and interface conventions to recognize their domain names but treat them in some special way.
...
The SSAC wishes to ensure that the ICANN Board and ICANN community are aware of discussions and ongoing work in multiple venues to more fully define what a namespace is, and how to avoid potential side effects, including name collisions, across the broad set of name resolution systems and expectations for their use."
"SAC079 - SSAC Advisory on the Changing Nature of IPv4 Address Semantics.  In this advisory, the SSAC considers the changing role of Internet Protocol Version 4 (IPv4) addresses caused by the increasing scarcity, and subsequent exhaustion, of IPv4 addresses. The exhaustion of the IPv4 address supply has been predicted since the end of the 1980s. However, the large scale adoption of mobile devices and their associated IPv4 addressing needs accelerated the exhaustion timetable, and placed increased pressure on network operators to conserve IPv4 addresses. This pressure has resulted in a marked increase in the use of Network Address Translation (NAT) technologies, altering the attributability characteristics of IPv4 addresses, and requiring changes to their interpretation by parties wishing to use them as endpoint identifiers."
"Borg, Omega, and   Kubernetes.  Lessons learned from three container management systems over a decade."
"Variable Rate Image Compression with Recurrent Neural Networks.  A large fraction of Internet traffic is now driven by requests from mobile devices with relatively small screens and often stringent bandwidth requirements. Due to these factors, it has become the norm for modern graphics-heavy websites to transmit low-resolution, low-bytecount image previews (thumbnails) as part of the initial page load process to improve apparent page responsiveness. Increasing thumbnail compression beyond the capabilities of existing codecs is therefore a current research focus, as any byte savings will significantly enhance the experience of mobile device users. Toward this end, we propose a general framework for variable-rate image compression and a novel architecture based on convolutional and deconvolutional LSTM recurrent networks. Our models address the main issues that have prevented autoencoder neural networks from competing with existing image compression algorithms: (1) our networks only need to be trained once (not per-image), regardless of input image dimensions and the desired compression rate; (2) our networks are progressive, meaning that the more bits are sent, the more accurate the image reconstruction; and (3) the proposed architecture is at least as efficient as a standard purpose-trained autoencoder for a given number of bits. On a large-scale benchmark of 32??32 thumbnails, our LSTM-based approaches provide better visual quality than (headerless) JPEG, JPEG2000 and WebP, with a storage size that is reduced by 10% or more."
"Improper Deep Kernels.  Neural networks have recently re-emerged as a powerful hypothesis class, yielding impressive classification accuracy in multiple domains. However, their training is a non-convex optimization problem which poses theoretical and practical challenges. Here we address this difficulty by turning to ``improper'' learning of neural nets. In other words, we learn a classifier that is not a neural net but is competitive with the best neural net model given a sufficient number of training examples. Our approach relies on a novel kernel construction scheme in which the kernel is a result of integration over the set of all possible instantiation of neural models. It turns out that the corresponding integral can be evaluated in closed-form via a simple recursion. Thus we translate the non-convex, hard learning problem of a neural net to a SVM with an appropriate kernel. We also provide sample complexity results which depend on the stability of the optimal neural net."
"Transforming Dependency Structures to Logical Forms for Semantic Parsing.  The strongly typed syntax of grammar formalisms such as CCG, TAG, LFG and HPSG offers a synchronous framework for deriving syntactic
structures and semantic logical forms.  In contrast - partly due to the lack of a strong type system - dependency structures are easy to annotate and have become a widely used form of syntactic analysis for many languages.  However, the lack of a type system makes a formal
mechanism for deriving logical forms from dependency structures challenging.  We address this by introducing a robust system based on the lambda calculus for deriving neo-Davidsonian logical forms from dependency trees.  These logical forms are then used for semantic parsing of natural language to Freebase. Experiments on the Free917 and WebQuestions datasets show that our representation is superior to the original dependency trees and that it outperforms a CCG-based representation on this task. Compared to prior work, we obtain the strongest result to date on Free917 and competitive results on WebQuestions."
"What Teachers Should Know about the Bootstrap: Resampling in the Undergraduate Statistics Curriculum.  Bootstrapping has enormous potential in statistics education and practice, but there are subtle issues and ways to go wrong. For example, the common combination of nonparametric bootstrapping and bootstrap percentile confidence intervals is less accurate than using t-intervals for small samples, though more accurate for larger samples. My goals in this article are to provide a deeper understanding of bootstrap methods??how they work, when they work or not, and which methods work better??and to highlight pedagogical issues. Supplementary materials for this article are available online."
"VOICE MORPHING THAT IMPROVES TTS QUALITY USING AN OPTIMAL DYNAMIC  FREQUENCY WARPING-AND-WEIGHTING TRANSFORM.  Dynamic Frequency Warping (DFW) is widely used to align spec-
tra of different speakers. It has long been argued that frequency warping captures inter-speaker differences but DFW practice always involves a tricky preprocessing part to remove spectral tilt. The DFW residual is successfully used in Voice Morphing to improve the quality and the similarity of synthesized speech but the estimation of the DFW residual remains largely heuristic and sub-optimal This paper presents a dynamic programming algorithm that simultaneously estimates the Optimal Frequency Warping and Weighting transform (ODFWW) and therefore needs no preprocessing step and fine-tuning while source/target-speaker data are matched using the Matching-Minimization algorithm [1]. The transform is used to morph the output of a state-of-the-art Vocaine-based [2] TTS synthesizer in order to generate different voices in runtime with only +8% computational overhead. Some morphed TTS voices exhibit significantly higher quality than the original one as morphing seems to ??correct?? the voice characteristics of the TTS voice."
"THE MATCHING-MINIMIZATION ALGORITHM, THE INCA ALGORITHM AND A  MATHEMATICAL FRAMEWORK FOR VOICE CONVERSION WITH UNALIGNED  CORPORA..  This paper presents a mathematical framework that is suitable for voice conversion and adaptation in speech processing. Voice conversion is formulated as a search for the optimal correspondances between a set of source-speaker spectra and a set of target-speaker spectra under a transform that compensates speaker differences. It is possible to simultaneously recover a bi-directional mapping between two sets of vectors that is a parametric mapping (a transform) in one direction and a non-parametric mapping (correspondences) in the reverse direction. An algorithm referred to as Matching-Minimization (MM) is formally derived with proven convergence and an optimal closed-form solution for each step. The algorithm is closely related to the asymmetric-1 variant of the well-known INCA algorithm [1] for which we also provide a proof within the same framework. The differences between MM and INCA are delineated both theoretically and experimentally. MM outperforms INCA in all scenarios. Like INCA, MM does not require parallel corpora. Unlike INCA, MM is suitable when only a few adaptation data are available."
"A FREQUENCY-WEIGHTED POST-FILTERING TRANSFORM FOR COMPENSATION OF  THE OVER-SMOOTHING EFFECT IN HMM-BASED SPEECH SYNTHESIS.  Over-smoothing is one of the major sources of quality degradation in statistical parametric speech synthesis. Many methods have been proposed to compensate over-smoothing with the speech parameter generation algorithm considering Global Variance (GV) being one of the most successfull. This paper models over-smoothing as a radial relocation of poles and zeros of the spectral envelope towards the origin of the z-plane and uses radial scaling to enhance spectral peaks and to deepen spectral valeys. The radial scaling technique is improved by introducing over-emphasis, spectral-tilt compensation and frequency weighting. Listening test results indicate that the proposed method is 11%-13% more preferable than GV while it has less algorithmic delay (only 5 ms) and computational complexity."
"An Empirical Study of Practitioners?? Perspectives on Green Software Engineering.  The energy consumption of software is an increasing concern as the use of mobile applications, embedded systems, and data center-based services expands. While research in green software engineering is correspondingly increasing, little is known about the current practices and perspectives of software engineers in the field. This paper describes the first empirical study of how practitioners think about energy when they write requirements, design, construct, test, and maintain their software. We report findings from a quantitative, targeted survey of 464 practitioners from ABB, Google, IBM, and Microsoft, which was motivated by and supported with qualitative data from 18 in-depth interviews with Microsoft employees. The major findings and implications from the collected data contextualize existing green software engineering research and suggest directions for researchers aiming to develop strategies and tools to help practitioners improve the energy usage of their applications."
"Order matters: Sequence to sequence for sets.  Sequences have become first class citizens in supervised learning thanks to the resurgence of recurrent neural networks. Many complex tasks that require mapping from or to a sequence of observations can now be formulated with the sequence-to-sequence (seq2seq) framework which employs the chain rule to efficiently represent the joint probability of sequences. In many cases, however, variable sized inputs and/or outputs might not be naturally expressed as sequences. For instance, it is not clear how to input a set of numbers into a model where the task is to sort them; similarly, we do not know how to organize outputs when they correspond to random variables and the task is to model their unknown joint probability. In this paper, we first show using various examples that the order in which we organize input and/or output data matters significantly when learning an underlying model. We then discuss an extension of the seq2seq framework that goes beyond sequences and handles input sets in a principled way. In addition, we propose a loss which, by searching over possible orders during training, deals with the lack of structure of output sets. We show empirical evidence of our claims regarding ordering, and on the modifications to the seq2seq framework on benchmark language modeling and parsing tasks, as well as two artificial tasks - sorting numbers and estimating the joint probability of unknown graphical models."
"SSD: Single Shot MultiBox Detector.  We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of bounding box priors over different aspect ratios and scales per feature map location. At prediction time, the network generates confidences that each prior corresponds to objects of interest and produces adjustments to the prior to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that requires object proposals, such as R-CNN and MultiBox, because it completely discards the proposal generation step and encapsulates all the computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on ILSVRC DET and PASCAL VOC dataset confirm that SSD has comparable performance with methods that utilize an additional object proposal step and yet is 100-1000x faster. Compared to other single stage methods, SSD has similar or better performance, while providing a unified framework for both training and inference."
"Distilling the Knowledge in a Neural Network.  A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
"Achieving Rapid Response Times in Large Online Services.  Today??s large-scale web services provide rapid responses to interactive requests by applying large amounts of computational resources to massive datasets. They typically operate in warehouse-sized datacenters and run on clusters of machines that are shared across many kinds of interactive and batch jobs. As these systems distribute work to ever larger numbers of machines and sub-systems in order to provide interactive response times, it becomes increasingly difficult to tightly control latency variability across these machines, and often the 95%ile and 99%ile response times suffer in an effort to improve average response times. As systems scale up, simply stamping out all sources of variability does not work. Just as fault-tolerant techniques needed to be developed when guaranteeing fault-free operation by design became unfeasible, techniques that deliver predictably low service-level latency in the presence of highly-variable individual components are increasingly important at larger scales. In this talk, I??ll describe a collection of techniques and practices lowering response times in large distributed systems whose components run on shared clusters of machines, where pieces of these systems are subject to interference by other tasks, and where unpredictable latency hiccups are the norm, not the exception. Some of the techniques adapt to trends observed over periods of a few minutes, making them effective at dealing with longer-lived interference or resource contention. Others react to latency anomalies within a few milliseconds, making them suitable for mitigating variability within the context of a single interactive request. I??ll discuss examples of how these techniques are used in various pieces of Google??s systems infrastructure and in various higher-level online services."
"Distributed Representations of Words and Phrases and their Compositionality.  The recently introduced continuous Skip-gram model is an efficient method for
learning high-quality distributed vector representations that capture a large number
of precise syntactic and semantic word relationships. In this paper we present
several extensions that improve both the quality of the vectors and the training
speed. By subsampling of the frequent words we obtain significant speedup and
also learn more regular word representations. We also describe a simple alternative
to the hierarchical softmax called negative sampling.
An inherent limitation of word representations is their indifference to word order
and their inability to represent idiomatic phrases. For example, the meanings of
??Canada?? and ??Air?? cannot be easily combined to obtain ??Air Canada??. Motivated
by this example, we present a simple method for finding phrases in text, and show
that learning good vector representations for millions of phrases is possible."
"Evolution and Future Directions of Large-scale Storage and Computation Systems at Google.  Underlying the many products and services offered by Google is a collection of systems and tools that simplify the storage and processing of large-scale data sets. These systems are intended to work well in Google's computational environment of large numbers of commodity machines connected by commodity networking hardware. Our systems handle issues like storage reliability and availability in the face of machine failures, and our processing tools make it relatively easy to write robust computations that run reliably and efficiently on thousands of machines. In this talk I'll highlight some of the systems we have built and are currently developing, and discuss some challenges and future directions for new systems."
"Attention for fine-grained categorization.  This paper presents experiments extending the work of Ba et al. (2014) on recurrent neural models for attention into less constrained visual environments, specifically fine-grained categorization on the Stanford Dogs data set. In this work we use an RNN of the same structure but substitute a more powerful visual network and perform large-scale pre-training of the visual network outside of the attention RNN. Most work in attention models to date focuses on tasks with toy or more constrained visual environments, whereas we present results for fine-grained categorization better than the state-of-the-art GoogLeNet classification model. We show that our model learns to direct high resolution attention to the most discriminative regions without any spatial supervision such as bounding boxes, and it is able to discriminate fine-grained dog breeds moderately well even when given only an initial low-resolution context image and narrow, inexpensive glimpses at faces and fur patterns. This and similar attention models have the major advantage of being trained end-to-end, as opposed to other current detection and recognition pipelines with hand-engineered components where information is lost. While our model is state-of-the-art, further work is needed to fully leverage the sequential input."
"Document embedding with paragraph vectors.  Paragraph Vectors has been recently proposed as an unsupervised method for learning distributed representations for pieces of texts. In their work, the authors showed that the method can learn an embedding of movie review texts which can be leveraged for sentiment analysis. That proof of concept, while encouraging, was rather narrow. Here we consider tasks other than sentiment analysis, provide a more thorough comparison of Paragraph Vectors to other document modelling algorithms such as Latent Dirichlet Allocation, and evaluate performance of the method as we vary the dimensionality of the learned representation. We benchmarked the models on two document similarity data sets, one from Wikipedia, one from arXiv. We observe that the Paragraph Vector method performs significantly better than other methods, and propose a simple improvement to enhance embedding quality. Somewhat surprisingly, we also show that much like word embeddings, vector operations on Paragraph Vectors can perform useful semantic results."
"Deep Networks With Large Output Spaces.  Deep neural networks have been extremely successful at various image, speech, video recognition tasks because of their ability to model deep structures within the data. However, they are still prohibitively expensive to train and apply for problems containing millions of classes in the output layer. Based on the observation that the key computation common to most neural network layers is a vector/matrix product, we propose a fast locality-sensitive hashing technique to approximate the actual dot product enabling us to scale up the training and inference to millions of output classes. We evaluate our technique on three diverse large-scale recognition tasks and show that our approach can train large-scale models at a faster rate (in terms of steps/total time) compared to baseline methods."
"Rethinking the Inception Architecture for Computer Vision.  Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set."
"Adversarial Autoencoders.  Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set."
"Remedying Web Hijacking: Notification Effectiveness and Webmaster Comprehension.  As miscreants routinely hijack thousands of vulnerable web servers weekly for cheap hosting and traffic acquisition, security services have turned to notifications both to alert webmasters of ongoing incidents as well as to expedite recovery. In this work we present the first large-scale measurement study on the effectiveness of combinations of browser, search, and direct webmaster notifications at reducing the duration a site remains compromised. Our study captures the life cycle of 760,935 hijacking incidents from July, 2014?? June, 2015, as identified by Google Safe Browsing and Search Quality. We observe that direct communication with webmasters increases the likelihood of cleanup by over 50% and reduces infection lengths by at least 62%. Absent this open channel for communication, we find browser interstitials??while intended to alert visitors to potentially harmful content??correlate with faster remediation. As part of our study, we also explore whether webmasters exhibit the necessary technical expertise to address hijacking incidents. Based on appeal logs where webmasters alert Google that their site is no longer compromised, we find 80% of operators successfully clean up symptoms on their first appeal. However, a sizeable fraction of site owners do not address the root cause of compromise, with over 12% of sites falling victim to a new attack within 30 days. We distill these findings into a set of recommendations for improving web security and best practices for webmasters."
"A Neural Conversational Model.  Conversational modeling is an important task in natural language understanding and machine intelligence. Although previous approaches exist, they are often restricted to specific domains (e.g., booking an airline ticket) and require hand-crafted rules. In this paper, we present a simple approach for this task which uses the recently proposed sequence to sequence framework. Our model converses by predicting the next sentence given the previous sentence or sentences in a conversation. The strength of our model is that it can be trained end-to-end and thus requires much fewer hand-crafted rules. We find that this straightforward model can generate simple conversations given a large conversational training dataset. Our preliminary results suggest that, despite optimizing the wrong objective function, the model is able to converse well. It is able extract knowledge from both a domain specific dataset, and from a large, noisy, and general domain dataset of movie subtitles. On a domain-specific IT helpdesk dataset, the model can find a solution to a technical problem via conversations. On a noisy open-domain movie transcript dataset, the model can perform simple forms of common sense reasoning. As expected, we also find that the lack of consistency is a common failure mode of our model."
"Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition.  We present Listen, Attend and Spell (LAS), a neural network that learns to transcribe speech utterances to characters. Unlike traditional DNN-HMM models, this model learns all the components of a speech recognizer jointly. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention based recurrent network decoder that emits characters as outputs. The network produces character sequences without making any independence assumptions between the characters. This is the key improvement of LAS over previous end-to-end CTC models. On a subset of the Google voice search task, LAS achieves a word error rate (WER) of 14.1% without a dictionary or a language model, and 10.3% with language model rescoring over the top 32 beams. By comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0%."
"Neural Programmer: Inducing Latent Programs with Gradient Descent.  Deep neural networks have achieved impressive supervised classification performance in many tasks including image recognition, speech recognition, and sequence to sequence learning. However, this success has not been translated to applications like question answering that may involve complex arithmetic and logic reasoning. A major limitation of these models is in their inability to learn even simple arithmetic and logic operations. For example, it has been shown that neural networks fail to learn to add two binary numbers reliably. In this work, we propose Neural Programmer, an end-to-end differentiable neural network augmented with a small set of basic arithmetic and logic operations. Neural Programmer can call these augmented operations over several steps, thereby inducing compositional programs that are more complex than the built-in operations. The model learns from a weak supervision signal which is the result of execution of the correct program, hence it does not require expensive annotation of the correct program itself. The decisions of what operations to call, and what data segments to apply to are inferred by Neural Programmer. Such decisions, during training, are done in a differentiable fashion so that the entire network can be trained jointly by gradient descent. We find that training the model is difficult, but it can be greatly improved by adding random noise to the gradient. On a fairly complex synthetic table-comprehension dataset, traditional recurrent networks and attentional models perform poorly while Neural Programmer typically obtains nearly perfect accuracy."
"Multi-task Sequence to Sequence Learning.  Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. To date, most of its applications focused on only one task and not much work explored this framework for multiple tasks. This paper examines three multi-task learning (MTL) settings for sequence to sequence models: (a) the oneto-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder can be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting - where multiple encoders and decoders are shared, which is the case with unsupervised objectives and translation. Our results show that training on a small amount of parsing and image caption data can improve the translation quality between English and German by up to 1.5 BLEU points over strong single-task baselines on the WMT benchmarks. Furthermore, we have established a new state-of-the-art result in constituent parsing with 93.0 F1. Lastly, we reveal interesting properties of the two unsupervised learning objectives, autoencoder and skip-thought, in the MTL context: autoencoder helps less in terms of perplexities but more on BLEU scores compared to skip-thought."
"Addressing the Rare Word Problem in Neural Machine Translation.  Neural Machine Translation (NMT) is a new approach to machine translation that has shown promising results that are comparable to traditional approaches. A significant
weakness in conventional NMT systems is their inability to correctly translate very rare words: end-to-end NMTs tend to have relatively small vocabularies with a single unk symbol that represents every possible out-of-vocabulary (OOV) word. In this paper, we propose and implement an effective technique to address this problem. We train an NMT system on data that is augmented by the output
of a word alignment algorithm, allowing the NMT system to emit, for each OOV word in the target sentence, the position of its corresponding word in the source sentence.
This information is later utilized in a post-processing step that translates every OOV word using a dictionary. Our experiments on the WMT??14 English to French translation task show that this method provides a substantial improvement of up to 2.8 BLEU points over an equivalent NMT
system that does not use this technique. With 37.5 BLEU points, our NMT system is the first to surpass the best result achieved on a WMT??14 contest task."
"Distributed Representations of Sentences and Documents.  Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering
of the words and they also ignore semantics of the words. For example, ??powerful,?? ??strong?? and ??Paris?? are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm
represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-ofwords models. Empirical results show that Paragraph
Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks."
"Exploiting Similarities among Languages for Machine Translation.  Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90% precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs."
"Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis.  Previous work on action recognition has focused on
adapting hand-designed local features, such as SIFT or
HOG, from static images to the video domain. In this paper,
we propose using unsupervised feature learning as a
way to learn features directly from video data. More specifically, we present an extension of the Independent Subspace Analysis algorithm to learn invariant spatio-temporal features from unlabeled video data. We discovered that, despite its simplicity, this method performs surprisingly well when combined with deep learning techniques such as stacking and convolution to learn hierarchical representations. By replacing hand-designed features with our learned features, we achieve classification results superior to all previous published results on the Hollywood2, UCF, KTH and YouTube action recognition datasets. On the challenging Hollywood2 and YouTube action datasets we obtain 53.3% and 75.8% respectively, which are approximately 5% better than the current best published results. Further benefits of this method, such as the ease of training and the efficiency of training and prediction, will also be discussed. You can download our code and learned spatio-temporal features here: http://ai.stanford.edu/???wzou/"
"Provisioning 1 Gb/s symmetrical services with next-generation passive optical network technologies.  Service providers spend billions upgrading their broadband access networks to the latest access standards. Fiber has become the technology of choice in the medium and long term, thanks to its speed, reach, and future-proofness. A differential advantage of fiber over other broadband access technologies is that it makes it possible for operators to deliver symmetric-rate services. Most of today's commercial offers based on regular PON range from 10 to 100 Mb/s of committed information rate, and higher rates are advertised as peak rates with unspecified guarantees. In this article we focus on delivering symmetrical 1 Gb/s access to residential users with a target temporal guarantee at the least cost using next-generation PON technologies. We compare four NG-PON standard access technologies, GPON, XGPON, WDM-PON, and the emerging TWDM-PON, from technical and economic perspectives. The study shows that if a service provider wants to keep up with the growing user traffic pattern in the long run, only TWDM-PON can provide 1 Gb/s nearly guaranteed at a moderate cost with respect to the fully dedicated 1 Gb/s point-to-point connection available in WDM-PON technologies."
"Planning, Optimization and Operation of Access and Ethernet  Optical Networks for the Provisioning of High-Speed  Symmetrical Services.  Today more than ever before, service providers are under enormous pressure to reduce operational costs while increasing business effectiveness. This often means optimizing cur-
rent infrastructure investments and implementing novel networking technologies. Customer demand is quickly evolving away from traditional one-way content consumption to a much more participative social network model that demands high capacity bidirectional information flows. Ethernet-based networking technology has become ubiquitous in both the enterprise and home broadband arenas. The combination of simplicity and rigorous specification has permitted a degree of integration and commoditization that other networking technologies could not achieve. On the other hand, optical fiber has become the technology of choice in the medium and long term in the access and metro networks, thanks to its speed, reach and future-proof. While it seems clear that fiber is the right technology to support emerging services in the access network, there are a large variety of technologies available in the industry. This thesis reviews the main technologies available today for access and metro networks, and proposes contributions about network planning and optimization. Firstly, main innovations added to Ethernet are analyzed, namely improvements related to scalability, OAM functionality and forwarding capabilities, in order to permit Ethernet to assume a much larger role in metro networks. After that, some of the Ethernet enhancements previously discussed are applied to access networks. Specifically, physical and link-layer information are combined in order to effectively set management procedures for passive optical networks (PON). We propose an Integrated Troubleshooting Box (ITB) and show its applicability in a number of realistic troubleshooting scenarios, including failure situations involving either the feeder fibre or one of its branches. Secondly, this thesis explores fiber access protocols available in the industry to support high-speed symmetrical services. It compares four Next-Generation PON protocols from a performance and economical perspective in a real scenario, and analyzes under which conditions 1 Gb/s symmetrical services can operate by selecting the right parameters of quality of service, oversubscription and split ratio. Finally, a novel access network planning method is proposed to provide 1Gb/s symmetrical services in a mixed environment of business and residential services. In this case, a converged cost-optimized access network is implemented, using Integer Linear Programming, which guarantees respective service level specifications in a real scenario."
"A Simple Way to Initialize Recurrent Networks of Rectified Linear Units.  Learning long term dependencies in recurrent networks is difficult due to vanishing
and exploding gradients. To overcome this difficulty, researchers have developed
sophisticated optimization techniques and network architectures. In this
paper, we propose a simpler solution that use recurrent neural networks composed
of rectified linear units. Key to our solution is the use of the identity matrix or its
scaled version to initialize the recurrent weight matrix. We find that our solution is
comparable to a standard implementation of LSTMs on our four benchmarks: two
toy problems involving long-range temporal structures, a large language modeling
problem and a benchmark speech recognition problem."
"Guest Editorial: Deep Learning.  Deep Learning methods aim at learning feature hierarchies.
Applications of deep learning to vision tasks date back to convolutional
networks in the early 1990s. These methods have
been the subject of a recent surge of interest for two main
reasons: when labeled data is scarce, unsupervised learning
algorithms can learn useful feature hierarchies. When
labeled data is abundant, supervised methods can be used
to train very large networks on very large datasets through
the use of high-performance computers. Such large networks
have been shown to outperform previous state-of-theart
methods on several perceptual tasks, including categorylevel
object recognition, object detection and semantic
segmentation."
"Adding Gradient Noise Improves Learning for Very Deep Networks.  Deep feedforward and recurrent networks have achieved impressive results in many perception and language processing applications. This success is partially attributed to architectural innovations such as convolutional and long short-term memory networks. The main motivation for these architectural innovations is that they capture better domain knowledge, and importantly are easier to optimize than more basic architectures. Recently, more complex architectures such as Neural Turing Machines and Memory Networks have been proposed for tasks including question answering and general computation, creating a new set of optimization challenges. In this paper, we discuss a low-overhead and easy-to-implement technique of adding gradient noise which we find to be surprisingly effective when training these very deep architectures. The technique not only helps to avoid overfitting, but also can result in lower training loss. This method alone allows a fully-connected 20-layer deep network to be trained with standard gradient descent, even starting from a poor initialization. We see consistent improvements for many complex models, including a 72% relative reduction in error rate over a carefully-tuned baseline on a challenging question-answering task, and a doubling of the number of accurate binary multiplication models learned across 7,000 random restarts. We encourage further application of this technique to additional complex modern architectures."
"Neural GPUs Learn Algorithms.  Learning an algorithm from examples is a fundamental problem that has been
widely studied. It has been addressed using neural networks too, in particular by
Neural Turing Machines (NTMs). These are fully differentiable computers that
use backpropagation to learn their own programming. Despite their appeal NTMs
have a weakness that is caused by their sequential nature: they are not parallel and
are are hard to train due to their large depth when unfolded.
We present a neural network architecture to address this problem: the Neural
GPU. It is based on a type of convolutional gated recurrent unit and, like the
NTM, is computationally universal. Unlike the NTM, the Neural GPU is highly
parallel which makes it easier to train and efficient to run.
An essential property of algorithms is their ability to handle inputs of arbitrary
size. We show that the Neural GPU can be trained on short instances of an algorithmic
task and successfully generalize to long instances. We verified it on a
number of tasks including long addition and long multiplication of numbers represented
in binary. We train the Neural GPU on numbers with up-to 20 bits and
observe no errors whatsoever while testing it, even on much longer numbers.
To achieve these results we introduce a technique for training deep recurrent networks:
parameter sharing relaxation. We also found a small amount of dropout
and gradient noise to have a large positive effect on learning and generalization."
"Graph Searching Games and Width Measures for Directed Graphs.  In cops and robber games a number of cops tries to capture a robber in a graph. A variant of
these games on undirected graphs characterises tree width by the least number of cops needed to
win. We consider cops and robber games on digraphs and width measures (such as DAG-width,
directed tree width or D-width) corresponding to them. All of them generalise tree width and
the game characterising it.
For the DAG-width game we prove that the problem to decide the minimal number of cops
required to capture the robber (which is the same as deciding DAG-width), is PSPACE-complete,
in contrast to most other similar games. We also show that the cop-monotonicity cost for directed
tree width games cannot be bounded by any function. As a consequence, D-width is not bounded
in directed tree width, refuting a conjecture by Safari.
A large number of directed width measures generalising tree width has been proposed in the
literature. However, only very little was known about the relation between them, in particular
about whether classes of digraphs of bounded width in one measure have bounded width in
another. In this paper we establish an almost complete order among the most prominent width
measures with respect to mutual boundedness."
"An Online Sequence-to-Sequence Model Using Partial Conditioning.  Sequence-to-sequence models have achieved impressive results on various tasks. However, they are unsuitable for tasks that require incremental predictions to be made as more data arrives. This is because they generate an output sequence conditioned on an entire input sequence. In this paper, we present a new model that can make incremental predictions as more input arrives, without redoing the entire computation. Unlike sequence-to-sequence models, our method computes the next-step distribution conditioned on the partial input sequence observed and the partial sequence generated. It accomplishes this goal using an encoder recurrent neural network (RNN) that computes features at the same frame rate as the input, and a transducer RNN that operates over blocks of input steps. The transducer RNN extends the sequence produced so far using a local sequence-to-sequence model. During training, our method uses alignment information to generate supervised targets for each block. Approximate alignment is easily available for tasks such as speech recognition, action recognition in videos, etc. During inference (decoding), beam search is used to find the most likely output sequence for an input sequence. This decoding is performed online - at the end of each block, the best candidates from the previous block are extended through the local sequence-to-sequence model. On TIMIT, our online method achieves 19.8% phone error rate (PER). For comparison with published sequence-to-sequence methods, we used a bidirectional encoder and achieved 18.7% PER compared to 17.6% from the best reported sequence-to-sequence model. Importantly, unlike sequence-to-sequence our model is minimally impacted by the length of the input. On artificially created longer utterances, it achieves 20.9% with a unidirectional model, compared to 20% from the best bidirectional sequence-to-sequence models."
"Recurrent Neural Networks for Noise Reduction in Robust ASR.  Recent work on deep neural networks as acoustic models for automatic speech recognition (ASR) have demonstrated substantial performance improvements. We introduce a model which uses a deep recurrent auto encoder neural network to denoise input features for robust ASR. The model is trained on stereo (noisy and clean) audio features to predict clean features given noisy input. The model makes no assumptions about how noise affects the signal, nor the existence of distinct noise environments. Instead, the model can learn to model any type of distortion or additive noise given sufficient training data. We demonstrate the model is competitive with existing feature denoising approaches on the Aurora2 task, and outperforms a tandem approach where deep networks are used to predict phoneme posteriors directly."
"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning.  Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge."
"Multilingual Language Processing From Bytes.  Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge."
"Report of the Inquiry into the 2015 British general election opinion polls.  Executive Summary
The opinion polls in the weeks and months leading up to the 2015 General Election substantially underestimated the lead of the Conservatives over Labour in the national vote share. This resulted in a strong belief amongst the public and key stakeholders that the election would be a dead heat and that a hung-parliament and coalition government would ensue. In historical terms, the 2015 polls were some of the most inaccurate since election polling first began in the UK in 1945. However, the polls have been nearly as inaccurate in other elections but have not attracted as much attention because they correctly ndicated the winning party. 
The Inquiry considered eight different potential causes of the polling miss and assessed the evidence in support of each of them. Our conclusion is that the primary cause of the polling miss in 2015 was unrepresentative samples. The methods the pollsters used to collect samples of voters systematically over-represented Labour supporters and under-represented Conservative supporters. The statistical adjustment procedures applied to the raw data did not mitigate this basic problem to any notable degree. The other putative causes can have made, at most, only a small contribution to the total error. We were able to replicate all published estimates for the final polls using raw microdata, so we can exclude the possibility that flawed analysis, or use of inaccurate weighting targets on the part of the pollsters, contributed to the polling miss. The procedures used by the pollsters to handle postal voters, overseas voters, and unregistered voters made no detectable contribution to the polling errors. There may have been a very modest ??late swing?? to the Conservatives between the final polls and Election Day, although this can have contributed ?? at most ?? around one percentage point to the error on the Conservative lead. We reject deliberate misreporting as a contributory factor in the polling miss on the grounds that it cannot easily be reconciled with the results of the re-contact surveys carried out by the pollsters and with two random surveys undertaken after the election. Evidence from several different sources does not support differential turnout misreporting making anything but, at most, a very small contribution to the polling errors. There was no difference between online and phone modes in the accuracy of the final polls. However, over the 2010-2015 parliament and in much of the election campaign, phone polls produced somewhat higher estimates of the Conservative vote share (1 to 2 percentage points). It is not possible to say what caused this effect, given the many confounded differences between the two modes. Neither is it possible to say which was the more accurate mode on the basis of this evidence. The decrease in the variance on the estimate of the Conservative lead in the final week of the campaign is consistent with herding - where pollsters make design and reporting decisions that cause published estimates to vary less than expected, given their sample sizes. Our interpretation of the evidence is that this convergence was unlikely to have been the result of deliberate collusion, or other forms of malpractice by the pollsters."
"Text-To-Speech with cross-lingual Neural Network-based grapheme-to-phoneme models.  Modern Text-To-Speech (TTS) systems need to increasingly deal with multilingual input. Navigation, social and news are all domains with a large proportion of foreign words. However, when typical monolingual TTS voices are used, the synthesis quality on such input is markedly lower. This is because traditional TTS derives pronunciations from a lexicon or a Grapheme-To-Phoneme (G2P) model which was built using a pre-defined sound inventory and a phonotactic grammar for one language only. G2P models perform poorly on foreign words, while manual lexicon development is labour-intensive, expensive and requires extra storage. Furthermore, large phoneme inventories and phonotactic grammars contribute to data sparsity in unit selection systems. We present an automatic system for deriving pronunciations for foreign words that utilises the monolingual voice design and can rapidly scale to many languages. The proposed system, based on a neural network cross-lingual G2P model, does not increase the size of the voice database, doesn't require large data annotation efforts, is designed not to increase data sparsity in the voice, and can be sized to suit embedded applications."
"Random Walk Initialization for Training Very Deep Feedforward Networks.  Training very deep networks is an important open problem in machine learning. One of many difficulties is that the norm of the back-propagated error gradient can grow or decay exponentially. Here we show that training very deep feed-forward networks (FFNs) is not as difficult as previously thought. Unlike when back-propagation is applied to a recurrent network, application to an FFN amounts to multiplying the error gradient by a different random matrix at each layer. We show that the successive application of correctly scaled random matrices to an initial vector results in a random walk of the log of the norm of the resulting vectors, and we compute the scaling that makes this walk unbiased. The variance of the random walk grows only linearly with network depth and is inversely proportional to the size of each layer. Practically, this implies a gradient whose log-norm scales with the square root of the network depth and shows that the vanishing gradient problem can be mitigated by increasing the width of the layers. Mathematical analyses and experimental results using stochastic gradient descent to optimize tasks related to the MNIST and TIMIT datasets are provided to support these claims. Equations for the optimal matrix scaling are provided for the linear and ReLU cases."
"A Field Guide to Personalized Reserve Prices.  We study the question of setting and testing reserve prices in single item auctions when the bidders are not identical. At a high level, there are two generalizations of the standard second price auction: in the lazy version we first determine the winner, and then apply reserve prices; in the eager version we first discard the bidders not meeting their reserves, and then determine the winner among the rest. We show that the two versions have dramatically different properties: lazy reserves are easy to optimize, and A/B test in production, whereas eager reserves always lead to higher welfare, but their optimization is NP-complete, and naive A/B testing will lead to incorrect conclusions. Despite their different characteristics, we show that the overall revenue for the two scenarios is always within a factor of 2 of each other, even in the presence of correlated bids. Moreover, we prove that the eager auction dominates the lazy auction on revenue whenever the bidders are independent or symmetric. We complement our theoretical results with simulations on real world data that show that even suboptimally set eager reserve prices are preferred from a revenue standpoint."
"Revisiting Distributed Synchronous SGD.  The recent success of deep learning approaches for domains like speech recognition (Hinton et al., 2012) and computer vision (Ioffe &amp; Szegedy, 2015) stems from many algorithmic improvements but also from the fact that the size of available training data has grown significantly over the years, together with the computing power, in terms of both CPUs and GPUs. While a single GPU often provides algorithmic simplicity and speed up to a given scale of data and model, there exist an operating point where a distributed implementation of training algorithms for deep architectures becomes necessary. 
Previous works have been focusing on asynchronous SGD training, which works well up to a few dozens of workers for some models. In this work, we show that synchronous SGD training, with the help of backup workers, can not only achieve better accuracy, but also reach convergence faster with respect to wall time, i.e. use more workers more efficiently."
"Smart Reply: Automated Response Suggestion for Email.  In this paper we propose and investigate a novel end-to-end method for automatically generating short email responses, called Smart Reply. It generates semantically diverse suggestions that can be used as complete email responses with just one tap on mobile. The system is currently used in Inbox by Gmail and is responsible for assisting with 10% of all mobile responses. It is designed to work at very high throughput and process hundreds of millions of messages daily. The system exploits state-of-the-art, large-scale deep learning. We describe the architecture of the system as well as the challenges that we faced while building it, like response diversity and scalability. We also introduce a new method for semantic clustering of user-generated content that requires only a modest amount of explicitly labeled data."
"BilBOWA: Fast Bilingual Distributed Representations without Word Alignments.  We introduce BilBOWA (Bilingual Bag-ofWords without Alignments), a simple and computationally-efficient model for learning
bilingual distributed representations of words which can scale to large monolingual datasets and does not require word-aligned parallel training
data. Instead it trains directly on monolingual data and extracts a bilingual signal from a smaller set of raw-text sentence-aligned data. This is achieved using a novel sampled bag-of-words
cross-lingual objective, which is used to regularize two noise-contrastive language models for efficient cross-lingual feature learning. We show that bilingual embeddings learned using the proposed
model outperform state-of-the-art methods on a cross-lingual document classification task as well as a lexical translation task on WMT11 data."
"GPUCC - An Open-Source GPGPU Compiler.  Graphics Processing Units have emerged as powerful accelerators for massively parallel, numerically intensive workloads. The two dominant software models for these devices are NVIDIA??s CUDA and the cross-platform OpenCL standard. Until now, there has not been a fully open-source compiler targeting the CUDA environment, hampering general compiler and architecture research and making deployment difficult in datacenter or supercomputer environments. In this paper, we present gpucc, an LLVM-based, fully open-source, CUDA compatible compiler for high performance computing. It performs various general and CUDA-specific optimizations to generate high performance code. The Clang-based frontend supports modern language features such as those in C++11 and C++14. Compile time is 8% faster than NVIDIA??s toolchain (nvcc) and it reduces compile time by up to 2.4x for pathological compilations (&gt;100 secs), which tend to dominate build times in parallel build environments. Compared to nvcc, gpucc??s runtime performance is on par for several open-source benchmarks, such as Rodinia (0.8% faster), SHOC (0.5% slower), or Tensor (3.7% faster). It outperforms nvcc on internal large-scale end-to-end benchmarks by up to 51.0%, with a geometric mean of 22.9%."
"Improving the Robustness of Deep Neural Networks via Stability Training.  In this paper we address the issue of output instability
of deep neural networks: small perturbations in the visual
input can significantly distort the feature embeddings and
output of a neural network. Such instability affects many
deep architectures with state-of-the-art performance on a
wide range of computer vision tasks. We present a general
stability training method to stabilize deep networks against
small input distortions that result from various types of common
image processing, such as compression, rescaling, and
cropping. We validate our method by stabilizing the stateof-the-art
Inception architecture [11] against these types of
distortions. In addition, we demonstrate that our stabilized
model gives robust state-of-the-art performance on largescale
near-duplicate detection, similar-image ranking, and
classification on noisy datasets."
"LLORMA: Local Low-Rank Matrix Approximation.  Matrix  approximation  is  a  common  tool  in  recommendation  systems,  text  mining,  and computer vision.  A prevalent assumption in constructing matrix approximations is that the partially observed matrix is low-rank.  In this paper, we propose, analyze, and experiment with two procedures, one parallel and the other global, for constructing local matrix approximations.  The two approaches approximate the observed matrix as a weighted sum of low-rank matrices.  These matrices are limited to a local region of the observed matrix. We analyze the accuracy of the proposed local low-rank modeling.  Our experiments show improvements in prediction accuracy over classical approaches for recommendation tasks."
"Local Low-Rank Matrix Approximation.  Matrix approximation is a common tool in recommendation systems, text mining, and computer vision. A prevalent assumption in constructing matrix approximations is that the partially observed matrix is of low-rank. We propose a new matrix approximation model where we assume instead that the matrix is locally of low-rank, leading to a representation of the observed matrix as a weighted sum of low-rank matrices. We analyze the accuracy of the proposed local low-rank modeling. Our experiments show improvements in prediction accuracy over classical approaches for recommendation tasks."
"Matrix Approximation under Local Low-Rank Assumption.  Matrix approximation is a common tool in machine learning for building accurate prediction models for recommendation systems, text mining, and computer vision. A prevalent assumption in constructing matrix approximations is that the partially observed matrix is of low-rank. We propose a new matrix approximation model where we assume instead that the matrix is only locally of low-rank, leading to a representation of the observed matrix as a weighted sum of low-rank matrices. We analyze the accuracy of the proposed local low-rank modeling. Our experiments show improvements of prediction accuracy in recommendation tasks."
"Discovering Structure in the Universe of Attribute Names.  Recently, search engines have invested significant effort to answering entity--attribute queries from structured data, but have focused mostly on queries for frequent attributes. In parallel, several research efforts have demonstrated that there is a long tail of attributes, often thousands per class of entities, that are of interest to users. Researchers are beginning to leverage these new collections of attributes to expand the ontologies that power search engines and to recognize entity--attribute queries. Because of the sheer number of potential attributes, such tasks require us to impose some structure on this long and heavy tail of attributes. This paper introduces the problem of organizing the attributes by expressing the compositional structure of their names as a rule-based grammar. These rules offer a compact and rich semantic interpretation of multi-word attributes, while generalizing from the observed attributes to new unseen ones. The paper describes an unsupervised learning method to generate such a grammar automatically from a large set of attribute names. Experiments show that our method can discover a precise grammar over 100,000 attributes of {\sc Countries} while providing a 40-fold compaction over the attribute names. Furthermore, our grammar enables us to increase the precision of attributes from 47\% to more than 90\% with only a minimal curation effort. Thus, our approach provides an efficient and scalable way to expand ontologies with attributes of user interest."
"""We call it Hi-Fi"": Exposing Indian Households to High Speed Broadband Wireless Internet.  As access to the Internet improves globally, we argue that speed is
important for accelerated and full-fledged exploration of the
Internet. We provided high-speed broadband service plans and
Wi-Fi access points to 12 Internet-using Indian households in
Delhi and Lucknow for three months. We sought to understand
uptake, trajectories, behaviours, and developmental impact of the
speed of the Internet on the daily lives of our respondents through
a multi-phased study (baseline, in-situ usage, follow-up). Our
findings suggest that high-speed Wi-Fi led to 2-3x increase in
online activity and higher-bandwidth activities; spurred Internet
skilling and instrumental uses online; shifted the time and space of
device access; created social currency; and transformed the
construct of Internet for the households. Ultimately, 8 out of the
12 households switched to high-speed Wi-Fi after the study
concluded. We discuss considerations for policy and infrastructure
deployments in introducing and onboarding new and existing
users living with slow speeds to higher-quality networks"
"Listen, Attend and Spell.  We present Listen, Attend and Spell (LAS), a neural network that learns to transcribe speech utterances to characters. Unlike traditional DNN-HMM models, this model learns all the components of a speech recognizer jointly. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits characters as outputs. The network produces character sequences without making any independence assumptions between the characters. This is the key improvement of LAS over previous end-to-end CTC models. On a subset of the Google voice search task, LAS achieves a word error rate (WER) of 14.1% without a dictionary or a language model, and 10.3% with language model rescoring over the top 32 beams. By comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0%."
"Pointer Networks.  We introduce a new neural architecture to learn the conditional probability of an
output sequence with elements that are discrete tokens corresponding to positions
in an input sequence. Such problems cannot be trivially addressed by existent approaches
such as sequence-to-sequence [1] and Neural Turing Machines [2], because
the number of target classes in each step of the output depends on the length
of the input, which is variable. Problems such as sorting variable sized sequences,
and various combinatorial optimization problems belong to this class. Our model
solves the problem of variable size output dictionaries using a recently proposed
mechanism of neural attention. It differs from the previous attention attempts in
that, instead of using attention to blend hidden units of an encoder to a context
vector at each decoder step, it uses attention as a pointer to select a member of
the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net).
We show Ptr-Nets can be used to learn approximate solutions to three challenging
geometric problems ?? finding planar convex hulls, computing Delaunay triangulations,
and the planar Travelling Salesman Problem ?? using training examples
alone. Ptr-Nets not only improve over sequence-to-sequence with input attention,
but also allow us to generalize to variable size output dictionaries. We show that
the learnt models generalize beyond the maximum lengths they were trained on.
We hope our results on these tasks will encourage a broader exploration of neural
learning for discrete problems."
"Learning to Rank with Selection Bias in Personal Search.  Click-through data has proven to be a critical resource for improving search ranking quality. Though a large amount of click data can be easily collected by search engines, various biases make it difficult to fully leverage this type of data. In the past, many click models have been proposed and successfully used to estimate the relevance for individual query-document pairs in the context of web search. These click models typically require a large quantity of clicks for each individual pair and this makes them difficult to apply in systems where click data is highly sparse due to personalized corpora and information needs, e.g., personal search. In this paper, we study the problem of how to leverage sparse click data in personal search and introduce a novel selection bias problem and address it in the learning-to-rank framework. This paper proposes a few bias estimation methods, including a novel query-dependent one that captures queries with similar results and can successfully deal with sparse data. We empirically demonstrate that learning-to-rank that accounts for query-dependent selection bias yields significant improvements in search effectiveness through online experiments with one of the world's largest personal search engines."
"XRay: A Function Call Tracing System.  Debugging high throughput, low-latency C/C++ systems in production is hard. At Google we developed XRay, a function call tracing system that allows Google engineers to get accurate function call traces with negligible overhead when off and moderate overhead when on, suitable for services deployed in production. XRay enables efficient function call entry/exit logging with high accuracy timestamps, and can be dynamically enabled and disabled. This white paper describes the XRay tracing system and its implementation. It also describes future plans with open sourcing XRay and engaging open source communities."
"Selection and Combination of Hypotheses for Dialectal Speech Recognition.  While research has often shown that building dialect-specific Automatic Speech Recognizers is the optimal approach to dealing with dialectal variations of the same language, we have observed that dialect-specific recognizers do not always output the best recognitions. Often enough, another dialectal recognizer outputs a better recognition than the dialect-specific one. In this paper, we present two methods to select and combine the best decoded hypothesis from a pool of dialectal recognizers. We follow a Machine Learning approach and extract features from the Speech Recognition output along with Word Embeddings and use Shallow Neural Networks for classification. Our experiments using Dictation and Voice Search data from the main four Arabic dialects show good WER improvements for the hypothesis selection scheme, reducing the WER by 2.1 to 12.1% depending on the test set, and promising results for the hypotheses combination scheme."
"AutoFDO: Automatic Feedback-Directed Optimization for Warehouse-Scale Applications.  AutoFDO is a system to simplify real-world deployment of feedback-directed optimization (FDO). The system works by sampling hardware performance monitors on production machines and using those profiles in to guide optimization. Profile data is stale by design, and we have implemented compiler features to deliver stable speedup across releases. The resulting performance is a geomean of 10.5% improvement on our benchmarks. AutoFDO achieves 85% of the gains of traditional FDO, despite imprecision due to sampling and information lost in the compilation pipeline. The system is deployed to hundreds of binaries at Google, and it is extremely easy to enable; users need only to add some flags to their release build. To date, AutoFDO has increased the number of FDO users at Google by 8X and has doubled the number of cycles spent in FDO-optimized binaries. Over half of CPU cycles used are now spent in some flavor of FDO-optimized binaries."
"Budget Allocation using Weakly Coupled, Constrained Markov Decision Processes.  AutoFDO is a system to simplify real-world deployment of feedback-directed optimization (FDO). The system works by sampling hardware performance monitors on production machines and using those profiles in to guide optimization. Profile data is stale by design, and we have implemented compiler features to deliver stable speedup across releases. The resulting performance is a geomean of 10.5% improvement on our benchmarks. AutoFDO achieves 85% of the gains of traditional FDO, despite imprecision due to sampling and information lost in the compilation pipeline. The system is deployed to hundreds of binaries at Google, and it is extremely easy to enable; users need only to add some flags to their release build. To date, AutoFDO has increased the number of FDO users at Google by 8X and has doubled the number of cycles spent in FDO-optimized binaries. Over half of CPU cycles used are now spent in some flavor of FDO-optimized binaries."
"API Design Reviews at Scale.  The number of APIs produced by Google's various business units grew at an astounding rate over the last decade, the result of which was a user experience containing wild inconsistencies and usability problems. There was no single issue that dominated the usability problems; rather, users suffered a death from a thousand papercuts. A lightweight, scalable, distributed design review process was put into place that has improved our APIs and the efficacy of our many API designers. Challenges remain, but the API design reviews at scale program has started successfully."
"Being good at doing good: Design precepts for Social Justice HCI projects.  In this short position paper, I propose four precepts for engaging in design and HCI projects with social justice goals, based on over 8 years of experience working on research, design and service projects with homeless people and non-profit service agencies. I would like to
discuss these precepts with other attendees at the Exploring Social Justice, Design and HCI workshop with the goal of developing a set of considerations that others can use when engaging in social justice HCI."
"TTS for Low Resource Languages: A Bangla Synthesizer.  We present a text-to-speech (TTS) system designed for the dialect of Bengali spoken in Bangladesh. This work is part of an ongoing effort to address the needs of under-resourced languages. We propose a process for streamlining the bootstrapping of TTS systems for under-resourced languages. First, we use crowdsourcing to collect the data from multiple ordinary speakers, each speaker recording small amount of sentences.  Second, we leverage an existing text normalization system for a related language (Hindi) to bootstrap a linguistic front-end for Bangla.  Third, we employ statistical techniques to construct multi-speaker acoustic models using Long Short-Term Memory Recurrent Neural Network (LSTM-RNN) and Hidden Markov Model (HMM) approaches. We then describe our experiments that show that the resulting TTS voices score well in terms of their perceived quality as measured by Mean Opinion Score (MOS) evaluations."
"Building Statistical Parametric Multi-speaker Synthesis for Bangladeshi Bangla.  We present a text-to-speech (TTS) system designed for the dialect of Bengali spoken in Bangladesh. This work is part of an ongoing effort to address the needs of new under-resourced languages. We propose a process for streamlining the bootstrapping of TTS systems for under-resourced languages. First, we use crowdsourcing to collect the data from multiple ordinary speakers, each speaker recording small amount of sentences.  Second, we leverage an existing text normalization system for a related language (Hindi) to bootstrap a linguistic front-end for Bangla.  Third, we employ statistical techniques to construct multi-speaker acoustic models using Long Short-term Memory Recurrent Neural Network (LSTM-RNN) and Hidden Markov Model (HMM) approaches. We then describe our experiments that show that the resulting TTS voices score well in terms of their perceived quality as measured by Mean Opinion Score (MOS) evaluations."
"Hiring Site Reliability Engineers.  Operating distributed systems at scale requires an unusual set of skills??problem solving, programming, system design, networking, and OS internals??which are difficult to find in one person. At Google, we??ve found some ways to hire Site Reliability Engineers, blending both software and systems skills to help keep a high standard for new SREs across our many teams and sites, including standardizing the format of our interviews and the unusual practice of making hiring decisions by committee. Adopting similar practices can help your SRE or DevOps team grow by consistently hiring excellent coworkers."
"The Systems Engineering Side of Site Reliability Engineering.  In order to run the company??s numerous services as efficiently and reliably as possible, Google??s Site Reliability Engineering (SRE) organization leverages the expertise of two main disciplines: Software Engineering and Systems Engineering. The roles of Software Engineer (SWE) and Systems Engineer (SE) lie at the two poles of the SRE continuum of skills and interests. While Site Reliability Engineers tend to be assigned to one of these two buckets, there is much overlap between the two job roles, and the knowledge exchange between the two job roles is rather fluid."
"Site Reliability Engineering: How Google Runs Production Systems.  The overwhelming majority of a software system??s lifespan is spent in use, not in design or implementation. So, why does conventional wisdom insist that software engineers focus primarily on the design and development of large-scale computing systems? In this collection of essays and articles, key members of Google??s Site Reliability Team explain how and why their commitment to the entire lifecycle has enabled the company to successfully build, deploy, monitor, and maintain some of the largest software systems in the world. You??ll learn the principles and practices that enable Google engineers to make systems more scalable, reliable, and efficient??lessons directly applicable to your organization. This book is divided into four sections:"
"DASS: Digital Advertising System Simulation.  We describe a Digital Advertising System Simulation (DASS) for modeling advertising and its impact on user behavior. DASS is both flexible and general, and can be applied to research on a wide range of topics, such as digital attribution, ad fatigue, campaign optimization, and marketing mix modeling. This paper introduces the basic DASS simulation framework and illustrates its application to digital attribution. We show that common position-based attribution models fail to capture the true causal effects of advertising across several simple scenarios. These results lay a groundwork for the evaluation of more complex attribution models, and the development of improved models."
"Improving Resource Efficiency at Scale with Heracles.  User-facing, latency-sensitive services, such as websearch, underutilize their computing resources during daily periods of low traffic. Reusing those resources for other tasks is rarely done in production services since the contention for shared resources can cause latency spikes that violate the service-level objectives of latency-sensitive tasks. The resulting under-utilization hurts both the affordability and energy efficiency of large-scale datacenters. With the slowdown in technology scaling caused by the sunsetting of Moore??s law, it becomes important to address this opportunity. We present Heracles, a feedback-based controller that enables the safe colocation of best-effort tasks alongside a latency-critical service. Heracles dynamically manages multiple hardware and software isolation mechanisms, such as CPU, memory, and network isolation, to ensure that the latency-sensitive job meets latency targets while maximizing the resources given to best-effort tasks. We evaluate Heracles using production latency-critical and batch workloads from Google and demonstrate average server utilizations of 90% without latency violations across all the load and colocation scenarios that we evaluated."
"Measuring Cross-Device Online Audiences.  We extend the work of Koehler, Skvortsov, and Vos (2013) to measure cross-device online audiences.
The method performs demographic corrections in the usual way device-by-device. A new method
that converts cross-device cookie counts to user counts is introduced. We provide practical recipes
for fitting this transformation function and then demonstrate its use using online panel data from
Japan."
"Idle Time Garbage Collection Scheduling.  Efficient garbage collection is increasingly important in today's managed language runtime systems that demand low latency, low memory consumption, and high throughput. Garbage collection may pause the application for many milliseconds to identify live memory, free unused memory, and compact fragmented regions of memory, even when employing concurrent garbage collection. In animation-based applications that require 60 frames per second, these pause times may be observable, degrading user experience. This paper introduces idle time garbage collection scheduling to increase the responsiveness of applications by hiding expensive garbage collection operations inside of small, otherwise unused idle portions of the application's execution, resulting in smoother animations. Additionally we take advantage of idleness to reduce memory consumption while allowing higher memory use when high throughput is required. We implemented idle time garbage collection scheduling in V8, an open-source, production JavaScript virtual machine running within Chrome. We present performance results on various benchmarks running popular webpages and show that idle time garbage collection scheduling can significantly improve latency and memory consumption. Furthermore, we introduce a new metric called frame time discrepancy to quantify the quality of the user experience and precisely measure the improvements that idle time garbage collection scheduling provides for a WebGL-based game benchmark. Idle time garbage collection scheduling is shipped and enabled by default in Chrome."
Oversubscription Dimensioning of Next-Generation PONs with Different Service Levels.  This letter provides a methodology for planning Passive Optical Networks using the oversubscription concept and shows its applicability in the dimensioning of 1 Gb/s access to both basic (i.e. residential) and premium (i.e. business) users with different service level requirements and activity patterns. We show that only Next-Generation PON networks can reach a large number of users with acceptable service levels.
"Course Builder Skill Maps.  In this paper, we present a new set of features
introduced in Course Builder that allow instructors to
add skill maps to their courses. We show how skill
maps can be used to provide up-to-date and actionable
information on students' learning behavior and
performance."
"Cloak of Visibility: Detecting When Machines Browse a Different Web.  The contentious battle between web services and miscreants involved in blackhat search engine optimization and malicious advertisements has driven the underground to develop increasingly sophisticated techniques that hide the true nature of malicious sites. These web cloaking techniques hinder the effectiveness of security crawlers and potentially expose Internet users to harmful content. In this work, we study the spectrum of blackhat cloaking techniques that target browser, network, or contextual cues to detect organic visitors. As a starting point, we investigate the capabilities of ten prominent cloaking services marketed within the underground. This includes a first look at multiple IP blacklists that contain over 50 million addresses tied to the top five search engines and tens of anti-virus and security crawlers. We use our findings to develop an anti-cloaking system that detects split-view content returned to two or more distinct browsing profiles with an accuracy of 95.5% and a false positive rate of 0.9% when tested on a labeled dataset of 94,946 URLs. We apply our system to an unlabeled set of 135,577 search and advertisement URLs keyed on high-risk terms (e.g., luxury products, weight loss supplements) to characterize the prevalence of threats in the wild and expose variations in cloaking techniques across traffic sources. Our study provides the first broad perspective of cloaking as it affects Google Search and Google Ads and underscores the minimum capabilities necessary of security crawlers to bypass the state of the art in mobile, rDNS, and IP cloaking."
"Rethinking Connection Security Indicators.  We propose a new set of browser security indicators, based on user research and an understanding of the design challenges faced by browsers. To motivate the need for new security indicators, we critique existing browser security indicators and survey 1,329 people about Google Chrome's indicators. We then evaluate forty icons and seven complementary strings by surveying thousands of respondents about their perceptions of the candidates. Ultimately, we select and propose three indicators. Our proposed indicators have been adopted by Google Chrome, and we hope to motivate others to update their security indicators as well."
"Ephemeral Identifiers:  Mitigating Tracking &amp; Spoofing Threats to BLE Beacons.  Bluetooth Smart (also known as Bluetooth Low Energy) beacons broadcast their presence in order to enable proximity-based applications by observer devices. This results in a privacy and security exposure: broadcast devices are typically susceptible to tracking and spoofing based on the IDs used by the beacons. We introduce a scheme consisting of cloud-based Ephemeral Identifiers (EID) which allows only authorized parties to properly identify the beacons broadcast; it mitigates the basic tracking and security threats while keeping high utility. We outline a formal model of privacy which is obtained with our scheme, present its implementation, and discuss possible extensions. The proposal outlined here is the basis for Google??s Eddystone standard, supported by some thirty industry partners. We supply an open source implementation for all the components of the system."
"Physical and Virtual Cell Phone Sensors for Traffic Control: Algorithms and Deployment Impact.  Decades of research have been directed towards
improving the timing of traffic lights. The ubiquity of cell phones
among drivers has created the opportunity to design new sensors
for traffic light controllers. These new sensors, which search for
radio signals that are constantly emanating from cell phones,
hold the hope of replacing the typical induction-loop sensors that
are installed within road pavements. A replacement to induction
sensors is desired as they require significant roadwork to install,
frequent maintenance and checkups, are sensitive to proper
repairs and installation work, and the construction techniques,
materials, and even surrounding unrelated ground work can be
sources of failure. However, before cell phone sensors can be
widely deployed, users must become comfortable with the passive
use of their cell phones by municipalities for this purpose. Despite
complete anonymization, public privacy concerns may remain.
This presents a chicken-and-egg problem: without showing the
benefits of using cell phones for traffic monitoring, users may
not be willing to allow this use. In this paper, we show that by
carefully training the traffic light controllers, we can unlock the
benefits of these sensors when only a small fraction of users allow
their cell phones to be used. Surprisingly, even when there is only
small percentage of opted-in users, the new traffic controllers
provide large benefits to all drivers"
"Learning Typographic Style.  Typography is a ubiquitous art form that affects our understanding, perception, and trust in what we read. Thousands of different font-faces have been created with enormous variations in the characters. In this paper, we learn the style of a font by analyzing a small subset of only four letters. From these four letters, we learn two tasks. The first is a discrimination task: given the four letters and a new candidate letter, does the new letter belong to the same font? Second, given the four basis letters, can we generate all of the other letters with the same characteristics as those in the basis set? We use deep neural networks to address both tasks, quantitatively and qualitatively measure the results in a variety of novel manners, and present a thorough investigation of the weaknesses and strengths of the approach."
"A Simple and Efficient Method to Handle Sparse Preference Data Using Domination Graphs: An Application to YouTube.  The phenomenal growth of the number of videos on YouTube provides enormous potential
for users to find content of interest to them. Unfortunately, as the size of the repository
grows, the task of discovering high-quality content becomes more daunting. To address this,
YouTube occasionally asks users for feedback on videos. In one such event (the YouTube
Comedy Slam), users were asked to rate which of two videos was funnier. This yielded sparse
pairwise data indicating a participant??s relative assessment of two videos. Given this data,
several questions immediately arise: how do we make inferences for uncompared pairs, overcome
noisy, and usually contradictory, data, and how do we handle severely skewed, real-world,
sampling? To address these questions, we introduce the concept of a domination-graph, and
demonstrate a simple and scalable method, based on the Adsorption algorithm, to efficiently
propagate preferences through the graph. Before tackling the publicly available YouTube data,
we extensively test our approach on synthetic data by attempting to recover an underlying,
known, rank-order of videos using similarly created sparse preference data."
"Reducing Vehicle Emissions via Machine Learning for Traffic Signal Program Selection (Extended Abstract).  Real-time optimization of traffic flow addresses important practical
problems: reducing a driver's wasted time, improving city-wide
efficiency, reducing gas emissions, and improving air quality.  Many
current implementations and research studies that address traffic
signal control construct a light controller's program (whether
adaptive or static) by segmenting the day into divisions in which
distinct traffic patterns are expected:  rush hours,
weekends, nights, etc. We consider the problem of automatically adapting a set of traffic
lights to changing conditions based upon the distribution of observed
traffic-density in surrounding areas.  Unlike previous techniques
which specify an a priori set number of unique flow patterns,
we assume an over-complete set of traffic patterns. A combination of
machine learning approaches are used to create a diverse set of
traffic-light programs that can be instantiated when new traffic flow
patterns are recognized.  We have observed significant reduction in
expected emissions and delays, while being agnostic to the number of
underlying distinct patterns in the traffic."
"Labeling the Features Not the Samples: Efficient Video Classification with Minimal Supervision.  Feature selection is essential for effective visual recognition.
We propose an efficient joint classifier learning
and feature selection method that discovers sparse,
compact representations of input features from a vast
sea of candidates, with an almost unsupervised formulation.
Our method requires only the following knowledge,
which we call the feature sign??whether or not
a particular feature has on average stronger values over
positive samples than over negatives. We show how this
can be estimated using as few as a single labeled training
sample per class. Then, using these feature signs,
we extend an initial supervised learning problem into an
(almost) unsupervised clustering formulation that can
incorporate new data without requiring ground truth
labels. Our method works both as a feature selection
mechanism and as a fully competitive classifier. It has
important properties, low computational cost and excellent
accuracy, especially in difficult cases of very limited
training data. We experiment on large-scale recognition
in video and show superior speed and performance
to established feature selection approaches such
as AdaBoost, Lasso, greedy forward-backward selection,
and powerful classifiers such as SVM."
"Density Estimation using Real NVP.  Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations."
"A Week to Remember: The Impact of Browser Warning Storage Policies.  When someone decides to ignore an HTTPS error warning,
how long should the browser remember that decision? If
they return to the website in five minutes, an hour, a day,
or a week, should the browser show them the warning again
or respect their previous decision? There is no clear industry
consensus, with eight major browsers exhibiting four different
HTTPS error exception storage policies. Ideally, a browser would not ask someone about the same
warning over and over again. If a user believes the warning
is a false alarm, repeated warnings undermine the browser??s
trustworthiness without providing a security benefit. However,
some people might change their mind, and we do not
want one security mistake to become permanent. We evaluated six storage policies with a large-scale, multimonth
field experiment. We found substantial differences
between the policies and selected the policy with the most
desirable characteristics. Google Chrome 45 adopted our
proposal, and it has proved successful since deployed. Subsequently,
we ran Mechanical Turk and GCS surveys to learn
about user expectations for warnings. Respondents generally
lacked knowledge about Chrome??s new storage policy,
but we remain satisfied with our proposal due to the behavioral
benefits we have observed in the field."
"Greedy Column Subset Selection: New Bounds and Distributed Algorithms.  The problem of matrix column subset selection
has recently attracted a large body of research,
with feature selection serving as one obvious and
important application. Among the techniques
that have been applied to solve this problem, the
greedy algorithm has been shown to be quite
effective in practice. However, our theoretical
guarantees on its performance have not been ex-
plored thoroughly, especially in a distributed set-
ting. In this paper, we study the greedy algorithm
for the column subset selection problem from a
theoretical and empirical perspective and show
its effectiveness in a distributed setting. In par-
ticular, we provide an improved approximation
guarantee for the greedy algorithm, and present
the first distributed implementation of this algo-
rithm with provable approximation factors. We
use the idea of randomized composable core-
sets, developed recently in the context of sub-
modular maximization. Finally, we validate the
effectiveness of this distributed algorithm via an
empirical study."
"Stack-propagation: Improved Representation Learning for Syntax.  Traditional syntax models typically leverage part-of-speech (POS) information by constructing features from hand-tuned templates. We demonstrate that a better approach is to utilize POS tags as a regularizer of learned representations. We propose a simple method for learning a stacked pipeline of models which we call ??stack-propagation??. We apply this to dependency parsing and tagging,  where we use the hidden layer of the tagger network as a representation of the input tokens for the parser. At test time, our parser does not require predicted POS tags. On 19 languages from the Universal Dependencies, our method is 1.3% (absolute) more accurate than a state-of-the-art graph-based approach and 2.7%  more accurate than the most comparable greedy model."
"Globally Normalized Transition-Based Neural Networks.  We introduce a globally normalized transition-based neural network
model that achieves state-of-the-art part-of-speech tagging,
dependency parsing and sentence compression results.  Our model is a
simple feed-forward neural network that operates on a task-specific
transition system, yet achieves comparable or better accuracies than
recurrent models.
We discuss the importance of global as opposed to local normalization:
a key insight is that the label bias problem implies that
globally
normalized models can be strictly more expressive 
than locally normalized models."
"Crowdsourcing a Gold Standard for Medical Relation  Extraction with CrowdTruth.  In this paper, we make the following contributions: (1) a comparison of the quality and efficacy of annotations for medical relation extraction provided by both crowd and medical experts, showing that crowd annotations are equivalent to those of experts, with appropriate processing; (2) an openly available dataset of 900 English sentences for medical relation extraction, centering primarily on the cause relation, that have been processed with disagreement analysis and by experts."
"Fast, Compact, and High Quality LSTM-RNN Based Statistical Parametric Speech Synthesizers for Mobile Devices.  In this paper, we make the following contributions: (1) a comparison of the quality and efficacy of annotations for medical relation extraction provided by both crowd and medical experts, showing that crowd annotations are equivalent to those of experts, with appropriate processing; (2) an openly available dataset of 900 English sentences for medical relation extraction, centering primarily on the cause relation, that have been processed with disagreement analysis and by experts."
"Open and Closed Schema for Aligning Knowledge and Text Collections..  When it comes to knowledge bases most people's first thought are structured sources such as the Freebase/Wikidata and their relationship to similarly structured web sources such as Wikipedia. A lot of additional and interesting ""knowledge"" though is captured in unstructured databases constructed in a less supervised manner using open information extraction techniques. In this talk we'll discuss some of the differences between open/closed schema knowledge bases including the ideas of objective vs subjective content as well as freshness and trust. We'll give an overview on approaches to aligning such data sources in a way that their relative strengths can be combined and finish with applications of such alignments; particularly around open question and answer systems."
"Building a RAPPOR with the Unknown: Privacy-Preserving Learning of Associations and Data Dictionaries.  Techniques based on randomized response enable the collection of potentially sensitive data from clients in a privacy-preserving manner with strong local differential privacy guarantees. A recent such technology, RAPPOR, enables estimation of the marginal frequencies of a set of strings via privacy-preserving crowdsourcing. However, this original estimation process relies on a known dictionary of possible strings; in practice, this dictionary can be extremely large and/or  unknown. In this paper, we propose a novel decoding algorithm for the RAPPOR mechanism that enables the estimation of ""unknown unknowns,'' i.e., strings we do not know we should be estimating. To enable learning without explicit dictionary knowledge, we develop methodology for estimating the joint distribution of multiple variables collected with RAPPOR. Our contributions are not RAPPOR-specific, and can be generalized to other local differential privacy mechanisms for learning distributions of string-valued random variables."
"Apples and Oranges: Detecting Least-Privilege Violators with Peer Group Analysis.  Clustering software into peer groups based on its apparent functionality allows for simple, intuitive categorization of software that can, in particular, help identify which software uses comparatively more privilege than is necessary to implement its functionality. Such relative comparison can improve the security of a software ecosystem in a number of ways. For example, it can allow market operators to incentivize software developers to adhere to the principle of least privilege, e.g., by encouraging users to use alternative, less-privileged applications for any desired functionality. This paper introduces software peer group analysis, a novel technique to identify least privilege violation and rank software based on the severity of the violation. We show that peer group analysis is an effective tool for detecting and estimating the severity of least privilege violation. It provides intuitive, meaningful results, even across different definitions of peer groups and security-relevant privileges. Our evaluation is based on empirically applying our analysis to over a million software items, in two different online software markets, and on a validation of our assumptions in a medium-scale user study."
"Data-driven software security: Models and methods.  For computer software, our security models, policies, 
mechanisms, and means of assurance were primarily conceived 
and developed before the end of the 1970??s. However,
since that time, software has changed radically: it is thousands
of times larger, comprises countless libraries, layers, and services,
and is used for more purposes, in far more complex ways. It is
worthwhile to revisit our core computer security concepts. For
example, it is unclear whether the Principle of Least Privilege
can help dictate security policy, when software is too complex for
either its developers or its users to explain its intended behavior. One possibility is to take an empirical, data-driven approach
to modern software, and determine its exact, concrete behavior
via comprehensive, online monitoring. Such an approach can be
a practical, effective basis for security??as demonstrated by its
success in spam and abuse fighting??but its use to constrain
software behavior raises many questions. In particular, three
questions seem critical. First, can we efficiently monitor the
details of how software is behaving, in the large? Second, is it
possible learn those details without intruding on users?? privacy?
Third, are those details a good foundation for security policies
that constrain how software should behave? This paper outlines what a data-driven model for software
security could look like, and describes how the above three
questions can be answered affirmatively. Specifically, this paper
briefly describes methods for efficient, detailed software monitoring, 
as well as methods for learning detailed software statistics
while providing differential privacy for its users, and, finally, how
machine learning methods can help discover users?? expectations
for intended software behavior, and thereby help set security
policy. Those methods can be adopted in practice, even at very
large scales, and demonstrate that data-driven software security
models can provide real-world benefits."
"Capacity planning for the Google backbone network.  Google operates one of the largest backbone networks in the world. In this
talk, we present optimization and simulation techniques we use to design the
network topology and provision its capacity to achieve conflicting objectives
such as scale, cost, availability, and latency."
"A practical algorithm for balancing the max-min fairness and throughput objectives in traffic engineering.  One of the goals of traffic engineering is to achieve a
flexible trade-off between fairness and throughput so that users
are satisfied with their bandwidth allocation and the network
operator is satisfied with the utilization of network resources. In
this paper, we propose a novel way to balance the throughput
and fairness objectives with linear programming. It allows the
network operator to precisely control the trade-off by bounding
the fairness degradation for each commodity compared to the
max-min fair solution or the throughput degradation compared
to the optimal throughput. We also present improvements to a
previous algorithm that achieves max-min fairness by solving a
series of linear programs. We significantly reduce the number
of steps needed when the access rate of commodities is limited.
We extend the algorithm to two important practical use cases:
importance weights and piece-wise linear utility functions for
commodities. Our experiments on synthetic and real networks
show that our algorithms achieve a significant speedup and
provide practical insights on the trade-off between fairness and
throughput."
"Cross-Lingual Morphological Tagging for Low-Resource Languages.  Morphologically rich languages often lack the annotated linguistic resources required to develop accurate natural language processing tools. We propose models suitable for training morphological taggers with rich tagsets for low-resource languages without using direct supervision. Our approach extends existing approaches of projecting part-of-speech tags across languages, using bitext to infer constraints on the possible tags for a given word type or token. We propose a tagging model using Wsabie, a discriminative embedding-based model with rank-based learning. In our evaluation on 11 languages, on average this model performs on par with a baseline weakly-supervised HMM, while being more scalable. Multilingual experiments show that the method performs best when projecting between related language pairs. Despite the inherently lossy projection, we show that the morphological tags predicted by our models improve the downstream performance of a parser by +0.6 LAS on average."
"Cache Content Selection Policies for Streaming Video Services.  The majority of internet traffic is now dominated
by streamed video content. As video quality continues to increase,
the strain that streaming traffic places on the network
infrastructure also increases. Caching content closer to users, e.g.,
using Content Distribution Networks, is a common solution to
reduce the load on the network. A simple approach to selecting
what to put in regional caches is to put the videos that are
most popular globally across the entire customer base. However,
this approach ignores distinct regional taste. In this paper we
explore the question of how a video content provider could go
about determining whether or not they should use a cache filling
policy based solely upon global popularity or take into account
regional tastes as well. We propose a model that captures the
overlap between inter-regional and intra-regional preferences. We
focus on movie content and derive a synthetic model that captures
??taste?? using matrix factorization, similarly to the method used
in recommender systems. Our model enables us to widely explore
the parameter space, and derive a set of metrics providers can
use to determine whether populating caches according to regional
of global tastes provides better cache performance."
"Robust Large-Scale Machine Learning in the Cloud.  The convergence behavior of many distributed machine learning (ML) algorithms can be sensitive to the number of machines being used or to changes in the computing environment. As a result, scaling to a large number of machines can be challenging. In this paper, we describe a new scalable coordinate descent (SCD) algorithm for generalized linear models whose convergence behavior is always the same, regardless of how much SCD is scaled out and regardless of the computing environment. This makes SCD highly robust and enables it to scale to massive datasets on low-cost commodity servers. Experimental results on a real advertising dataset in Google are used to demonstrate SCD's cost effectiveness and scalability. Using Google's internal cloud, we show that SCD can provide near linear scaling using thousands of cores for 1 trillion training examples on a petabyte of compressed data. This represents 10,000x more training examples than the `large-scale' Netflix prize dataset. We also show that SCD can learn a model for 20 billion training examples in two hours for about $10."
"Goods: Organizing Google's Datasets.  Enterprises increasingly rely on structured datasets to run their businesses. These datasets take a variety of forms, such as structured files, databases, spreadsheets, or even services that provide access to the data. The datasets often reside in different storage systems, may vary in their formats, may change every day. In this paper, we present <em>Goods</em>, a project to rethink how we organize structured datasets at scale, in a setting where teams use diverse and often idiosyncratic ways to produce the datasets and where there is no centralized system for storing and querying them. <em>Goods</em> extracts metadata ranging from salient information about each dataset (owners, timestamps, schema) to relationships among datasets, such as similarity and provenance. It then exposes this metadata through services that allow engineers to find datasets within the company,
to monitor datasets, to annotate them in order to enable others to use their datasets, and to analyze relationships between them. We discuss the technical challenges that we had to overcome in order
to crawl and infer the metadata for billions of datasets, to maintain the consistency of our metadata catalog at scale, and to expose the metadata to users. We believe that many of the lessons that we
learned are applicable to building large-scale enterprise-level data management systems in general."
"SWIFT: Using task-based parallelism, fully asynchronous communication, and graph partition-based domain decomposition for strong scaling on more than 100000 cores..  We present a new open-source cosmological code, called \swift, designed to solve the equations of hydrodynamics using a particle-based approach (Smooth Particle Hydrodynamics) on hybrid shared / distributed-memory architectures. \swift was designed from the bottom up to provide excellent {\em strong scaling} on both commodity clusters (Tier-2 systems) and Top100-supercomputers (Tier-0 systems), without relying on architecture-specific features or specialized accelerator hardware. This performance is due to three main computational approaches: \begin{itemize} \item \textbf{Task-based parallelism} for shared-memory parallelism, which provides fine-grained load balancing and thus strong scaling on large numbers of cores. \item \textbf{Graph-based domain decomposition}, which uses the task graph to decompose the simulation domain such that the {\em work}, as opposed to just the {\em data}, as is the case with most partitioning schemes, is equally distributed across all nodes. \item \textbf{Fully dynamic and asynchronous communication}, in which communication is modelled as just another task in the task-based scheme, sending data whenever it is ready and deferrin on tasks that rely on data from other nodes until it arrives. \end{itemize} In order to use these approaches, the code had to be re-written from scratch, and the algorithms therein adapted to the task-based paradigm. As a result, we can show upwards of 60\% parallel efficiency for moderate-sized problems when increasing the number of cores 512-fold, on both x86-based and Power8-based architectures."
"Efficient and Scalable Algorithms for Smoothed Particle Hydrodynamics on Hybrid Shared/Distributed-Memory Architectures.  This paper describes a new fast and implicitly parallel approach to neighbour-finding in multi-resolution Smoothed Particle Hydrodynamics (SPH) simulations. This new approach is based on hierarchical cell decompositions and sorted interactions, within a task-based formulation. It is shown to be faster than traditional tree-based codes, and to scale better than domain decomposition-based approaches on hybrid shared/distributed-memory parallel architectures, e.g. clusters of multi-cores, achieving a 40?? speedup over the Gadget-2 simulation code."
"Shasta: Interactive Reporting at Scale.  We describe Shasta, a middleware system built at Google to support interactive reporting in complex user-facing applications related to Google??s Internet advertising business. Shasta targets applications with challenging requirements: First, user query latencies must be low. Second, underlying transactional data stores have complex ??read-unfriendly?? schemas, placing significant transformation logic between stored data and the read-only views that Shasta exposes to its clients. This transformation logic must be expressed in a way that scales to large and agile engineering teams. Finally, Shasta targets applications with strong data freshness requirements, making it challenging to precompute query results using common techniques such as ETL pipelines or materialized views. Instead, online queries must go all the way from primary storage to userfacing views, resulting in complex queries joining 50 or more tables.
<br/>
<br/>
Designed as a layer on top of Google??s F1 RDBMS and Mesa data warehouse, Shasta combines language and system techniques to meet these requirements. To help with expressing complex view specifications, we developed a query language called RVL, with support for modularized view templates that can be dynamically compiled into SQL. To execute these SQL queries with low latency at scale, we leveraged and extended F1??s distributed query engine with facilities such as safe execution of C++ and Java UDFs. To reduce latency and increase read parallelism, we extended F1 storage with a distributed read-only in-memory cache. The system we describe is in production at Google, powering critical applications used by advertisers and internal sales teams. Shasta has significantly improved system scalability and software engineering efficiency compared to the middleware solutions it replaced."
"Collective Entity Resolution with Multi-Focal Attention.  Entity resolution is the task of linking each mention of an entity in text to the corresponding record in a knowledge base (KB).  Coherence models for entity resolution encourage all referring expressions in a document to resolve to entities that are related in the KB. We explore attention-like mechanisms for coherence, where the evidence for each candidate is based on a small set of strong relations, rather than relations to all other entities in the document. The rationale is that document-wide support may simply not exist  for non-salient entities, or entities not densely connected in the KB. Our proposed system outperforms state-of-the-art systems on the CoNLL 2003, TAC KBP 2010, 2011
and 2012 tasks."
"Optimizing Distributed Actor Systems for Dynamic Interactive Services.  Distributed actor systems are widely used for developing interactive scalable cloud services, such as social networks and on-line games. By modeling an application as a dynamic set of lightweight communicating ??actors??, developers can easily build complex distributed applications, while the underlying runtime system deals with low-level complexities of a distributed environment. We present ActOp ?? a data-driven, application-independent runtime mechanism for optimizing end-to-end service latency of actor-based distributed applications. ActOp targets the two dominant factors affecting latency: the overhead of remote inter-actor communications across servers, and the intra-server queuing delay. ActOp automatically identifies frequently communicating actors and migrates them to the same server transparently to the running application. The migration decisions are driven by a novel scalable distributed graph partitioning algorithm which does not rely on a single server to store the whole communication graph, thereby enabling efficient actor placement even for applications with rapidly changing graphs (e.g., chat services). Further, each server autonomously reduces the queuing delay by learning an internal queuing model and configuring threads according to instantaneous request rate and application demands. We prototype ActOp by integrating it with Orleans ?? a popular open-source actor system [4, 13]. Experiments with realistic workloads show latency improvements of up to 75% for the 99th percentile, up to 63% for the mean, with up to 2x increase in peak system throughput."
"Sense Anaphoric Pronouns: Am I One?.  This paper focuses on identity-of-sense anaphoric relations, in which the sense is shared but not the referent. We are not restricted to the pronoun ""one"", the focus of the small body of previous NLP work on this phenomenon, but look at a wider range of pronouns (""that"", ""some"", ""another"", etc.). We develop annotation guidelines, enrich a third of English OntoNotes with sense anaphora annotations, and shed light onto this phenomenon from a corpus-based perspective. We release the annotated data as part of the SAnaNotes corpus. We also use this corpus to develop a learning-based classifier to identify sense anaphoric uses, showing both the power and limitations of local features."
"Neural Random Access Machines.  Deep Neural Networks (DNNs) have achieved great success in supervised learning tasks mainly due to their ??depth??, allowing DNNs to represent functions whose implementation requires some sequential computation, which turned out to be sufficient to solve many previously-intractable problems. Given that depth was a key ingredient in the success of DNNs, it is plausible that much deeper neural models ?? namely, models that are computationally uni-
versal ?? would be able to solve much harder problems using less training data. The Neural Turing Machine is the first neural network model of this kind, which has been able to learn to solve a number of algorithmic tasks from input-output examples using backpropagation. Although the Neural Turing Machine is compu-
tationally universal, it used a memory addressing mechanism that does not allow for pointers, which makes the implementation of a number of natural algorithms cumbersome. In this paper, we propose and investigate a computationally universal model that can manipulate and dereference pointers. Pointer manipulation is a natural opera-
tion, so it is interesting to determine whether they can be learned with backprop-
agation as well. We evaluate the new model on a number of simple algorithmic tasks whose solution requires pointer manipulation and dereferencing. Our results show that the proposed model can learn to solve algorithmic tasks of such type, provided that they are not too difficult."
"Neural Network Adaptive Beamforming for Robust Multichannel Speech Recognition.  Joint multichannel enhancement and acoustic modeling using neural networks has shown promise over the past few years. However, one shortcoming of previous work [1,2,3] is that the filters learned during training are fixed for decoding, potentially limiting the ability of these models to adapt to previously unseen or changing conditions. In this paper we explore a neural network adaptive beamforming (NAB) technique to address this issue. Specifically, we use LSTM layers to predict time domain beamforming filter coefficients at each input frame. These filters are convolved with the framed time domain input signal and summed across channels, essentially performing FIR filter-and-sum beamforming using the dynamically adapted filter. The beamformer output is passed into a waveform CLDNN acoustic model [4] which is trained jointly with the filter prediction LSTM layers. We find that the proposed NAB model achieves a 12.7% relative improvement in WER over a single channel model [4] and reaches similar performance to a ``factored'' model architecture which utilizes several fixed spatial filters [3] on a 2,000-hour Voice Search task, with a 17.9% decrease in computational cost."
"Multi-Language Multi-Speaker Acoustic Modeling for LSTM-RNN based Statistical Parametric Speech Synthesis.  Building text-to-speech (TTS) systems requires large amounts of high quality speech recordings and annotations, which is a challenge to collect especially considering the variation in spoken languages around the world. Acoustic modeling techniques that could utilize inhomogeneous data are hence important as they allow us to pool more data for training. This paper presents a long short-term memory (LSTM) recurrent neural network (RNN) based statistical parametric speech synthesis system that uses data from multiple languages and speakers. It models language variation through cluster adaptive training and speaker variation with speaker dependent output layers. Experimental results have shown that the proposed multilingual TTS system can synthesize speech in multiple languages from a single model while maintaining naturalness. Furthermore, it can be adapted to new languages with only a small amount of data."
"Modeling Time-Frequency Patterns with LSTM vs. Convolutional Architectures for LVCSR Tasks.  Various neural network architectures have been proposed in the literature to model 2D correlations in the input signal, including convolutional layers, frequency LSTMs and 2D LSTMs such as time-frequency LSTMs, grid LSTMs and ReNet LSTMs. It has been argued that frequency LSTMs can model translational variations similar to CNNs, and 2D LSTMs can model even more variations [1], but no proper comparison has been done for speech tasks.  While convolutional layers have been a popular technique in speech tasks, this paper compares convolutional and LSTM architectures to model time-frequency patterns as the first layer in an LDNN [2] architecture. This comparison is particularly interesting when the convolutional layer degrades performance, such as in noisy conditions or when the learned filterbank is not constant-Q [3]. We find that grid-LDNNs offer the best performance of all techniques, and provide between a 1-4% relative improvement over an LDNN and CLDNN on 3 different large vocabulary Voice Search tasks."
"DeepMath - Deep Sequence Models for Premise Selection.  We study the effectiveness of neural sequence models for premise selection in automated theorem proving, one of the main bottlenecks in the formalization of mathematics. We propose a two stage approach for this task that yields good results for the premise selection task on the Mizar corpus while avoiding the hand-engineered features of existing state-of-the-art models. To our knowledge, this is the first time deep learning has been applied to theorem proving."
"Virtual Adversarial Training for Semi-Supervised Text Classification.  Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations. We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting."
"Generating Sentences from a Continuous Space.  The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling."
"A No-reference Perceptual Quality Metric for Videos Distorted by Spatially Correlated Noise.  Assessing the perceptual quality of video is critical for monitoring and optimizing video processing pipelines. In this paper, we focus on predicting the perceptual quality for videos distorted by noise. Existing video quality metrics are generally focus on ``white"", i.e., spatially un-correlated noise. However, white noise is very rare in realistic videos. Based on our analysis of the noise correlation patterns on a large and comprehensive video set, we build a video database that simulates the commonly encountered noise patterns. Using the database, we develop a perceptual quality metric that explicitly incorporates the noise pattern in quality prediction. Experimental results show that the proposed algorithm presents very high correlation with the perceptual quality of noisy videos."
"Design patterns for container-based distributed systems.  In the late 1980s and early 1990s, object-oriented programming revolutionized software development, popularizing the approach of building of applications as collections of modular components. Today we are seeing a similar revolution in distributed system development, with the increasing popularity of microservice architectures built from containerized software components. Containers are particularly well-suited as the fundamental ??object?? in distributed systems by virtue of the walls they erect at the container boundary. As this architectural style matures, we are seeing the emergence of design patterns, much as we did for objectoriented programs, and for the same reason ?? thinking in terms of objects (or containers) abstracts away the lowlevel details of code, eventually revealing higher-level patterns that are common to a variety of applications and algorithms. This paper describes three types of design patterns that we have observed emerging in container based distributed systems: single-container patterns for container management, single-node patterns of closely cooperating containers, and multi-node patterns for distributed algorithms. Like object oriented patterns before them, these patterns for distributed computation encode best practices, simplify development, and make the systems where they are used more reliable."
"The 2015 Top Picks In Computer Architecture (Guest Editor's Introduction).  It is our pleasure to introduce the 2015 Top Picks in Computer Architecture. We co-chaired the Selection Committee that had the formidable task of selecting the best computer architecture papers that were published in conferences in the previous year. Many excellent papers are published every year, and choosing among them is challenging, not least because of the need to define ??best.?? The committee identified 11 papers as being Top Picks this year. The range of topics is wide and reflects the healthy broadening of what the community considers to be computer architecture."
"Efficient Large Scale Video Classification.  Video classification has advanced tremendously over the recent years. A large part of the improvements in video classification had to do with the work done by the image classification community and the use of deep convolutional networks (CNNs) which produce competitive results with hand- crafted motion features. These networks were adapted to use video frames in various ways and have yielded state of the art classification results. We present two methods that build on this work, and scale it up to work with millions of videos and hundreds of thousands of classes while maintaining a low computational cost. In the context of large scale video processing, training CNNs on video frames is extremely time consuming, due to the large number of frames involved. We propose to avoid this problem by training CNNs on either YouTube thumbnails or Flickr images, and then using these networks' outputs as features for other higher level classifiers. We discuss the challenges of achieving this and propose two models for frame-level and video-level classification. The first is a highly efficient mixture of experts while the latter is based on long short term memory neural networks. We present results on the Sports-1M video dataset (1 million videos, 487 classes) and on a new dataset which has 12 million videos and 150,000 labels."
"Security Keys: Practical Cryptographic Second Factors for the Modern Web.  The security of online user accounts is often protected
by no more than a weak password. We present ??Security Key??, a second-factor device based on open standards that protects users against phishing and man-in-the-middle attacks. The user carries a single device and
can self-register it with any online web service that supports the standard. The devices are simple to implement
and deploy, are not encumbered by patents, are simple to
use, privacy preserving, and secure against strong attackers. We have shipped support for Security Keys in one of
the mainstream web browsers. In addition, multiple device vendors produce security keys.
In this work, we demonstrate that Security Keys lead
to both an increased level of security and user satisfaction by analyzing a two year deployment which began
within our 50,000 person corporation and has extended
to our consumer-facing web applications. The Security
Key design has been standardized by the FIDO Alliance,
an organization with more than 170 member companies
spanning the industry."
"Blockout: Dynamic Model Selection for Hierarchical Deep Networks.  Most deep architectures for image classification ?? even those that are trained to classify a large number of diverse categories ?? learn shared image representations with a single combined model. Intuitively, however, categories that are more visually similar should share more information than those that are very different. While hierarchical deep networks address this problem by learning separate features for subsets of related categories, current implementations require simplified models using fixed architectures specified with heuristic clustering methods. Instead, we propose Blockout, a method for regularization and model selection that simultaneously learns both the model architecture and parameters jointly with end-to-end training. Inspired by dropout, our approach gives a novel parametrization of hierarchical architectures that allows for structure learning using simple back-propagation. To demonstrate the utility of our approach, we evaluate Blockout on the CIFAR and ImageNet datasets demonstrating improved classification accuracy, better regularization performance, faster training, and a clear separation of nodes into hierarchical structures."
"An Internet-Wide Analysis of Traffic Policing.  Large flows like videos consume significant
bandwidth. Some ISPs actively manage these high volume
flows with techniques like policing, which enforces a flow
rate by dropping excess traffic. While the existence of policing
is well known, our contribution is an Internet-wide study
quantifying its prevalence and impact on video quality metrics.
We developed a heuristic to identify policing from
server-side traces and built a pipeline to deploy it at scale on
hundreds of servers worldwide within one of the largest online
content providers. Using a dataset of 270 billion packets
served to 28,400 client ASes, we find that, depending on region,
up to 7% of lossy transfers are policed. Loss rates are
on average 6?? higher when a trace is policed, and it impacts
video playback quality. We show that alternatives to policing,
like pacing and shaping, can achieve traffic management
goals while avoiding the deleterious effects of policing."
"Modular Composition of Coordination Services.  Coordination services like ZooKeeper, etcd, Doozer, and Consul are increasingly used by distributed applications for consistent, reliable, and high-speed coordination. When applications execute in multiple geographic regions, coordination service deployments trade-off between performance, (achieved by using independent services in separate regions), and consistency. We present a system design for modular composition of services that addresses this trade-off. We implement ZooNet, a prototype of this concept over ZooKeeper. ZooNet allows users to compose multiple instances of the service in a consistent fashion, facilitating applications that execute in multiple regions. In ZooNet, clients that access only local data suffer no performance penalty compared to working with a standard single ZooKeeper. Clients that use remote and local ZooKeepers show up to 7x performance improvement compared to consistent solutions available today."
"Wide &amp; Deep Learning for Recommender Systems.  Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide &amp; Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide &amp; Deep significantly increased app acquisitions compared with wide-only and deep-only models."
"Learning Personalized Pronunciations for Contact Names Recognition.  Person name is intrinsically difficult to pronounce due to its variety of origin and non-regularity. This poses significant challenges to contact dialing by voice. In order to mitigate the pronunciation problem for contact names,
we propose using personalized pronunciation learning: people can use their own pronunciations for their contact names. We achieve this by implicitly learning from users' corrections in real time, providing a seamless user experience improvement. We show that personalized pronunciation significantly reduces word error for difficult contact names by 15% relatively."
"Statistical performance of support vector machines.  The support vector machine (SVM) algorithm is well known to the computer
learning community for its very good practical results. The goal of the
present paper is to study this algorithm from a statistical perspective, using
tools of concentration theory and empirical processes.
Our main result builds on the observation made by other authors that the
SVM can be viewed as a statistical regularization procedure. From this point
of view, it can also be interpreted as a model selection principle using a penalized
criterion. It is then possible to adapt general methods related to model
selection in this framework to study two important points: (1) what is the
minimum penalty and how does it compare to the penalty actually used in the
SVM algorithm; (2) is it possible to obtain ??oracle inequalities?? in that setting,
for the specific loss function used in the SVM algorithm? We show that
the answer to the latter question is positive and provides relevant insight to the
former. Our result shows that it is possible to obtain fast rates of convergence
for SVMs."
"Learning using Large Datasets.  This contribution develops a theoretical framework that takes into account
the effect of approximate optimization on learning algorithms. The analysis
shows distinct tradeoffs for the case of small-scale and large-scale learning
problems. Small-scale learning problems are subject to the usual approximation??
estimation tradeoff. Large-scale learning problems are subject to a qualitatively different
tradeoff involving the computational complexity of the underlying optimization
algorithms in non-trivial ways. For instance, a mediocre optimization algorithms,
stochastic gradient descent, is shown to perform very well on large-scale
learning problems."
The Tradeoffs of Large Scale Learning.  This contribution develops a theoretical framework that takes into account the effect of approximate optimization on learning algorithms. The analysis shows distinct tradeoffs for the case of small-scale and large-scale learning problems. Small-scale learning problems are subject to the usual approximation??estimation tradeoff. Large-scale learning problems are subject to a qualitatively different tradeoff involving the computational complexity of the underlying optimization algorithms in non-trivial ways.
"Multi-Language Online Handwriting Recognition.  We describe Google's online handwriting recognition system that currently supports 22 scripts and 97 languages. The system's focus is on fast, high-accuracy text entry for mobile, touch-enabled devices. We use a combination of state-of-the-art components and combine them with novel additions in a flexible framework. This architecture allows us to easily transfer improvements between languages and scripts. This made it possible to build recognizers for languages that, to the best of our knowledge, are not handled by any other online handwriting recognition system. The approach also enabled us to use the same architecture both on very powerful machines for recognition in the cloud as well as on mobile devices with more limited computational power by changing some of the settings of the system. In this paper we give a general overview of the system architecture and the novel components, such as unified time- and position-based input interpretation, trainable segmentation, minimum-error rate training for feature combination, and a cascade of pruning strategies. We present experimental results for different setups. The system is currently publicly available in several Google products, for example in Google Translate and as an input method for Android devices."
"Recurrent Dropout without Memory Loss.  This paper presents a novel approach to recurrent neural network (RNN) regularization. Differently from the widely adopted dropout method, which is applied to forward connections of feed-forward architectures or RNNs, we propose to drop neurons directly in recurrent connections in a way that does not cause loss of long-term memory. Our approach is as easy to implement and apply as the regular feed-forward dropout and we demonstrate its effectiveness for the most popular recurrent networks: vanilla RNNs, Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks. Our experiments on three NLP benchmarks show consistent improvements even when combined with conventional feed-forward dropout."
"Why Google Stores Billions of Lines of Code in a Single Repository.  Google's monolithic repository provides a common source of truth for tens of thousands of developers around the world. This article outlines the scale of Google??s codebase, describes Google??s custom-built monolithic source repository, and discusses the reasons behind choosing this model. We provide background on the systems and workflows that make managing and working productively with a large repository feasible. We also review the advantages and trade-offs of this model of source code management."
"Cross Panel Imputation.  Many empirical micro-economics studies rely on consumer panels. For example, TV and web metering panels track TV and online usage of individuals. Sometimes more than one panel are available although these panels use different metering technologies and are subject to varying degrees of missingness. The problem we consider here is how to combine imputation based on two panels which have similar but not identical statistical characteristics. In the US, we have two two-screen panels, panel A (TV + desktop) and panel B(desktop + mobile) which are both calibrated to the US internet population. We want to estimate a count of ad impressions across all three-screens. As desktop impressions are metered in both panels, we fit a joint imputation model by pooling observed desktop impression counts across panels. After imputation on panel B, we fit a truncated negative binomial hurdle regression of mobile impression count over desktop impression count, demographic information, etc. And then, for each panelist in the panel A, we predict his/her mobile impression counts. In this way, we 'impute' mobile impressions in the panel A to facilitate three-screens measurements."
"Deep Learning with Differential Privacy.  Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality."
"Passive Taxonomy of Wifi Clients using MLME Frame Contents.  In supporting Wifi networks it is useful to identify the type of client device connecting to an AP. Knowing the type of client can guide troubleshooting steps, allow searches for known issues, or allow specific workarounds to be implemented in the AP. For support purposes a passive method which analyzes normal traffic is preferable to active methods, which often send obscure combinations of packet options which might trigger client bugs. We have developed a method of passive client identification which observes the contents of Wifi management frames including Probes and Association requests. We show that the management frames populated by modern Wifi chipsets and device drivers are quite distinguishable, making it possible in many cases to identify the model of the device. Supplementing information from the Wifi management frames with additional information from DHCP further extends the set of clients which can be distinguished."
"A Perceptual Visibility Metric for Banding Artifacts.  Banding is a common video artifact caused by compressing low texture regions with coarse quantization. Relatively few previous attempts exist to address banding and none incorporate subjective testing for calibrating the measurement. In this paper, we propose a novel metric that incorporates both edge length and contrast across the edge to measure video banding. We further introduce both reference and non-reference metrics. Our results demonstrate that the new metrics have a very high correlation with subjective assessment and certainly outperforms PSNR, SSIM, and VQM."
"When Recommendation Goes Wrong - Anomalous Link Discovery in Recommendation Networks.  We present a secondary ranking system to find and remove
erroneous suggestions from a geospatial recommendation system.
We discover such anomalous links by ??double checking??
the recommendation system??s output to ensure that it
is both structurally cohesive, and semantically consistent.
Our approach is designed for the Google Related Places
Graph, a geographic recommendation system which provides
results for hundreds of millions of queries a day. We model
the quality of a recommendation between two geographic entities
as a function of their structure in the Related Places
Graph, and their semantic relationship in the Google Knowledge
Graph. To evaluate our approach, we perform a large scale human
evaluation of such an anomalous link detection system. For
the long tail of unpopular entities, our models can predict
the recommendations users will consider poor with up to
42% higher mean precision (29 raw points) than the live
system. Results from our study reveal that structural and semantic
features capture different facets of relatedness to human
judges. We characterize our performance with a qualitative
analysis detailing the categories of real-world anomalies our
system is able to detect, and provide a discussion of additional
applications of our method."
"Leveraging Contextual Cues for Generating Basketball Highlights.  The massive growth of sports videos has resulted in a need for automatic generation of sports highlights that are comparable in quality to the hand-edited highlights produced by broadcasters such as ESPN. Unlike previous works that mostly use audio-visual cues derived from the video, we propose an approach that additionally leverages contextual cues derived from the environment that the game is being played in. The contextual cues provide information about the excitement levels in the game, which can be ranked and selected to automatically produce high-quality basketball highlights. We introduce a new dataset of 25 NCAA games along with their play-by-play stats and the ground-truth excitement data for each basket. We explore the informativeness of five different cues derived from the video and from the environment through user studies. Our experiments show that for our study participants, the highlights produced by our system are comparable to the ones produced by ESPN for the same games."
"Unsupervised Learning for Physical Interaction through Video Prediction.  A core challenge for an agent learning to interact with the world is to predict how its actions affect objects in its environment. Many existing methods for learning the dynamics of physical interactions require labeled object information. However, to scale real-world interaction learning to a variety of scenes and objects, acquiring labeled data becomes increasingly impractical. To learn about physical object motion without labels, we develop an action-conditioned video prediction model that explicitly models pixel motion, by predicting a distribution over pixel motion from previous frames. Because our model explicitly predicts motion, it is partially invariant to object appearance, enabling it to generalize to previously unseen objects. To explore video prediction for real-world interactive agents, we also introduce a dataset of 50,000 robot interactions involving pushing motions, including a test set with novel objects. In this dataset, accurate prediction of videos conditioned on the robot's future actions amounts to learning a ""visual imagination"" of different futures based on different courses of action. Our experiments show that our proposed method not only produces more accurate video predictions, but also more accurately predicts object motion, when compared to prior methods."
"Grounded compositional semantics for finding and describing images with sentences.  Previous work on Recursive Neural Networks (RNNs) shows that these models can produce compositional feature vectors for accurately representing and classifying sentences or images.
However, the sentence vectors of previous models cannot accurately represent visually grounded meaning. We introduce the DTRNN model which uses dependency trees to embed sentences into a vector space in order to retrieve images that are described by those sentences. Unlike previous RNN-based models which use constituency trees, DT-RNNs naturally focus on the action and agents in a sentence. They are better able to abstract from the details of word order and syntactic expression. DT-RNNs outperform other recursive and recurrent neural networks, kernelized CCA and a bag-of-words baseline on the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image."
"Fastfood-computing hilbert space expansions in loglinear time.  Despite their successes, what makes kernel methods difficult to use in many large scale problems is the fact that computing the decision function is typically expensive, especially at prediction time. In this paper, we overcome this difficulty by proposing Fastfood, an approximation that accelerates such computation significantly. Key to Fastfood is the observation that Hadamard matrices when combined with diagonal Gaussian matrices exhibit properties similar to dense Gaussian random matrices. Yet unlike the latter, Hadamard and diagonal matrices are inexpensive to multiply and store. These
two matrices can be used in lieu of Gaussian matrices in Random Kitchen Sinks (Rahimi &amp; Recht, 2007) and thereby speeding up the computation for a large range of kernel functions. Specifically, Fastfood requires O(n log d) time and O(n) storage to compute n non-linear basis functions in d dimensions, a significant improvement from O(nd) computation and storage, without sacrificing accuracy. We prove that the approximation is unbiased and has low variance. Extensive experiments show that we achieve similar accuracy to full kernel expansions and Random Kitchen Sinks while being 100x faster and using 1000x less memory. These improvements, especially in terms of memory usage, make kernel methods more practical for applications that have large training sets and/or require real-time prediction."
"Exploring the limits of language modeling.  This paper shows recent advances for large scale neural language modeling, a task central to language understanding. Our goal is to show how well large neural language models can perform on a large LM benchmark corpus, for which we chose the One Billion Word Benchmark. Using various techniques, our best single model significantly improves state-of-the-art perplexity from 51.3 to 30.0, while an ensemble of models sets a new record by improving perplexity from 41.0 to 23.7."
"Frodo: Take off the ring! Practical, Quantum-Secure Key Exchange from LWE.  Abstract: Lattice-based cryptography offers some of the most attractive primitives believed to be resistant to quantum computers. Following increasing interest from both companies and government agencies in building quantum computers, a number of works have proposed instantiations of practical post-quantum key exchange protocols based on hard problems in ideal lattices, mainly based on the Ring Learning With Errors (R-LWE) problem. While ideal lattices facilitate major efficiency and storage benefits over their non-ideal counterparts, the additional ring structure that enables these advantages also raises concerns about the assumed difficulty of the underlying problems. Thus, a question of significant interest to cryptographers, and especially to those currently placing bets on primitives that will withstand quantum adversaries, is how much of an advantage the additional ring structure actually gives in practice. Despite conventional wisdom that generic lattices might be too slow and unwieldy, we demonstrate that LWE-based key exchange is quite practical: our constant time implementation requires around 1.3ms computation time for each party; compared to the recent NewHope R-LWE scheme, communication sizes increase by a factor of 4.7??, but remain under 12 KiB in each direction. Our protocol is competitive when used for serving web pages over TLS; when partnered with ECDSA signatures, latencies increase by less than a factor of 1.6??, and (even under heavy load) server throughput only decreases by factors of 1.5?? and 1.2?? when serving typical 1 KiB and 100 KiB pages, respectively. To achieve these practical results, our protocol takes advantage of several innovations. These include techniques to optimize communication bandwidth, dynamic generation of public parameters (which also offers additional security against backdoors), carefully chosen error distributions, and tight security parameters."
"Robust and Probabilistic Failure-Aware Placements.  Motivated by the growing complexity of heterogeneity and hierarchical organization in data centers, in this paper we address the problem of placing copies of tasks on machines with the goal of increasing availability in the presence of failures. We consider two models of failures: adversarial and probabilistic. In the adversarial model, each node has a weight (higher weight implying higher reliability) and the adversary can remove any subset of nodes of total weight at most a given bound W and our goal is to find a placement that incurs the least disruption against such an adversary. In the probabilistic model, each node has a probability of failure and we want to find a placement that maximizes the probability that at least K out of N tasks are available at any time. The problems in the first category covering adversarial failures lie in $\Sigma_2$, the second level of the polynomial hierarchy. We show that a basic variant, which we call RobustFAP, is co-NP-hard, and an all-or-nothing version of RobustFAP is $\Sigma_2$-complete. Our first algorithmic result here is is a PTAS for RobustFAP, a key ingredient of which is a solution to a fractional variant of RobustFAP. Our second algorithmic result is for fractional HierRobustFAP over hierarchies: a new Generalized Spreading algorithm, which is simultaneously optimal for all W. This algorithm generalizes the classical notion of {\em max-min fairness} to work with nodes of differing capacities, differing reliability weights and hierarchical structures. We then apply randomized rounding to obtain an algorithm for integral RobustFAP over hierarchies. For the probabilistic version ProbFAP, we first focus on the single level problem and give an algorithm that achieves an additive \eps approximation in the failure probability while giving up a (1 + \eps) multiplicative factor in the number of failures. We then extend the result to the hierarchical version achieving a \eps additive approximation in failure probability while giving up a (L + \eps) multiplicative factor in the number of failures, where L is the number of levels in the hierarchy."
"Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection.  Motivated by the growing complexity of heterogeneity and hierarchical organization in data centers, in this paper we address the problem of placing copies of tasks on machines with the goal of increasing availability in the presence of failures. We consider two models of failures: adversarial and probabilistic. In the adversarial model, each node has a weight (higher weight implying higher reliability) and the adversary can remove any subset of nodes of total weight at most a given bound W and our goal is to find a placement that incurs the least disruption against such an adversary. In the probabilistic model, each node has a probability of failure and we want to find a placement that maximizes the probability that at least K out of N tasks are available at any time. The problems in the first category covering adversarial failures lie in $\Sigma_2$, the second level of the polynomial hierarchy. We show that a basic variant, which we call RobustFAP, is co-NP-hard, and an all-or-nothing version of RobustFAP is $\Sigma_2$-complete. Our first algorithmic result here is is a PTAS for RobustFAP, a key ingredient of which is a solution to a fractional variant of RobustFAP. Our second algorithmic result is for fractional HierRobustFAP over hierarchies: a new Generalized Spreading algorithm, which is simultaneously optimal for all W. This algorithm generalizes the classical notion of {\em max-min fairness} to work with nodes of differing capacities, differing reliability weights and hierarchical structures. We then apply randomized rounding to obtain an algorithm for integral RobustFAP over hierarchies. For the probabilistic version ProbFAP, we first focus on the single level problem and give an algorithm that achieves an additive \eps approximation in the failure probability while giving up a (1 + \eps) multiplicative factor in the number of failures. We then extend the result to the hierarchical version achieving a \eps additive approximation in failure probability while giving up a (L + \eps) multiplicative factor in the number of failures, where L is the number of levels in the hierarchy."
"Towards Principled Unsupervised Learning.  General unsupervised learning is a long-standing conceptual problem in machine learning. Supervised learning is successful because it can be solved by the minimization of the training error cost function. Unsupervised learning is not as successful, because the unsupervised objective may be unrelated to the supervised task of interest. For an example, density modelling and reconstruction have often been used for unsupervised learning, but they did not produced the sought-after performance gains, because they have no knowledge of the supervised tasks. 
In this paper, we present an unsupervised cost function which we name the Output Distribution Matching (ODM) cost, which measures a divergence between the distribution of predictions and distributions of labels. The ODM cost is appealing because it is consistent with the supervised cost in the following sense: a perfect supervised classifier is also perfect according to the ODM cost. Therefore, by aggressively optimizing the ODM cost, we are almost guaranteed to improve our supervised performance whenever the space of possible predictions is exponentially large. 
We demonstrate that the ODM cost works well on number of small and semi-artificial datasets using no (or almost no) labelled training cases. Finally, we show that the ODM cost can be used for one-shot domain adaptation, which allows the model to classify inputs that differ from the input distribution in significant ways without the need for prior exposure to the new domain."
"Recurrent Neural Network Regularization.  We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation."
"Understanding the Challenges of Designing  and Developing Multi-Device Experiences.  As the number of computing devices available to users continues to grow, personal computing increasingly involves using multiple devices together. However, support for multi-device interactions has fallen behind users' desire to leverage the diverse capabilities of the devices that surround them. In this paper, we report on an interview study of 29 designers and developers in which we investigate the barriers to creating useful, usable, and delightful multi-device experiences. We uncovered three key challenges: 1) the difficulty in designing the interactions between devices, 2) the complexity of adapting interfaces to different platform UI standards, and 3) the lack of tools and methods for testing multi-device user experiences. We discuss the technological and business factors behind these challenges and potential ways to lower the barriers they impose."
"Pricing a low-regret seller.  As the number of ad exchanges has grown, publishers have turned to low regret learning algorithms to decide which exchange offers the best price for their inventory. This in turn opens the following question for the exchange: how to set prices to attract as many sellers as possible and maximize revenue. In this work we formulate this precisely as a learning problem, and present algorithms showing that by simply knowing that the counterparty is using a low regret algorithm is enough for the exchange to have its own low regret learning algorithm to find the optimal price."
"Where to sell: Simulating auctions from learning algorithms.  Ad Exchange platforms connect online publishers and advertisers and facilitate selling billions of impressions every day. We study these environments from the perspective of a publisher who wants to find the profit maximizing exchange to sell his inventory. Ideally, the publisher would run an auction among exchanges. However, this is not possible due to technological and other practical considerations. The publisher needs to send each impression to one of the exchanges with an asking price. We model the problem as a variation of multi-armed bandits where exchanges (arms) can behave strategically in order to maximizes their own profit. We propose mechanisms that find the best exchange with sub-linear regret and have desirable incentive properties."
"Intuitions, analytics, and killing ants: Inference literacy of high school-educated adults in the US.  Analytic systems increasingly allow companies to draw inferences about users?? characteristics, yet users may not fully understand these systems due to their complex and often unintuitive nature. In this paper, we investigate inference literacy: the beliefs and misconceptions people have about how companies collect and make inferences from their data. We interviewed 21 non-student participants with a high school education, finding that few believed companies can make the type of deeply personal inferences that companies now routinely make through machine learning. Instead, most participant??s inference literacy beliefs clustered around one of two main concepts: one cluster believed companies make inferences about a person based largely on a priori stereotyping, using directly gathered demographic data; the other cluster believed that companies make inferences based on
computer processing of online behavioral data, but often expected these inferences to be limited to straightforward intuitions. We also find evidence that cultural models related to income and ethnicity influence the assumptions that users make about their own role in the data economy. We share implications for research, design, and policy on tech savviness, digital inequality, and potential inference literacy interventions."
"Managing your Private and Public Data: Bringing down Inference Attacks against your Privacy.  We propose a practical methodology to protect a user??s private data, when he wishes to publicly release data that is correlated with his private data, in the hope of getting some utility. Our approach relies on a general statistical inference framework that captures the privacy threat under inference
attacks, given utility constraints. Under this framework, data is distorted before it is released, according to a probabilistic privacy mapping. This mapping is obtained by solving a convex optimization problem, which minimizes information leakage under a distortion constraint. We address practical challenges encountered when applying this theoretical framework to real world
data. On one hand, the design of optimal privacy-preserving mechanisms requires knowledge of the prior distribution linking private data and data to be released, which is often unavailable in practice. On the other hand, the optimization may become untractable when data assumes values in large size alphabets, or is high dimensional. Our work makes three major contributions.
First, we provide bounds on the impact of a mismatched prior on the privacy-utility tradeoff. Second, we show how to reduce the optimization size by introducing a quantization step, and how to generate privacy mappings under quantization. Third, we evaluate our method on two datasets, including a new dataset that we collected, showing correlations between political convictions
and TV viewing habits. We demonstrate that good privacy properties can be achieved with limited distortion so as not to undermine the original purpose of the publicly released data, e.g. recommendations."
"DeepStereo: Learning to Predict New Views From the World's Imagery.  Deep networks have recently enjoyed enormous success when applied to recognition and classification problems in computer vision [22, 32], but their use in graphics problems has been limited ([23, 7] are notable recent exceptions). In this work, we present a novel deep architecture that per- forms new view synthesis directly from pixels, trained from a large number of posed image sets. In contrast to tradi- tional approaches which consist of multiple complex stages of processing, each of which require careful tuning and can fail in unexpected ways, our system is trained end-to-end. The pixels from neighboring views of a scene are presented to the network which then directly produces the pixels of the unseen view. The benefits of our approach include gen- erality (we only require posed image sets and can easily apply our method to different domains), and high quality results on traditionally difficult scenes. We believe this is due to the end-to-end nature of our system which is able to plausibly generate pixels according to color, depth, and tex- ture priors learnt automatically from the training data. We show view interpolation results on imagery from the KITTI dataset [12], from data from [1] as well as on StreetView images. To our knowledge, our work is the first to apply deep learning to the problem of new view synthesis from sets of real-world, natural imagery."
"Conversational Contextual Cues: The Case of Personalization and History for Response Ranking.  We investigate the task of modeling open-domain, multi-turn, unstructured, multi-
participant, conversational dialogue. We specifically study the effect of incorporating different elements of the conversation. Unlike previous efforts, which focused on modeling messages and responses, we extend the modeling to long context and participant??s history. Our system does not rely
on handwritten rules or engineered features; instead, we train deep neural networks on a large conversational dataset. In particular, we exploit the structure of Reddit comments and posts to extract 2.1 billion messages and 133 million conversations. We evaluate our models on the task
of predicting the next response in a conversation, and we find that modeling both context and participants improves prediction accuracy."
"Real-Time Loop Closure in 2D LIDAR SLAM.  Portable laser range-finders, further referred to as LIDAR, and simultaneous localization and mapping (SLAM) are an efficient method of acquiring as-built floor plans. Generating and visualizing floor plans in real-time helps the operator assess the quality and coverage of capture data. Building a portable capture platform necessitates operating under limited computational resources. We present the approach used in our backpack mapping platform which achieves real-time mapping and loop closure at a 5 cm resolution. To achieve real-time loop closure, we use a branch-and-bound approach for computing scan-to-submap matches as constraints. We provide experimental results and comparisons to other well known approaches which show that, in terms of quality, our approach is competitive with established techniques. IEEE-copyrighted article, http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7487258"
"Computational multiqubit tunnelling in programmable quantum annealers.  Quantum tunnelling is a phenomenon in which a quantum state traverses energy barriers higher than the energy of the state itself. Quantum tunnelling has been hypothesized as an advantageous physical resource for optimization in quantum annealing. However, computational multiqubit tunnelling has not yet been observed, and a theory of co-tunnelling under high- and low-frequency noises is lacking. Here we show that 8-qubit tunnelling plays a computational role in a currently available programmable quantum annealer. We devise a probe for tunnelling, a computational primitive where classical paths are trapped in a false minimum. In support of the design of quantum annealers we develop a nonperturbative theory of open quantum dynamics under realistic noise characteristics. This theory accurately predicts the rate of many-body dissipative quantum tunnelling subject to the polaron effect. Furthermore, we experimentally demonstrate that quantum tunnelling outperforms thermal hopping along classical paths for problems with up to 200 qubits containing the computational primitive."
"Product Reservoir Computing: Time-Series Computation with Multiplicative Neurons.  Echo state networks (ESN), a type of reservoir computing (RC) architecture, are efficient and accurate artificial neural systems for time series processing and learning. An ESN consists of a core of recurrent neural networks, called a reservoir, with a small number of tunable parameters to generate a highdimensional representation of an input, and a readout layer which is easily trained using regression to produce a desired output from the reservoir states. Certain computational tasks involve realtime
calculation of high-order time correlations, which requires nonlinear transformation either in the reservoir or the readout layer. Traditional ESN employs a reservoir with sigmoid or tanh function neurons. In contrast, some types of biological neurons obey response curves that can be described as a product unit rather than a sum and threshold. Inspired by this class of neurons, we introduce a RC architecture with a reservoir of product nodes for time series computation. We find that the product RC shows
many properties of standard ESN such as short-term memory and nonlinear capacity. On standard benchmarks for chaotic prediction tasks, the product RC maintains the performance of a standard nonlinear ESN while being more amenable to mathematical analysis. Our study provides evidence that such networks are powerful in highly nonlinear tasks owing to highorder statistics generated by the recurrent product node reservoir."
"Dirichlet-Hawkes Processes with Applications to Clustering Continuous-Time Document Streams.  Clusters in document streams, such as online news articles, can be induced by their textual contents, as well as by the temporal dynamics of their arriving patterns. Can we leverage both sources of information to obtain a better clustering of the documents, and distill information that is not possible to extract using contents only? In this paper, we propose a novel random process, referred to as the Dirichlet-Hawkes process, to take into account both information in a unified framework. A distinctive feature of the proposed model is that the preferential attachment of items to clusters according to cluster sizes, present in Dirichlet processes, is now driven according to the intensities of cluster-wise self-exciting temporal point processes, the Hawkes processes. This new model establishes a previously unexplored connection between Bayesian Nonparametrics and temporal Point Processes, which makes the number of clusters grow to accommodate the increasing complexity of online streaming contents, while at the same time adapts to the ever changing dynamics of the respective continuous arrival time. We conducted large-scale experiments on both synthetic and real world news articles, and show that Dirichlet-Hawkes processes can recover both meaningful topics and temporal
dynamics, which leads to better predictive performance in terms of content perplexity and arrival time of future documents."
"Neural Random-Access Machines.  In this paper, we propose and investigate a new neural network architecture called Neural Random Access Machine. It can manipulate and dereference pointers to an external variable-size random-access memory. The model is trained from pure input-output examples using backpropagation. 
We evaluate the new model on a number of simple algorithmic tasks whose solutions require pointer manipulation and dereferencing. Our results show that the proposed model can learn to solve algorithmic tasks of such type and is capable of operating on simple data structures like linked-lists and binary trees. For easier tasks, the learned solutions generalize to sequences of arbitrary length. Moreover, memory access during inference can be done in a constant time under some assumptions."
"Buyer and Nonprofit Levers to Improve Supplier Environmental Performance.  Material IQ (MiQ) is a new decision tool designed by GreenBlue to help suppliers safely share sensitive chemical-toxicity data with their customers. As GreenBlue takes MiQ to market, it must determine under what market conditions to promote the use of MiQ and when to recommend that a buyer use its implementation as an opportunity to work with an existing supplier. We study GreenBlue??s problem in two parts. First, we investigate when a buyer can use a wholesale-price premium and/or buyer-supplier cost sharing to improve a supplier??s environmental performance. Based on our findings, we then develop insights into GreenBlue??s strategy. We model both a single-supplier and a supplier-competition setting. We find that in the single-supplier setting, if the buyer??s optimal strategy is to offer the supplier a premium, then he also fully subsidizes her investment cost to build quality. By developing the supplier??s capabilities, the buyer can increase the impact of the premium he offers.  In the supplier-competition setting, although cost sharing is less effective as a lever, cases can occur in which the buyer chooses to share costs and prevent the incumbent supplier from having to compete. From GreenBlue??s perspective, promoting the use of MiQ and cost sharing are often viable strategies when there exists a one-to-one relationship between a buyer and a supplier. However, GreenBlue??s strategy becomes more restricted when competition exists between suppliers. Only when the relative market awareness of quality is high and there is a dominant party in the supply chain should GreenBlue recommend the use of MiQ."
"Web Browser Workload Characterization for Power Management on HMP Platforms.  The volume of mobile web browsing traffic has significantly
increased as well as the complexity of the mobile websites
mandating high-performance JavaScript engines such as Google??s
V8 to be used on mobile devices. Although there has been a
significant improvement in performance of JavaScript engine
on mobile phones in recent years, the power consumption re-
duction has not been addressed much. This paper presents
a case study for power management of JavaScript engine
V8 from Google in web browsers on a heterogeneous multi-
processing (HMP) platform. We analyze the detailed traces
of the thread workload generated by the web browser and
JavaScript engine, and discuss the power saving potentials
in relation to power management policies on Android. We
believe that this work will lead to development of practi-
cal power management techniques considering thread allo-
cation, dynamic voltage and frequency scaling (DVFS) and
power-gating."
"Perspective-aware manipulation of portrait photos.  This paper introduces a method to modify the apparent relative pose and distance between camera and subject given a single portrait photo. Our approach fits a full perspective camera and a parametric 3D head model to the portrait, and then builds a 2D warp in the image plane to approximate the effect of a desired change in 3D. We show that this model is capable of correcting objectionable artifacts such as the large noses sometimes seen in ??selfies,?? or to deliberately bring a distant camera closer to the subject. This framework can also be used to re-pose the subject, as well as to create stereo pairs from an input portrait. We show convincing results on both an existing dataset as well as a new dataset we captured to validate our method."
"Learning N-gram Language Models from Uncertain Data.  We present a new algorithm for efficiently training n-gram language models on uncertain data, and illustrate its use for semi-supervised language model adaptation.  We compute the probability that an n-gram occurs k times in the sample of uncertain data, and use the resulting histograms to derive a generalized Katz backoff model.  We compare semi-supervised adaptation of language models for YouTube video speech recognition in two conditions: when using full lattices with our new algorithm versus just the 1-best output from the baseline speech recognizer.  Unlike 1-best methods, the new algorithm provides models that yield solid improvements over the baseline on the full test set, and, further, achieves these gains without hurting performance on any of the set of channels.  We show that channels with the most data yielded the largest gains.  The algorithm was implemented via a new semiring in the OpenFst library and will be released as part of the OpenGrm ngram library."
"Generalized Transition-based Dependency Parsing.  In this paper, we present a transition-base parsing framework where a specific parser type is instantiated in terms of a set of abstract control parameters that constrain transitions between parser states. These
parameters enable a generalization across a range of transition-based parsing algorithms, including Arc-eager, Arc-standard,
and Easy-first. This generalization provides a unified framework that allows us to describe and compare various transition-based parsing approaches from a theoretical and empirical perspective. This includes
both previously studied transition systems, but potentially new systems as
well."
"Investigating Commercial Pay-Per-Install and the Distribution of Unwanted Software.  In this work, we explore the ecosystem of commercial pay-per-install (PPI) and the role it plays in the proliferation of unwanted software. Commercial PPI enables companies to bundle their applications with more popular software in return for a fee, effectively commoditizing access to user devices. We develop an analysis pipeline to track the business relationships underpinning four of the largest commercial PPI networks and classify the software families bundled. In turn, we measure their impact on end users and enumerate the distribution techniques involved. We find that unwanted ad injectors, browser settings hijackers, and cleanup utilities dominate the software families buying installs. Developers of these families pay $0.10--$1.50 per install---upfront costs that they recuperate by monetizing users without their consent or by charging exorbitant subscription fees. Based on Google Safe Browsing telemetry, we estimate that PPI networks drive over 60 million download attempts every week---nearly three times that of malware. While anti-virus and browsers have rolled out defenses to protect users from unwanted software, we find evidence that PPI networks actively interfere with or evade detection. Our results illustrate the deceptive practices of some commercial PPI operators that persist today."
"PlaNet - Photo Geolocation with Convolutional Neural Networks.  Is it possible to determine the location of a photo from just its pixels? While the general problem seems exceptionally difficult, photos often contain cues such as landmarks, weather patterns, vegetation, road markings, and architectural details, which in combination allow to infer the location. In computer vision, this problem is usually approached using image retrieval methods. In contrast, we pose the problem as one of classification by subdividing the surface of the earth into thousands of multi-scale geographic cells, and train a deep network using millions of geotagged images. We show that the resulting model, called PlaNet, outperforms previous approaches and even attains superhuman accuracy in some cases. Moreover, we extend our model to photo albums by combining it with a long short-term memory (LSTM) architecture. By learning to exploit temporal coherence to geolocate uncertain photos, this model achieves a 50% performance improvement over the single-image model."
"Detecting Events and Key Actors in Multi-Person Videos.  Multi-person event recognition is a challenging task, often with many people active in the scene but only a small subset contributing to an actual event. In this paper, we propose a model which learns to detect events in such videos while automatically ""attending"" to the people responsible for the event. Our model does not use explicit annotations regarding who or where those people are during training and testing. In particular, we track people in videos and use a recurrent neural network (RNN) to represent the track features. We learn time-varying attention weights to combine these features at each time-instant. The attended features are then processed using another RNN for event detection/classification. Since most video datasets with multiple people are restricted to a small number of videos, we also collected a new basketball dataset comprising 257 basketball games with 14K event annotations corresponding to 11 event classes. Our model outperforms state-of-the-art methods for both event classification and detection on this new dataset. Additionally, we show that the attention mechanism is able to consistently localize the relevant players."
"The Abuse Sharing Economy: Understanding the Limits of Threat Exchanges.  The underground commoditization of compromised hosts suggests a tacit capability where miscreants leverage the same machine---subscribed by multiple criminal ventures---to simultaneously profit from spam, fake account registration, malicious hosting, and other forms of automated abuse. To expedite the detection of these commonly abusive hosts, there are now multiple industry-wide efforts that aggregate abuse reports into centralized threat exchanges. In this work, we investigate the potential benefit of global reputation tracking and the pitfalls therein. We develop our findings from a snapshot of 45 million IP addresses abusing six Google services including Gmail, YouTube, and ReCaptcha between April 7--April 21, 2015. We estimate the scale of end hosts controlled by attackers, expose underground biases that skew the abuse perspectives of individual web services, and examine the frequency that criminals re-use the same infrastructure to attack multiple, heterogeneous services. Our results indicate that an average Google service can block 14% of abusive traffic based on threats aggregated from seemingly unrelated services, though we demonstrate that outright blacklisting incurs an untenable volume of false positives."
"Temporal Localization of Fine-Grained Actions in Videos by Domain Transfer from Web Images.  We address the problem of fine-grained action localization from temporally untrimmed web videos. We assume that only weak video-level annotations are available for training. The goal is to use these weak labels to identify temporal segments corresponding to the actions, and learn models that generalize to unconstrained web videos. We find that web images queried by action names serve as well-localized highlights for many actions, but are noisily labeled. To solve this problem, we propose a simple yet effective method that takes weak video labels and noisy image labels as input, and generates localized action frames as output. This is achieved by cross-domain transfer between video frames and web images, using pre-trained deep convolutional neural networks. We then use the localized action frames to train action recognition models with long short-term memory networks. We collect a fine-grained sports action data set FGA-240 of more than 130,000 YouTube videos. It has 240 fine-grained actions under 85 sports activities. Convincing results are shown on the FGA-240 data set, as well as the THUMOS 2014 localization data set with untrimmed training videos."
"Webly-supervised Video Recognition by Mutually Voting for Relevant Web Images and Web Video Frames.  Video recognition usually requires a large amount of training samples, which are expensive to be collected. An alternative and cheap solution is to draw from the large-scale images and videos from the Web. With modern search engines, the top ranked images or videos are usually highly correlated to the query, implying the potential to harvest the labeling-free Web images and videos for video recognition. However, there are two key difficulties that prevent us from using the Web data directly. First, they are typically noisy and may be from a completely different domain from that of users?? interest (e.g. cartoons). Second, Web videos are usually untrimmed and very lengthy, where some query-relevant frames are often hidden in between the irrelevant ones. A question thus naturally arises: to what extent can such noisy Web images and videos be utilized for labeling-free video recognition? In this paper, we propose a novel approach to mutually voting for relevant Web images and video frames, where two forces are balanced, i.e. aggressive matching and passive video frame selection. We validate our approach on three large-scale video recognition datasets."
"Dex-Net 1.0: A cloud-based network of 3D objects for robust grasp planning using a Multi-Armed Bandit model with correlated rewards.  This paper presents the Dexterity Network (Dex-Net) 1.0, a dataset of 3D object models and a sampling-based planning algorithm to explore how Cloud Robotics can be used for robust grasp planning. The algorithm uses a Multi- Armed Bandit model with correlated rewards to leverage prior grasps and 3D object models in a growing dataset that currently includes over 10,000 unique 3D object models and 2.5 million parallel-jaw grasps. Each grasp includes an estimate of the probability of force closure under uncertainty in object and gripper pose and friction. Dex-Net 1.0 uses Multi-View Convolutional Neural Networks (MV-CNNs), a new deep learning method for 3D object classification, to provide a similarity metric between objects, and the Google Cloud Platform to simultaneously run up to 1,500 virtual cores, reducing experiment runtime by up to three orders of magnitude. Experiments suggest that correlated bandit techniques can use a cloud-based network of object models to significantly reduce the number of samples required for robust grasp planning. We report on system sensitivity to variations in similarity metrics and in uncertainty in pose and friction. Code and updated information is available at http://berkeleyautomation.github.io/dex-net/."
"DieHard: reliable scheduling to survive correlated failures in cloud data centers.  In large scale data centers, a single fault can lead to correlated failures of several physical machines and the tasks running on them, simultaneously. Such correlated failures can severely damage the reliability of a service or a job. This paper models the impact of stochastic and correlated failures on job reliability in a data center. We focus on correlated failures caused by power outages or failures of network components, on jobs running multiple replicas of identical tasks. We present a statistical reliability model and an approximation technique for computing a job??s reliability in the presence of correlated failures. In addition, we address the problem of scheduling a job with reliability constraints. We formulate the scheduling problem as an optimization problem, with the aim being to achieve the desired reliability with the minimum number of extra tasks. We present a scheduling algorithm that approximates the minimum number of required tasks and a placement to achieve a desired job reliability.

We study the efficiency of our algorithm using an analytical approach and by simulating a cluster with different failure sources and reliabilities. The results show that the algorithm can effectively approximate the minimum number of extra tasks required to achieve the job??s reliability."
"Concrete Problems in AI Safety.  Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention
to the potential impacts of AI technologies on society. In this paper we discuss one such
potential impact: the problem of accidents in machine learning systems, defined as unintended
and harmful behavior that may emerge from poor design of real-world AI systems. We present a
list of five practical research problems related to accident risk, categorized according to whether
the problem originates from having the wrong objective function (??avoiding side effects?? and
??avoiding reward hacking??), an objective function that is too expensive to evaluate frequently
(??scalable supervision??), or undesirable behavior during the learning process (??safe exploration??
and ??distributional shift??). We review previous work in these areas as well as suggesting research
directions with a focus on relevance to cutting-edge AI systems. Finally, we consider
the high-level question of how to think most productively about the safety of forward-looking
applications of AI."
"API Usability at Scale.  Designing and maintaining useful and usable APIs remains challenging. At Google we manage hundreds of APIs. In this article we report on the experience of doing so and describe six on-going challenges: resource allocation, empirically-grounded guidelines, communicating issues, supporting API evolution over time, usable auth, and usable client libraries at scale."
"Automatically Scheduling Halide Image Processing Pipelines.  The Halide image processing language has proven to be an effective system for authoring high-performance image processing code. Halide programmers need only provide a high-level strategy for mapping an image processing pipeline to a parallel machine (a schedule), and the Halide compiler carries out the mechanical task of generating platform-specific code that implements the schedule. Unfortunately, designing high-performance schedules for complex image processing pipelines requires substantial knowledge of modern hardware architecture and code-optimization techniques. In this paper we provide an algorithm for automatically generating high-performance schedules for Halide programs. Our solution extends the function bounds analysis already present in the Halide compiler to automatically perform locality and parallelism-enhancing global program transformations typical of those employed by expert Halide developers. The algorithm does not require costly (and often impractical) auto-tuning, and, in seconds, generates schedules for a broad set of image processing benchmarks that are performance-competitive with, and often better than, schedules manually authored by expert Halide developers on server and mobile CPUs, as well as GPUs."
"Distributed representation and estimation of WFST-based n-gram models.  We present methods for partitioning a weighted finite-state transducer (WFST) representation of an n-gram language model into multiple shards, each of which is a stand-alone WFST n-gram model in its own right, allowing processing with existing algorithms. After independent estimation, including normalization, smoothing and pruning on each shard, the shards can be merged into a single WFST that is identical to the model that would have resulted from estimation without sharding. We then present an approach that uses data partitions in conjunction with WFST sharding to estimate models on orders-of-magnitude more data than would have otherwise been feasible with a single process. We present some numbers on shard characteristics when large models are trained from a very large data set.  Functionality to support distributed n-gram modeling has been added to the OpenGrm library."
"WikiReading: A Novel Large-scale Language Understanding Task over Wikipedia.  We present WikiReading, a large-scale natural language understanding task and publicly-available dataset with 18 million instances. The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the corresponding Wikipedia articles. The task contains a rich variety of challenging classification and extraction sub-tasks, making it well-suited for end-to-end models such as deep neural networks (DNNs). We compare various state-of-the-art DNN-based architectures for document classification, information extraction, and question answering. We find that models supporting a rich answer space, such as word or character sequences, perform best. Our best-performing model, a word-level sequence to sequence model with a mechanism to copy out-of-vocabulary words, obtains an accuracy of 71.8%."
"Deep Neural Networks for YouTube Recommendations.  We present WikiReading, a large-scale natural language understanding task and publicly-available dataset with 18 million instances. The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the corresponding Wikipedia articles. The task contains a rich variety of challenging classification and extraction sub-tasks, making it well-suited for end-to-end models such as deep neural networks (DNNs). We compare various state-of-the-art DNN-based architectures for document classification, information extraction, and question answering. We find that models supporting a rich answer space, such as word or character sequences, perform best. Our best-performing model, a word-level sequence to sequence model with a mechanism to copy out-of-vocabulary words, obtains an accuracy of 71.8%."
"Computer Vision for Active and Assisted Living.  Chapter on Computer Vision for Active and Assisted Living - State of the art review of main applications, methods and image processing stages."
"Full Resolution Image Compression with Recurrent Neural Networks.  This paper presents a set of full-resolution lossy image compression methods based on neural networks. Each of the architectures we describe can provide variable compression rates during deployment without requiring retraining of the network: each network need only be trained once. All of our architectures consist of a recurrent neural network (RNN)-based encoder and decoder, a binarizer, and a neural network for entropy coding. We compare RNN types (LSTM, associative LSTM) and introduce a new hybrid of GRU and ResNet. We also study ""one-shot"" versus additive reconstruction architectures and introduce a new scaled-additive framework. We compare to previous work, showing improvements of 4.3%-8.8% AUC (area under the rate-distortion curve), depending on the perceptual metric used. As far as we know, this is the first neural network architecture that is able to outperform JPEG at image compression across most bitrates on the rate-distortion curve on the Kodak dataset images, with and without the aid of entropy coding."
"A Piggyback System for Joint Entity Mention Detection and Linking in Web Queries.  In this paper we study the problem of linking open-domain web-search queries towards entities drawn from the full entity inventory of Wikipedia articles. We introduce SMAPH- 2 to attack this problem, a second-order approach that, by piggybacking on a web search engine, alleviates the noise and irregularities that characterize the language of queries and puts queries in a larger context in which it is easier to make sense of them. The key algorithmic idea under- lying SMAPH-2 is to first discover a candidate set of entities and then link-back those entities to their mentions occurring in the input query. This allows us to confine the possible concepts pertinent to the query to only the ones really mentioned in it. The link-back is implemented via a collective disambiguation step based upon a supervised ranking model that makes one joint prediction for the annotation of the complete query optimizing directly the F1 mea- sure. We evaluate both known features, such as word em- beddings and semantic relatedness among entities, and several novel features such as an approximate distance between mentions and entities (which can handle spelling errors). We demonstrate that SMAPH-2 achieves state-of-the-art on the ERD@SIGIR2014 benchmark. We also publish GERDAQ, a novel dataset we built specifically for web-query entity linking via a crowdsourcing effort, and show that SMAPH- 2 outperforms the benchmarks by comparable margins on GERDAQ."
"DROWN: Breaking TLS using SSLv2.  We present DROWN, a novel cross-protocol attack that
can decrypt passively collected TLS sessions from upto-date
clients by using a server supporting SSLv2 as a
Bleichenbacher RSA padding oracle. We present two versions
of the attack. The more general form exploits a combination
of thus-far unnoticed protocol flaws in SSLv2
to develop a new and stronger variant of the Bleichenbacher
attack. A typical scenario requires the attacker
to observe 1,000 TLS handshakes, then initiate 40,000
SSLv2 connections and perform 2
50 offline work to decrypt
a 2048-bit RSA TLS ciphertext. (The victim client
never initiates SSLv2 connections.) We implemented the
attack and can decrypt a TLS 1.2 handshake using 2048-
bit RSA in under 8 hours using Amazon EC2, at a cost
of $440. Using Internet-wide scans, we find that 33% of
all HTTPS servers and 22% of those with browser-trusted
certificates are vulnerable to this protocol-level attack,
due to widespread key and certificate reuse.
For an even cheaper attack, we apply our new techniques
together with a newly discovered vulnerability in
OpenSSL that was present in releases from 1998 to early
2015. Given an unpatched SSLv2 server to use as an
oracle, we can decrypt a TLS ciphertext in one minute on
a single CPU??fast enough to enable man-in-the-middle
attacks against modern browsers. 26% of HTTPS servers
are vulnerable to this attack.
We further observe that the QUIC protocol is vulnerable
to a variant of our attack that allows an attacker to
impersonate a server indefinitely after performing as few
as 225 SSLv2 connections and 265 offline work.
We conclude that SSLv2 is not only weak, but actively
harmful to the TLS ecosystem."
"Large-Scale Analysis of Viewing Behavior: Towards Measuring Satisfaction with Mobile Proactive Systems.  Recently, proactive systems such as Google Now and Microsoft Cortana have become increasingly popular in reforming the way users access information on mobile devices. In these systems, relevant content is presented to users based on their context without a query in the form of information cards that do not require a click to satisfy the users. As a result, prior approaches based on clicks cannot provide reliable measurements of user satisfaction with such systems. It is also unclear how much of the previous findings regarding good abandonment with reactive Web searches can be applied to these proactive systems due to the intrinsic difference in user intent, the greater variety of content types and their presentations. In this paper, we present the first large-scale analysis of viewing behavior based on the viewport (the visible fraction of a Web page) of the mobile devices, towards measuring user satisfaction with the information cards of the mobile proactive systems. In particular, we identified and analyzed a variety of factors that may influence the viewing behavior, including biases from ranking positions, the types and attributes of the information cards, and the touch interactions with the mobile devices. We show that by modeling the various factors we can better measure user satisfaction with the mobile proactive systems, enabling stronger statistical power in large-scale online A/B testing."
"CSP Is Dead, Long Live CSP! On the Insecurity of Whitelists and the Future of Content Security Policy.  Content Security Policy is a web platform mechanism designed to mitigate cross-site scripting (XSS), the top security vulnerability in modern web applications. In this paper, we take a closer look at the practical benefits of adopting CSP and identify significant flaws in real-world deployments that result in bypasses in 94.72% of all distinct policies.
We base our Internet-wide analysis on a search engine corpus of approximately 100 billion pages from over 1 billion hostnames; the result covers CSP deployments on 1,680,867 hosts with 26,011 unique CSP policies ?? the most comprehensive study to date. We introduce the security-relevant aspects of the CSP specification and provide an in-depth analysis of its threat model, focusing on XSS protections. We identify three common classes of CSP bypasses and explain how they subvert the security of a policy. We then turn to a quantitative analysis of policies deployed on the Internet in order to understand their security benefits. We observe that 14 out of the 15 domains most commonly whitelisted for loading scripts contain unsafe endpoints; as a consequence, 75.81% of distinct policies use script whitelists that allow attackers to bypass CSP. In total, we find that 94.68% of policies that attempt to limit script execution are ineffective, and that 99.34% of hosts with CSP use policies that offer no benefit against XSS. Finally, we propose the ??strict-dynamic?? keyword, an addition to the specification that facilitates the creation of policies based on cryptographic nonces, without relying on domain whitelists. We discuss our experience deploying such a nonce-based policy in a complex application and provide guidance to web authors for improving their policies."
Annotating Topic Development in Information Seeking Queries.  This paper contributes to the limited body of empirical research into the domain of discourse structure of information seeking queries. In this paper we describe the development of an annotation schema for coding topic development in information seeking queries and the initial observations from a pilot sample of query sessions. The main idea explored is the relationship between constant and variable discourse entities and their role in tracking changes in the topic progression. We argue that the topicalized entities remain stable across discourse moves and can be identified by a simple mechanism where anaphora resolution is a precursor. We also claim that a corpus annotated in this framework can be used as training data for dialogue management and computational semantics systems.
"Train faster, generalize better: Stability of stochastic gradient descent.  We show that any model trained by a stochastic gradient method with few
iterations has vanishing generalization error. Our results apply to both
convex and non-convex optimization under standard Lipschitz and smoothness assumptions. Our bounds hold in cases where existing uniform convergence bounds do not apply, for instance, if there is no explicit form of
regularization and the model capacity far exceeds the sample size. Conceptually, our findings help explain the widely observed empirical success of training large models with gradient descent methods. They further underscore the  importance of reducing training time beyond the obvious benefit of saving time."
"Domain Separation Networks.  The cost of large scale data collection and annotation often makes the application of machine learning algorithms to new tasks or datasets prohibitively expensive. One approach circumventing this cost is training models on synthetic data where annotations are provided automatically. Despite their appeal, such models often fail to generalize from synthetic to real images, necessitating domain adaptation algorithms to manipulate these models before they can be successfully applied. Existing approaches focus either on mapping representations from one domain to the other, or on learning to extract features that are invariant to the domain from which they were extracted. However, by focusing only on creating a mapping or shared representation between the two domains, they ignore the individual characteristics of each domain. We suggest that explicitly modeling what is unique to each domain can improve a model's ability to extract domain-invariant features. Inspired by work on private-shared component analysis, we explicitly learn to extract image representations that are partitioned into two subspaces: one component which is private to each domain and one which is shared across domains. Our model is trained not only to perform the task we care about in the source domain, but also to use the partitioned representation to reconstruct the images from both domains. Our novel architecture results in a model that outperforms the state-of-the-art on a range of unsupervised domain adaptation scenarios and additionally produces visualizations of the private and shared representations enabling interpretation of the domain adaptation process."
"Cross-lingual projection for class-based language models.  This paper presents a cross-lingual projection
technique for training class-based
language models. We borrow from previous
success in projecting POS tags and
NER mentions to that of a trained classbased
language model. We use a CRF
to train a model to predict when a sequence
of words is a member of a given
class and use this to label our language
model training data. We show that we can
successfully project the contextual cues
for these classes across pairs of languages
and retain a high quality class model in
languages with no supervised class data.
We present empirical results that show the
quality of the projected models as well
as their effect on the down-stream speech
recognition objective. We are able to
achieve over half the reduction of WER
when using the projected class models as
compared to models trained on human annotations."
"Contextual prediction models for speech recognition.  We introduce an approach to biasing language models towards
known contexts without requiring separate language models or
explicit contextually-dependent conditioning contexts. We do
so by presenting an alternative ASR objective, where we predict
the acoustics and words given the contextual cue, such as
the geographic location of the speaker. A simple factoring of the
model results in an additional biasing term, which effectively
indicates how correlated a hypothesis is with the contextual cue
(e.g., given the hypothesized transcript, how likely is the user??s
known location). We demonstrate that this factorization allows
us to train relatively small contextual models which are effective
in speech recognition. An experimental analysis shows both a
perplexity reduction and a significant word error rate reductions
on a voice search task when using the user??s location as a contextual
cue."
"Molecular graph convolutions: moving beyond fingerprints.  Molecular ??fingerprints?? encoding structural
information are the workhorse of cheminfor-
matics and machine learning in drug discovery
applications. However, fingerprint representa-
tions necessarily emphasize particular aspects
of the molecular structure while ignoring others,
rather than allowing the model to make data-
driven decisions. We describe molecular graph
convolutions, a fully integrated machine learn-
ing architecture for learning from undirected
graphs, such as small molecules. Graph convo-
lutions use a simple encoding of the molecular
graph (atoms, bonds, distances, etc.), allowing
the model to take full advantage of information
in the graph structure."
"A Neural Transducer.  Sequence-to-sequence models have achieved impressive results on various tasks. However, they are unsuitable for tasks that require incremental predictions to be made as more data arrives or tasks that have long input sequences and output sequences. This is because they generate an output sequence conditioned on an entire input sequence. In this paper, we present a Neural Transducer that can make incremental predictions as more input arrives, without redoing the entire computation. Unlike sequence-to-sequence models, the Neural Transducer computes the next-step distribution conditioned on the partially observed input sequence and the partially generated sequence. At each time step, the transducer can decide to emit zero to many output symbols. The data can be processed using an encoder and presented as input to the transducer. The discrete decision to emit a symbol at every time step makes it difficult to learn with conventional backpropagation. It is however possible to train the transducer by using a dynamic programming algorithm to generate target discrete decisions. Our experiments show that the Neural Transducer works well in settings where it is required to produce output predictions as data come in. We also find that the Neural Transducer performs well for long sequences even when attention mechanisms are not used."
"Deep Learning Games.  We investigate a reduction of supervised learning to game playing that reveals new connections and learning methods.  For convex one-layer problems, we demonstrate an equivalence between global minimizers of the training problem and Nash equilibria in a simple game.  We then show how the game can be extended to general acyclic neural networks with differentiable convex gates, establishing a bijection between the Nash equilibria and critical (or KKT) points of the deep learning problem.  Based on these connections we investigate alternative learning methods, and find that regret matching can achieve competitive training performance while producing sparser models than current deep learning approaches."
"Exponential expressivity in deep neural networks through transient chaos.  We combine Riemannian geometry with the mean field theory of high dimensional chaos to study the nature of signal propagation in generic, deep neural networks with random weights. Our results reveal an order-to-chaos expressivity phase transition, with networks in the chaotic phase computing nonlinear functions whose global curvature grows exponentially with depth but not width. We prove this generic class of deep random functions cannot be efficiently computed by any shallow network, going beyond prior work restricted to the analysis of single functions. Moreover, we formalize and quantitatively demonstrate the long conjectured idea that deep networks can disentangle highly curved manifolds in input space into flat manifolds in hidden space. Our theoretical analysis of the expressive power of deep networks broadly applies to arbitrary nonlinearities, and provides a quantitative underpinning for previously abstract notions about the geometry of deep functions."
"Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity.  We develop a general duality between neural networks and compositional kernels, striving towards a better understanding of deep learning. We show that initial representations generated by common random initializations are sufficiently rich to express all functions in the dual kernel space. Hence, though the training objective is hard to optimize in the worst case, the initial weights form a good starting point for optimization. Our dual view also reveals a pragmatic and aesthetic perspective of neural networks and underscores their expressive power."
"Attend, Infer, Repeat: Fast Scene Understanding with Generative Models.  We present a framework for efficient inference in structured image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps. We use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects - counting, locating and classifying the elements of a scene - without any supervision, e.g., decomposing 3D images with various numbers of objects in a single forward pass of a neural network. We further show that the networks produce accurate inferences when compared to supervised counterparts, and that their structure leads to improved generalization."
"Lower Frame Rate Neural Network Acoustic Models.  Recently neural network acoustic models trained with Connectionist
Temporal Classification (CTC) were proposed as an alternative approach
to conventional cross-entropy trained neural network acoustic models which output frame-level decisions every 10ms~\cite{senior15asru}. As opposed to
conventional models, CTC learns an alignment jointly with the acoustic
model, and outputs a \textit{blank} symbol in addition to the
regular acoustic state units. This allows the CTC model to run with a
lower frame rate, outputting decisions every 30ms rather than 10ms as
in conventional models, thus improving overall system latency. In this
work, we explore how conventional models behave with lower frame
rates. On a large vocabulary Voice Search task, we will show that with
conventional models, we can slow the frame rate to 40ms while improving WER by 3\% relative over a CTC-based model."
"Harvesting the Low-hanging Fruits: Defending Against  Automated Large-Scale Cyber-Intrusions  by Focusing on the Vulnerable Population.  The orthodox paradigm to defend against automated social-engineering attacks in large-scale socio-technical systems is reactive and victim-agnostic. Defenses generally focus on identifying the attacks/ attackers (e.g., phishing emails, social-bot infiltration, malware offered for download). To change the status quo, we propose to identify, even if imperfectly, the vulnerable user population, that is, the users that are likely to fall victim to such attacks. Once identified, information about the vulnerable population can be used in two ways. First, the vulnerable population can be influenced by the defender through several means including: education, specialized user experience, extra protection layers and watchdogs. In the same vein, information about the vulnerable population ultimately be used also be used to fine-tune and reprioritize defense mechanisms to offer differentiated protection, possibly at the cost of additional friction generated by the defense mechanism. Secondly, information about the user population can be used to identify an attack (or compromised users) based on differences between the general and the vulnerable population. This paper considers the implications of the proposed paradigm on existing defenses in three areas (phishing of user credentials, malware distribution and socialbot infiltration) and discusses how using knowledge of the vulnerable population can enable more robust defenses."
"Pynini: A Python library for weighted finite-state grammar compilation.  We present Pynini, an open-source library allowing users to compile weighted finite-state transducers (FSTs) and pushdown transducers from strings, context-dependent rewrite rules, and recursive transition networks. Pynini uses the OpenFst library for encoding, modifying, and applying WFSTs, as well as a powerful generic optimization routine. We describe the design of this library and the algorithms and interfaces used for FST and PDT compilation and optimization, and illustrate its use for a natural language processing application."
"Unsupervised Word Segmentation and Lexicon  Discovery Using Acoustic Word Embeddings.  In settings where only unlabelled speech data is available, speech technology needs to be developed without transcriptions, pronunciation dictionaries, or language modelling text. A similar problem is faced when modelling infant language acquisition. In these cases, categorical linguistic structure needs to be discovered directly from speech audio. We present a novel unsupervised Bayesian model that segments unlabelled speech and clusters the segments into hypothesized word groupings. The result is a complete unsupervised tokenization of the input speech in terms of discovered word types. In our approach, a potential word segment (of arbitrary length) is embedded in a fixed-dimensional acoustic vector space. The model, implemented as a Gibbs sampler, then builds a whole-word acoustic model in this space while jointly performing segmentation. We report word error rates in a small-vocabulary connected digit recognition task by mapping the unsupervised decoded output to ground truth transcriptions. The model achieves around 20% error rate, outperforming a previous HMM-based system by about 10% absolute. Moreover, in contrast to the baseline, our model does not require a pre-specified vocabulary size."
"Flash reliability in the field: The expected and the unexpected.  The use of solid state drives based on NAND flash technology is continuously growing. As more data either lives on flash or is being cached on flash, data durability and availability critically depend on flash reliability. 
This paper provides a detailed field study of flash reliability based on data collected over 6 years in a large-scale data center production environment. The data spans many millions of drive days, ten different drive models, different flash technologies (MLC and SLC) and feature sizes (ranging from 24nm to 50nm). The paper analyses this data in order to derive a better understanding of flash reliability in the field, including the most prevalent types of errors and hardware failures and their frequency, and how different factors impact flash reliability."
"Recent Advances in Google Real-time HMM-driven Unit Selection Synthesizer.  This paper presents advances in Google's hidden Markov model (HMM)-driven unit selection speech synthesis system. We describe several improvements to the run-time system; these include minimal
latency, high-quality and fast refresh cycle for new voices. Traditionally unit selection synthesizers are limited in terms of the amount of data they can handle and the real applications they
are built for. That is even more critical for real-life large-scale applications where high-quality is expected and low latency is required given the available computational resources. In this paper we present an optimized engine to handle a large database at runtime, a composite unit search approach for combining diphones and phrase-based units.  In addition a new voice building strategy for handling big
databases and keeping the building times low is presented."
"Importance Weighting Without Importance Weights: An Efficient Algorithm for Combinatorial Semi-Bandits.  We propose a sample-efficient alternative for importance weighting for situations where one only
has sample access to the probability distribution that generates the observations. Our new method,
called Geometric Resampling (GR), is described and analyzed in the context of online combinatorial
optimization under semi-bandit feedback, where a learner sequentially selects its actions from a
combinatorial decision set so as to minimize its cumulative loss. In particular, we show that
the well-known Follow-the-Perturbed-Leader (FPL) prediction method coupled with Geometric
Resampling yields the first computationally efficient reduction from offline to online optimization
in this setting. We provide a thorough theoretical analysis for the resulting algorithm, showing that
its performance is on par with previous, inefficient solutions. Our main contribution is showing
that, despite the relatively large variance induced by the GR procedure, our performance guarantees
hold with high probability rather than only in expectation. As a side result, we also improve the
best known regret bounds for FPL in online combinatorial optimization with full feedback, closing
the perceived performance gap between FPL and exponential weights in this setting."
"Abstract Data Types in Object-Capability Systems.  The distinctions between the two forms of procedural data abstraction -- abstract data types and objects -- are well known. An abstract data type provides an opaque type declaration, and an implementation that manipulates the modules of the abstract type, while an object uses procedural abstraction to hide an individual implementation. The object-capability model has been proposed to en- able object-oriented programs to be written securely, and has been adopted by a number of practical languages including JavaScript, E, and Newspeak. This short paper addresses the question: how can we implement abstract data types in an object-capability language?"
Improving topic clustering on search queries with word co-occurrence and bipartite graph co-clustering.  Uncovering common themes from a large number of unorganized search queries is a primary step to mine insights about aggregated user interests. Common topic modeling techniques for document modeling often face sparsity problems with search query data as these are much shorter than documents. We present two novel techniques that can discover semantically meaningful topics in search queries: i) word co-occurrence clustering generates topics from words frequently occurring together; ii) weighted bigraph clustering uses URLs from Google search results to induce query similarity and generate topics. We exemplify our proposed methods on a set of Lipton brand as well as make-up &amp; cosmetics queries. A comparison to standard LDA clustering demonstrates the usefulness and improved performance of the two proposed methods.
"Permission and Authority Revisited: towards a formalization.  Miller??s notions of permissions and authority are foundational to the analysis of object-capability programming. Informal definitions of these concepts were given in Miller??s thesis. In this paper we propose definitions for permissions and authority, based on a small object-oriented calculus. We quantify their bounds (current, eventual, behavioral, topological), and delineate the relationships between these definitions."
"Decision Alignment (extended abstract).  When one object makes a request of another, why do we expect that the second object's behavior correctly satisfies the first object's wishes? The need to cope with such <em>principal-agent problems</em> shapes programming practice as much as it shapes human organizations and economies. However, the literature about such plan coordination issues among humans is almost disjoint from the literature about these issues among objects. Even the terms used are unrelated."
Carrier Recovery in Coherent Optical Communication Systems.  This book chapter presents a systematic review on carrier recovery technologies recently developed for high-spectral efficiency coherent transmission systems.
"Scalable Learning of Non-Decomposable Objectives.  Modern retrieval systems are often driven by an underlying machine learning model. The goal of such systems is to identify and possibly rank the few most relevant items for a given query or context. Thus, such systems are typically evaluated using a ranking-based performance metric such as the area under the precision-recall curve, the F?? score, precision at fixed recall, etc. Obviously, it is desirable to train such systems to optimize the metric of interest. In practice, due to the scalability limitations of existing approaches for optimizing such objectives, large-scale retrieval systems are instead trained to maximize classification accuracy, in the hope that performance as measured via the true objective will also be favorable. In this work we present a unified framework that, using straightforward building block bounds, allows for highly scalable optimization of a wide range of ranking-based objectives. We demonstrate the advantage of our approach on several real-life retrieval problems that are significantly larger than those considered in the literature, while achieving substantial improvement in performance over the accuracy-objective baseline."
"Semantic Video Trailers.  Query-based video summarization is the task of creating a brief visual trailer, which captures the parts of the video (or a collection of videos) that are most relevant to the user-issued query. In this paper, we propose an unsupervised label propagation approach for this task. Our approach effectively captures the multimodal semantics of queries and videos using state-of-the-art deep neural networks and creates a summary that is both semantically coherent and visually attractive. We describe the theoretical framework of our graph-based approach and empirically evaluate its effectiveness in creating relevant and attractive trailers. Finally, we showcase example video trailers generated by our system."
"The Left Hand of Equals.  When is one object equal to another object? While object identity is fundamental to object-oriented systems, object equality, although tightly intertwined with identity, is harder to pin down. The distinction between identity and equality is reflected in object-oriented languages, almost all of which provide two variants of ??equality??, while some provide many more. Programmers can usually override at least one of these forms of equality, and can always define their own methods to distinguish their own objects. This essay takes a reflexive journey through fifty years of identity and equality in object-oriented languages, and ends somewhere we did not expect: a ??left-handed?? equality relying on trust and grace."
"Distributed Authorization in Vanadium.  This paper presents an authorization model for distributed systems that operate with limited internet connectivity. Reliable internet access remains a luxury for a majority of the world??s population. Even for those who can afford it, a dependence on internet connectivity leads to sub-optimal user experiences. With a focus on decentralized deployment, this model is suitable for scenarios where devices right next to each other (such as a sensor or a friend??s phone) should be able to communicate securely in a peer-to-peer manner. The model combines several known techniques from previous work on SPKI/SDSI [26, 19], Macaroons [10], and the vast literature on trust management [12]. It has been deployed as part of the open-source framework Vanadium [6] that offers a set of tools, libraries and services for developing secure, distributed applications that can run over a network of devices."
"Reward Augmented Maximum Likelihood for Neural Structured Prediction.  A key problem in structured output prediction is direct optimization of the task reward function that matters for test evaluation. This paper presents a simple and computationally efficient approach to incorporate task reward into a maximum likelihood framework. We establish a connection between the log-likelihood and regularized expected reward objectives, showing that at a zero temperature, they are approximately equivalent in the vicinity of the optimal solution. We show that optimal regularized expected reward is achieved when the conditional distribution of the outputs given the inputs is proportional to their exponentiated (temperature adjusted) rewards. Based on this observation, we optimize conditional log-probability of edited outputs that are sampled proportionally to their scaled exponentiated reward. We apply this framework to optimize edit distance in the output label space. Experiments on speech recognition and machine translation for neural sequence to sequence models show notable improvements over a maximum likelihood baseline by using edit distance augmented maximum likelihood."
"Picasso: Lightweight Device Class Fingerprinting for Web Clients.  In this work we present Picasso: a lightweight device class fingerprinting protocol that allows a server to verify the software and hardware stack of a mobile or desktop client. As an example, Picasso can distinguish between traffic sent by an authentic iPhone running Safari on iOS from an emulator or desktop client spoofing the same configuration. Our fingerprinting scheme builds on unpredictable yet stable noise introduced by a client's browser, operating system, and graphical stack when rendering HTML5 canvases. Our algorithm is resistant to replay and includes a hardware-bound proof of work that forces a client to expend a configurable amount of CPU and memory to solve challenges. We demonstrate that Picasso can distinguish 52 million Android, iOS, Windows, and OSX clients running a diversity of browsers with 100% accuracy. We discuss applications of Picasso in abuse fighting, including protecting the Play Store or other mobile app marketplaces from inorganic interactions; or identifying login attempts to user accounts from previously unseen device classes."
"A Staircase Transform Coding Scheme for Screen Content Video Coding.  Screen content videos that typically contain computer generated texts and graphics are getting more demanding in nowadays online video service. They involve a great amount of circumstances that are not commonly seen in natural videos, including sharp edge transition and repetitive pattern, which make their statistical characteristics distinct from those of natural videos. This makes it questionable about the efficacy of the conventional discrete cosine transform (DCT), which builds on the Gauss-Markov model assumption that leads to a base-band signal, on coding the computer-generated graphics. This work exploits a class of staircase transforms. Unlike the DCT whose bases are samplings of sinusoidal functions, the staircase transforms have their bases sampled from staircase functions, which naturally better approximate the sharp transitions often encountered in the context of screen content. As an alternative transform kernel, the staircase transform is integrated into a hybrid transform coding scheme, in conjunction with DCT. It is experimentally shown that the proposed approach provides an average of 2.9% compression performance gains in terms of BD-rate reduction. A perceptual comparison further demonstrates that the use of staircase transform achieves substantial reduction in ringing artifact due to the Gibbs phenomenon."
"A DYNAMIC MOTION VECTOR REFERENCING SCHEME FOR VIDEO CODING.  Video codec exploits temporal redundancy of video signal, in the form of motion compensated prediction, to achieve superior compression performance. The coding of motion vectors takes a large portion of the total rate cost. Prior research utilizes the spatial and temporal correlations of the motion field to improve the coding efficiency of the motion information. It typically constructs a candidate pool composed of a fixed number of reference motion vectors and allows the codec to select and reuse the one that best approximates the motion activity of the current block. This largely disconnects the entropy coding process from the true boundary conditions, since it is masked by the fix-length candidate list, and hence could potentially cause sub-optimal coding performance. An alternative motion vector referencing scheme is proposed in this work to fully accommodate the dynamic nature of the boundary conditions for compression efficiency. It adaptively extends or shortens the candidate list according to the actual number of available reference motion vectors. The associated probability model accounts for the likelihood that an individual motion vector candidate is used. A complementary motion vector candidate ranking system is also presented here. It is experimentally shown that the proposed scheme achieves considerable compression performance gains across all the test sets."
"Learning for Efficient Supervised Query Expansion via  Two-stage Feature Selection.  Query expansion (QE) is a well known technique to improve retrieval effectiveness, which expands original queries with extra terms that are predicted to be relevant. A recent trend in the literature is Supervised Query Expansion(SQE), where supervised learning is introduced to better select expansion terms. However, an important but neglected issue for SQE is its efficiency, as applying SQE in retrieval
can be much more time-consuming than applying Unsupervised Query Expansion (UQE) algorithms. In this paper, we point out that the cost of SQE mainly comes from term feature extraction, and propose a Two-stage Feature Selection framework (TFS) to address this problem. The first stage is adaptive expansion decision, which determines if a query is suitable for SQE or not. For unsuitable queries, SQE is skipped and no term features are extracted at all, which reduces the most time cost. For those suitable queries, the second stage is cost constrained feature selection, which
chooses a subset of effective yet inexpensive features for supervised learning. Extensive experiments on four corpora (including three academic and one industry corpus) show that our TFS framework can substantially reduce the time cost for SQE, while maintaining its effectiveness."
"Bilateral Guided Upsampling.  We present an algorithm to accelerate a large class of image processing operators. Given a low-resolution reference input and output pair, we model the operator by fitting local curves that map the input to the output. We can then produce a full-resolution output by evaluating these low-resolution curves on the full-resolution input. We demonstrate that this faithfully models state-of-the-art operators for tone mapping, style transfer, and recoloring. The curves are computed by lifting the input into a bilateral grid and then solving for the 3D array of affine matrices that best maps input color to output color per x, y, intensity bin. We enforce a smoothness term on the matrices which prevents false edges and noise amplification. We can either globally optimize this energy, or quickly approximate a solution by locally fitting matrices and then enforcing smoothness by blurring in grid space. This latter option reduces to joint bilateral upsampling or the guided filter depending on the choice of parameters. The cost of running the algorithm is reduced to the cost of running the original algorithm at greatly reduced resolution, as fitting the curves takes about 10 ms on mobile devices, and 1-2 ms on desktop CPUs, and evaluating the curves can be done with a simple GPU shader."
"Burst photography for high dynamic range and low-light imaging on mobile cameras.  Cell phone cameras have small apertures, which limits the number of photons they can gather, leading to noisy images in low light. They also have small sensor pixels, which limits the number of electrons each pixel can store, leading to limited dynamic range. We describe a computational photography pipeline that captures, aligns, and merges a burst of frames to reduce noise and increase dynamic range. Our system has several key features that help make it robust and efficient. First, we do not use bracketed exposures. Instead, we capture frames of constant exposure, which makes alignment more robust, and we set this exposure low enough to avoid blowing out highlights. The resulting merged image has clean shadows and high bit depth, allowing us to apply standard HDR tone mapping methods. Second, we begin from Bayer raw frames rather than the demosaicked RGB (or YUV) frames produced by hardware Image Signal Processors (ISPs) common on mobile platforms. This gives us more bits per pixel and allows us to circumvent the ISP's unwanted tone mapping and spatial denoising. Third, we use a novel FFT-based alignment algorithm and a hybrid 2D/3D Wiener filter to denoise and merge the frames in a burst. Our implementation is built atop Android's Camera2 API, which provides per-frame camera control and access to raw imagery, and is written in the Halide domain-specific language (DSL). It runs in 4 seconds on device (for a 12 Mpix image), requires no user intervention, and ships on several mass-produced cell phones."
"Sparse Non-negative Matrix Language Modeling.  We present Sparse Non-negative Matrix (SNM) estimation, a novel probability estimation technique for language modeling that can efficiently incorporate arbitrary features. We evaluate SNM language models on two corpora: the One Billion Word Benchmark and a subset of the LDC English Gigaword corpus. Results show that SNM language models trained with n-gram features are a close match for the well-established Kneser-Ney models. The addition of skip-gram features yields a model that is in the same league as the state-of-the-art recurrent neural network language models, as well as complementary: combining the two modeling techniques yields the best known result on the One Billion Word Benchmark. On the Gigaword corpus further improvements are observed using features that cross sentence boundaries. The computational advantages of SNM estimation over both maximum entropy and neural network estimation are probably its main strength, promising an approach that has large flexibility in combining arbitrary features and yet scales gracefully to large amounts of data. Presented at EMNLP 2016 (Austin, Texas), see slide deck: https://research.google.com/pubs/pub45647.html."
"Liveness becomes Entelechy - A scheme for L6.  Integrated development environments have provided increasingly powerful tools for software creation, and yet the creation of complex computer programs remains difficult and time-consuming.  Liveness in a programming environment has been identified as one direction in which to pursue further improvements in programmer productivity.  We propose a scheme for achieving strategically predictive liveness, that is a scheme which can predict and evaluate considerable features of an application. The scheme exploits statistical properties of code to allow for  synthesis and evaluation of code that is most likely to be useful to the developer. We hypothesise that this will help inculcate liveness into mainstream technical practice."
"Users Really Do Plug in USB Drives They Find.  We investigate the anecdotal belief that end users will pick up and plug in USB flash drives they find by completing a controlled experiment in which we drop 297 flash drives on a large university campus. We find that the attack is effective with an estimated success rate of 45??98% and expeditious with the first drive connected in less than six minutes. We analyze the types of drives users connected and survey those users to understand their motivation and security profile. We find that a drive??s appearance does not increase attack success. Instead, users connect the drive with the altruistic intention of finding the owner. These individuals are not technically incompetent, but are rather typical community members who appear to take more recreational risks then their peers. We conclude with lessons learned and discussion on how social engineering attacks ??while less technical?? continue to be an effective attack vector that our community has yet to successfully address."
"Recycling Randomness with Structure for Sublinear time Kernel Expansions.  We propose a scheme for recycling Gaussian random vectors into structured matrices to approximate
various kernel functions in sublinear time via random embeddings. Our framework includes the Fastfood construction of Le et al. (2013) as a special case, but also extends to Circulant, Toeplitz and Hankel matrices, and the broader family of structured matrices that are characterized by the concept of lowdisplacement rank. We introduce notions of coherence and graph-theoretic structural constants
that control the approximation quality, and prove unbiasedness and low-variance properties of random feature maps that arise within our framework.
For the case of low-displacement matrices, we show how the degree of structure and randomness can be controlled to reduce statistical variance at the cost of increased computation and storage requirements. Empirical results strongly support our theory and justify the use of a broader family of structured matrices for scaling up kernel methods using random features."
"Quantization based Fast Inner Product Search.  We propose a quantization based approach for fast approximate Maximum Inner Product Search (MIPS). Each database vector is quantized in multiple subspaces via a set of codebooks, learned directly by minimizing the inner product quantization error. Then, the inner product of a query to a database vector is approximated as the sum of inner products with the subspace quantizers. Different from recently proposed LSH approaches to MIPS, the database vectors and queries do not need to be augmented in a higher dimensional feature space. We also provide a theoretical analysis of the proposed approach, consisting of the concentration results under mild assumptions. Furthermore, if a small sample of example queries is given at the training time, we propose a modified codebook learning procedure which further improves the accuracy. Experimental results on a variety of datasets including those arising from deep neural networks show that the proposed approach significantly outperforms the existing state-of-the-art."
"Binary embeddings with structured hashed projections.  We consider the hashing mechanism for constructing binary embeddings, that involves pseudo-random projections followed by nonlinear (sign function) mappings. The pseudo-random projection is described by a matrix, where not all entries are independent random variables but instead a fixed ""budget of randomness"" is distributed across the matrix. Such matrices can be efficiently stored in sub-quadratic or even linear space, provide reduction in randomness usage (i.e. number of required random values), and very often lead to computational speed ups. We prove several theoretical results showing that projections via various structured matrices followed by nonlinear mappings accurately preserve the angular distance between input high-dimensional vectors. To the best of our knowledge, these results are the first that give theoretical ground for the use of general structured matrices in the nonlinear setting. In particular, they generalize previous extensions of the Johnson-Lindenstrauss lemma and prove the plausibility of the approach that was so far only heuristically confirmed for some special structured matrices. Consequently, we show that many structured matrices can be used as an efficient information compression mechanism. Our findings build a better understanding of certain deep architectures, which contain randomly weighted and untrained layers, and yet achieve high performance on different learning tasks. We empirically verify our theoretical findings and show the dependence of learning via structured hashed projections on the performance of neural network as well as nearest neighbor classifier."
"A Decomposable Attention Model for Natural Language Inference.  We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements."
"Buchi, Lindenbaum, Tarski: A Program Analysis Appetizer.  One can prove that a program program satisfies a correctness property in different ways. The deductive approach uses logic and is automated using decision procedures and proof assistants. The automata-theoretic approach reduces questions about programs to algorithmic questions about automata. In the abstract interpretation approach, programs and their properties are expressed in terms of fixed points in lattices and reasoning uses fixed point approximation techniques. We describe a research programme to establish precise, mathematical correspondences between these approaches and to develop new analyzers using these results. The theoretical tools we use are the theorems of Buchi that relate automata and logic and a construction of Lindenbaum and Tarski for generating lattices from logics. This research has lead to improvements in existing tools and we anticipate further theoretical and practical consequences."
"Chained Predictions Using Convolutional Neural Networks.  In this paper, we present an adaptation of the sequence-to-sequence model for structured output prediction in vision tasks. In this model the output variables for a given input are predicted sequentially using neural networks. The prediction for each output variable depends not only on the input but also on the previously predicted output variables. The model is applied to spatial localization tasks and uses convolutional neural networks (CNNs) for processing input images and a multi-scale deconvolutional architecture for making spatial predictions at each time step. We explore the impact of weight sharing with a recurrent connection matrix between consecutive predictions, and compare it to a formulation where these weights are not tied. Untied weights are particularly suited for problems with a fixed sized structure, where different classes of output are predicted in different steps. We show that chained predictions achieve top performing results on human pose estimation from single images and videos."
"The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition.  Current approaches for fine-grained recognition do the following: First, recruit experts to annotate a dataset of images, optionally also collecting more structured data in the form of part annotations and bounding boxes. Second, train a model utilizing this data. Toward the goal of solving fine-grained recognition, we introduce an alternative approach, leveraging free, noisy data from the web and simple, generic methods of recognition. This approach has benefits in both performance and scalability. We demonstrate its efficacy on four fine-grained datasets, greatly exceeding existing state of the art without the manual collection of even a single label, and furthermore show first results at scaling to more than 10,000 fine-grained categories. Quantitatively, we achieve top-1 accuracies of 92.3% on CUB-200-2011, 85.4% on Birdsnap, 93.4% on FGVC-Aircraft, and 80.8% on Stanford Dogs without using their annotated training sets. We compare our approach to an active learning approach for expanding fine-grained datasets."
"Generation and Comprehension of Unambiguous Object Descriptions.  We propose a method that can generate an unambiguous
description (known as a referring expression) of a specific
object or region in an image, and which can also comprehend
or interpret such an expression to infer which object
is being described. We show that our method outperforms
previous methods that generate descriptions of objects
without taking into account other potentially ambiguous
objects in the scene. Our model is inspired by recent
successes of deep learning methods for image captioning,
but while image captioning is difficult to evaluate, our task
allows for easy objective evaluation. We also present a new
large-scale dataset for referring expressions, based on MSCOCO.
We have released the dataset and a toolbox for visualization
and evaluation, see https://github.com/
mjhucla/Google_Refexp_toolbox."
"On the Efficient Representation and Execution of Deep Acoustic Models.  In this paper we present a simple and computationally efficient quantization scheme that enables us to reduce the resolution of the parameters of a neural network from 32-bit floating point values to 8-bit integer values. The proposed quantization scheme leads to significant memory savings and enables the use of optimized hardware instructions for integer arithmetic, thus significantly reducing the cost of inference. Finally, we propose a 'quantization aware' training process that applies the proposed scheme during network training and find that it allows us to recover most of the loss in accuracy introduced by quantization. We validate the proposed techniques by applying them to a long short-term memory-based acoustic model on an open-ended large vocabulary speech recognition task."
"Minimally Supervised Number Normalization.  We propose two models for verbalizing numbers, a key component in speech recognition and synthesis systems. The first model uses an end-to-end recurrent neural network. The second model, drawing inspiration from the linguistics literature, uses finite-state transducers constructed with a minimal amount of training data. While both models achieve near-perfect performance, the latter model can be trained using several orders of magnitude less data than the former, making it particularly useful for low-resource languages."
"CNN Architectures for Large-Scale Audio Classification.  Convolutional Neural Networks (CNNs) have proven very effective
in image classification and have shown promise for audio classification.
We apply various CNN architectures to audio and investigate
their ability to classify videos with a very large scale data set of 70M
training videos (5.24 million hours) with 30,871 labels. We examine
fully connected Deep Neural Networks (DNNs), AlexNet [1],
VGG [2], Inception [3], and ResNet [4]. We explore the effects of
training with different sized subsets of the 70M training videos. Additionally
we report the effect of training over different subsets of
the 30,871 labels. While our dataset contains video-level labels, we
are also interested in Acoustic Event Detection (AED) and train a
classifier on embeddings learned from the video-level task on AudioSet
[5]. We find that derivatives of image classification networks
do well on our audio classification task, that increasing the number
of labels we train on provides some improved performance over subsets
of labels, that performance of models improves as we increase
training set size, and that a model using embeddings learned from
the video-level task do much better than a baseline on the AudioSet
classification task."
"Semantic Model for Fast Tagging of Word Lattices.  This paper introduces a semantic tagger that inserts tags into a word lattice, such as one produced by a real-time large-vocabulary speech recognition system. Benefits of such a tagger the ability to rescore speech recognition hypotheses based on this metadata, as well as providing rich annotations to clients downstream. We focus on the domain of spoken search queries and voice commands, which can be useful for building an intelligent assistant. We explore a method to distill a preexisting very large semantic model into a lightweight tagger. This is accomplished by constructing a joint distribution of tagged n-grams from a supervised training corpus, then deriving a conditional distribution for a given lattice. With 300 classes, the tagger achieves a precision of 88.2% and recall of 93.1% on 1-best paths in speech recognition lattices with 2.8ms median latency."
"Jump: Virtual Reality Video.  We present Jump, a practical system for capturing high resolution, omnidirectional stereo (ODS) video suitable for wide scale consumption in currently available virtual reality (VR) headsets. Our system consists of a video camera built using off-the-shelf components and a fully automatic stitching pipeline capable of capturing video content in the ODS format. We have discovered and analyzed the distortions inherent to ODS when used for VR display as well as those introduced by our capture method and show that they are small enough to make this approach suitable for capturing a wide variety of scenes. Our stitching algorithm produces robust results by reducing the problem to one of pairwise image interpolation followed by compositing. We introduce novel optical flow and compositing methods designed specifically for this task. Our algorithm is temporally coherent and efficient, is currently running at scale on a distributed computing platform, and is capable of processing hours of footage each day."
"L-EnsNMF: Boosted Local Topic Discovery via Ensemble of Nonnegative Matrix Factorization.  Nonnegative matrix factorization (NMF) has been widely applied in many domains. In document analysis, it has been increasingly used in topic modeling applications, where a set of underlying topics are revealed by a low-rank factor matrix from NMF. However, it is often the case that the resulting topics give only general topic information in the data, which tends not to convey much information. To tackle this problem, we propose a novel ensemble model of nonnegative matrix factorization for discovering high-quality local topics. Our method leverages the idea of an ensemble model, which has been successful in supervised learning, into an unsupervised topic modeling context. That is, our model successively performs NMF given a residual matrix obtained from previous stages and generates a sequence of topic sets. Our algorithm for updating the input matrix has novelty in two aspects. The first lies in utilizing the residual matrix inspired by a state-of-the-art gradient boosting model, and the second stems from applying a sophisticated local weighting scheme on the given matrix to enhance the locality of topics, which in turn delivers high-quality, focused topics of interest to users. We evaluate our proposed method by comparing it against other topic modeling methods, such as a few variants of NMF and latent Dirichlet allocation, in terms of various evaluation measures representing topic coherence, diversity, coverage, computing time, and so on. We also present qualitative evaluation on the topics discovered by our method using several real-world data sets."
"YouTube-8M: A Large-Scale Video Classification Benchmark.  Many recent advancements in Computer Vision are attributed to large datasets. Open-source software packages for Machine Learning and inexpensive commodity hardware have reduced the barrier of entry for exploring novel approaches at scale.  It is possible to train models over millions of examples within a few days. Although large-scale datasets exist for image understanding, such as ImageNet, there are no comparable size video classification datasets. In this paper, we introduce YouTube-8M, the largest multi-label video classification dataset, composed of ~8 million videos---500K hours of video---annotated with a vocabulary of 4803 visual entities. To get the videos and their (multiple) labels, we used the YouTube Data APIs. We filtered the video labels (Freebase topics) using both automated and manual curation strategies, including by asking Mechanical Turk workers if the labels are visually recognizable. Then, we decoded each video at one-frame-per-second, and used a Deep CNN pre-trained on ImageNet to extract the hidden representation immediately prior to the classification layer. Finally, we compressed the frame features and make both the features and video-level labels available for download. The dataset contains frame-level features for over 1.9 billion video frames and 8 million videos, making it the largest public multi-label video dataset. We trained various (modest) classification models on the dataset, evaluated them using popular evaluation metrics, and report them as baselines. Despite the size of the dataset, some of our models train to convergence in less than a day on a single machine using the publicly-available TensorFlow framework. We plan to release code for training a basic TensorFlow model and for computing metrics. We show that pre-training on large data generalizes to other datasets like Sports-1M and ActivityNet. We achieve state-of-the-art on ActivityNet, improving mAP from 53.8% to 77.8%. We hope that the unprecedented scale and diversity of YouTube-8M will lead to advances in video understanding and representation learning."
"Incremental, iterative data processing with timely dataflow.  We describe the timely dataflow model for distributed computation and its implementation in the Naiad system. The model supports stateful iterative and incremental computations. It enables both low-latency stream processing and high-throughput batch processing, using a new approach to coordination that combines asynchronous and fine-grained synchronous execution. We describe two of the programming frameworks built on Naiad: GraphLINQ for parallel graph processing, and differential dataflow for nested iterative and incremental computations. We show that a general-purpose system can achieve performance that matches, and sometimes exceeds, that of specialized systems."
"Complex Linear Projection (CLP): A Discriminative Approach to Joint Feature Extraction and Acoustic Modeling.  State-of-the-art automatic speech recognition (ASR) systems
typically rely on pre-processed features. This paper studies
the time-frequency duality in ASR feature extraction methods
and proposes extending the standard acoustic model with a
complex-valued linear projection layer to learn and optimize
features that minimize standard cost functions such as cross
entropy. The proposed Complex Linear Projection (CLP) features
achieve superior performance compared to pre-processed
Log Mel features."
"Evolve or Die: High-Availability Design Principles Drawn from Google's Network Infrastructure.  Maintaining the highest levels of availability for content providers is challenging in the face of scale, network evolution, and complexity. Little, however, is known about the network failures large content providers are susceptible to, and what mechanisms they employ to ensure high availability. From a detailed analysis of over 100 high-impact failure events within Google??s network, encompassing many
data centers and two WANs, we quantify several dimensions of availability failures. We find that failures are evenly distributed across different network types and across data, control, and management planes, but that a large number of failures happen when a network management operation is in progress within the network. We discuss some of these failures in detail, and also describe our design principles for high availability motivated by these failures. These include using defense in depth, maintaining consistency across planes, failing open on large failures, carefully preventing and avoiding failures, and assessing root cause quickly. Our findings suggest that, as networks become more complicated, failures lurk everywhere, and, counter-intuitively, continuous incremental evolution of the network can, when applied together with our design principles, result in a more robust network."
"Trumpet: Timely and Precise Triggers in Data Centers.  As data centers grow larger and strive to provide tight performance and availability SLAs, their monitoring infrastructure must move from passive systems that provide aggregated inputs to human operators, to active systems that enable programmed control. In this paper, we propose Trumpet, an
event monitoring system that leverages CPU resources and end-host programmability, to monitor every packet and report events at millisecond timescales. Trumpet users can express many network-wide events, and the system efficiently detects these events using triggers at end-hosts. Using careful design, Trumpet can evaluate triggers by inspecting every packet at full line rate even on future generations of NICs, scale to thousands of triggers per end-host while bounding packet processing delay to a few microseconds, and report events to a controller within 10 milliseconds, even in the presence of attacks. We demonstrate these properties using an implementation of Trumpet, and also show that it allows operators to describe new network events such as detecting correlated bursts and loss, identifying the root cause of transient congestion, and detecting short-term anomalies at the scale of a data center tenant."
"Scalable in-situ qubit calibration during repetitive error detection.  We present a method to optimize physical qubit parameters while error detection is running. We demonstrate how gate optimization can be parallelized in a large-scale qubit array. Additionally we show that the presented method can be used to simultaneously compensate for independent or correlated qubit parameter drifts. Our method is O(1) scalable to systems of arbitrary size, providing a path towards controlling the large numbers of qubits needed for a fault-tolerant quantum computer."
"Onix: a distributed control platform for large-scale production networks.  Computer networks lack a general control paradigm, as traditional networks do not provide any network-wide management abstractions. As a result, each new function (such as routing) must provide its own state distribution, element discovery, and failure recovery mechanisms. We believe this lack of a common control platform has significantly hindered the development of flexible, reliable and feature-rich network control planes. To address this, we present Onix, a platform on top of which a network control plane can be implemented as a distributed system. Control planes written within Onix operate on a global view of the network, and use basic state distribution primitives provided by the platform. Thus Onix provides a general API for control plane implementations, while allowing them to make their own trade-offs among consistency, durability, and scalability."
"On The Existence of Epipolar Matrices.  This paper considers the foundational question of the existence of a fundamental (resp. essential) 
matrix given $m$ point correspondences in two views. 
We present a complete answer for the existence of fundamental matrices for any value of $m$. Using examples we disprove the widely held beliefs that 
fundamental matrices always exist whenever $m \leq 7$. At the same time, we prove that they exist 
unconditionally when $m \leq 5$. Under a mild genericity condition, we show that an essential matrix always exists when $m \leq 4$. We also characterize the six and seven point configurations in two views for which all matrices satisfying the epipolar constraint have rank at most one."
"Federated Optimization: Distributed Machine Learning for On-Device Intelligence.  We introduce a new and increasingly relevant setting for distributed optimization in machine learning, where the data defining the optimization are unevenly distributed over an extremely large number of nodes. The goal is to train a high-quality centralized model. We refer to this setting as Federated Optimization. In this setting, communication efficiency is of the utmost importance and minimizing the number of rounds of communication is the principal goal. A motivating example arises when we keep the training data locally on users' mobile devices instead of logging it to a data center for training. In federated optimization, the devices are used as compute nodes performing computation on their local data in order to update a global model. We suppose that we have extremely large number of devices in the network --- as many as the number of users of a given service, each of which has only a tiny fraction of the total data available. In particular, we expect the number of data points available locally to be much smaller than the number of devices. Additionally, since different users generate data with different patterns, it is reasonable to assume that no device has a representative sample of the overall distribution. We show that existing algorithms are not suitable for this setting, and propose a new algorithm which shows encouraging experimental results for sparse convex problems. This work also sets a path for future research needed in the context of federated optimization."
"A cloud-based large-scale distributed video analysis system.  Digital content consumption is exploding thanks to the advances of the distributed cloud-computing infrastructures and the consumer electronics. Further challenges have been posed to engineers and researchers to satisfy the ever increasing user needs not only for high quality video delivery, but also for richer experience. In order to support various video analysis tasks in addition to transcoding, a software platform is designed based on the Google cloud computing infrastructure with the features to be flexible, scalable, robust, and secure. In this paper, we discuss the scope, requirements, constraints, features of such a system, the problems we met and how they are resolved."
"Bootstrapping User-Defined Body Tapping Recognition with Offline-Learned Probabilistic Representation.  To address the increasing functionality (or information) overload of smartphones, prior research has explored a variety of methods to extend the input vocabulary of mobile devices. In particular, body tapping has been previously proposed as a technique that allows the user to quickly access a target functionality by simply tapping at a specific location of the body with a smartphone. Though compelling, prior work often fell short in enabling users?? unconstrained tapping locations or behaviors. To address this problem, we developed a novel recognition method that combines both offline??before the system sees any user-defined gestures??and online learning to reliably recognize arbitrary, user-defined body tapping gestures, only using a smartphone??s built-in sensors. Our experiment indicates that our method significantly outperforms baseline approaches in several usage conditions. In particular, provided only with a single sample per location, our accuracy is 30.8% over an SVM baseline and 24.8% over a template matching method. Based on these findings, we discuss how our approach can be generalized to other user-defined gesture problems."
"NetBricks: Taking the V out of NFV.  The move from hardware middleboxes to software network
functions, as advocated by NFV, has proven more challenging
than expected. Developing new NFs remains a tedious
process, requiring that developers repeatedly rediscover
and reapply the same set of optimizations, while current
techniques for providing isolation between NFs (using
VMs or containers) incur high performance overheads. In
this paper we describe NetBricks, a new NFV framework
that tackles both these problems. For building NFs we take
inspiration from modern data analytics frameworks (e.g.,
Spark and Dryad) and build a small set of customizable network
processing elements. We also embrace type checking
and safe runtimes to provide isolation in software, rather
than rely on hardware isolation. NetBricks provides the
same memory isolation as containers and VMs, without
incurring the same performance penalties. To improve I/O
efficiency, we introduce a novel technique called zero-copy
software isolation."
"Knowledge Vault: A Web-Scale Approach to Probabilistic Knowledge Fusion.  Recent years have witnessed a proliferation of large-scale
knowledge bases, including Wikipedia, Freebase, YAGO, Microsoft??s
Satori, and Google??s Knowledge Graph. To increase
the scale even further, we need to explore automatic
methods for constructing knowledge bases. Previous approaches
have primarily focused on text-based extraction,
which can be very noisy. Here we introduce Knowledge
Vault, a Web-scale probabilistic knowledge base that combines
extractions from Web content (obtained via analysis of
text, tabular data, page structure, and human annotations)
with prior knowledge derived from existing knowledge repositories.
We employ supervised machine learning methods for
fusing these distinct information sources. The Knowledge
Vault is substantially bigger than any previously published
structured knowledge repository, and features a probabilistic
inference system that computes calibrated probabilities
of fact correctness. We report the results of multiple studies
that explore the relative utility of the different information
sources and extraction methods."
"EYEORG: A Platform For Crowdsourcing Web Quality Of Experience Measurements.  Tremendous effort has gone into the ongoing battle to make
webpages load faster. This effort has culminated in new protocols
(QUIC, SPDY, and HTTP/2) as well as novel content
delivery mechanisms. In addition, companies like Google
and SpeedCurve investigated how to measure ??page load
time?? (PLT) in a way that captures human perception. This is
challenging since even estimating when a web page is loaded
has proven to be difficult in modern web pages. In this paper
we present Eyeorg [13], a platform for crowdsourcing web
quality of experience measurements. Eyeorg overcomes the
scaling and automation challenges of recruiting users and
collecting consistent user-perceived quality measurements.
We validate Eyeorg??s capabilities via a set of 100 trusted
participants. Next, we showcase its functionalities via three
measurement campaigns, each involving 1,000 paid participants,
to 1) study the quality of several PLT metrics, 2) compare
HTTP/1.1 and HTTP/2 performance, and 3) assess the
impact of online advertisements and ad blockers on user experience.
We find that commonly used, and even novel and
sophisticated PLT metrics fail to represent actual human perception
of PLT, that the performance gains from HTTP/2
are imperceivable in some circumstances, and that not all
ad blockers are created equal."
"Exploring the Steps of Verb Phrase Ellipsis.  Verb Phrase Ellipsis is a well-studied topic in theoretical linguistics but has received little attention as a computational problem. Here we propose a decomposition of the overall resolution problem into three tasks??target detection, antecedent head resolution, and antecedent boundary detection??and implement a number of computational approaches for each one. We also explore the relationships among these tasks by attempting joint learning over different combinations. Our new decomposition of the problem yields significantly improved performance on publicly available datasets, including a newly contributed one."
"FAUCET: Deploying SDN in the Enterprise.  Since the publication of OpenFlow: Enabling Innovation in Campus Networks in 2008, there has been a lot of published work and experience with SDN and OpenFlow in large networks and in datacenters, including at Google. In this article we will discuss an open source SDN controller, FAUCET. FAUCET was created to bring the benefits of SDN to a typical enterprise network and has been deployed in various settings, including the Open Networking Foundation, which runs an instance of FAUCET as their office network. FAUCET delivers high forwarding performance using switch hardware, while enabling operators to add features to their networks and deploy them quickly, in many cases without needing to change (or even reboot) hardware - and interoperates with neighboring non-SDN network devices."
Learn and disseminate. Mario Callegaro of Google Uk on why we need to share knowledge..  Simon Chadwick from Research World interviews Mario Callegaro about why we need to share knowledge
"Sparse Non-negative Matrix Language Modeling (EMNLP presentation).  We present Sparse Non-negative Matrix (SNM) estimation, a novel probability estimation technique for language modeling that can efficiently incorporate arbitrary features. We evaluate SNM language models on two corpora: the One Billion Word Benchmark and a subset of the LDC English Gigaword corpus. Results show that SNM language models trained with n-gram features are a close match for the well-established Kneser-Ney models. The addition of skip-gram features yields a model that is in the same league as the state-of-the-art recurrent neural network language models, as well as complementary: combining the two modeling techniques yields the best known result on the One Billion Word Benchmark. On the Gigaword corpus further improvements are observed using features that cross sentence boundaries. The computational advantages of SNM estimation over both maximum entropy and neural network estimation are probably its main strength, promising an approach that has large flexibility in combining arbitrary features and yet scales gracefully to large amounts of data."
"Federated Learning: Strategies for Improving Communication Efficiency.  Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model with training data distributed over a large number of clients each with unreliable and relatively slow network connections. We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model. The typical clients in this setting are mobile phones, and communication efficiency is of utmost importance. In this paper, we propose two ways to reduce the uplink communication costs. The proposed methods are evaluated on the application of training a deep neural network to perform image classification. Our best approach reduces the upload communication required to train a reasonable model by two orders of magnitude."
"When Does Improved Targeting Increase Revenue?.  In second-price auctions, we find that improved targeting via enhanced information disclosure decreases revenue when there are two bidders and increases revenue if there are at least four symmetric bidders with values drawn from a distribution with a monotone hazard rate. With asymmetries, improved targeting increases revenue if the most frequent winner wins less than 30.4% of the time under a model in which shares are well defined, but can decrease revenue otherwise. We derive analogous results for position auctions. Finally, we show that revenue can vary nonmonotonically with the number of bidders who are able to take advantage of improved targeting."
"Evaluation Metrics of Service-Level Reliability  Monitoring Rules of a Big Data Service.  This paper presents new metrics to evaluate the reliability monitoring rules of a large-scale big data service. Our target service uses manually-tuned, service-level reliability monitoring rules. Using the measurement data, we identify two key technical challenges in operating our target monitoring system. In order to improve the operational efficiency, we characterize how those rules were manually tuned by the domain experts. The characterization results provide useful information to operators supposed to regularly tune such rules. Using the actual production failure data, we evaluate the same monitoring rules by using standard metrics and the presented metrics. Our evaluation results show the strengths and weaknesses of each metric and show that the presented metrics can further help operators recognize when and which rules need to be re-tuned."
"Expert and Non-Expert Attitudes towards (Secure) Instant Messaging.  In this paper, we present results from an online survey with 1,510 participants and an interview study with 31 participants on (secure) mobile instant messaging. Our goal was to uncover how much of a role security and privacy played in people's decisions to use a mobile instant messenger. In the interview study, we recruited a balanced sample of IT security experts and non-experts, as well as an equal split of users of mobile instant messengers that are advertised as being more secure and/or private (e.g., Threema) than traditional mobile IMs. Our results suggest that peer influence is what primarily drives people to use a particular mobile IM, even for secure/private IMs, and that security and privacy play minor roles."
"Related Event Discovery.  In this paper, we consider the problem of discovering local events on the web, where events are entities extracted from pages with Schema.org annotations. Examples of such local events include small venue concerts, farmers markets, sports activities, etc. Given an event entity, we propose a graph-based framework for retrieving a ranked list of related events that a user is likely to be interested in or to attend. We demonstrate that this framework can be easily extended to the keyword search scenario as well, where the user issues a query to a search engine, hoping to find relevant events to attend. Due to the difficulty of obtaining ground-truth labels for event entities, which are temporal and are constrained by location, our general retrieval framework is unsupervised, and its graph-based formulation addresses (a) the challenge of feature sparseness and noisiness, and (b) semantic mismatch problem in a self-contained and principled manner. To validate our methods, we collect human annotations and conduct a comprehensive empirical study, analyzing the performance of our methods with regard to relevance, recall, and diversity. This study shows that our graph-based framework is significantly better than any individual feature for both entity and keyword search scenarios, and can be further improved with minimal supervision. Finally, we demonstrate that our framework can be useful in understanding the temporal and the localized nature of the events on the web."
"Learning from User Interactions in Personal Search via Attribute Parameterization.  User interaction data (e.g., click data) has proven to be a powerful signal for learning-to-rank models in web search. However, such models require observing multiple interactions across many users for the same query-document  pair to achieve statistically meaningful gains. Therefore, utilizing user interaction data for improving search over personal, rather than public, content is a challenging problem. First, the documents (e.g., emails or private files) are not shared across users. Second, user search queries are of personal nature (e.g., [alice's address]) and may not generalize well across users. In this paper, we propose a solution to these challenges, by projecting user queries and documents into a multi-dimensional space of fine-grained and semantically coherent attributes. We then introduce a novel parameterization technique to overcome sparsity in the multi-dimensional attribute space. Attribute parameterization enables effective usage of cross-user interactions for improving personal search quality -- which is a first such published result, to the best of our knowledge. Experiments with a dataset derived from interactions of users of one of the worlds' largest personal search engines demonstrate the effectiveness of the proposed attribute parameterization technique."
"Equality of Opportunity in Supervised Learning.  We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. 
In line with other studies, our notion is oblivious: it depends only on the joint statistics of the predictor, the target and the protected attribute, but not on interpretation of individualfeatures. We study the inherent limits of defining and identifying biases based on such oblivious measures, outlining what can and cannot be inferred from different oblivious tests. 
We illustrate our notion using a case study of FICO credit scores."
"Using Fast Weights to Attend to the Recent Past.  Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These ""fast weights"" can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proved very helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns."
"Can Active Memory Replace Attention?.  Several mechanisms to focus attention of a neural network on selected parts of its input or memory have been used successfully in deep learning models in recent years. Attention has improved image classification, image captioning, speech recognition, generative models, and learning algorithmic tasks, but it had probably the largest impact on neural machine translation. 
Recently, similar improvements have been obtained using alternative mechanisms that do not focus on a single part of a memory but operate on all of it in parallel, in a uniform way. Such mechanism, which we call active memory, improved over attention in algorithmic tasks, image processing, and in generative modelling. 
So far, however, active memory has not improved over attention for most natural language processing tasks, in particular for machine translation. We analyze this shortcoming in this paper and propose an extended model of active memory that matches existing attention models on neural machine translation and generalizes better to longer sentences. We investigate this model and explain why previous active memory models did not succeed. Finally, we discuss when active memory brings most benefits and where attention can be a better choice."
"Orthogonal Random Features.  We present an intriguing discovery related to Random Fourier Features: in Gaussian kernel approximation, replacing the random Gaussian matrix by a properly scaled random orthogonal matrix significantly decreases kernel approximation error. We call this technique Orthogonal Random Features (ORF), and provide theoretical and empirical justification for this behavior. Motivated by this discovery, we further propose Structured Orthogonal Random Features (SORF), which uses a class of structured discrete orthogonal matrices to speed up the computation. The method reduces the time cost from O(d^2) to O(dlogd), where d is the data dimensionality, with almost no compromise in kernel approximation quality compared to ORF. Experiments on several datasets verify the effectiveness of ORF and SORF over the existing methods. We also provide discussions on using the same type of discrete orthogonal structure for a broader range of applications."
"Private Service Discovery and Mutual Authentication for the Internet of Things.  Automatic service discovery is essential to realizing the full potential of the Internet of Things
(IoT). While discovery protocols like Multicast DNS, Apple AirDrop, and Bluetooth Low Energy
have gained widespread adoption across both IoT and mobile devices, most of these protocols
do not offer any form of privacy control for the service, and often leak sensitive information such
as service type, device hostname, device owner??s identity, and more in the clear. To address the need for better privacy in both the IoT and the mobile landscape, we develop
two protocols for private service discovery and private mutual authentication. Our protocols
provide private and authentic service advertisements, zero round-trip (0-RTT) mutual
authentication, and are provably secure in the Canetti-Krawczyk key-exchange model. In contrast
to alternatives, our protocols are lightweight and require minimal modification to existing
key-exchange protocols. We integrate our protocols into an existing open-source distributed applications framework, and provide benchmarks on multiple hardware platforms: Intel Edisons,
Raspberry Pis, smartphones, laptops, and desktops. Finally, we discuss some privacy limitations
of the Apple AirDrop protocol (a peer-to-peer file sharing mechanism) and show how to improve
the privacy of Apple AirDrop using our private mutual authentication protocol."
"Latent Attention For If-Then Program Synthesis.  Automatic translation from natural language descriptions into programs is a long-standing challenging problem. In this work, we consider a simple yet important sub-problem: translation from textual descriptions to If-Then programs. We devise a novel neural network architecture for this task which we train end-to-end. Specifically, we introduce Latent Attention, which computes multiplicative weights for the words in the description in a two-stage process with the goal of better leveraging the natural language structures that indicate the relevant parts for predicting program elements. Our architecture reduces the error rate by 28.57% compared to prior art. We also propose a one-shot learning scenario of If-Then program synthesis and simulate it with our existing dataset. We demonstrate a variation on the training procedure for this scenario that outperforms the original procedure, significantly closing the gap to the model trained with all data."
"A PTAS for Planar Group Steiner Tree via Bootstrapping Approximation.  We present the first polynomial-time approximation scheme (PTAS), i.e., (1 + ??)-approximation algorithm for any constant ?? &gt; 0, for the planar group Steiner tree problem (in which each group lies on a boundary of a face). This result improves on the best previous approximation factor of O(log n(log log n)O(1) ). We achieve this result via a novel and powerful technique called bootstrapping approximation, which allows one to bootstrap from a superconstant approximation factor (even superpolynomial in the input size) all the way down to a PTAS. This is in contrast with the popular ex- isting approach for planar PTASs of constructing light-weight spanners in one iteration, which notably requires a constant-factor approximate solution to start from. Bootstrapping approximation removes the main barrier for designing PTASs for problems which have no known constant-factor approxima- tion (even on planar graphs), and thus can be used to obtain PTASs for several difficult-to-approximate problems. Our second major contribution required for the planar group Steiner tree PTAS is a spanner con- struction, which reduces the graph to have total weight within a factor of the optimal solution while approximately preserving the optimal solution. This is particularly challenging because group Steiner tree requires deciding which terminal in each group to connect by the tree, making it much harder than recent previous approaches to construct spanners for planar TSP by Klein (FOCS??05 &amp; SICOMP??08), subset TSP by Klein (STOC??06), Steiner tree by Borradaile, Klein, and Mathieu (SODA??07 &amp; TALG??09), and Steiner forest by Bateni, Hajiaghayi, and Marx (STOC??10 &amp; JACM??11) (and its improvement to an efficient PTAS by Eisenstat, Klein, and Mathieu (SODA??12)). The spanner construction algorithm first constructs a ??pre-spanner?? by several steps. we divide the groups in a solution into minimal and non- minimal groups according to whether the optimal solution includes one or multiple vertices of the group. Next we make sure every vertex in a minimal group reached by the optimal solution is in the pre-spanner. By adding more to the pre-spanner, we guarantee that the vertices of (nonminimal) groups reached by the optimal solution but not in the spanner, called tips, exist for only a constant number of nonminimal groups. Next we make sure each such tip of a nonminimal group is first weakly and then via yet another involved step strongly isolated. We are then able to handle strongly isolated tips of one group via another structural result of optimum solutions. Finally we invoke the known spanner construction for Steiner tree as a black box on top of our already constructed pre-spanner to construct an actual spanner. We iterate all the above a polynomial number of times via the bootstrapping approximation technique to obtain the desired PTAS for planar group Steiner tree. Our PTAS for planar group Steiner tree implies the first PTAS for geometric Euclidean group Steiner tree with obstacles, as well as a (2 + ?? )-approximation algorithm for group TSP with obstacles, improv- ing over the best previous constant-factor approximation algorithms. By contrast, we show that planar group Steiner forest, a slight generalization of planar group Steiner tree, is APX-hard on planar graphs of treewidth 3, even if the groups are pairwise disjoint and every group is a vertex or an edge."
"Distributed Mean Estimation with Limited Communication.  Motivated by the need for distributed optimization algorithms with  low communication cost, we study communication efficient algorithms  to perform distributed mean estimation. We study the scenarios in which each client sends one bit per dimension. We first show that for d dimensional data with n clients, a naive stochastic rounding approach yields a mean squared error Theta(d/n). We then show by applying a structured random rotation of the data (an O(dlogd) algorithm), the error can be reduced to O(logd/n). The methods we show in this paper do not depend on the distribution of the data."
"Using Machine Learning to Improve the Email Experience.  Email is an essential communication medium for billions of people, with most users relying on web-based email services. Two recent trends are changing the email experience: smartphones have become the primary tool for accessing online services including email, and machine learning has come of age. 
Smartphones have a number of compelling properties (they are location-aware, usually with us, and allow us to record and share photos and videos), but they also have a few limitations, notably limited screen size and small and tedious virtual keyboards.  Over the past few years, Google researchers and engineers have leveraged machine learning to ameliorate these weaknesses, and in the process created novel experiences.  In this talk, I will give three examples of machine learning improving the email experience. The first example describes how we are improving email search. Displaying the most relevant results as the query is being typed is particularly useful on smartphones due to the aforementioned limitations.  Combining hand-crafted and machine-learned rankers is powerful, but training learned rankers requires a relevance-labeled training set. User privacy prohibits us from employing raters to produce relevance labels. Instead, we leverage implicit feedback (namely clicks) provided by the users themselves. Using click logs as training data in a learning-to-rank setting is intriguing, since there is a vast and continuous supply of fresh training data.  However, the click stream is biased towards queries that receive more clicks -- e.g. queries for which we already return the best result in the top-ranked position.  I will summarize our work~\cite{Pointer} on neutralizing that bias. The second example describes how we extract key information from appointment and reservation emails and surface it at the appropriate time as a reminder on the user's smartphone.  Our basic approach~\cite{Juicer} is to learn the templates that were used to generate these emails, use these templates to extract key information such as times and dates, store the extracted records in a personal information store, and surface them at the right time (taking contextual information such as estimated transit time into account). The third example describes Smart Reply~\cite{SmartReply}, a system that offers a set of three short responses to those incoming emails for which a short response is appropriate, allowing users to respond quickly with just a few taps, without typing or involving voice-to-text transcription.  The basic approach is to learn a model of likely short responses to original emails from the corpus, and then to apply the model whenever a new message arrives. Other considerations include offering a set of responses that are all appropriate and yet diverse, and triggering only when sufficiently confident that each responses is of high quality and appropriate."
"The Fast Bilateral Solver.  We present the bilateral solver, a novel algorithm for edge-aware smoothing that combines the flexibility and speed of simple filtering approaches with the accuracy of specialized domain-specific optimization algorithms. Our single technique is capable of matching or improving upon state-of-the-art results on several different computer vision tasks (stereo, depth superresolution, colorization, and semantic segmentation) while being 10-1000 times faster than competing approaches. The bilateral solver is fast, robust, straightforward to generalize to new domains, and capable of being easily integrated into deep learning pipelines."
"Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large Vocabulary Speech Recognition.  We present results that show it is possible to build a competitive, greatly simplified, large vocabulary continuous speech recognition system with whole words as acoustic units. We model the output vocabulary of about 100,000 words directly using deep bi-directional LSTM RNNs with CTC loss. The model is trained on 125,000 hours of semi-supervised acoustic training data, which enables us to alleviate the data sparsity problem for word models. We show that the CTC word models work very well as an end-to-end all-neural speech recognition model without the use of traditional context-dependent sub-word phone units that require a pronunciation lexicon, and without any language model removing the need to decode. We demonstrate that the CTC word models perform better than a strong, more complex, state-of-the-art baseline with sub-word units."
"Multilingual Code-switching Identification via LSTM Recurrent Neural Networks.  This paper describes the HHU-UH-G system submitted to the EMNLP 2016 Second Workshop on Computational Approaches to Code Switching. Our system ranked first place for Arabic (MSA-Egyptian) with an F1-score of 0.83 and second place for Spanish-English with an F1-score of 0.90. The HHU-UHG system introduces a novel unified neural network architecture for language identification in code-switched tweets for both Spanish-English and MSA-Egyptian dialect. The system makes use of word and character level representations to identify code-switching. For the MSA-Egyptian dialect the system does not rely on any kind of language-specific knowledge or linguistic resources such as, Part Of Speech (POS) taggers, morphological analyzers, gazetteers or word lists to obtain state-of-the-art performance."
"Explicit Fine grained Syntactic and Semantic Annotation of the Idafa Construction in Arabic.  Idafa in traditional Arabic grammar is an umbrella construction that covers several phenomena including what is expressed in English as noun-noun compounds and Saxon &amp; Norman genitives. Additionally, Idafa participates in some other constructions, such as quantifiers, quasi-prepositions, and adjectives. Identifying the various types of the Idafa construction (IC) is of importance to Natural Language Processing (NLP) applications. Noun-Noun compounds exhibit special behaviour in most languages impacting their semantic interpretation. Hence distinguishing them could have an impact on downstream NLP applications. The most comprehensive computational syntactic representation of the Arabic language is found in the LDC Arabic Treebank (ATB). Despite its coverage, ICs are not explicitly labeled in the ATB and furthermore, there is no clear distinction between ICs of noun-noun relations and other traditional ICs. Hence, we devise a detailed syntactic and semantic typification process of the IC phenomenon in Arabic. We target the ATB as a platform for this classification. We render the ATB annotated with explicit IC labels in addition to further semantic characterization which is useful for syntactic, semantic and cross language processing. Our typification of IC comprises 3 main syntactic IC types: False Idafas (FIC), Grammatical Idafas (GIC), and True Idafas (TIC), which are further divided into 10 syntactic subclasses. The TIC group is further classified into semantic relations. We devise a method for automatic IC labeling and compare its yield against the CATiB Treebank. Our evaluation shows that we achieve the same level of accuracy, but with the additional fine-grained classification into the various syntactic and semantic types."
"Multilingual Metaphor Processing: Experiments with Semi-Supervised and Unsupervised Learning.  Highly frequent in language and communication, metaphor represents a significant challenge for
Natural Language Processing (NLP) applications. Computational work on metaphor has traditionally
evolved around the use of hand-coded knowledge, making the systems hard to scale. Recent
years have witnessed a rise in statistical approaches to metaphor processing. However, these
approaches often require extensive human annotation effort and are predominantly evaluated
within a limited domain. In contrast, we experiment with weakly supervised and unsupervised
techniques ?? with little or no annotation ?? to generalize higher-level mechanisms of metaphor
from distributional properties of concepts. We investigate different levels and types of supervision
(learning from linguistic examples vs. learning from a given set of metaphorical mappings vs.
learning without annotation) in flat and hierarchical, unconstrained and constrained clustering
settings. Our aim is to identify the optimal type of supervision for a learning algorithm that
discovers patterns of metaphorical association from text. In order to investigate the scalability
and adaptability of our models, we applied them to data in three languages from different
language groups ?? English, Spanish and Russian, ?? achieving state-of-the-art results with
little supervision. Finally, we demonstrate that statistical methods can facilitate and scale up
cross-linguistic research on metaphor."
"Automatic Optimization of Data Perturbation Distributions for Multi-Style Training in Speech Recognition.  Speech recognition performance using deep neural network based acoustic models is known to degrade when the acoustic environment and the speaker population in the target utterances are significantly different from the conditions represented in the training data. To address these mismatched scenarios, multi-style training (MTR) has been used to perturb utterances in an existing uncorrupted and potentially mismatched training speech corpus to better match target domain utterances.  This paper addresses the problem of determining the distribution of perturbation levels for a given set of perturbation types that best matches the target speech utterances. An approach is presented that, given a small set of utterances from a target domain, automatically identifies an empirical distribution of perturbation levels that can be applied to utterances in an existing training set. 
Distributions are estimated for perturbation types that include acoustic background environments, reverberant room configurations, and speaker related variation like frequency and temporal warping. 
The end goal is for the resulting perturbed training set to characterize the variability in the target domain and thereby optimize ASR performance.  An experimental study is performed to evaluate the impact of this approach on ASR performance when the target utterances are taken from a simulated far-field acoustic environment."
G-RMI Object Detection.  We present our submission to the COCO 2016 Object Detection challenge.
"Length bias in Encoder Decoder Models and a Case for Global Conditioning.  Encoder-decoder networks are popular for modeling sequences probabilistically in many applications. These models use the power of the Long Short-Term Memory (LSTM) architecture to capture the full dependence among variables, unlike earlier models like CRFs that typically assumed conditional independence among non-adjacent variables. However in practice encoder-decoder models exhibit a bias towards short sequences that surprisingly gets worse with increasing beam size. In this paper we show that such phenomenon is due to a discrepancy between the full sequence margin and the per-element margin enforced by the locally conditioned training objective of a encoder-decoder model. The discrepancy more adversely impacts long sequences, explaining the bias towards predicting short sequences. For the case where the predicted sequences come from a closed set, we show that a globally conditioned model alleviates the above problems of encoder-decoder models. From a practical point of view, our proposed model also eliminates the need for a beam-search during inference, which reduces to an efficient dot-product based search in a vector-space."
Incentivizing Advertiser Networks to Submit Multiple Bids.  This paper illustrates a method for making side payments to advertiser networks that creates an incentive for the advertiser networks to submit the second-highest bids they received to an ad exchange and simultaneously ensures that the publishers will make more money on average in the short run as a result of adopting this scheme. We also illustrate how this payment scheme affects publisher payoffs in the long run after advertisers have a chance to modify their strategies in response to the changed incentives of the mechanism.
Machine Learning in an Auction Environment.  We consider a model of repeated online auctions in which an ad with an uncertain click-through rate faces a random distribution of competing bids in each auction and there is discounting of payoffs. We formulate the optimal solution to this explore/exploit problem as a dynamic programming problem and show that efficiency is maximized by making a bid for each advertiser equal to the advertiser??s expected value for the advertising opportunity plus a term proportional to the variance in this value divided by the number of impressions the advertiser has received thus far. We then use this result to illustrate that the value of incorporating active exploration in an auction environment is exceedingly small.
"The Zero Touch Network.  Large scale content and cloud infrastructure providers strive to offer the highest level of availability across the infrastructure stack. This however is not an easy feat given the fast pace of technology evolution, infrastructure expansion and global reach. Google??s network infrastructure has been built to achieve scale, efficiency and very high reliability by following a set of key architectural principles, which we refer to as the ??zero touch network??. Failures do happen in any global scale network infrastructure such as Google??s. By analyzing past failures, we found that a large number of them happened when a network management operation was in progress. To minimize such failures, we have built a network infrastructure where all network operations are automated, requiring no additional steps beyond the instantiation of intent. The network infrastructure is fully declarative and changes applied to individual network elements are derived by the network infrastructure from the high-level network-wide intent. Any network changes are automatically halted and automatically rolled-back by the management infrastructure if the network displays unintended behavior. Finally, the infrastructure does not allow operations which violate network policies. 
While it might be tempting to limit the rate at which the network evolves to minimize risk of network failures, we have internally come to the opposite conclusion. In a zero-touch-network, continuous incremental evolution results in a more robust infrastructure rather than in-frequent large changes."
"Optimal Content Placement for a Large-Scale VoD System.  IPTV service providers offering Video-on-Demand currently use servers at each metropolitan office to store all the videos in their library. With the rapid increase in library sizes, it will soon become infeasible to replicate the entire library at each office. We present an approach for intelligent content placement that scales to large library sizes (e.g., 100Ks of videos). We formulate the problem as a mixed integer program (MIP) that takes into account constraints such as disk space, link bandwidth, and content popularity. To overcome the challenges of scale, we employ a Lagrangian relaxation-based decomposition technique combined with integer rounding. Our technique finds a near-optimal solution (e.g., within 1-2%) with orders of magnitude speedup relative to solving even the LP relaxation via standard software.  We also present simple strategies to address practical issues such as popularity estimation, content updates, short-term popularity fluctuation, and frequency of placement updates. Using traces from an operational system, we show that our approach significantly outperforms simpler placement strategies. For instance, our MIP-based solution can serve all requests using only half the link bandwidth used by LRU or LFU cache replacement policies. We also investigate the trade-off between disk space and network bandwidth."
Scaling Large Data Center Interconnect: Technology Options and Challenges.  This is an invited OSA annual meeting talk. This talk reviews technology options and challenges to scale  bandwidth and power for large-scale data center interconnects.
"Pluggable DWDM: Considerations For Campus and Metro DCI Applications.  This is a ECOC workshop talk. We will discuss, from Google??s perspective, the challenges facing datacenter campus and metro interconnection networks, and the potential of using pluggable tunable DWDM for both campus and metro applications"
"Brand Impersonation Detection By Knowledge Verification On Text Containing Hyperlinks.  A system and method are disclosed of verifying authenticity of a message using embedded hyperlinks therein. The method works by checking that the hyperlinks the text points to match the companies/brands the text portrays. The verification makes use of a knowledgebase of company/brand information as a source for retrieving the known sites/pages/domains for those companies/brands. The method then estimates a probabilistic measure of identity checking based on matches between the companies/brands mentioned in the text and the outgoing links from the text. In a variation, the method may additionally use contextual information of the message text to determine authenticity to compute a coverage score. The coverage score is then input into a machine learning model that uses these along with other known indicators of genuineness to determine that the message or Web page is either genuine or not."
"Engineering Reliability into Sites.  This talk introduces Site Reliability Engineering (SRE) at Google,
explaining its purpose and describing the challenges it addresses.
SRE teams manage Google's many services and web sites from our offices in
Pittsburgh, New York, London, Sydney, Zurich, Los Angeles, Dublin, Mountain View, ...
They draw upon the Linux based computing resources
that are distributed in data centers around the world."
"Beyond Corp: The Access Proxy.  This article details the implementation of BeyondCorp's front end infrastructure. It focuses on the Access Proxy, the challenges we encountered in its implementation, and the resulting lessons we learned in its design and rollout. We also touch on some of the projects we're currently undertaking to improve the overall user experience for employees accessing internal applications. In migrating to the BeyondCorp model (previously discussed in BeyondCorp: A New Approach to Enterprise Security and BeyondCorp: Design to Deployment at Google), Google had to solve a number of problems. One particular challenge was figuring out how to enforce company policy across all our internal-only services. A conventional approach might integrate each back end with the device Trust Inferer in order to evaluate applicable policies; however, this approach would significantly slow the rate at which we're able to launch and change products. To address this challenge, Google implemented a centralized policy enforcement front end Access Proxy (AP)--to handle coarse-grained company policies. Our implementation of the AP is generic enough to let us implement logically different gateways using the same AP codebase. At the moment, Access Proxy implements both the Web Proxy and the SSH gateway components, according to the terminology used in the previous article. As the AP was the only mechanism that allowed employees to access internal HTTP services, all internal services were required to migrate behind the AP. Unsurprisingly, attempting to deal with only HTTP requests proved inadequate, so we had to provide solutions for additional protocols, many of which required end-to-end encryption (e.g. SSH). These additional protocols necessitated a number of client-side changes to ensure that the device was properly identified to the AP. The combination of the AP and an Access Control Engine (a shared ACL evaluator) for all entry points provided a couple of main benefits: a common logging point for all requests allowed us to perform forensic analysis more effectively, and we were also able to make changes to enforcement policies much more quickly and consistently."
"Semi-supervised Word Sense Disambiguation with Neural Models.  Determining the intended sense of words in text ?? word sense disambiguation (WSD) ?? is a long- standing problem in natural language processing. Recently, researchers have shown promising results using word vectors extracted from a neural network language model as features in WSD algorithms. However, a simple average or concatenation of word vectors for each word in a text loses the sequential and syntactic information of the text. In this paper, we study WSD with a sequence learning neural net, LSTM, to better capture the sequential and syntactic patterns of the text. To alleviate the lack of training data in all-words WSD, we employ the same LSTM in a semi-supervised label propagation classifier. We demonstrate state-of-the-art results, especially on verbs."
"Modeling Gesture-Typing Movements.  Word??Gesture keyboards allow users to enter text using continuous input strokes (also known as <i>gesture typing</i> or <i>shape writing</i>). We developed a production model of gesture typing input based on a human motor control theory of optimal control (specifically, modeling human drawing movements as a <i>minimization of jerk</i>??the third derivative of position). In contrast to existing models, which consider gestural input as a series of concatenated aiming movements and predict a user??s time performance, this descriptive theory of human motor control predicts the shapes and trajectories that users will draw. The theory is supported by an analysis of user-produced gestures that found qualitative and quantitative agreement between the shapes users drew and the minimum jerk theory of motor control. Furthermore, by using a small number of statistical <i>via-points</i> whose distributions reflect the sensorimotor noise and speed??accuracy trade-off in gesture typing, we developed a model of gesture production that can predict realistic gesture trajectories for arbitrary text input tasks. The model accurately reflects features in the figural shapes and dynamics observed from users and can be used to improve the design and evaluation of gestural input systems."
"A Cost???Benefit Study of Text Entry Suggestion Interaction.  Mobile keyboards often present error corrections and word completions (<em>suggestions</em>) as candidates for anticipated user input. However, these suggestions are not cognitively free: they require users to attend, evaluate, and act upon them. To understand this trade-off between suggestion savings and interaction costs, we conducted a text transcription experiment that controlled <em>interface assertiveness</em>: the tendency for an interface to present itself. Suggestions were either always present (<em>extraverted</em>), never present (<em>introverted</em>), or gated by a probability threshold (<em>ambiverted</em>). Results showed that although increasing the assertiveness of suggestions reduced the number of keyboard actions to enter text and was subjectively preferred, the costs of attending to and using the suggestions impaired average time performance."
"Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation.  We propose a simple, elegant solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no change in the model architecture from our base system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. The rest of the model, which includes encoder, decoder and attention, remains unchanged and is shared across all languages. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT using a single model without any increase in parameters, which is significantly simpler than previous proposals for Multilingual NMT. Our method often improves the translation quality of all involved language pairs, even while keeping the total number of model parameters constant. On the WMT'14 benchmarks, a single multilingual model achieves comparable performance for English-&gt;French and surpasses state-of-the-art results for English-&gt;German. Similarly, a single multilingual model surpasses state-of-the-art results for French-&gt;English and German-&gt;English on WMT'14 and WMT'15 benchmarks respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. In addition to improving the translation quality of language pairs that the model was trained with, our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and show some interesting examples when mixing languages."
"Interactive reinforcement learning for task-oriented dialogue management.  Dialogue management is the component of a dialogue system that determines the optimal action for the system to take at each turn. An important consideration for dialogue managers is the ability to adapt to new user behaviors unseen during training. In this paper, we investigate policy gradient based methods for interactive reinforcement learning where the agent receives action-specific feedback from the user and incorporates this feedback into its policy. We show that using the feedback to directly shape the policy enables a dialogue manager to learn new interactions faster compared to interpreting the feedback as a reward value."
"Multibrand geographic experiments.  In a geographic experiment to measure advertising effectiveness, some regions (hereafter GEOs) get increased advertising while others do not. This paper looks at running B &gt; 1 such experiments simultaneously on B different brands in G GEOs, and then using shrinkage methods to estimate
returns to advertising. There are important practical gains from doing this. Data from any one brand helps estimate the return of all other brands. We see this in both a frequentist and Bayesian formulation. As a result each individual experiment could be made smaller and less expensive when
they are analyzed together. We also provide an experimental design for multibrand experiments where half of the brands have increased spend in each GEO while half of the GEOs have increased spend for each brand. For G &gt; B the design is a two level factorial for each brand and simultaneously
a supersaturated design for the GEOs. Multiple simultaneous experiments also allow one to identify GEOs in which advertising is generally more effective. That cannot be done in the single brand experiments we consider."
"CogALex-V Shared Task: GHHH - Detecting Semantic Relations via Word Embeddings.  This paper describes our system submitted to the CogALex-2016 Shared Task on the Corpus-Based Identification of Semantic Relations. The evaluation results of our system on the test set are 88.1\% (79.0\% for TRUE only) f-measure for Task-1 on detecting semantic similarity, and 76.0\% (42.3\% when excluding RANDOM) for Task-2 on identifying more finer grained semantic relations. In our experiments, we try word analogy, linear regression, and multi-task Convolutional Neural Networks (CNN) with word embeddings from publicly available word vectors. We found that linear regression performs better in binary classification (Task-1), while CNN has better performance in multi-class semantic classification (Task-2). 
We assume that word analogy is more suited for deterministic answers rather than handling the ambiguity of one-to-many and many-to-many relationships. We also show that classifier performance could benefit from balancing the frequency of labels in the training data."
"The Power of Language Music: Arabic Lemmatization through Patterns.  Patterns play a pivotal role in Arabic morphological processing whether related to derivation or inflection. These patterns have not been yet adequately and fully utilized in computational processing of the language. The novel contribution of this paper is performing lemmatization (a high level lexical processing) without relying on a lookup dictionary. We use a machine learning classifier to predict the lemma pattern for a given stem, and use mapping rules to convert stems to their respective lemmas."
"Brand Attitudes and Search Engine Queries.  Search engines record the queries that users submit, including a large number of queries that include brand names. This data holds promise for assessing brand health. However, before adopting brand search volume as a brand metric, marketers should understand how brand search relates to traditional survey-based measures of brand attitudes, which have been shown to be predictive of sales. We investigate the relationship between brand attitudes and search engine queries using a unique micro-level data set collected from a panel of Google users who agreed to allow us to track their individual brand search behavior over eight weeks and link this search history to their responses to a brand attitude survey. Focusing on the smartphone and automotive markets, we find that users who are actively shopping in a category are more likely to search for any brand.  Further, as users move from being aware of a brand to intending to purchase a brand, they are increasingly more likely to search for that brand, with the greatest gains as customers go from recognition to familiarity and from familiarity to consideration. Additionally, users that own and use a particular automotive or smartphone brand are much more likely to search for that brand, even when they are not in market suggesting that a substantial volume of brand search in these categories is not related to shopping or product search. We discuss the implications of these findings for assessing brand health from search data."
"HolStep: a Machine Learning Dataset for Higher-Order Logic Theorem Proving.  Large computer-understandable proofs consist of millions of intermediate logical
steps. The vast majority of such steps originate from manually selected and manually
guided heuristics applied to intermediate goals. So far, machine learning has
generally not been used to filter or generate these steps. In this paper, we introduce
a new dataset based on Higher-Order Logic (HOL) proofs, for the purpose of developing
new machine learning-based theorem-proving strategies. We make this
dataset publicly available under the BSD license. We propose various machine
learning tasks that can be performed on this dataset, and discuss their significance
for theorem proving. We also benchmark a set of baseline deep learning models
suited for the tasks (including convolutional neural networks and recurrent neural
networks). The results of our baseline models shows the promise of applying deep
learning to HOL theorem proving."
"What??s your ML test score? A rubric for ML production systems.  Using machine learning in real-world production systems is complicated by a
host of issues not found in small toy examples or even large offline research
experiments. Testing and monitoring are key considerations for assessing the
production-readiness of an ML system. But how much testing and monitoring is
enough? We present an ML Test Score rubric based on a set of actionable tests to
help quantify these issues."
"Linguistic Wisdom from the Crowd.  Crowdsourcing for linguistic data typically aims to replicate expert annotations using simplified tasks. But an alternative goal??one that is especially relevant for research in the domains of language meaning and use??is to tap into people's rich experience as everyday users of language. Research in these areas has the potential to tell us a great deal about how language works, but designing annotation frameworks for crowdsourcing of this kind poses special challenges. In this paper we define and exemplify two approaches to linguistic data collection corresponding to these differing goals (<em>model-driven</em> and <em>user-driven</em>) and discuss some hybrid cases in which they overlap. We also describe some design principles and resolution techniques helpful for eliciting linguistic wisdom from the crowd."
"AutoMOS: Learning a non-intrusive assessor of naturalness-of-speech.  Developers of text-to-speech synthesizers (TTS) often make use of
  human raters to assess the quality of synthesized speech. We
  demonstrate that we can model human raters' mean opinion scores
  (MOS) of synthesized speech using a deep recurrent neural network
  whose inputs consist solely of a raw waveform. Our best models
  provide utterance-level estimates of MOS only moderately inferior to
  sampled human ratings, as shown by Pearson and Spearman
  correlations. When multiple utterances are scored and averaged,
  a scenario common in synthesizer quality assessment,
  we achieve correlations comparable to those of human raters.
  This model has a number of applications, such as the
  ability to automatically explore the parameter space of a speech
  synthesizer without requiring a human-in-the-loop.
  We explore a method of probing what the models have learned."
"A cortical-hippocampal-cortical loop of information processing during memory consolidation.  Developers of text-to-speech synthesizers (TTS) often make use of
  human raters to assess the quality of synthesized speech. We
  demonstrate that we can model human raters' mean opinion scores
  (MOS) of synthesized speech using a deep recurrent neural network
  whose inputs consist solely of a raw waveform. Our best models
  provide utterance-level estimates of MOS only moderately inferior to
  sampled human ratings, as shown by Pearson and Spearman
  correlations. When multiple utterances are scored and averaged,
  a scenario common in synthesizer quality assessment,
  we achieve correlations comparable to those of human raters.
  This model has a number of applications, such as the
  ability to automatically explore the parameter space of a speech
  synthesizer without requiring a human-in-the-loop.
  We explore a method of probing what the models have learned."
"Firmament: Fast, Centralized Cluster Scheduling at Scale.  Centralized datacenter schedulers can make high-quality
placement decisions when scheduling tasks in a cluster.
Today, however, high-quality placements come at
the cost of high latency at scale, which degrades response
time for interactive tasks and reduces cluster utilization.
This paper describes Firmament, a centralized scheduler
that scales to over ten thousand machines at subsecond
placement latency even though it continuously
reschedules all tasks via a min-cost max-flow (MCMF)
optimization. Firmament achieves low latency by using
multiple MCMF algorithms, by solving the problem incrementally,
and via problem-specific optimizations.
Experiments with a Google workload trace from a
12,500-machine cluster show that Firmament improves
placement latency by 20?? over Quincy [22], a prior
centralized scheduler using the same MCMF optimization.
Moreover, even though Firmament is centralized, it
matches the placement latency of distributed schedulers
for workloads of short tasks. Finally, Firmament exceeds
the placement quality of four widely-used centralized
and distributed schedulers on a real-world cluster,
and hence improves batch task response time by 6??"
"Optimizing Display Advertising Markets: Challenges and Directions.  Display advertising is the major source of revenue for service and content providers on the Internet. Here, the authors explain the prevalent mechanisms for selling display advertising, including reservation contracts and real-time bidding. Discussing some of the important challenges in this market from optimization and economic perspectives, they also survey recent results and directions for future research."
"Reservation Exchange Markets for Internet Advertising.  Internet display advertising industry follows two main business models. One model is based on direct deals between publishers and advertisers where they sign legal contracts containing terms of fulfillment for a future inventory. The second model is a spot market based on auctioning page views in real-time on advertising exchange (AdX) platforms such as DoubleClick's Ad Exchange, RightMedia, or AppNexus. These exchanges play the role of intermediaries who sell items (e.g. page-views) on behalf of a seller (e.g. a publisher) to buyers (e.g., advertisers) on the opposite side of the market. The computational and economics issues arising in this second model have been extensively investigated in recent times. In this work, we consider a third emerging model called reservation exchange market. A reservation exchange is a two-sided market between buyer orders for blocks of advertisers' impressions and seller orders for blocks of publishers' page views. The goal is to match seller orders to buyer orders while providing the right incentives to both sides. In this work we first describe the important features of mechanisms for efficient reservation exchange markets. We then address the algorithmic problems of designing revenue sharing schemes to provide a fair division between sellers of the revenue collected from buyers. A major conceptual contribution of this work is in showing that even though both clinching ascending auctions and VCG mechanisms achieve the same outcome from a buyer perspective, however, from the perspective of revenue sharing among sellers, clinching ascending auctions are much more informative than VCG auctions."
"Dynamic Auctions with Bank Accounts.  Consider a buyer with independent additive valuations
for a set of goods, and a seller who is constrained
to sell one item at a time in an online fashion.
If the seller is constrained to run independent
auctions for each item, then he would run Myerson??s
optimal auction for each item. If the seller is
allowed to use the full power of dynamic mechanism
design and have the auction for each item depend
on the outcome of the previous auctions, he is
able to perform much better. The main issues in implementing
such strategies in online settings where
items arrive over time are that the auction might
be too complicated or it makes too strong assumptions
on the buyer??s rationality or seller??s commitment
over time. This motivates us to explore a
restricted family of dynamic auctions that can be
implemented in an online fashion and without too
much commitment from the seller ahead of time.
In particular, we study a set of auction in which the
space of single-shot auctions is augmented with a
structure that we call bank account, a real number
for each node that summarizes the history so far.
This structure allows the seller to store deficits or
surpluses of buyer utility from each individual auction
and even them out on the long run. This is akin
to enforcing individual rationality constraint on average
rather than per auction. We also study the
effect of enforcing a maximum limit to the values
that bank account might grow, which means that
we enforce that besides the auction being individually
rational on average it is also not far from being
individually rational at any given interval. Interestingly,
even with these restrictions, we can achieve
significantly better revenue and social welfare compared
to separate Myerson auctions."
"Oblivious Dynamic Mechanism Design.  Despite their better revenue and welfare guarantees for repeated auctions, dynamic mechanisms have not been widely adopted in practice. This is partly due to their computational and implementation complexity, and also due to their unrealistic use of forecasting for future periods. We address the above shortcomings and present a new family of dynamic mechanisms that are computationally efficient and do not use any distribution knowledge of future periods. Our contributions are three-fold: We present the first polynomial-time dynamic incentive-compatible and ex-post individually rational mechanism for multiple periods and for any number of buyers that is a constant approximation to the optimal revenue. Unlike previous mechanisms, we require no expensive pre-processing step and in each period we run a simple auction that is a combination of virtual value maximizing auctions. We introduce the concept of obliviousness in dynamic mechanism design. A dynamic mechanism is oblivious if the allocation and pricing rule at each period does not depend on the type distributions in future periods. Our mechanism is oblivious and guarantees a 5-approximation compared to the optimal mechanism that knows all the distributions in advance. We develop a framework for characterizing, designing, and proving lower bounds for dynamic mechanisms (oblivious or not). In addition to the aforementioned positive results, we use this characterization to show that no oblivious mechanism can produce a better-than-2 approximation to the mechanism that knows all the distributions."
"Dynamic Mechanisms with Martingale Utilities.  We study the dynamic mechanism design problem of a seller who repeatedly sells independent items to a buyer with private values. In this setting, the seller could potentially extract the entire buyer surplus by running efficient auctions and charging an upfront participation fee at the beginning of the horizon. In some markets, such as internet advertising, participation fees are not practical since buyers expect to inspect items before purchasing them. This motivates us to study the design of dynamic mechanisms under successively more stringent requirements that capture the implicit business constraints of these markets. We first consider a periodic individual rationality constraint, which limits the mechanism to charge at most the buyer's value in each period. While this prevents large upfront participation fees, the seller can still design mechanisms that spread a participation fee across the first few auctions. These mechanisms have the unappealing feature that they provide close-to-zero buyer utility in the first auctions in exchange for higher utility in future auctions. To address this problem, we introduce a martingale utility constraint, which imposes the requirement that from the perspective of the buyer, the next item's expected utility is equal to the present one's. Our main result is providing a dynamic auction satisfying martingale utility and periodic individual rationality whose profit loss with respect to first-best (full extraction of buyer surplus) is optimal up to polylogarithmic factors. The proposed mechanism is a dynamic two-tier auction with a hard floor and a soft floor that allocates the item whenever the buyer's bid is above the hard floor and charges the minimum of the bid and the soft floor."
"Feature-based Dynamic Pricing.  We consider the problem faced by a firm that receives highly differentiated products in an online fashion and needs to price them in order to sell them to its customer base. Products are described by vectors of features and the market value of each product is linear in the values of the features. The firm does not initially know the values of the different features, but it can learn the values of the features based on whether products were sold at the posted prices in the past. This model is motivated by a question in online advertising, where impressions arrive over time and can be described by vectors of features. We first consider a multi-dimensional version of binary search over polyhedral sets, and show that it has exponential worst-case regret. We then propose a modification of the prior algorithm where uncertainty sets are replaced by their Lowner-John ellipsoids. We show that this algorithm has a worst-case regret that is quadratic in the dimensionality of the feature space and logarithmic in the time horizon."
"Almost Optimal Streaming Algorithms for Coverage Problems.  Maximum coverage and minimum set cover problems --collectively called coverage problems-- have been studied extensively in streaming models. However, previous research not only achieve sub-optimal approximation factors and space complexities, but also study a restricted set arrival model which makes an explicit or implicit assumption on oracle access to the sets, ignoring the complexity of reading and storing the whole set at once. In this paper, we address the above shortcomings, and present algorithms with improved approximation factor and improved space complexity, and prove that our results are almost tight. Moreover, unlike most of previous work, our results hold on a more general edge arrival model. More specifically, we present (almost) optimal approximation algorithms for maximum coverage and minimum set cover problems in the streaming model with an (almost) optimal space complexity of O~(n), i.e., the space is {\em independent of the size of the sets or the size of the ground set of elements}. These results not only improve over the best known algorithms for the set arrival model, but also are the first such algorithms for the more powerful {\em edge arrival} model. In order to achieve the above results, we introduce a new general sketching technique for coverage functions: This sketching scheme can be applied to convert an ??-approximation algorithm for a coverage problem to a $(1-\eps)\alpha$-approximation algorithm for the same problem in streaming, or RAM models. We show the significance of our sketching technique by ruling out the possibility of solving coverage problems via accessing (as a black box) a $(1 \pm \eps)$-approximate oracle (e.g., a sketch function) that estimates the coverage function on any subfamily of the sets."
"Distributed Coverage Maximization via Sketching.  Coverage problems are central in optimization and have a wide range of applications in data mining and machine learning. While several distributed algorithms have been developed for coverage problems, the existing methods suffer from several limitations, e.g., they all achieve either suboptimal approximation guarantees or suboptimal space and memory complexities. In addition, previous algorithms developed for submodular maximization assume oracle access, and ignore the computational complexity of communicating large subsets or computing the size of the union of the subsets in this subfamily. In this paper, we develop an improved distributed algorithm for the k-cover and the set cover with ?? outliers problems, with almost optimal approximation guarantees, almost optimal memory complexity, and linear communication complexity running in only four rounds of computation. Finally, we perform an extensive empirical study of our algorithms on a number of publicly available real data sets, and show that using sketches of size 30 to 600 times smaller than the input, one can solve the coverage maximization problem with quality very close to that of the state-of-the-art single-machine algorithm."
"Unsupervised Context Learning For Speech Recognition.  It has been shown in the literature that automatic speech
recognition systems can greatly benefit from contextual in-
formation [ref]. The contextual information can be used to
simplify the search and improve recognition accuracy. The
types of useful contextual information can include the name
of the application the user is in, the contents on the user??s
phone screen, user??s location, a certain dialog state, etc.
Building a separate language model for each of these types
of context is not feasible due to limited resources or limited
amount of training data.
In this paper we describe an approach for unsupervised
learning of contextual information and automatic building of
contextual (biasing) models. Our approach can be used to
build a large number of small contextual models from a lim-
ited amount of available unsupervised training data. We de-
scribe how n-grams relevant for a particular context are au-
tomatically selected as well as how an optimal size of a final
contextual model built is chosen. Our experimental results
show great accuracy improvements for several types of con-
text."
"Large-Scale Audio Event Discovery in One Million YouTube Videos.  Internet videos provide a virtually boundless source of audio with a conspicuous lack of localized annotations, presenting an ideal setting for unsupervised methods. With this motivation, we perform an unprecedented exploration into the large-scale discovery of recurring audio events in a diverse corpus of one million YouTube videos (45K hours of audio). Our approach is to apply a streaming, nonparametric clustering algorithm to both spectral features and out-of-domain neural audio embeddings, and use a small portion of manually annotated audio events to quantitatively estimate the intrinsic clustering performance. In addition to providing a useful mechanism for unsupervised active learning, we demonstrate the effectiveness of the discovered audio event clusters in two downstream applications.  The first is weakly-supervised learning, where we exploit the association of video-level metadata and cluster occurrences to temporally localize audio events. The second is informative activity detection, an unsupervised method for semantic saliency based on the corpus statistics of the discovered event clusters."
";login: Interrupt Reduction Projects.  Reducing interrupts using the methodology taken from Bigtable SRE: Relieving technical debt through short projects. This article begins by describing the landscape of work faced by Site Reliability Engineering (SRE) teams at Google: the types of work we undertake, the logistics of how SRE teams are organized across sites, and the inevitable toil we incur. Within this discussion, we focus on interrupts: how teams initially approached tickets, and why and how we implemented a better strategy. After providing a case study of how the ticket funnel was one such successful initiative, we offer practical advice about mapping what we learned to other organizations."
"Invent More, Toil Less.  This article is a follow-up to Vivek Rau's chapter ""Eliminating Toil"" in Site Reliability Engineering: How Google Runs Production Systems. We begin by recapping Vivek's definition of toil and Google's approach to balancing operational work with engineering project work. The Bigtable SRE case study then presents a concrete example of how one team at Google went about reducing toil. Finally, we leave readers with a series of best practices that should be helpful in reducing toil no matter the size or makeup of the organization."
"Toward Improving Digital Attribution Model Accuracy.  The accuracy of an attribution model is limited by the assumptions of the model, and the quality and completeness of the data available to the model.  Common digital attribution models on the market make a critical, yet hidden, assumption that ads only affect users by directly changing their propensity to convert.  These models assume that ad exposure does not change user behavior in other ways, such as driving additional website visits, generating branded searches, or creating awareness and interest in the advertiser.  In a previous paper, we described a Digital Advertising System Simulation (DASS) for modeling advertising and its impact on user behavior.  In this paper, we use this simulation to demonstrate that current models fail to accurately capture the true number of incremental conversions generated by ads that impact user behavior, and introduce an Upstream Data-Driven Attribution (UDDA) model to address this shortcoming.  We also demonstrate that development beyond UDDA is still required to address a lack of data completeness, and situations that include highly targeting advertising."
"Imagined Connectivities: Synthesized Conceptions of Public Wi-Fi Networks in Urban India.  India and other economies in the Global South are undergoing a proliferation in public Wi-Fi, with large-scale deployments from industry and government. In this paper, we report on a qualitative study on public Wi-Fi conceptions as held by urban Indians, a priori to connecting to a network. Our findings show that prior public Wi-Fi users and non-users alike raised a surprising range and depth of conceptions---ranging from suspicion of operators' intentions to monetize, to concerns about sexual image morphing, to fears of phone wipeouts, to aspiration---which were informed by popular media, BlueTooth cultures, and social learning. We found these conceptions of Wi-Fi networks to significantly influence adoption of public Wi-Fi. With enormous investments in public Wi-Fi initiatives, we call for network providers to address these deep conceptions among emerging users; by suggesting ways to build public awareness, better user experiences, and business model innovation."
"Locating the Internet in the Parks of Havana.  During the past two years, the public squares of Havana have been transformed from places where people stroll and children play to places where crowds gather to try to connect to the internet at all hours of the day and night. We present a qualitative inquiry of public WiFi hotspots in Havana, Cuba, and the possibilities of internet access these limited and expensive hotspots present to individuals, many of who are experiencing the internet for the first time. Drawing on fieldwork conducted in 2015-2016, we underscore the reconfigurations that have resulted from this access, as evolving internet users reconfigure their interactions with place, time, and individuals in their efforts to ``locate the internet.'' We also discuss the implications our findings have for the design of internet access interventions in Cuba and in other low-resource environments across the world, as well as the implications for social computing more generally."
"Topology and Control Innovation for Auxiliary Power Supply in Dimmable LED Drivers.  As the Internet of Things (IoT) continues to proliferate, connected and smart solutions are influencing more and more areas of our lives, as well as lighting sector. From system point of view, the additional smart features will require separate power and voltage domains from LED load, therefore, integrating an auxiliary (AUX) power supply into LED drivers is an ideal option to facilitate LED luminaire system design and reduce system cost and complexity. In this paper, a cost effective architecture based on Flyback topology is proposed for both constant current (CC) output for LED drive and constant voltage (CV) output for AUX supply. A novel nonlinear ramp based control scheme is proposed to decouple the main CC power train from the CV AUX supply and avoid LED output flickering. Small signal modeling is presented to highlight the advantages of this control scheme over conventional peak current mode control. This scheme has been implemented successfully for a 40W dimmable LED driver with a 12V 3W AUX supply."
"High quality agreement-based semi-supervised training data for acoustic modeling.  This paper describes a new technique to automatically obtain large high-quality training speech corpora for acoustic modeling. Traditional approaches select utterances based on confidence thresholds and other heuristics. We propose instead to use an ensemble approach: we transcribe each utterance using several recognizers, and only keep those on which they agree. The recognizers we use are trained on data from different dialects of the same language, and this diversity leads them to make different mistakes in transcribing speech utterances. In this work we show, however, that when they agree, this is an extremely strong signal that the transcript is correct. This allows us to produce automatically transcribed speech corpora that are superior in transcript correctness even to those manually transcribed by humans. Further more, we show that using the produced semi-supervised data sets, we can train new acoustic models which outperform those trained solely on previously available data sets."
"Large-Scale Image Retrieval with Attentive Deep Local Features.  We propose an attentive local feature descriptor suitable for large-scale image retrieval, referred to as DELF (DEep Local Feature). The new feature is based on convolutional neural networks, which are trained only with image-level annotations on a landmark image dataset. To identify semantically useful local features for image retrieval, we also propose an attention mechanism for keypoint selection, which shares most network layers with the descriptor. This framework can be used for image retrieval as a drop-in replacement for other keypoint detectors and descriptors, enabling more accurate feature matching and geometric verification. Our system produces reliable confidence scores to reject false positives---in particular, it is robust against queries that have no correct match in the database. To evaluate the proposed descriptor, we introduce a new large-scale dataset, referred to as Google-Landmarks dataset, which involves challenges in both database and query such as background clutter, partial occlusion, multiple landmarks, objects in variable scales, etc. We show that DELF outperforms the state-of-the-art global and local descriptors in the large-scale setting by significant margins."
"Adaptive Averaging in Accelerated Descent Dynamics.  We study accelerated descent dynamics for constrained convex optimization. This dynamics can be described naturally as a coupling of a dual variable accumulating gradients at a given rate ??(t), and a primal variable obtained as the weighted average of the mirrored dual trajectory, with weights w(t). Using a Lyapunov argument, we give sufficient conditions on ?? and w to achieve a desired convergence rate. As an example, we show that the replicator dynamics (an example of mirror descent on the simplex) can be accelerated using a simple averaging scheme. We then propose an adaptive averaging heuristic which adaptively computes the weights to speed up the decrease of the Lyapunov function. We provide guarantees on adaptive averaging, and give numerical experiments to compare it with existing heuristics, such as adaptive restarting. The experiments indicate that adaptive averaging performs at least as well as adaptive restarting, with significant improvements in some cases."
"Personal Tasks at Work: An Exploration.  Technology has changed the way that people work, socialize, and generally live their lives. Especially, technology has changed the way that those distinct parts of a person??s life intermingle. Previous work has largely focused on different styles of work-life balance, and most of the industry contributions to this problem focus on preventing work tasks from leaking into one??s personal life. However, little work has been done to investigate the ways that personal tasks impact the work day. We present the results of a survey of 93 employed people, as they relate to personal tasks that people do at work. We consider the context, consequences, and persistency of these tasks, and propose some design implications that could better reinforce the individual boundaries that people have between their work and personal lives."
"WaveNet: A Generative Model for Raw Audio.  This paper introduces WaveNet, a deep generative neural network trained end-to-end to model raw audio waveforms, which can be applied to text-to-speech and music generation. Current approaches to text-to-speech are focused on non-parametric, example-based generation (which stitches together short audio signal segments from a large training set), and parametric, model-based generation (in which a model generates acoustic features synthesized into a waveform with a vocoder). In contrast, we show that directly generating wideband audio signals at tens of thousands of samples per second is not only feasible, but also achieves results that significantly outperform the prior art. A single trained WaveNet can be used to generate different voices by conditioning on the speaker identity. We also show that the same approach can be used for music audio generation and speech recognition."
"On deconstructing ensemble models.  Consider a prediction problem with correlated predictors. In such a case, the best model specification, that is, the best subset of active predictors, can be ambiguous. In spite of this ambiguity, a forecast that informs a high-stakes decision warrants a compact, informative description of the model that produces it. For forecasts based on ensemble models, such descriptions are not straightforward.
Our example considers searches on google.com; each observation consists of one experiment changing the details in how the system responds to user queries. Our predictors measure the changes, relative to a contemporaneous control, of short-term metrics. Our response measures a shift in user behavior observable only after a longer term, also calculated relative to the control.
Our ensemble of models comes from a spike-and-slab regression. We represent each ensemble ?? each model ?? by its specification, a vector of booleans denoting the active predictors. To each such model we calculate its associated goodness of fit. Applying logic regression to predict goodness of fit as a function of the specification booleans, we obtain a metamodel. As a weighted sum of boolean expressions, the metamodel provides a description that is both parsimonious and illuminating"
"Modeling with Gamuts.  Consider predicting a response using an additive model. A gamut is an additional,
usually continuous variable that can segment data and any associated estimates; the
underlying model varies smoothly. Examples of gamuts: (a) for disease, ages of subjects,
their initial severity status, and/or cumulative exposure doses; (b) for learning,
measures of cumulative experience and/or engagement; and (c) for economic activity,
levels of income and/or spending. In previous work, gamuts have helped identify metric
changes, detect coefficient shifts, and formulate statistical narratives.
   Gamuts can be classified into four types: (1) gamuts exogenously specified and
known a priori; (2) those endogenously constructed and therefore latent; (3) gamuts
derived from an auxiliary model??s predictions; (4) gamuts chosen to optimize a predictive
model. Here we use gamuts of type (3) to parametrize model coefficients. By
construction, the in-sample goodness-of-fit is always improved, so we focus on out-of-sample
cross-validating methods. We also address computational issues."
"A Growing Long-term Episodic &amp; Semantic Memory.  The long-term memory of most connectionist systems lies entirely in the weights
of the system. Since the number of weights is typically fixed, this bounds the
total amount of knowledge that can be learned and stored. Though this is not
normally a problem for a neural network designed for a specific task, such a bound
is undesirable for a system that continually learns over an open range of domains.
To address this, we describe a lifelong learning system that leverages a fast, though
non-differentiable, content-addressable memory which can be exploited to encode
both a long history of sequential episodic knowledge and semantic knowledge over
many episodes for an unbounded number of domains. This opens the door for
investigation into transfer learning, and leveraging prior knowledge that has been
learned over a lifetime of experiences to new domains."
"TensorFlow Debugger: Debugging Dataflow Graphs for Machine Learning.  Debuggability is important in the development of machine-learning (ML) systems.
Several widely-used ML libraries, such as TensorFlow and Theano, are based on
dataflow graphs. While offering important benefits such as facilitating distributed
training, the dataflow graph paradigm makes the debugging of model issues more
challenging compared to debugging in the more conventional procedural paradigm.
In this paper, we present the design of the TensorFlow Debugger (tfdbg), a specialized
debugger for ML models written in TensorFlow. tfdbg provides features
to inspect runtime dataflow graphs and the state of the intermediate graph elements
(""tensors""), as well as simulating stepping on the graph. We will discuss the
application of this debugger in development and testing use cases."
Practical Cryptanalysis of Json Web Token and Galois Counter Mode's Implementations.  Practical Cryptanalysis of Json Web Token and Galois Counter Mode's Implementations
"Analyza: Exploring Data with Conversation.  We describe Analyza, a system that helps lay users explore
data. Analyza has been used within two large real world systems. The
first is a question-and-answer feature in a spreadsheet product. The
second provides convenient access to a revenue/inventory database
for a large sales force. Both user bases consist of users who do not
necessarily have coding skills, demonstrating Analyza's ability to
democratize access to data. We discuss the key design decisions in implementing this system.
For instance, how to mix structured and natural language modalities,
how to use conversation to disambiguate and simplify querying, how
to rely on the ``semantics'' of the data to compensate for the lack
of syntactic structure, and how to efficiently curate the data."
"Fast keyed hash/pseudo-random function using SIMD multiply and permute.  HighwayHash is a new pseudo-random function based on AVX2 multiply and permute instructions for thorough and fast hashing. It is 5.2 times as fast as SipHash for 1 KiB inputs. An open-source implementation is available under a permissive license. We discuss design choices and provide statistical analysis, speed measurements and preliminary cryptanalysis. Assuming it withstands further analysis, strengthened variants may also substantially accelerate file checksums and stream ciphers."
"Using instantaneous frequency and aperiodicity detection to estimate FO for high-quality speech synthesis.  This paper introduces a general and flexible framework for FO and aperiodicity analysis, specifically intended for high-quality speech synthesis and modification applications. The proposed framework consists of three subsystems: instantaneous frequency estimator and initial aperiodicity detector, FO trajectory tracker, and FO  refinement and aperiodicity extractor. A preliminary implementation of the proposed framework substantially outperformed (1/5 to 1/10 in terms of RMS FO estimation error) existing FO extractors in tracking ability of temporally varying FO trajectories. The front end aperiodicity detector consists of a complex-valued wavelet analysis filter with a highly selective temporal and spectral envelope. This front end aperiodicity detector uses a new measure that quantifies the deviation from periodicity. The measure is less sensitive to slow FM and AM and closely correlates with the signal to noise ratio.  The front end combines instantaneous frequency information over a set of filter outputs using the measure to yield an observation probability map. The second stage generates the initial FO trajectory using this map and signal power information. The final stage uses the deviation measure of each harmonic component and FO adaptive time warping to refine the FO estimate and aperiodicity estimation. The proposed framework is flexible to integrate other sources of instantaneous frequency when they provide relevant information."
"Who Broke the Build? Automatically Identifying Changes That Induce Test Failures In Continuous Integration at Google Scale.  Quickly identifying and fixing code changes that
introduce regressions is critical to keep the momentum on
software development, especially in very large scale software
repositories with rapid development cycles, such as at Google.
Identifying and fixing such regressions is one of the most
expensive, tedious, and time consuming tasks in the software
development life-cycle. Therefore, there is a high demand
for automated techniques that can help developers identify
such changes while minimizing manual human intervention.
Various techniques have recently been proposed to identify such
code changes. However, these techniques have shortcomings
that make them unsuitable for rapid development cycles as
at Google. In this paper, we propose a novel algorithm to
identify code changes that introduce regressions, and discuss
case studies performed at Google on 140 projects. Based on our
case studies, our algorithm automatically identifies the change
that introduced the regression in the top-5 among thousands of
candidates 82% of the time, and provides considerable savings
on manual work developers need to perform"
"AdaGAN: Boosting Generative Models.  Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) are an effective method for training generative models of complex data such as natural images. However, they are notoriously hard to train and can suffer from the problem of missing modes where the model is not able to produce examples in certain regions of the space. We propose an iterative procedure, called AdaGAN, where at every step we add a new component into a mixture model by running a GAN algorithm on a reweighted sample. This is inspired by boosting algorithms, where many potentially weak individual predictors are greedily aggregated to form a strong composite predictor. We prove that such an incremental procedure leads to convergence to the true distribution in a finite number of steps if each step is optimal, and convergence at an exponential rate otherwise. We also illustrate experimentally that this procedure addresses the problem of missing modes."
"Monte-Carlo tree search and rapid action value estimation in computer Go.  A new paradigm for search, based on Monte-Carlo simulation, has revolutionised the performance of computer Go programs. In this article we describe two extensions to the Monte-Carlo tree search algorithm, which significantly improve the effectiveness of the basic algorithm. When we applied these two extensions to the Go program MoGo, it became the first program to achieve dan   (master) level in 9??9 Go. In this article we survey the Monte-Carlo revolution in computer Go, outline the key ideas that led to the success of MoGo and subsequent Go programs, and provide for the first time a comprehensive description, in theory and in practice, of this extended framework for Monte-Carlo tree search."
"The grand challenge of computer Go: Monte Carlo tree search and extensions.  The ancient oriental game of Go has long been considered a grand challenge for artificial intelligence. For decades, computer Go has defied the classical methods in game tree search that worked so successfully for chess and checkers. However, recent play in computer Go has been transformed by a new paradigm for tree search based on Monte-Carlo methods. Programs based on Monte-Carlo tree search now play at human-master levels and are beginning to challenge top professional players. In this paper, we describe the leading algorithms for Monte-Carlo tree search and explain how they have advanced the state of the art in computer Go."
"Achieving Master Level Play in 9 x 9 Computer Go.  The UCT algorithm uses Monte-Carlo simulation to estimate the value of states in a search tree from the current state. However, the first time a state is encountered, UCT has no knowledge, and is unable to generalise from previous experience. We describe two extensions that address these weaknesses. Our first algorithm, heuristic UCT, incorporates prior knowledge in the form of a value function. The value function can be learned offline, using a linear combination of a million binary features, with weights trained by temporal-difference learning. Our second algorithm, UCT??RAVE, forms a rapid online generalisation based on the value of moves. We applied our algorithms to the domain of 9 ?? 9 Computer Go, using the program MoGo. Using both heuristic UCT and RAVE, MoGo became the first program to achieve human master level in competitive play."
"On the parallelization of Monte-Carlo planning.  Since their impressive successes in various areas of large-scale parallelization, recent techniques like UCT and
other Monte-Carlo planning variants (Kocsis and Szepesvari, 2006a) have been extensively studied (Coquelin
and Munos, 2007; Wang and Gelly, 2007). We here propose and compare various forms of parallelization of
bandit-based tree-search, in particular for our computer-go algorithm XYZ."
"Learning to Remember Rare Events.  Despite recent advances, memory-augmented deep neural networks are still limited
when it comes to life-long and one-shot learning, especially in remembering rare events. We present a large-scale life-long memory module for use in deep learning. The module exploits fast nearest-neighbor algorithms for efficiency and thus scales to large memory sizes. Except for the nearest-neighbor query, the module is fully differentiable and trained end-to-end with no extra supervision.  It operates in a life-long manner, i.e., without the need to reset it during training. Our memory module can be easily added to any part of a supervised neural network. To show its versatility we add it to a number of networks, from simple convolutional ones tested on image classification to deep sequence-to-sequence and recurrent-convolutional models. In all cases, the enhanced network gains the ability to remember and do life-long one-shot learning. Our module remembers training examples shown many thousands of steps in the past and it can successfully generalize from them. We set new state-of-the-art for one-shot learning on the Omniglot dataset and demonstrate, for the first time, life-long one-shot learning in recurrent neural networks on a large-scale machine translation task."
"HyperNetworks.  This work explores hypernetworks: an approach of using one network, also known as a hypernetwork, to generate the weights for another network.  We apply hypernetworks to generate adaptive weights for recurrent networks. In this case, hypernetworks can be viewed as a relaxed form of weight-sharing across layers. In our implementation, hypernetworks are are trained jointly with the main network in an end-to-end fashion.  Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks."
"Bisimulation and trace equivalence in an approximate probabilistic context.  This work introduces a notion of approximate probabilistic trace equivalence for labelled Markov chains, and relates this new concept to the known notion of approximate probabilistic bisimulation. In particular this work shows that the latter notion induces a tight upper bound on the approximation between finite-horizon traces, as expressed by a total variation distance. As such, this work extends corresponding results for exact notions and analogous results for non-probabilistic models. This bound can be employed to relate the closeness in satisfaction probabilities over bounded linear-time properties, and allows for probabilistic model checking of concrete models via abstractions. The contribution focuses on both finite-state and uncountable-state labelled Markov chains, and claims two main applications: firstly, it allows an upper bound on the trace distance to be decided for finite state systems; secondly, it can be used to synthesise discrete approximations to continuous-state models with arbitrary precision."
"Ubiq: A Scalable and Fault-tolerant Log Processing Infrastructure.  Most of today??s Internet applications are data-centric and generate vast amounts of data (typically, in the form of event logs) that needs to be processed and analyzed for detailed reporting, enhancing user experience and increasing monetization. In this paper, we describe the architecture of Ubiq, a geographically distributed framework for processing continuously growing log files in real time with high scalability, high availability and low latency. The Ubiq framework fully tolerates infrastructure degradation and datacenter-level outages without any manual intervention. It also guarantees exactly-once semantics for application pipelines to process logs in the form of event bundles. Ubiq has been in production for Google??s advertising system for many years and has served as a critical log processing framework for hundreds of pipelines. Our production deployment demonstrates linear scalability with machine resources, extremely high availability even with underlying infrastructure failures, and an end-to-end latency of under a minute."
"Structured adaptive and random spinners for fast machine learning computations.  We consider an efficient computational framework for speeding up several machine learning algorithms with almost no loss of accuracy. The proposed framework relies on projections via structured matrices that we call Structured Spinners, which are formed as products of three structured matrix-blocks that incorporate rotations. The approach is highly generic, i.e. i) structured matrices under consideration can either be fully-randomized or learned, ii) our structured family contains as special cases all previously considered structured schemes, iii) the setting extends to the non-linear case where the projections are followed by non-linear functions, and iv) the method finds numerous applications including kernel approximations via random feature maps, dimensionality reduction algorithms, new fast cross-polytope LSH techniques, deep learning, convex optimization algorithms via Newton sketches, quantization with random projection trees, and more. The proposed framework comes with theoretical guarantees characterizing the capacity of the structured model in reference to its unstructured counterpart and is based on a general theoretical principle that we describe in the paper. As a consequence of our theoretical analysis, we provide the first theoretical guarantees for one of the most efficient existing LSH algorithms based on the HD3HD2HD1 structured matrix [Andoni et al., 2015]. The exhaustive experimental evaluation confirms the accuracy and efficiency of structured spinners for a variety of different applications."
"Discrete Distribution Estimation under Local Privacy.  Randomized Response: a new RAPPOR-like mechanism for private histogram computation that has better utility and smaller message size than RAPPOR for ""large"" epsilon. Generalized Local Differential Privacy: a generalization of local differential privacy that allows the privacy level to vary across the alphabet. The generalization allows system designers to put privacy where it matters without paying the cost where it doesn't, admitting an elegant unification of local differential privacy with the classic Warner 1965 mechanism."
"Coevolutionary Latent Feature Processes for Continuous-Time User-Item Interactions.  Matching users to the right items at the right time is a fundamental task in recommendation systems. As users interact with different items over time, users?? and items?? feature may evolve and co-evolve over time. Traditional models based on static latent features or discretizing time into epochs can become ineffective for capturing the fine-grained temporal dynamics in the user-item interactions. We propose a coevolutionary latent feature process model that accurately captures the coevolving nature of users?? and items?? feature. To learn parameters, we design an efficient convex optimization algorithm with a novel low rank space sharing constraints. Extensive experiments on diverse real-world datasets demonstrate significant improvements in user behavior prediction compared to state-of-the-arts."
"Optimal Scheduling of a Constellation of Earth-Imaging Satellites, for Maximal Data Throughput and Efficient Human Management.  A mixed-integer linear program (MILP) approach to scheduling a constellation of Earth-imaging satellites is presented. The algorithm optimizes the assignment of imagery collects, image data downlinks, and ""health \&amp; safety"" contacts, generating schedules for all satellites and ground stations in a network. Hardware-driven constraints (e.g., the limited agility of the satellites) and operations-driven constraints (e.g., guaranteeing a minimum contact frequency for each satellite) are both addressed. Of critical importance to the use of this algorithm in real-world operations, it runs fast enough to allow for human operator interaction. This is achieved by a novel partitioning of the problem into distinct MILPs for downlink scheduling and image scheduling, with a dynamic programming (DP) heuristic providing a stand-in for imaging activity when scheduling the downlinks."
"Deep Network Guided Proof Search.  Deep learning techniques lie at the heart of several significant AI advances and break- throughs in the past years including object recognition and detection, image captioning, machine translation, speech recognition and synthesis, and playing the game of go.
Automated first-order theorem provers can aid in the formalization and verification of mathematical theorems and play a crucial role in program analysis, theory reasoning, security, interpolation, and system verification.
Here we suggest deep learning based guidance to the proof process of E Prover. We train and compare several deep neural network models on the traces of existing ATP proofs of Mizar statements and use them to select processed clauses during proof search. We give experimental evidence that with a hybrid, two-phase approach, deep learning based guidance can significantly reduce the average number of proof search steps while increasing the number of theorems proved.
Using a few proof guidance strategies that leverage deep neural networks, we have found first-order proofs of 7.18% of the first-order logic translations of the Mizar Mathematical Library theorems that did not previously have ATP generated proofs. This increases the ratio of statements in the corpus with ATP generated proofs from 56% to 59%."
"BBR: Congestion-Based Congestion Control.  By all accounts, today??s Internet is not moving data as well as it should. Most of the world??s cellular users experience delays of seconds to minutes; public Wi-Fi in airports and conference venues is often worse. Physics and climate researchers need to exchange petabytes of data with global collaborators but find their carefully engineered multi-Gbps infrastructure often delivers at only a few Mbps over intercontinental distances.6 These problems result from a design choice made when TCP congestion control was created in the 1980s??interpreting packet loss as ??congestion.??13 This equivalence was true at the time but was because of technology limitations, not first principles. As NICs (network interface controllers) evolved from Mbps to Gbps and memory chips from KB to GB, the relationship between packet loss and congestion became more tenuous. Today TCP??s loss-based congestion control??even with the current best of breed, CUBIC11??is the primary cause of these problems. When bottleneck buffers are large, loss-based congestion control keeps them full, causing bufferbloat. When bottleneck buffers are small, loss-based congestion control misinterprets loss as a signal of congestion, leading to low throughput. Fixing these problems requires an alternative to loss-based congestion control. Finding this alternative requires an understanding of where and how network congestion originates."
"Adversarial Machine Learning at Scale.  Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a ""label leaking"" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process."
"Adversarial examples in the physical world.  Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera."
"Density estimation using Real NVP.  Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations."
"Understanding deep learning requires rethinking generalization.  Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. 
Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. 
We interpret our experimental findings by comparison with traditional models."
"Neural Combinatorial Optimization with Reinforcement Learning.  This paper presents a framework to tackle combinatorial optimization problems using neural networks and reinforcement learning. We focus on the traveling salesman problem (TSP) and train a recurrent network that, given a set of city coordinates, predicts a distribution over different city permutations. Using negative tour length as the reward signal, we optimize the parameters of the recurrent network using a policy gradient method. We compare learning the network parameters on a set of training graphs against learning them on individual test graphs. Despite the computational expense, without much engineering and heuristic designing, Neural Combinatorial Optimization achieves close to optimal results on 2D Euclidean graphs with up to 100 nodes. Applied to the KnapSack, another NP-hard problem, the same method obtains optimal solutions for instances with up to 200 items."
"Categorical Reparameterization with Gumbel-Softmax.  Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification."
"HyperNetworks.  This work explores hypernetworks: an approach of using a one network, also known as a hypernetwork, to generate the weights for another network. Hypernetworks provide an abstraction that is similar to what is found in nature: the relationship between a genotype - the hypernetwork - and a phenotype - the main network. Though they are also reminiscent of HyperNEAT in evolution, our hypernetworks are trained end-to-end with backpropagation and thus are usually faster. The focus of this work is to make hypernetworks useful for deep convolutional networks and long recurrent networks, where hypernetworks can be viewed as relaxed form of weight-sharing across layers. Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve near state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks. Our results also show that hypernetworks applied to convolutional networks still achieve respectable results for image recognition tasks compared to state-of-the-art baseline models while requiring fewer learnable parameters."
"Unsupervised Pretraining for Sequence to Sequence Learning.  Sequence to sequence models are successful tools for supervised sequence learning tasks, such as machine translation. Despite their success, these models still require much labeled data and it is unclear how to improve them using unlabeled data, which is much less expensive to obtain. In this paper, we present simple changes that lead to a significant improvement in the accuracy of seq2seq models when the labeled set is small. Our method intializes the encoder and decoder of the seq2seq model with the trained weights of two language models, and then all weights are jointly fine-tuned with labeled data. An additional language modeling loss can be used to regularize the model during fine-tuning. We apply this method to low-resource tasks in machine translation and abstractive summarization and find that it significantly improves the subsequent supervised models. Our main finding is that the pretraining accelerates training and improves generalization of seq2seq models, achieving state-of-the-art results on the WMT English???German task. Our model obtains an improvement of 1.3 BLEU from the previous best models on both WMT'14 and WMT'15 English???German. Our ablation study shows that pretraining helps seq2seq models in different ways depending on the nature of the task: translation benefits from the improved generalization whereas summarization benefits from the improved optimization."
"Learning a Natural Language Interface with Neural Programmer.  Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. The task is often approached by mapping natural language queries to logical forms or programs that provide the desired response when executed on the database. To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce such programs on a real-world dataset. We enhance the objective function of Neural Programmer, a neural network with built-in discrete operations, and apply it on WikiTableQuestions, a natural language question-answering dataset. The model is trained end-to-end with weak supervision of question-answer pairs, and does not require domain-specific grammars, rules, or annotations that are key elements in previous approaches to program induction. The main experimental result in this paper is that a single Neural Programmer model achieves 34.2% accuracy using only 10,000 examples with weak supervision. An ensemble of 15 models, with a trivial combination technique, achieves 37.2% accuracy, which is competitive to the current state-of-the-art accuracy of 37.1% obtained by a traditional natural language semantic parser."
"Neural Architecture Search with Reinforcement Learning.  Learning a natural language interface for database tables is a challenging task that involves deep language understanding and multi-step reasoning. The task is often approached by mapping natural language queries to logical forms or programs that provide the desired response when executed on the database. To our knowledge, this paper presents the first weakly supervised, end-to-end neural network model to induce such programs on a real-world dataset. We enhance the objective function of Neural Programmer, a neural network with built-in discrete operations, and apply it on WikiTableQuestions, a natural language question-answering dataset. The model is trained end-to-end with weak supervision of question-answer pairs, and does not require domain-specific grammars, rules, or annotations that are key elements in previous approaches to program induction. The main experimental result in this paper is that a single Neural Programmer model achieves 34.2% accuracy using only 10,000 examples with weak supervision. An ensemble of 15 models, with a trivial combination technique, achieves 37.2% accuracy, which is competitive to the current state-of-the-art accuracy of 37.1% obtained by a traditional natural language semantic parser."
"Learning to Protect Communications with Adversarial Neural Cryptography.  We ask whether neural networks can learn to use secret keys to protect information from other neural networks. Specifically, we focus on ensuring confidentiality properties in a multiagent system, and we specify those properties in terms of an adversary. Thus, a system may consist of neural networks named Alice and Bob, and we aim to limit what a third neural network named Eve learns from eavesdropping on the communication between Alice and Bob. We do not prescribe specific cryptographic algorithms to these neural networks; instead, we train end-to-end, adversarially. We demonstrate that the neural networks can learn how to perform forms of encryption and decryption, and also how to apply these operations selectively in order to meet confidentiality goals."
"Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data.  Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information. To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees for training data: Private Aggregation of Teacher Ensembles (PATE). The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as ??teachers?? for a ??student?? model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student??s privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student??s training) and formally, in terms of differential privacy. These properties hold even if an adversary can not only query the student but also inspect its internal workings. Compared with previous work, the approach imposes only weak assumptions on how teachers are trained: it applies to any model, including non-convex models like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN thanks to an improved privacy analysis and semi-supervised learning."
"Unrolled Generative Adversarial Networks.  We introduce a method to stabilize Generative Adversarial Networks (GANs) by defining the generator objective with respect to an unrolled optimization of the discriminator. This allows training to be adjusted between using the optimal discriminator in the generator's objective, which is ideal but infeasible in practice, and using the current value of the discriminator, which is often unstable and leads to poor solutions. We show how this technique solves the common problem of mode collapse, stabilizes training of GANs with complex recurrent generators, and increases diversity and coverage of the data distribution by the generator."
"Conditional Image Synthesis With Auxiliary Classifier GANs.  Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data."
"Intelligible Language Modeling with Input Switched Affine Networks.  The computational mechanisms by which nonlinear recurrent neural networks (RNNs) achieve their goals remains an open question. There exist many problem domains where intelligibility of the network model is crucial for deployment. Here we introduce a recurrent architecture composed of input-switched affine transformations, in other words an RNN without any nonlinearity and with one set of weights per input. We show that this architecture achieves near identical performance to traditional architectures on language modeling of Wikipedia text, for the same number of model parameters. It can obtain this performance with the potential for computational speedup compared to existing methods, by precomputing the composed affine transformations corresponding to longer input sequences. As our architecture is affine, we are able to understand the mechanisms by which it functions using linear methods. For example, we show how the network linearly combines contributions from the past to make predictions at the current time step. We show how representations for words can be combined in order to understand how context is transferred across word boundaries. Finally, we demonstrate how the system can be executed and analyzed in arbitrary bases to aid understanding."
"A Learned Representation For Artistic Style.  The diversity of painting styles represents a rich visual vocabulary for the construction of an image. The degree to which one may learn and parsimoniously capture this visual vocabulary measures our understanding of the higher level features of paintings, if not images in general. In this work we investigate the construction of a single, scalable deep network that can parsimoniously capture the artistic style of a diversity of paintings. We demonstrate that such a network generalizes across a diversity of artistic styles by reducing a painting to a point in an embedding space. Importantly, this model permits a user to explore new painting styles by arbitrarily combining the styles learned from individual paintings. We hope that this work provides a useful step towards building rich models of paintings and offers a window on to the structure of the learned representation of artistic style."
"Identity Matters in Deep Learning.  An emerging design principle in deep learning is that each layer of a deep artificial neural network should be able to easily express the identity transformation. This idea not only motivated various normalization techniques, such as \emph{batch normalization}, but was also key to the immense success of \emph{residual networks}. 
In this work, we put the principle of \emph{identity parameterization} on a more solid theoretical footing alongside further empirical progress. We first give a strikingly simple proof that arbitrarily deep linear residual networks have no spurious local optima. The same result for linear feed-forward networks in their standard parameterization is substantially more delicate. Second, we show that residual networks with ReLu activations have universal finite-sample expressivity in the sense that the network can represent any function of its sample provided that the model has more parameters than the sample size. 
Directly inspired by our theory, we experiment with a radically simple residual architecture consisting of only residual convolutional layers and ReLu activations, but no batch normalization, dropout, or max pool. Our model improves significantly on previous all-convolutional networks on the CIFAR10, CIFAR100, and ImageNet classification benchmarks."
"Latent Sequence Decompositions.  We present the Latent Sequence Decompositions (LSD) framework. LSD decomposes sequences with variable lengthed output units as a function of both the input sequence and the output sequence. We present a training algorithm which samples valid extensions and an approximate decoding algorithm. We experiment with the Wall Street Journal speech recognition task. Our LSD model achieves 12.9% WER compared to a character baseline of 14.8% WER. When combined with a convolutional network on the encoder, we achieve 9.6% WER."
"Regularizing Neural Networks by Penalizing Confident Output Distributions.  We systematically explore regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. Furthermore, we connect a maximum entropy based confidence penalty to label smoothing through the direction of the KL divergence. We exhaustively evaluate the proposed confidence penalty and label smoothing on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and the confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyperparameters, suggesting the wide applicability of these regularizers."
"Unsupervised Perceptual Rewards for Imitation Learning.  Reward function design and exploration time are arguably the biggest obstacles to the deployment of reinforcement learning (RL) agents in the real world. In many real-world tasks, designing a reward function takes considerable hand engineering and often requires additional sensors to be installed just to measure whether the task has been executed successfully. Furthermore, many interesting tasks consist of multiple implicit intermediate steps that must be executed in sequence. Even when the final outcome can be measured, it does not necessarily provide feedback on these intermediate steps. To address these issues, we propose leveraging the abstraction power of intermediate visual representations learned by deep models to quickly infer perceptual reward functions from small numbers of demonstrations. We present a method that is able to identify key intermediate steps of a task from only a handful of demonstration sequences, and automatically identify the most discriminative features for identifying these steps. This method makes use of the features in a pre-trained deep model, but does not require any explicit specification of sub-goals. The resulting reward functions can then be used by an RL agent to learn to perform the task in real-world settings. To evaluate the learned reward, we present qualitative results on two real-world tasks and a quantitative evaluation against a human-designed reward function. We also show that our method can be used to learn a real-world door opening skill using a real robot, even when the demonstration used for reward learning is provided by a human using their own hand. To our knowledge, these are the first results showing that complex robotic manipulation skills can be learned directly and without supervised labels from a video of a human performing the task. Supplementary material and data are available at https://sermanet.github.io/rewards"
"Improving Policy Gradient by Exploring Under-appreciated Rewards.  This paper presents a novel form of policy gradient for model-free reinforcement learning (RL) with improved exploration properties. Current policy-based methods use entropy regularization to encourage undirected exploration of the reward landscape, which is ineffective in high dimensional spaces with sparse rewards. We propose a more directed exploration strategy that promotes exploration of under-appreciated reward regions. An action sequence is considered under-appreciated if its log-probability under the current policy under-estimates its resulting reward. The proposed exploration strategy is easy to implement, requiring small modifications to an implementation of the REINFORCE algorithm. We evaluate the approach on a set of algorithmic tasks that have long challenged RL methods. Our approach reduces hyper-parameter sensitivity and demonstrates significant improvements over baseline methods. Our algorithm successfully solves a benchmark multi-digit addition task and generalizes to long sequences. This is, to our knowledge, the first time that a pure RL method has solved addition using only reward feedback."
"Adversarial Training Methods for Semi-Supervised Text Classification.  Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations. We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting."
"TensorFlow: Learning Functions at Scale.  TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Its computational model is based on dataflow graphs with mutable state. Graph nodes may be mapped to different machines in a cluster, and within each machine to CPUs, GPUs, and other devices. TensorFlow supports a variety of applications, but it particularly targets training and inference with deep neural networks. It serves as a platform for research and for deploying machine learning systems across many areas, such as speech recognition, computer vision, robotics, information retrieval, and natural language processing. In this talk, we describe TensorFlow and outline some of its applications. We also discuss the question of what TensorFlow and deep learning may have to do with functional programming. Although TensorFlow is not purely functional, many of its uses are concerned with optimizing functions (during training), then with applying those functions (during inference). These functions are defined as compositions of simple primitives (as is common in functional programming), with internal data representations that are learned rather than manually designed. TensorFlow is joint work with many other people in the Google Brain team and elsewhere. More information is available at tensorflow.org."
"A Minimalistic Approach to Sum-Product Network Learning for Real Applications.  Sum-Product Networks (SPNs) are a class of expressive yet tractable hierarchical graphical models. LearnSPN is a structure learning algorithm for SPNs that uses hierarchical co-clustering to simultaneously identifying similar entities and similar features. The original LearnSPN algorithm assumes that all the variables are discrete and there is no missing data. We introduce a practical, simplified version of LearnSPN, MiniSPN, that runs faster and can handle missing data and heterogeneous features common in real applications. We demonstrate the performance of MiniSPN on standard benchmark datasets and on two datasets from Google??s Knowledge Graph exhibiting high missingness rates and a mix of discrete and continuous features."
"Audio Set: An ontology and human-labeled dataset for audio events.  Audio event recognition, the human-like ability to identify and relate sounds
from audio, is a nascent problem in machine perception. Comparable problems such
as object detection in images have reaped enormous benefits from comprehensive
datasets -- principally ImageNet. This paper describes the creation of 
Audio Set, a large-scale dataset of manually-annotated audio events that
endeavors to bridge the gap in data availability between image and audio
research. Using a carefully structured hierarchical ontology of 635 audio
classes guided by the literature and manual curation, we collect data from human
labelers to probe the presence of specific audio classes in 10 second segments
of YouTube videos. Segments are proposed for labeling using searches based on
metadata, context (e.g., links), and content analysis.  The
result is a dataset of unprecedented breadth and size that will, we hope,
substantially stimulate the development of high-performance audio event
recognizers."
Multi-Task Convolutional Music Models.  The paper is itself a short abstract for BayLearn.
"Audio Deepdream: Optimizing raw audio with convolutional networks.  The hallucinatory images of DeepDream opened up the floodgates for a recent wave of artwork generated by neural networks. In this work, we take first steps to applying this to audio. We believe a key to solving this problem is training a deep neural network to perform a music perception task on raw audio. Consequently, we have followed in the footsteps of Van den Oord et al and trained a network to predict embeddings that were themselves the result of a collaborative filtering model. A key difference is that we learn features directly from the raw audio, which creates a chain of differentiable functions from raw audio to high level features. We then use gradient descent on the network to extract samples of ""dreamed"" audio."
"Using Perceptual Metrics for Something Other Than Compression.  This talk considers several situations in which Perceptual Metrics add value, but are underexplored in the academic community. We present our recently published material on Noise and Banding metrics and also an overview of our transcoding pipeline."
"Taming Google-Scale Continuous Testing.  This talk considers several situations in which Perceptual Metrics add value, but are underexplored in the academic community. We present our recently published material on Noise and Banding metrics and also an overview of our transcoding pipeline."
"Distributed submodular cover: Succinctly summarizing massive data.  How can one find a subset, ideally as small as possible, that well represents a
massive dataset? I.e., its corresponding utility, measured according to a suitable
utility function, should be comparable to that of the whole dataset. In this paper,
we formalize this challenge as a submodular cover problem. Here, the utility is
assumed to exhibit submodularity, a natural diminishing returns condition prevalent
in many data summarization applications. The classical greedy algorithm is
known to provide solutions with logarithmic approximation guarantees compared
to the optimum solution. However, this sequential, centralized approach is impractical
for truly large-scale problems. In this work, we develop the first distributed
algorithm ?? DISCOVER ?? for submodular set cover that is easily implementable
using MapReduce-style computations. We theoretically analyze our approach,
and present approximation guarantees for the solutions returned by DISCOVER.
We also study a natural trade-off between the communication cost and the number
of rounds required to obtain such a solution. In our extensive experiments,
we demonstrate the effectiveness of our approach on several applications, including
active set selection, exemplar based clustering, and vertex cover on tens of
millions of data points using Spark."
"Fast Constrained Submodular Maximization: Personalized Data Summarization.  Can we summarize multi-category data based on
user preferences in a scalable manner? Many
utility functions used for data summarization satisfy
submodularity, a natural diminishing returns
property. We cast personalized data summarization
as an instance of a general submodular
maximization problem subject to multiple
constraints. We develop the first practical and
FAst coNsTrained submOdular Maximization algorithm,
FANTOM, with strong theoretical guarantees.
FANTOM maximizes a submodular function
(not necessarily monotone) subject to the intersection
of a p-system and l knapsacks constrains.
It achieves a (1+)(p+1)(2p+2l+1)/p
approximation guarantee with only O(
nrp log(n)/\epsilon)
query complexity (n and r indicate the size of
the ground set and the size of the largest feasible
solution, respectively). We then show how
we can use FANTOM for personalized data summarization.
In particular, a p-system can model
different aspects of data, such as categories or
time stamps, from which the users choose. In
addition, knapsacks encode users?? constraints including
budget or time. In our set of experiments,
we consider several concrete applications:
movie recommendation over 11K movies, personalized
image summarization with 10K images,
and revenue maximization on the YouTube
social networks with 5000 communities. We observe
that FANTOM constantly provides the highest
utility against all the baselines."
"Locally adaptive optimization: adaptive seeding for monotone submodular functions.  The Adaptive Seeding problem is an algorithmic challenge motivated by influence maximization in social networks: One seeks to select among certain accessible nodes in a network, and then select, adaptively, among neighbors of those nodes as they become accessible in order to maximize a global objective function. More generally, adaptive seeding is a stochastic optimization framework where the choices in the first stage affect the realizations in the second stage, over which we aim to optimize. Our main result is a (1 - 1/e)^2-approximation for the adaptive seeding problem for any monotone submodular function. While adaptive policies are often approximated via non-adaptive policies, our algorithm is based on a novel method we call locally-adaptive policies. These policies combine a non-adaptive global structure, with local adaptive optimizations. This method enables the (1 - 1/e)^2-approximation for general monotone submodular functions and circumvents some of the impossibilities associated with non-adaptive policies. We also introduce a fundamental problem in submodular optimization that may be of independent interest: given a ground set of elements where every element appears with some small probability, find a set of expected size at most k that has the highest expected value over the realization of the elements. We show a surprising result: there are classes of monotone submodular functions (including coverage) that can be approximated almost optimally as the probability vanishes. For general monotone submodular functions we show via a reduction from Planted-Clique that approximations for this problem are not likely to be obtainable. This optimization problem is an important tool for adaptive seeding via non-adaptive policies, and its hardness motivates the introduction of locally-adaptive policies we use in the main result."
"ICON: Inferring Temporal Constraints from Natural Language API Descriptions.  Temporal constraints of an Application Programming
Interface (API) are the allowed sequences of method
invocations in the API governing the secure and robust operation
of client software using the API. These constraints are typically
described informally in natural language API documents, and
therefore are not amenable to existing constraint-checking tools.
Manually identifying and writing formal temporal constraints
from API documents can be prohibitively time-consuming and
error-prone. To address this issue, we propose ICON: an approach
based on Machine Learning (ML) and Natural Language
Processing (NLP) for identifying and inferring formal temporal
constraints. To evaluate our approach, we use ICON to infer and
formalize temporal constraints from the Amazon S3 REST API,
the PayPal Payment REST API, and the java.io package
in the JDK API. Our results indicate that ICON can effectively
identify temporal constraint sentences (from over 4000 human annotated
API sentences) with the average 79.0% precision and
60.0% recall. Furthermore, our evaluation demonstrates that
ICON achieves an accuracy of 70% in inferring 77 formal
temporal constraints from these APIs."
"Segmenting Two-Sided Markets.  Recent years have witnessed the rise of many successful ecommerce marketplaces like the Amazon marketplace, Uber, AirBnB, and Upwork, where a central platform mediates economic transactions between buyers and sellers. A common feature of many of these two-sided marketplaces is that the platform has full control over search and discovery, but prices are determined by the buyers and sellers. Motivated by this, we study the algorithmic aspects of market segmentation via directed discovery in two-sided markets with endogenous prices. We consider a model where an online platform knows each buyer/seller??s characteristics, and associated demand/supply elasticities. Moreover, the platform can use discovery mechanisms (search/recommendation/etc.) to control which buyers/sellers are visible to each other. This leads to a segmentation of the market into pools, following which buyers and sellers endogenously determine market clearing transaction prices within each pool. The aim of the platform is to maximize the resulting volume of transactions/welfare in the market. We develop efficient algorithms with provable guarantees under a variety of assumptions on the demand and supply functions. We also test the validity of our assumptions on demand curves inferred from NYC taxicab log-data, as well as show the performance of our algorithms on synthetic experiments."
"Expanders via Local Edge Flips.  Designing distributed and scalable algorithms to improve network connectivity is a central topic in peer-to-peer networks. In this paper we focus on the following well-known problem: given an n-node d-regular network for d=??(logn), we want to design a decentralized, local algorithm that transforms the graph into one that has good connectivity properties (low diameter, expansion, etc.) without affecting the sparsity of the graph. To this end, Mahlmann and Schindelhauer introduced the random ""flip"" transformation, where in each time step, a random pair of vertices that have an edge decide to <code>swap a neighbor'. They conjectured that performing O(nd) such flips at random would convert any connected d-regular graph into a d-regular expander graph, with high probability. However, the best known upper bound for the number of steps is roughly O(n^17d^23), obtained via a delicate Markov chain comparison argument. 
Our main result is to prove that a natural instantiation of the random flip produces an expander in at most O(n^2d^2???logn) steps, with high probability. Our argument uses a potential-function analysis based on the matrix exponential, together with the recent beautiful results on the higher-order Cheeger inequality of graphs. We also show that our technique can be used to analyze another well-studied random process known as the</code>random switch', and show that it produces an expander in O(nd) steps with high probability."
"Generating Music by Fine-Tuning Recurrent Neural Networks with Reinforcement Learning.  Supervised learning with next-step prediction is a common way to train a sequence prediction model; however, it suffers from known failure modes and is notoriously difficult to train models to learn certain properties, such as having a coherent global structure. Reinforcement learning can be used to impose arbitrary properties on generated data by choosing appropriate reward functions. In this paper we propose a novel approach for sequence training, where we refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from stochastic optimal control (SOC). We explore the usefulness of our approach in the context of music gener- ation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note-RNN is then refined using RL, where the reward function is a combination of rewards based on rules of music theory, as well as the output of another trained Note-RNN. We show that this combination of ML and RL can not only produce more pleasing melodies, but that it can significantly reduce unwanted behaviors and failure modes of the RNN."
"Whole-Page Optimization and Submodular Welfare Maximization with Online Bidders.  In the context of online ad serving, display ads may appear on different types of webpages, where each page includes several ad slots and therefore multiple ads can be shown on each page. The set of ads that can be assigned to ad slots of the same page needs to satisfy various prespecified constraints including exclusion constraints, diversity constraints, and the like. Upon arrival of a user, the ad serving system needs to allocate a set of ads to the current webpage respecting these per-page allocation constraints. Previous slot-based settings ignore the important concept of a page and may lead to highly suboptimal results in general. In this article, motivated by these applications in display advertising and inspired by the submodular welfare maximization problem with online bidders, we study a general class of page-based ad allocation problems, present the first (tight) constant-factor approximation algorithms for these problems, and confirm the performance of our algorithms experimentally on real-world datasets. A key technical ingredient of our results is a novel primal-dual analysis for handling free disposal, which updates dual variables using a ??level function?? instead of a single level and unifies with previous analyses of related problems. This new analysis method allows us to handle arbitrarily complicated allocation constraints for each page. Our main result is an algorithm that achieves a 1 &amp;minus frac 1 e &amp;minus o(1)-competitive ratio. Moreover, our experiments on real-world datasets show significant improvements of our page-based algorithms compared to the slot-based algorithms. Finally, we observe that our problem is closely related to the submodular welfare maximization (SWM) problem. In particular, we introduce a variant of the SWM problem with online bidders and show how to solve this problem using our algorithm for whole-page optimization."
"SAC090 - SSAC Advisory on the Stability of the Domain Namespace.  This advisory is concerned only with the risks to security and stability that arise from
ambiguity in the use of the domain namespace. Because no one owns (or can own) the
domain namespace, and programmers and network managers cannot be prevented from
creating their own names and naming scopes, these risks arise regardless of how policy
debates about authority or oversight are resolved. Therefore, the observations and
recommendations in this advisory are directed at mitigating clearly identified risks and
developing policies that provide practical guidance to software and system developers,
rather than debating whether or not private network operators should use the domain
namespace, or who (if anyone) should have the authority to declare and enforce exclusive
uses for specific individual domain name labels or categories of labels."
"RSSAC002 version 2 -  RSSAC Advisory on Measurements of the Root Server System.  RSSAC has begun work to determine a list of parameters that define the
desired service trends for the root zone system. These parameters include the measured
latency in the distribution of the root zone, the frequency of the updates, and their size.
With knowledge of these parameters in hand, RSSAC can then seek to produce estimates
of acceptable root zone size dynamics to ensure the overall system works within a set of
parameters. The future work to define these parameters will involve RSSAC working
closely with the root server operators to gather best practice estimates for the size and
update frequency of the root zone."
"RSSAC024 - Key Technical Elements of Potential Root Operators.  In this document the RSSAC defines key technical elements of potential new root
operators that would be a critical part of any potential root server operator designation
process.
RSSAC001 (??Service Expectations of Root Servers??) and RFC 7720 (??DNS Root Name
Service Protocol and Deployment Requirements??) are considered as starting points;
alone, they are insufficient to evaluate potential operators.
Non-technical aspects, such as trustworthiness, ethos, funding, business models,
openness, community participation and politics are out of scope for this document. The
RSSAC believes these non-technical aspects to be important, and although this document
does not address them, we expect them to be part of the overall evaluation. The elements
defined in this document are designed to be:
?? Technical in nature
?? As specific as possible
?? Provable, documentable, and/or measurable"
"Towards Acoustic Model Unification Across Dialects.  Research has shown that acoustic model performance typically decreases when evaluated on a dialectal variation of the same language that was not used during training. Similarly, models simultaneously trained on a group of dialects tend to under-perform when compared to dialect-specific models. In this paper, we report on our efforts towards building a unified acoustic model that can serve a multi-dialectal language. Two techniques are presented: Distillation and MTL. In Distillation, we use an ensemble of dialect-specific acoustic models and distill its knowledge in a single model. In MTL, we utilize MultiTask Learning to train a unified acoustic model that learns to distinguish dialects as a side task. We show that both techniques are superior to the naive model that is trained on all dialectal data, reducing word error rates by 4.2% and 0.6%, respectively. And, while achieving this improvement, neither technique degrades the performance of the dialect-specific models by more than 3.4%."
The State of Continuous Integration Testing @Google.  This presentation deck speaks to some of the key problems and challenges facing the internal Google software engineering stack and invites industry and academic collaboration to help improve the state of the art of software testing throughout the industry.
"Recurrent Recommender Networks.  Recommender systems traditionally assume that user profiles and movie attributes are static. Temporal dynamics are purely reactive, that is, they are inferred after they are observed, e.g. after a user's taste has changed or based on hand-engineered temporal bias corrections for movies. We propose Recurrent Recommender Networks (RRN) that are able to predict future behavioral trajectories. This is achieved by endowing both users and movies with a Long Short-Term Memory (LSTM) autoregressive model that captures dynamics, in addition to a more traditional low-rank factorization. On multiple real-world datasets, our model offers excellent prediction accuracy and it is very compact, since we need not learn latent state but rather just the state transition function."
Generative Model-Based Text-to-Speech Synthesis.  Recent progress in generative modeling has improved the naturalness of synthesized speech significantly.  In this talk I will summarize these generative model-based approaches for speech synthesis and describe possible future directions.
"Understanding and Improving JVM GC Work Stealing at the Data Center Scale.  Garbage collection (GC) is a critical part of performance in managed run-time systems such as the OpenJDK Java Virtual Machine (JVM). With a large number of latency sensitive applications written in Java the performance of the JVM is imperative. Java application servers run in data centers on a large number of multi-core hardware. Thus load balancing in multi-threaded GC phases is critical. Dynamic load balancing in the JVM GC is achieved through work stealing, a well known and effective method to balance tasks across threads. This paper analyzes the JVM work stealing behavior, and introduces a novel work stealing technique that improves performance, GC CPU utilization, scalability, and cost of Jobs running on Google??s data-centers. We analyze both the Dacapo benchmark suite as well as Google??s data-center jobs. Our results show that the Gmail front-end server shows a 15-20% GC CPU reduction, and a 5% CPU performance improvement. Our analysis of a sample of ~59K jobs shows that GC CPU utilization improves by 38% geomean and 12% weighted geomean, GC pause time improves by 16% geomean, 20% weighted geomean and full GC pause time improves by 34% geomean, 12% weighted geomean."
"Privacy Requirements: Present &amp; Future.  Software systems are increasingly more and more
open, handle large amounts of personal or other sensitive data
and are intricately linked with the daily lives of individuals
and communities. This poses a range of privacy requirements.
Such privacy requirements are typically treated as instances of
requirements pertaining to compliance, traceability, access control,
verification or usability. Though important, such approaches
assume that the scope for the privacy requirements can be
established a-priori and that such scope does not vary drastically
once the system is deployed. User data and information, however,
exists in an open, hyper-connected and potentially ??unbounded??
environment. Furthermore, ??privacy requirements - present?? and
??privacy requirements - future?? may differ significantly as the
privacy implications are often emergent a-posteriori. Effective
treatment of privacy requirements, therefore, requires techniques
and approaches that fit with the inherent openness and fluidity
of the environment through which user data and information
flows are shared. This paper surveys state of the art and present
some potential directions in the way privacy requirements should
be treated. We reflect on the limitations of existing approaches
with regards to unbounded privacy requirements and highlight a
set of key challenges for requirements engineering research with
regards to managing privacy in such unbounded settings."
"Contextual Language Model Adaptation Using Dynamic Classes.  Recent focus on assistant products has increased the need for extremely
flexible speech systems that adapt
well to specific users' needs. An important aspect of this is enabling users to
make voice commands referencing their own personal data, such as favorite songs,
application names, and contacts. Recognition accuracy for common commands such
as playing music and sending text messages can be greatly improved if we know a
user's preferences. In the past, we have addressed this problem using class-based language models
that allow for query-time injection of class instances. However, this approach
is limited by the need to train class-based models ahead of time. In this work, we present a significantly more flexible system for query-time
injection of user context. Our system dynamically injects the classes
into a non-class-based language model. We remove the need to select the classes
at language model training time. Instead, our system can vary the classes on a
per-client, per-use case, or even a per-request basis. With the ability to inject new classes per-request outlined in this work, our
speech system can support a diverse set of use cases by
taking advantage of a wide range of contextual information specific to each
use case."
"Situational Context for Ranking in Personal Search.  Modern search engines leverage a variety of sources, beyond the traditional query-document content similarity, to improve their ranking performance. Among them, query context has attracted attention in prior work. Previously, query context was mainly modeled by user search history, either long-term or short-term, to help the ranking of future queries. In this paper, we focus on situational context, i.e., the contextual features of the current search request that are independent from both query content and user history. As an example, situational context can depend on search request time and location. We propose two context-aware ranking models based on neural networks. The first model learns a low-dimensional deep representation from the combination of contextual features. The second model extends the first model by leveraging binarized contextual features in addition to the high-level abstractions learned from a deep network. The existing context-aware ranking models are mainly based on search history, especially click data that can be gathered from the search engine logs. Although context-aware models have been widely explored in web search, their influence on search scenarios where click data is highly sparse is relatively unstudied. The focus of this paper, personal search (e.g., email search or on-device search) is one of such scenarios. We evaluate our models using the click data collected from one of the world's largest personal search engines. The experiments demonstrate that the proposed models significantly outperform the baselines which do not take context into account. These results indicate the importance of situational context for personal search, and open up a venue for further exploration of situational context in other search scenarios."
"Composite Ethical Frameworks for IoT and other Emerging Technologies.  (From Conclusion)
With the advent of innovations like IoT (which move rapidly and include machine-enhanced learning), the ethical responsibility for the engineers and designers of such technologies becomes magnified. Today??s engineering and R&amp;D should be informed by considering established deontological codes, as well as a combination of teleological and virtue ethics approaches that can help guide the emergent practices and anticipation of consequences. Programmers and systems engineers who contribute to the evolution of the Internet of Things need to feel empowered by ethical considerations (e.g. codes of professional associations) to resist release of products that do not meet standards of safety, reliability, privacy and resilience."
"IoT Safety and Security as Shared Responsibility.  As the things around us become more and more internetworked, users?? safety must be the first priority for all hardware and software providers. In the context of  the Internet of Things, this paper puts forward a definition of ??digital safety?? as distinction from ??online security?? and discusses how multistakeholder governance can be applied to address safety challenges."
"Taking the Internet to the Next Physical Level.  We have come a long way since the IEEE Computer article by Mark Weiser in which he  envisioned small connected computers that ubiquitously enhance all aspects of our life. In this opening article of the IEEE IoT Connection, we put forward our analysis of the architectural leitmotifs we should pursue for the Internet of Things ecosystem in order to repeat the staggering success of the Internet that resulted in the introduction of the World Wide Web. By success, we mean the economic value, social and technological innovation these platforms  have brought to the world."
"Internet Governance Is Our Shared Responsibility.  This essay looks at the different roles that multistakeholder institutions play in the Internet governance ecosystem. We propose a model for thinking of Internet governance within the context of the Internet's layered model. We use the example of the negotiations in Dubai in 2012 at the World Conference on International Telecommunications as an illustration for why it is important for different institutions within the governance system to focus on their respective areas of expertise (e.g., the ITU, ICANN, and IGF). Several areas of conflict (a ""tussle"") are reviewed, such as the desire to promote more broadband infrastructure, a topic that is in the remit of the International Telecommunications Union, but also the recurring desire of countries like Russia and China to use the ITU to regulate content and restrict free expression on the Internet through onerous cybersecurity and spam provisions. We conclude that it is folly to try and regulate all these areas through an international treaty, and encourage further development of mechanisms for global debate like the Internet Governance Forum (IGF)."
"Bitrate Classification of Twice-Encoded Audio using Objective Quality Features.  Streaming services such as Google Play Music and Sound-Cloud handle terabytes of audio data every week. These services aim to encode audio with a balance between quality of experience (QoE) [1] for the end user, the size of the encoded audio files, and the processing cost of the encoding. Users may upload files to a streaming service that have already been encoded because the user wants to reduce file size to decrease upload time. The same audio encoded as a 3 MB uncompressed WAV, a 510 KB 256kb/s AAC-LC, or a 250 KB 128 kb/s Opus all seem similar in quality to expert listeners [2]. Streaming services encode audio to a number of bitrates and formats to provide the best experience for users of different devices. For example, mobile users may prefer to compromise quality to limit bandwidth consumption. Services do not encode to bitrates higher than that of the uploaded files as there will be no increase in quality. Determining the lowest bitrate of the files allows the streaming service to forgo encoding the files to bitrates higher than that of the uploaded files, saving on processing and storage space."
"Email Category Prediction.  According to recent estimates, about 90% of consumer received emails are machine-generated. Such messages include shopping receipts, promotional campaigns, newsletters, booking confirmations, etc. Most such messages are created by populating a fixed template with a small amount of personalized information, such as name, salutation, reservation numbers, dates, etc. Web mail providers (Gmail, Hotmail, Yahoo) are leveraging the structured nature of such emails to extract salient information and use it to improve the user experience: e.g. by automatically entering reservation data into a user calendar, or by sending alerts about upcoming shipments. To facilitate these extraction tasks it is helpful to classify templates according to their category, e.g. restaurant reservations or bill reminders, since each category triggers a particular user experience. Recent research has focused on discovering the causal thread of templates, e.g. inferring that a shopping order is usually followed by a shipping confirmation, an airline booking is followed by a confirmation and then by a ??ready to check in?? message, etc. Gamzu et al. took this idea one step further by implementing a method to predict the template category of future emails for a given user based on previously received templates. The motivation is that predicting future emails has a wide range of potential applications, including better user experiences (e.g. warning users of items ordered but not shipped), targeted advertising (e.g. users that recently made a flight reservation may be interested in hotel reservations), and spam classification (a message that is part of a legitimate causal thread is unlikely to be spam). The gist of the Gamzu et al. approach is modeling the problem as a Markov chain, where the nodes are templates or temporal events (e.g. the first day of the month). This paper expands on their work by investigating the use of neural networks for predicting the category of emails that will arrive during a fixed-sized time window in the future. We consider two types of neural networks: multi-layer perceptrons (MLP), a type of feedforward neural network; and long short-term memory (LSTM), a type of recurrent neural network. For each type of neural network, we explore the effects of varying their configuration (e.g. number of layers or number of neurons) and hyper-parameters (e.g. drop-out ratio). We find that the prediction accuracy of neural networks vastly outperforms the Markov chain approach, and that LSTMs perform slightly better than MLPs. We offer some qualitative interpretation of our findings and identify some promising future directions."
"Variance Reduction for Large Scale Revenue Optimization.  A significant part of optimizing revenue for ad auctions is setting a
good reserve (or minimum) price. Set it too low, and the impression
may yield little revenue, set it too high and there may not anyone
willing to buy the item. Previous work has looked at predicting this
value directly, however, the strongly non-convex objective function
makes this a challenging proposition. In contrast, motivated by the
fact that computing an optimal reserve price for a set of bids is
easy, we propose a clustering approach, first finding a good partition
of the data, and then finding an optimal reserve price for each
partition. In this work, we take a major step in this direction: we
derive the specific objective function that corresponds to revenue
optimization in auctions, and give algorithms that optimize it."
Robust Stochastic Linear Bandits.  We analyze the problem of linear bandits under heavy tailed noise and provide favorable regret guarantees.
"Adaptation Based on Generalized Discrepancy.  We present a new algorithm for domain adaptation improving upon a discrepancy mini- mization algorithm, (DM), previously shown to outperform a number of algorithms for this problem. Unlike many previous proposed solutions for domain adaptation, our algorithm does not consist of a fixed reweighting of the losses over the training sample. Instead, the reweighting depends on the hypothesis sought. The algorithm is derived from a less con- servative notion of discrepancy than the DM algorithm. We call this quantity generalized discrepancy. We present a detailed description of our algorithm and show that it can be formulated as a convex optimization problem. We also give a detailed theoretical analysis of its learning guarantees which helps us select its parameters. Finally, we report the results of experiments demonstrating that it improves upon discrepancy minimization in several tasks."
"Learning mobile phone battery consumptions.  We introduce a novel, data-driven way for predicting battery consumption of apps. The state-of-the-art models used to blame  battery consumption on apps are based on micro-benchmark experiments. These experiments are carried out on controlled setups where one can measure how much battery is consumed by each internal resource (CPU, bluetooth, WiFi...). The battery blame allocated to an app is simply the sum of the  blames of the resources consumed by the app. We argue that this type of models do not capture the way phones work ""in the wild"" and propose instead to train a regression model using data collected from logs. We  show that this type of learning is correct in the sense that under some assumptions, we can recover the true battery discharge rate of each component. We present experimental results where we consistently do better predictions than a model trained on micro-benchmarks."
Capacity Planning at Scale.  Have you ever bought machines? What if you need to even build datacenters? How can you predict how many you are going to need in two years from now? How can you make efficient use of all the resources you suddenly got? What if you are missing some resources? Can we automate all these stuff and integrate with our continuous delivery? These are just a few questions anyone planning a large computer fleet always make. This talk will cover some of the approaches and tooling that can be used to effectively plan for the demand of services and how to cover it on the most efficient manner.
"Deep Variational Information Bottleneck.  We present a variational approximation to the information bottleneck of Tishby et al. (1999). This variational approach allows us to parameterize the information bottleneck model using a neural network and leverage the reparameterization trick for efficient training. We call this method ""Deep Variational Information Bottleneck"", or Deep VIB. We show that models trained with the VIB objective outperform those that are trained with other forms of regularization, in terms of generalization performance and robustness to adversarial attack."
"Improved generator objectives for GANs.  We present a new framework to understand GAN training as alternating density ratio estimation with divergence minimization. This provides a new interpretation for the GAN generator objective used in practice and explains the problem of poor sample diversity. Furthermore, we derive a family of objectives that target arbitrary f-divergences without minimizing a lower bound, and use them to train generative image models that target either improved sample quality or sample diversity."
"Recent Books and Journals Articles in Public Opinion, Survey Methods, Survey Statistics, and Big Data. 2016 Update.  Welcome to the 8th edition of this column on recent books and journal articles in the field of public opinion, survey methods, survey statistics, and Big Data. This year I officially added Big Data to the title as there is very strong interest on the topic and
surveys and Big data are becoming more and more interrelated.
This article is an update of the April 2015 article. Like the previous year, the books are organized by topic; this should help the readers to focus on their interests. It is unlikely to list all new books in the field; I did my best scouting different resources and websites, but I take full responsibility for any omission. The list is also focusing only on books published in English language and available for purchase (as ebook or in print) at the time of this review (October 2016). Books are listed based on the relevance to the topic, and no judgment is made in terms of quality of the content. We let the readers do so."
"Postmortem Action Items: Plan the Work and Work the Plan.  This article follows up SRE Book chapter ??Postmortem Culture: Learning from Failure."" Here, we address the challenges in designing an appropriate action item plan and then executing that plan. We discuss best practices for developing high-quality action items (AIs) for a postmortem, plus methods of ensuring these AIs actually get implemented."
"Deep Metric Learning via Facility Location.  Learning the representation and the similarity metric in an end-to-end fashion with deep networks have demonstrated outstanding results for clustering and retrieval. However, these recent approaches still suffer from the performance degradation stemming from the local metric training procedure which is unaware of the global structure of the embedding space. 
We propose a global metric learning scheme for optimizing the deep metric embedding with the learnable clustering function and the clustering metric (NMI) in a novel structured prediction framework. 
Our experiments on CUB200-2011, Cars196, and Stanford online products datasets show state of the art performance both on the clustering and retrieval tasks measured in the NMI and Recall@K evaluation metrics."
"Template Induction over Unstructured Email Corpora.  Unsupervised template induction over email data is a central component in applications such as information extraction, document classification, and auto-reply. The benefits of automatically generating such templates are known for structured data, e.g. machine generated HTML emails. However much less work has been done in performing the same task over unstructured email data. We propose a technique for inducing high quality templates from plain text emails at scale based on the suffix array data structure. We evaluate this method against an industry-standard approach for finding similar content based on shingling, running both algorithms over two corpora: a synthetically created email corpus for a high level of experimental control, as well as user-generated emails from the well-known Enron email corpus. Our experimental results show that the proposed method is more robust to variations in cluster quality than the baseline and templates contain more text from the emails, which would benefit extraction tasks by identifying transient parts of the emails. Our study indicates templates induced using suffix arrays contain approximately half as much noise (measured as entropy) as templates induced using shingling. Furthermore, the suffix array approach is substantially more scalable, proving to be an order of magnitude faster than shingling even for modestly-sized training clusters. Public corpus analysis shows that email clusters contain on average 4 segments of common phrases, where each of the segments contains on average 9 words, thus showing that templatization could help users reduce the email writing effort by an average of 35 words per email in an assistance or auto-reply related task."
"A Data-Driven Large-Scale Optimization Approach for Task-Specific Physics Realism in Real-Time Robotics Simulation.  Physics-based simulation of robots requires mod-
els of the simulated robots and their environment. For a realistic
simulation behavior, these models must be accurate. Their
physical properties such as geometric and kinematic values,
as well as dynamic parameters such as mass, inertia matrix
and friction, must be modelled. Unfortunately, this problem is
hard for at least two reasons. First, physics engines designed
for simulation of rigid bodies in real-time cannot accurately
describe many common real world phenomena, e.g. (drive)
friction and grasping. Second, classical parameter identification
algorithms are well-studied and efficient, but often necessitate
significant manual engineering effort and may not be applicable
due to application constraints. Thus, we present a data-
driven general purpose tool, which allows to optimize model
parameters for (task-specific) realistic simulation behavior. Our
approach directly uses the simulator and the model under
optimization to improve model parameters. The optimization
process is highly distributed and uses a hybrid optimization
approach based on metaheuristics and the Ceres non-linear
least squares solver. The user only has to provide a configuration
file that specifies which model parameter to optimize together
with realism criteri"
"Multichannel Signal Processing with Deep Neural Networks for Automatic Speech Recognition.  Multichannel ASR systems commonly separate speech enhancement, including localization, beamforming and postfiltering, from acoustic modeling. In this paper, we perform multichannel enhancement jointly with acoustic modeling in a deep neural network framework. Inspired by beamforming, which leverages differences in the fine time structure of the signal at different microphones to filter energy arriving from different directions, we explore modeling the raw time-domain waveform directly. We introduce a neural network architecture which performs multichannel filtering in the first layer of the network and show that this network learns to be robust to varying target speaker direction of arrival, performing as well as a model that is given oracle knowledge of the true target speaker direction.
%
Next, we show how performance can be improved by \emph{factoring} the first layer to separate the multichannel spatial filtering operation from a single channel filterbank which computes a frequency decomposition.
%
We also introduce an adaptive variant, which updates the spatial filter coefficients at each time frame based on the previous inputs.
%
Finally we demonstrate that these approaches can be implemented more efficiently in the frequency domain. Overall, we find that such multichannel neural networks give a relative word error rate improvement of more than 5\% compared to a traditional beamforming-based multichannel ASR system and more than 10\% compared to a single channel waveform model."
"Trainable Frontend For Robust and Far-Field Keyword Spotting.  Robust and far-field speech recognition is critical to enable true hands-free communication. In far-field conditions, signals are attenuated due to distance. To improve robustness to loudness variation, we introduce a novel frontend called per-channel energy normalization (PCEN). The key ingredient of PCEN is the use of an automatic gain control based dynamic compression to replace the widely used static (such as log or root) compression. We evaluate PCEN on the keyword spotting task. On our large rerecorded noisy and far-field eval sets, we show that PCEN significantly improves recognition performance. Furthermore, we model PCEN as neural network layers and optimize high-dimensional PCEN parameters jointly with the keyword spotting acoustic model. The trained PCEN frontend demonstrates significant further improvements without increasing model complexity or inference-time cost."
"Flatstart-CTC: a new acoustic model training procedure for speech recognition.  We present a new procedure to train acoustic models from scratch for large vocabulary speech
recognition requiring no previous model for alignments or boot-strapping.
We augment the Connectionist Temporal Classification (CTC) objective function to allow training of acoustic models directly
from a parallel corpus of audio data and transcribed data. With this augmented CTC function
we train a phoneme recognition acoustic model directly from the written-domain transcript. Further, 
we outline a mechanism to generate a context-dependent phonemes from a CTC model trained to predict phonemes 
and ultimately train a second CTC model to predict these context-dependent phonemes. Since this approach does not 
require training of any previous non-CTC model it drastically reduces the overall data-to-model training time from 
30 days to 10 days. Additionally, models obtain from this flatstart-CTC procedure outperform the state-of-the-art by XX-XX\%."
"Predicting Pronunciations with Syllabification and Stress with Recurrent Neural Networks.  Word pronunciations, consisting of phoneme sequences and the associated syllabification and stress patterns, are vital for both speech recognition and text-to-speech (TTS) systems. For speech recognition phoneme sequences for words may be learned from audio data. We train recurrent neural network (RNN) based models to predict the syllabification and stress pattern for such pronunciations making them usable for TTS. We find these RNN models significantly outperform naive rulebased models for almost all languages we tested. Further, we find additional improvements to the stress prediction model by using the spelling as features in addition to the phoneme sequence. Finally, we train a single RNN model to predict the phoneme sequence, syllabification and stress for a given word. For several languages, this single RNN outperforms similar models trained specifically for either phoneme sequence or stress prediction. We report an exhaustive comparison of these approaches for twenty languages."
"Multi-Accent Speech Recognition with Hierarchical Grapheme Based Models.  We explore the viability of grapheme-based
recognition specifically how it compares to phoneme-based
equivalents. We utilize the CTC loss to train models to directly
predict graphemes, we also train models with hierarchical
CTC and show that they improve on previous CTC models.
We also explore how the grapheme and phoneme models
scale with large data sets, we consider a single acoustic training
data set where we combine various dialects of English from
US, UK, India and Australia. We show that by training a single
grapheme-based model on this multi-dialect data set we create
a accent-robust ASR system"
"Traffic Lights with Auction-Based Controllers: Algorithms and Real-World Data.  Real-time optimization of traffic flow addresses important practical problems: reducing a driver's wasted time, improving city-wide efficiency, reducing gas emissions and improving air quality. Much of the current research in traffic-light optimization relies on extending the capabilities of traffic lights to either communicate with each other or communicate with vehicles. However, before such capabilities become ubiquitous, opportunities exist to improve traffic lights by being more responsive to current traffic situations within the current, already deployed, infrastructure. In this paper, we introduce a traffic light controller that employs bidding within micro-auctions to efficiently incorporate traffic sensor information; no other outside sources of information are assumed. We train and test traffic light controllers on large-scale data collected from opted-in Android cell-phone users over a period of several months in Mountain View, California and the River North neighborhood of Chicago, Illinois. The learned auction-based controllers surpass (in both the relevant metrics of road-capacity and mean travel time) the currently deployed lights, optimized static-program lights, and longer-term planning approaches, in both cities, measured using real user driving data."
"Submodular Optimization Over Sliding Windows.  Maximizing submodular functions under cardinality constraints lies at
the core of numerous data mining and machine learning applications,
including data diversification, data summarization, and coverage problems.
In this work, we study this question in the context of data
streams, where elements arrive one at a time, and we want to
design low-memory and fast update-time algorithms that maintain a good solution. 
Specifically, we focus on the sliding window
model, where we are asked to maintain a solution that considers only
the last $W$ items. In this context, we provide the first non-trivial algorithm that
maintains a provable approximation of the optimum using space sublinear in the size of the window.
In particular we give a  $\nicefrac{1}{3} - \epsilon$ approximation algorithm that uses space polylogarithmic in
the spread of the values of the elements,
$\Spread$, and linear in the solution size $k$ for any constant $\epsilon &gt; 0$. At the same
time, processing each element only requires a
polylogarithmic number of evaluations of the function itself. When a
better approximation is desired, we show a different algorithm that, at the cost of using more memory, provides a $\nicefrac{1}{2} - \epsilon$ 
approximation, and allows a tunable trade-off between average update time and space. This algorithm matches the best known approximation guarantees for submodular optimization in insertion-only streams, a less general formulation of the problem. We demonstrate the efficacy of the algorithms on a number of real
world datasets, showing that their practical performance far exceeds
the theoretical bounds. The algorithms preserve high quality solutions in streams with millions of items, while storing a
negligible fraction of them."
"Commercialize Quantum Technologies in Five Years.  Masoud Mohseni, Peter Read, Hartmut Neven and colleagues at Google's Quantum AI Laboratory set out investment opportunities on the road to the ultimate quantum machines."
"Human and Machine Hearing: Extracting Meaning from Sound.  Human and Machine Hearing is the first book to comprehensively describe how human hearing works and how to build machines to analyze sounds in the same way that people do. Drawing on over thirty-five years of experience in analyzing hearing and building systems, Richard F. Lyon explains how we can now build machines with close-to-human abilities in speech, music, and other sound-understanding domains. He explains human hearing in terms of engineering concepts, and describes how to incorporate those concepts into machines for a wide range of modern applications. The details of this approach are presented at an accessible level, to bring a diverse range of readers, from neuroscience to engineering, to a common technical understanding. The description of hearing as signal-processing algorithms is supported by corresponding open-source code, for which the book serves as motivating documentation."
"Detecting Cancer Metastases on Gigapixel Pathology Images.  This paper presents a convolutional neural network (CNN) approach for segmenting gigapixel pathology images into normal and cancerous pixels to aid breast cancer diagnosis. Each year, the treatment decisions for more than 230, 000 patients in the U.S. hinges on whether the cancer has metastasized away from the breast. Metastasis detection is currently performed by pathologists reviewing large expanses of biological tissues. This is labor intensive and error-prone. We present an automated approach to detect and localize tumors as small as 100??100 pixels in digitized microscopy images sized 100, 000??100, 000 pixels or larger. Our method leverages the Inception (V3) neural network architecture and obtains state-of-the-art results on the Camelyon16 dataset in the challenging lesion-level tumor detection task. At 8 false positives per image, we detect 92.4% of the tumors, relative to 82.7% by the previous best automated approach. For comparison, a human pathologist achieved 73.2% sensitivity using exhaustive search. We also achieved an image-level AUC above 97% on both the Camelyon16 test set, and another independent set of 110 slides. In addition, we discovered two Camelyon16 slides in the training set that were erroneously labeled normal. Our approach could considerably reduce false negative rates in metastasis detection."
"Unsupervised Pixel-level Domain Adaptation with Generative Adversarial Networks.  Collecting well-annotated image datasets to train modern machine learning algorithms is prohibitively expensive for many tasks. One appealing alternative is rendering synthetic data where ground-truth annotations are generated automatically. Unfortunately, models trained purely on rendered images often fail to generalize to real images. To address this shortcoming, prior work introduced unsupervised domain adaptation algorithms that attempt to map representations between the two domains or learn to extract features that are domain-invariant. In this work, we present a new approach that learns, in an unsupervised manner, a transformation in the pixel space from one domain to the other. Our generative adversarial network (GAN)-based method adapts source-domain images to appear as if drawn from the target domain. Our approach not only produces plausible samples, but also outperforms the state-of-the-art on a number of unsupervised domain adaptation scenarios by large margins. Finally, we demonstrate that the adaptation process generalizes to object classes unseen during training."
"Robust Adversarial Reinforcement Learning.  Deep neural networks coupled with fast simulation and improved computation have led to recent successes in the field of reinforcement learning (RL). However, most current RL-based approaches fail to generalize since: (a) the gap between simulation and real world is so large that policy-learning approaches fail to transfer; (b) even if policy learning is done in real world, the data scarcity leads to failed generalization from training to test scenarios (e.g., due to different friction or object masses). Inspired from H-infinity control methods, we note that both modeling errors and differences in training and test scenarios can be viewed as extra forces/disturbances in the system. This paper proposes the idea of robust adversarial reinforcement learning (RARL), where we train an agent to operate in the presence of a destabilizing adversary that applies disturbance forces to the system. The jointly trained adversary is reinforced -- that is, it learns an optimal destabilization policy. We formulate the policy learning as a zero-sum, minimax objective function. Extensive experiments in multiple environments (InvertedPendulum, HalfCheetah, Swimmer, Hopper and Walker2d) conclusively demonstrate that our method (a) improves training stability; (b) is robust to differences in training/test conditions; and c) outperform the baseline even in the absence of the adversary."
"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.  The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost."
"Decomposing Motion and Content for Natural Video Sequence Prediction.  We propose a deep neural network for the prediction of future frames in natural video sequences. To effectively handle complex evolution of pixels in videos, we propose to decompose the motion and content, two key components generating dynamics in videos. Our model is built upon the Encoder-Decoder Convolutional Neural Network and Convolutional LSTM for pixel-level prediction, which independently capture the spatial layout of an image and the corresponding temporal dynamics. By independently modeling motion and content, predicting the next frame reduces to converting the extracted content features into the next frame content by the identified motion features, which simplifies the task of prediction. Our model is end-to-end trainable over multiple time steps, and naturally learns to decompose motion and content without separate training. We evaluate the proposed network architecture the human activity videos using KTH, Weizmann action, and UCF-101 datasets. We show state-of-the-art performance in comparison to recent approaches. To the best of our knowledge, this is the first end-to-end trainable network architecture with motion and content separation to model the spatio-temporal dynamics for pixel-level future prediction in natural videos."
"Deep Information Propagation.  We study the behavior of untrained neural networks whose weights and biases are randomly distributed using mean field theory. We show the existence of depth scales that naturally limit the maximum depth of signal propagation through these random networks. Our main practical result is to show that random networks may be trained precisely when information can travel through them. Thus, the depth scales that we identify provide bounds on how deep a network may be trained for a specific choice of hyperparameters. As a corollary to this, we argue that in networks at the edge of chaos, one of these depth scales diverges. Thus arbitrarily deep networks may be trained only sufficiently close to criticality. We show that the presence of dropout destroys the order-to-chaos critical point and therefore strongly limits the maximum trainable depth for random networks. Finally, we develop a mean field theory for backpropagation and we show that the ordered and chaotic phases correspond to regions of vanishing and exploding gradient respectively."
"Capacity and Trainability in Recurrent Neural Networks.  Two potential bottlenecks on the expressiveness of recurrent neural networks (RNNs) are their ability to store information about the task in their parameters, and to store information about the input history in their units. We show experimentally that all common RNN architectures achieve nearly the same per-task and per-unit capacity bounds with careful training, for a variety of tasks and stacking depths. They can store an amount of task information which is linear in the number of parameters, and is approximately 5 bits per parameter. They can additionally store approximately one real number from their input history per hidden unit. We further find that for several tasks it is the per-task parameter capacity bound that determines performance. These results suggest that many previous results comparing RNN architectures are driven primarily by differences in training effectiveness, rather than differences in capacity. Supporting this observation, we compare training difficulty for several architectures, and show that vanilla RNNs are far more difficult to train, yet have slightly higher capacity. Finally, we propose two novel RNN architectures, one of which is easier to train than the LSTM or GRU for deeply stacked architectures."
"A Deep Matrix Factorization Method for Learning Attribute Representations.  Semi-Non-negative Matrix Factorization is a technique that learns a low-dimensional representation of a dataset that lends itself to a clustering interpretation. It is possible that the mapping between this new representation and our original data matrix contains rather complex hierarchical information with implicit lower-level hidden attributes, that classical one level clustering methodologies cannot interpret. In this work we propose a novel model, Deep Semi-NMF, that is able to learn such hidden representations that allow themselves to an interpretation of clustering according to different, unknown attributes of a given dataset. We also present a semi-supervised version of the algorithm, named Deep WSF, that allows the use of (partial) prior information for each of the known attributes of a dataset, that allows the model to be used on datasets with mixed attribute knowledge. Finally, we show that our models are able to learn low-dimensional representations that are better suited for clustering, but also classification, outperforming Semi-Non-negative Matrix Factorization, but also other state-of-the-art methodologies variants."
"On the expressive power of deep neural net-works.  We propose a novel approach to the problem of neural network expressivity, which seeks to characterize how structural properties of a neural network family affect the functions it is able to compute. Understanding expressivity is a classical issue in the study of neural networks, but it has remained challenging at both a conceptual and a practical level. Our approach is based on an interrelated set of measures of expressivity, unified by the novel notion of trajectory length, which measures how the output of a network changes as the input sweeps along a one-dimensional path. We show how our framework provides insight both into randomly initialized networks (the starting point for most standard optimization methods) and for trained networks. Our findings can be summarized as follows: 
(1) The complexity of the computed function grows exponentially with depth. We design measures of expressivity that capture the non-linearity of the computed function. These measures grow exponentially with the depth of the network architecture, due to the way the network transforms its input. 
(2) All weights are not equal (initial layers matter more). We find that trained networks are far more sensitive to their lower (initial) layer weights: they are much less robust to noise in these layer weights, and also perform better when these weights are optimized well."
"Tuning Recurrent Neural Networks With Reinforcement Learning.  This paper proposes a general method for improving the structure and quality of sequences generated by a recurrent neural network (RNN), while maintaining information originally learned from data, as well as sample diversity. An RNN is first pre-trained on data using maximum likelihood estimation (MLE), and the probability distribution over the next token in the sequence learned by this model is treated as a prior policy. Another RNN is then trained using reinforcement learning (RL) to generate higher-quality outputs that account for domain-specific incentives while retaining proximity to the prior policy of the MLE RNN. To formalize this objective, we derive novel off-policy RL methods for RNNs from KL-control. The effectiveness of the approach is demonstrated on two applications; 1) generating novel musical melodies, and 2) computational molecular generation. For both problems, we show that the proposed method improves the desired properties and structure of the generated sequences, while maintaining information learned from data."
"Generating High-Quality and Informative Conversation Responses with Sequence-to-Sequence Models.  Building general-purpose conversation agents is a very challenging task, but necessary on the road toward intelligent agents that can interact with humans in natural language. Neural conversation models -- purely data-driven systems trained end-to-end on dialogue corpora -- have shown great promise recently, yet they often produce short and generic responses. This work presents new training and decoding methods that improve the quality, coherence, and diversity of long responses generated using sequence-to-sequence models. Our approach adds self-attention to the decoder to maintain coherence in longer responses, and we propose a practical approach, called the glimpse-model, for scaling to large datasets. We introduce a stochastic beam-search algorithm with segment-by-segment reranking which lets us inject diversity earlier in the generation process. We trained on a combined data set of over 2.3B conversation messages mined from the web. In human evaluation studies, our method produces longer responses overall, with a higher proportion rated as acceptable and excellent as length increases, compared to baseline sequence-to-sequence models with explicit length-promotion. A back-off strategy produces better responses overall, in the full spectrum of lengths."
"Adversarial Evaluation of Dialogue Models.  The recent application of RNN encoder-decoder models has resulted in substantial progress in fully data-driven dialogue systems, but evaluation remains a challenge. An adversarial loss could be a way to directly evaluate the extent to which generated dialogue responses sound like they came from a human. This could reduce the need for human evaluation, while more directly evaluating on a generative task. In this work, we investigate this idea by training an RNN to discriminate a dialogue model's samples from human-generated samples. Although we find some evidence this setup could be viable, we also note that many issues remain in its practical application. We discuss both aspects and conclude that future work is warranted."
"Beyond Globally Optimal: Focused Learning for Improved Recommendations.  When building a recommender system, how can we ensure that all items are modeled well?  Classically, recommender systems are built, optimized, and tuned to improve a global prediction objective, such as root mean squared error.  However, as we demonstrate, these recommender systems often leave many items badly-modeled and thus under-served. Further, we give both empirical and theoretical evidence that no single matrix factorization, under current state-of-the-art methods, gives optimal results for each item. As a result, we ask: how can we learn additional models to improve the recommendation quality for a specified subset of items?  We offer a new technique called focused learning, based on hyperparameter optimization and a customized matrix factorization objective.  Applying focused learning on top of weighted matrix factorization, factorization machines, and LLORMA, we demonstrate prediction accuracy improvements on multiple datasets.  For instance, on MovieLens we achieve as much as a 17% improvement in prediction accuracy for niche movies, cold-start items, and even the most badly-modeled items in the original model."
"A framework for technology design for emerging markets.  A billion new people are expected to come online in the next couple of years--almost entirely from emerging markets. A majority of these new users are coming online through a mobile phone.  Decreased costs of production of smartphones, shifting localities for labour, rollout of Internet pipes, and aspirational desires have led to rapid growth and uptake of the Internet in emerging markets. Today, Internet software can be made available to millions across the world instantly. Technologies, though, enter heterogenous cultural, social and economic milieus in their traversals. The HCI community can play a critical role in bringing insights on practices, values, and infrastructures in these contexts to the design process. In this article, we share the process of creation and dissemination of a research-led framework for design for emerging markets at Google and externally. In the second half, we discuss the framework, principles and implementation."
"The little Engine that Could: Regularization by Denoising (RED).  Image denoising has reached impressive heights in performance and quality -- almost as good as it can ever get. But is this the only way in which tasks in image processing can exploit the image denoising engine? In this paper we offer Regularization by Denoising (RED): using the denoising engine in defining the regularization of the inverse problem. We propose an explicit image-adaptive Laplacian-based regularization functional, making the overall objective functional clear and well-defined. With a complete flexibility to choose the iterative optimization procedure for minimizing the above functional, RED is capable of incorporating any image denoising algorithm, treat general inverse problems very effectively, and is guaranteed to converge to the globally optimal result. As examples of its utility, we test this approach and demonstrate state-of-the-art results in the image deblurring and super-resolution problems."
"Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search.  In principle, reinforcement learning and policy search methods can enable robots to learn highly complex and general skills that may allow them to function amid the complexity and diversity of the real world. However, training a policy that generalizes well across a wide range of real-world conditions requires far greater quantity and diversity of experience than is practical to collect with a single robot. Fortunately, it is possible for multiple robots to share their experience with one another, and thereby, learn a policy collectively. In this work, we explore distributed and asynchronous policy learning as a means to achieve generalization and improved training times on challenging, real-world manipulation tasks. We propose a distributed and asynchronous version of Guided Policy Search and use it to demonstrate collective policy learning on a vision-based door opening task using four robots. We show that it achieves better generalization, utilization, and training times than the single robot alternative."
"Path Integral Guided Policy Search.  We present a policy search method for learning complex feedback control policies that map from high-dimensional sensory inputs to motor torques, for manipulation tasks with discontinuous contact dynamics. We build on a prior technique called guided policy search (GPS), which iteratively optimizes a set of local policies for specific instances of a task, and uses these to train a complex, high-dimensional global policy that generalizes across task instances. We extend GPS in the following ways: (1) we propose the use of a model-free local optimizer based on path integral stochastic optimal control (PI2), which enables us to learn local policies for tasks with highly discontinuous contact dynamics; and (2) we enable GPS to train on a new set of task instances in every iteration by using on-policy sampling: this increases the diversity of the instances that the policy is trained on, and is crucial for achieving good generalization. We show that these contributions enable us to learn deep neural network policies that can directly perform torque control from visual input. We validate the method on a challenging door opening task and a pick-and-place task, and we demonstrate that our approach substantially outperforms the prior LQR-based local policy optimizer on these tasks. Furthermore, we show that on-policy sampling significantly increases the generalization ability of these policies."
"Cognitive Mapping and Planning for Visual Navigation.  We introduce a neural architecture for navigation in novel environments. Our proposed architecture learns to map from first-person viewpoints and plans a sequence of actions towards goals in the environment. The Cognitive Mapper and Planner (CMP) is based on two key ideas: a) a unified joint architecture for mapping and planning, such that the mapping is driven by the needs of the planner, and b) a spatial memory with the ability to plan given an incomplete set of observations about the world. CMP constructs a top-down belief map of the world and applies a differentiable neural net planner to produce the next action at each time step. The accumulated belief of the world enables the agent to track visited regions of the environment. Our experiments demonstrate that CMP outperforms both reactive strategies and standard memory-based architectures and performs well in novel environments. Furthermore, we show that CMP can also achieve semantically specified goals, such as 'go to a chair'."
"Chained predictions using convolutional neural networks.  In this paper, we present an adaptation of the sequence-to-sequence model for structured output prediction in vision tasks. In this model the output variables for a given input are predicted sequentially using neural networks. The prediction for each output variable depends not only on the input but also on the previously predicted output variables. The model is applied to spatial localization tasks and uses convolutional neural networks (CNNs) for processing input images and a multi-scale deconvolutional architecture for making spatial predictions at each time step. We explore the impact of weight sharing with a recurrent connection matrix between consecutive predictions, and compare it to a formulation where these weights are not tied. Untied weights are particularly suited for problems with a fixed sized structure, where different classes of output are predicted in different steps. We show that chained predictions achieve top performing results on human pose estimation from single images and videos."
"Towards Accurate Multi-person Pose Estimation in the Wild.  We propose a method for multi-person detection and 2-D keypoint localization (human pose estimation) that achieves state-of-the-art results on the challenging COCO keypoints task. It is a simple, yet powerful, top-down approach consisting of two stages. 
In the first stage, we predict the location and scale of boxes which are likely to contain people; for this we use the Faster RCNN detector with an Inception-ResNet architecture. In the second stage, we estimate the keypoints of the person potentially contained in each proposed bounding box. For each keypoint type we predict dense heatmaps and offsets using a fully convolutional ResNet. To combine these outputs we introduce a novel aggregation procedure to obtain highly localized keypoint predictions. We also use a novel form of keypoint-based Non-Maximum-Suppression (NMS), instead of the cruder box-level NMS, and a novel form of keypoint-based confidence score estimation, instead of box-level scoring. 
Our final system achieves average precision of 0.636 on the COCO test-dev set and the 0.628 test-standard sets, outperforming the CMU-Pose winner of the 2016 COCO keypoints challenge. Further, by using additional labeled data we obtain an even higher average precision of 0.668 on the test-dev set and 0.658 on the test-standard set, thus achieving a roughly 10% improvement over the previous best performing method on the same challenge."
"The Limits of Popularity-Based Recommendations, and the Role of Social Ties.  In this paper we introduce a mathematical model that captures some of the salient features of recommender systems that are based on popularity and that try to exploit social ties among the users. We show that, under very general conditions, the market always converges to a steady state, for which we are able to give an explicit form. Thanks to this we can tell rather precisely how much a market is altered by a recommendation system, and determine the power of users to influence others. Our theoretical results are complemented by experiments with real world social networks showing that social graphs prevent large market distortions in spite of the presence of highly influential users."
"Operating a UAV Mesh &amp; Internet Backhaul Network using Temporospatial SDN.  In this paper we describe an application of Temporospatial SDN (TS-SDN) to UAV networks. Airborne platforms (airplanes, balloons, airships) can be used to carry wireless communication nodes to provide direct-to-user as well as backhaul connections. Such networks also include ground nodes typically equipped with highly directional steerable transceivers. The physics of flight as well as state of the atmosphere lead to time-dynamic link metrics and availability. As nodes move around, the network topology and routing need to adjust to maintain connectivity. Further, mechanical aspects of the system, such as time required to mechanically steer antennas, makes the reactive repair approach more costly than in terrestrial applications. Instead, TS-SDN incorporates reasoning about physical evolution of the system to proactively adjust the network topology in anticipation of future changes. Using airborne networks under development at Google as an example, we discuss the benefits of the TS-SDN approach compared to reactive repair in terms of network availability. We also identify additional constraints one needs to account for when computing the network topology, such as non-interference with other stationary and moving sources. Existing SDN standards do not support scheduled updates necessary in a TS-SDN. We describe our extensions to control messages and software implementation used in field tests."
"Datacenter Interconnect and Networking: from Evolution to Holistic Revolution.  In this presentation, we will discuss Google??s intra-datacenter networks and interconnect. We will first review the evolution of datacenter interconnects and networking over the past decade, then outline future technology directions which will be needed to keep pace with the requirements and growth of the datacenter."
"Estimating Ad Effectiveness using Geo Experiments in a Time-Based Regression Framework.  Two previously published papers (Vaver and Koehler, 2011, 2012) describe
a model for analyzing geo experiments. This model was designed to measure
advertising effectiveness using the rigor of a randomized experiment with replication
across geographic units providing confidence interval estimates. While effective, this
geo-based regression (GBR) approach is less applicable, or not applicable at all,
for situations in which few geographic units are available for testing (e.g. smaller
countries, or subregions of larger countries) These situations also include the so-called
matched market tests, which may compare the behavior of users in a single
control region with the behavior of users in a single test region. To fill this gap, we
have developed an analogous time-based regression (TBR) approach for analyzing
geo experiments. This methodology predicts the time series of the counterfactual
market response, allowing for direct estimation of the cumulative causal effect at
the end of the experiment. In this paper we describe this model and evaluate its
performance using simulation."
"Greedy Maximization Framework for  Graph-based Influence Functions.  The study of graph-based submodular maximization problems was initiated in a seminal work of Kempe, Kleinberg, and Tardos (2003):  An {\em influence} function of subsets of nodes is defined by the graph structure and the aim is to find subsets of seed nodes with (approximately) optimal tradeoff of size and influence. Applications include viral  marketing, monitoring, and active learning of node labels. This powerful formulation was studied for (generalized) {\em coverage} functions, where the influence of a seed set on a node is the maximum utility of a seed item to the node, and for pairwise {\em utility} based on reachability, distances, or reverse ranks. We define a rich class of influence functions which unifies and extends previous work beyond coverage functions and specific utility functions. We present a meta-algorithm for  approximate greedy maximization with strong approximation quality guarantees and worst-case near-linear computation for all functions in our class. Our meta-algorithm generalizes a recent design by  Cohen et al (2014) that was specific for distance-based coverage functions."
"Pixel Recursive Super Resolution.  We present a pixel recursive super resolution model that synthesizes realistic details into images while enhancing their resolution. A low resolution image may correspond to multiple plausible high resolution images, thus modeling the super resolution process with a pixel independent conditional model often results in averaging different details--hence blurry edges. By contrast, our model is able to represent a multimodal conditional distribution by properly modeling the statistical dependencies among the high resolution image pixels, conditioned on a low resolution input. We employ a PixelCNN architecture to define a strong prior over natural images and jointly optimize this prior with a deep conditioning convolutional network. Human evaluations indicate that samples from our proposed model look more photo realistic than a strong L2 regression baseline."
"Perspective Transformer Nets: Learning Single-View 3D Object Reconstruction without 3D Supervision.  Understanding the 3D world is a fundamental problem in computer vision. However,
learning a good representation of 3D objects is still an open problem due
to the high dimensionality of the data and many factors of variation involved. In
this work, we investigate the task of single-view 3D object reconstruction from a
learning agent??s perspective. We formulate the learning process as an interaction
between 3D and 2D representations and propose an encoder-decoder network with
a novel projection loss defined by the perspective transformation. More importantly,
the projection loss enables the unsupervised learning using 2D observation without
explicit 3D supervision. We demonstrate the ability of the model in generating 3D
volume from a single 2D image with three sets of experiments: (1) learning from
single-class objects; (2) learning from multi-class objects and (3) testing on novel
object classes. Results show superior performance and better generalization ability
for 3D object reconstruction when the projection loss is involved."
"Geometry of 3D Environments and Sum of Squares Polynomials.  Motivated by applications in robotics and computer vision, we study problems related to spatial reasoning of a 3D environment using sublevel sets of polynomials. These include: tightly containing a cloud of points (e.g., representing an obstacle) with convex or nearly-convex basic semialgebraic sets, computation of Euclidean distances between two such sets, separation of two convex basic semalgebraic sets that overlap, and tight containment of the union of several basic semialgebraic sets with a single convex one. We use algebraic techniques from sum of squares optimization that reduce all these tasks to semidefinite programs of small size and present numerical experiments in realistic scenarios."
Training a Subsampling Mechanism in Expectation.  We describe a mechanism for subsampling sequences and show how to compute its expected output so that it can be trained with standard backpropagation. We test this approach on a simple toy problem and discuss its shortcomings.
"Massive Exploration of Neural Machine Translation Architectures.  Neural Machine Translation (NMT) has shown remarkable progress over the past few years with production systems now being deployed to end-users. One major drawback of current architectures is that they are expensive to train, typically requiring days to weeks of GPU time to converge. This makes exhaustive hyperparameter search, as is commonly done with other neural network architectures, prohibitively expensive. In this work, we present the first large-scale analysis of NMT architecture hyperparameters. We report empirical results and variance numbers for several hundred experimental runs, corresponding to over 250,000 GPU hours on the standard WMT English to German translation task. Our experiments lead to novel insights and practical advice for building and extending NMT architectures. As part of this contribution, we release an open-source NMT framework that enables researchers to easily experiment with novel techniques and reproduce state of the art results."
"PathNet: Evolution Channels Gradient Descent in Super Neural Networks.  For artificial general intelligence (AGI) it would be efficient if multiple users trained the same giant neural network, permitting parameter reuse, without catastrophic forgetting. PathNet is a first step in this direction. It is a neural network algorithm that uses agents embedded in the neural network whose task is to discover which parts of the network to re-use for new tasks. Agents are pathways (views) through the network which determine the subset of parameters that are used and updated by the forwards and backwards passes of the backpropogation algorithm. During learning, a tournament selection genetic algorithm is used to select pathways through the neural network for replication and mutation. Pathway fitness is the performance of that pathway measured according to a cost function. We demonstrate successful transfer learning; fixing the parameters along a path learned on task A and re-evolving a new population of paths for task B, allows task B to be learned faster than it could be learned from scratch or after fine-tuning. Paths evolved on task B re-use parts of the optimal path evolved on task A. Positive transfer was demonstrated for binary MNIST, CIFAR, and SVHN supervised learning classification tasks, and a set of Atari and Labyrinth reinforcement learning tasks, suggesting PathNets have general applicability for neural network training. Finally, PathNet also significantly improves the robustness to hyperparameter choices of a parallel asynchronous reinforcement learning algorithm (A3C)."
"Bridging the Gap Between Value and Policy Based Reinforcement Learning.  We formulate a new notion of softmax temporal consistency that generalizes the standard hard-max Bellman consistency usually considered in value based reinforcement learning (RL). In particular, we show how softmax consistent action values correspond to optimal policies that maximize entropy regularized expected reward. More importantly, we establish that softmax consistent action values and the optimal policy must satisfy a mutual compatibility property that holds across any state-action subsequence. Based on this observation, we develop a new RL algorithm, Path Consistency Learning (PCL), that minimizes the total inconsistency measured along multi-step subsequences extracted from both both on and off policy traces. An experimental evaluation demonstrates that PCL significantly outperforms strong actor-critic and Q-learning baselines across several benchmark tasks."
"REBAR: Low-variance, unbiased gradient estimates for discrete variable models.  Learning in models with discrete latent variables is challenging due to high variance gradient estimators. Generally, approaches have relied on control variates to reduce the variance of the REINFORCE estimator. Recent work (Jang et al. 2016; Maddison et al. 2016) has taken a different approach, introducing a continuous relaxation of discrete variables to produce low-variance, but biased, gradient estimates. In this work, we combine the two approaches through a novel control variate that produces low-variance, <i>unbiased</i> gradient estimates. Then, we introduce a novel continuous relaxation and show that the tightness of the relaxation can be adapted online, removing it as a hyperparameter. We show state-of-the-art variance reduction on several benchmark generative modeling tasks, generally leading to faster convergence to a better final log likelihood."
"Reverse Ranking by Graph Structure: Model and Scalable Algorithms.  Distances in a network capture relations between nodes  and are the basis of centrality, similarity, and influence measures.Often, however, the relevance of a node $u$ to a node $v$ is more precisely measured not by the magnitude of the distance, but by the number of nodes that are closer to $v$ than $u$. That is, by the {\em rank} of $u$ in an ordering of nodes by increasing distance from $v$. We identify and address fundamental challenges in rank-based graph mining.  We first consider single-source computation of reverse-ranks and design a ``Dijkstra-like''  algorithm which computes nodes in order of  increasing approximate reverse rank while only traversing edges adjacent to returned nodes. We then define {\em reverse-rank influence}, which naturally extends reverse nearest neighbors influence [Korn and Muthukrishnan 2000] and builds on a well studied distance-based influence. We present  near-linear algorithms for  greedy approximate reverse-rank influence maximization. The design relies on our single-source algorithm.  Our algorithms utilize near-linear  preprocessing of the network to compute all-distance sketches. As a contribution of independent interest, we present  a novel algorithm for computing these sketches, which have many other applications,  on multi-core architectures. We complement our algorithms by establishing the hardness of  computing {\em exact} reverse-ranks for a single source and {\em exact}  reverse-rank influence. This implies that when using near-linear algorithms,  the small relative errors we obtain are  the best we can currently hope for. Finally, we conduct an experimental evaluation on graphs with tens of  millions of edges, demonstrating both scalability and accuracy."
"Multi-Objective Weighted Sampling.  Key value data sets of the form ${(x,w_x)}$ where $w_x &gt;0$  are prevalent. Common queries over such data are {\em segment $f$-statistics} $Q(f,H) =   \sum_{x\in H}f(w_x)$, specified for a segment  $H$ of the keys and a function $f$.  Different choices of $f$ correspond to count, sum, moments, cap, and threshold statistics. When the data set is large, we can compute a smaller sample from which we can quickly estimate statistics. A weighted sample of keys taken with respect to $f(w_x)$ provides estimates with statistically guaranteed quality for $f$-statistics. Such a sample $S^{(f)}$ can be used to estimate $g$-statistics for $g\not=f$, but quality degrades with the disparity between $g$ and $f$. In this paper we address applications that require quality estimates for a set $F$ of different functions.  A naive solution is to compute and work with a different sample $S^{(f)}$ for each $f\in F$. Instead,  this can be achieved more effectively and seamlessly using a single  {\em multi-objective} sample $S^{(F)}$ of a much smaller size. We review multi-objective sampling schemes and place them in our context of estimating $f$-statistics.  We show that a multi-objective sample for $F$ provides quality estimates for any $f$ that is a positive linear combination of functions from $F$.   We then establish a surprising and powerful result when the target set $M$ is {\em all}  monotone non-decreasing functions, noting that $M$ includes most natural statistics. We provide efficient multi-objective sampling algorithms  for $M$ and show that a sample size of $k \ln n$ (where $n$ is the number of active keys) provides the same estimation quality, for any $f\in M$, as a dedicated weighted sample of size $k$ for $f$."
"Power-Normalized Cepstral Coefficients (PNCC) for Robust Speech Recognition.  This paper presents a new feature extraction algorithm
called power normalized Cepstral coefficients (PNCC) that
is motivated by auditory processing. Major new features of
PNCC processing include the use of a power-law nonlinearity
that replaces the traditional log nonlinearity used in MFCC
coefficients, a noise-suppression algorithm based on asymmetric
filtering that suppresses background excitation, and a module
that accomplishes temporal masking. We also propose the use
of medium-time power analysis in which environmental parameters
are estimated over a longer duration than is commonly
used for speech, as well as frequency smoothing. Experimental
results demonstrate that PNCC processing provides substantial
improvements in recognition accuracy compared to MFCC and
PLP processing for speech in the presence of various types of
additive noise and in reverberant environments, with only slightly
greater computational cost than conventional MFCC processing,
and without degrading the recognition accuracy that is observed
while training and testing using clean speech. PNCC processing
also provides better recognition accuracy in noisy environments
than techniques such as vector Taylor series (VTS) and the ETSI
advanced front end (AFE) while requiring much less computation.
We describe an implementation of PNCC using ??online
processing?? that does not require future knowledge of the input."
"Sound source separation algorithm using phase difference and angle distribution modeling near the target.  In this paper we present a novel two-microphone sound source
separation algorithm, which selects the signal from the target
direction while suppressing signals from other directions. In
this algorithm, which is referred to as Power Angle Information
Near Target (PAINT), we first calculate phase difference
for each time-frequency bin. From the phase difference, the angle
of a sound source is estimated. For each frame, we represent
the source angle distribution near the expected target location as
a mixture of a Gaussian and a uniform distributions and obtain
binary masks using hypothesis testing. Continuous masks are
calculated from the binary masks using the Channel Weighting
(CW) technique, and processed speech is synthesized using
IFFT and the OverLap-Add (OLA) method. We demonstrate
that the algorithm described in this paper shows better speech
recognition accuracy compared to conventional approaches and
our previous approaches"
"Robust speech recognition using temporal masking and thresholding algorithm.  In this paper, we present a new dereverberation algorithm called
Temporal Masking and Thresholding (TMT) to enhance the
temporal spectra of spectral features for robust speech recognition
in reverberant environments. This algorithm is motivated
by the precedence effect and temporal masking of human
auditory perception. This work is an improvement of our
previous dereverberation work called Suppression of Slowlyvarying
components and the falling edge of the power envelope
(SSF). The TMT algorithm uses a different mathematical
model to characterize temporal masking and thresholding compared
to the model that had been used to characterize the SSF
algorithm. Specifically, the nonlinear highpass filtering used
in the SSF algorithm has been replaced by a masking mechanism
based on a combination of peak detection and dynamic
thresholding. Speech recognition results show that the TMT
algorithm provides superior recognition accuracy compared to
other algorithms such as LTLSS, VTS, or SSF in reverberant
environments."
"A subband-based stationary-component suppression method using harmanics and power ratio for reverberant speech recognition.  This letter describes a preprocessing method called
subband-based stationary-component suppression method using
harmonics and power ratio (SHARP) processing for reverberant
speech recognition. SHARP processing extends a previous
algorithm called Suppression of Slowly varying components and
the Falling edge (SSF), which suppresses the steady-state portions
of subband spectral envelopes. The SSF algorithm tends
to over-subtract these envelopes in highly reverberant environments
when there are high levels of power in previous analysis
frames. The proposed SHARP method prevents excessive suppression
both by boosting the floor value using the harmonics in voiced
speech segments and by inhibiting the subtraction for unvoiced
speech by detecting frames in which power is concentrated in
high-frequency channels. These modifications enable the SHARP
algorithm to improve recognition accuracy by further reducing
the mismatch between power contours of clean and reverberated
speech. Experimental results indicate that the SHARP method
provides better recognition accuracy in highly reverberant environments
compared to the SSF algorithm. It is also shown that
the performance of the SHARP method can be further improved
by combining it with feature-space maximum likelihood linear
regression (fMLLR)."
"Net2Net: Accelerating Learning via Knowledge Transfer.  We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scratch. Our Net2Net technique accelerates the experimentation process by instantaneously transferring the knowledge from a previous network to each new deeper or wider network. Our techniques are based on the concept of function-preserving transformations between neural network specifications. This differs from previous approaches to pre-training that altered the function represented by a neural net when adding layers to it. Using our knowledge transfer mechanism to add depth to Inception modules, we demonstrate a new state of the art accuracy rating on the ImageNet dataset."
"Pinning Down Abuse on Google Maps.  In this paper, we investigate a new form of blackhat search engine optimization that targets local listing services like Google Maps. Miscreants register abusive business listings in an attempt to siphon search traffic away from legitimate businesses and funnel it to deceptive service industries---such as unaccredited locksmiths---or to traffic-referral scams, often for the restaurant and hotel industry. In order to understand the prevalence and scope of this threat, we obtain access to over a hundred-thousand business listings on Google Maps that were suspended for abuse. We categorize the types of abuse affecting Google Maps; analyze how miscreants circumvented the protections against fraudulent business registration such as postcard mail verification; identify the volume of search queries affected; and ultimately explore how miscreants generated a profit from traffic that necessitates physical proximity to the victim. This physical requirement leads to unique abusive behaviors that are distinct from other online fraud such as pharmaceutical and luxury product scams."
"Deep Learning for Explicitly Modeling Optimization Landscapes.  In all but the most trivial optimization problems, the structure of the solutions exhibit complex interdependencies between the input parameters. Decades of research with stochastic search techniques has shown the benefit of explicitly modeling the interactions between sets of parameters and the overall quality of the solutions discovered. We demonstrate a novel method, based on learning deep networks, to model the global landscapes of optimization problems. To represent the search space concisely and accurately, the deep networks must encode information about the underlying parameter interactions and their contributions to the quality of the solution. Once the networks are trained, the networks are probed to reveal parameter combinations with high expected performance with respect to the optimization task. These estimates are used to initialize fast, randomized, local search algorithms, which in turn expose more information about the search space that is subsequently used to refine the models. We demonstrate the technique on multiple optimization problems that have arisen in a variety of real-world domains, including: packing, graphics, job scheduling, layout and compression. The problems include combinatoric search spaces, discontinuous and highly non-linear spaces, and span binary, higher-cardinality discrete, as well as continuous parameters. Strengths, limitations, and extensions of the approach are extensively discussed and demonstrated."
"Uncanny Valleys in Declarative Language Design.  When people write programs in conventional programming languages, they over-specify how to solve the problem they have in mind. Over-specification prevents the language's implementation from making many optimization decisions, leaving programmers with this burden. In more declarative languages, programmers over-specify less, enabling the implementation to make more choices for them. As these decisions improve, programmers shift more attention from implementation to their real problems. This process easily overshoots. When under-specified programs almost always work well enough, programmers rarely need to think about implementation details. As their understanding of implementation choices atrophies, the controls provided so they can override these decisions become obscure. Our declarative language project, Yedalog, is in the midst of this dilemma. The improvements in question make our users more productive, so we cannot simply retreat back towards over-specification. To proceed forward instead, we must meet some of the expectations we prematurely provoked, and our implementation's behavior must help users learn expectations more aligned with our intended semantics. These are general issues. Discussing their concrete manifestation in Yedalog should help other declarative systems that come to face these issues."
"Geometry-Based Next Frame Prediction from Monocular Video.  We consider the problem of next frame prediction
from video input. A recurrent convolutional neural network is
trained to predict depth from monocular video input, which,
along with the current video image and the camera trajectory,
can then be used to compute the next frame. Unlike prior next-
frame prediction approaches, we take advantage of the scene
geometry and use the predicted depth for generating the next
frame prediction. Our approach can produce rich next frame
predictions which include depth information attached to each
pixel. Another novel aspect of our approach is that it predicts
depth from a sequence of images (e.g. in a video), rather than
from a single still image. We evaluate the proposed approach on the KITTI dataset,
a standard dataset for benchmarking tasks relevant to au-
tonomous driving. The proposed method produces results which
are visually and numerically superior to existing methods that
directly predict the next frame. We show that the accuracy of
depth prediction improves as more prior frames are considered."
"Learning with Proxy Supervision for End-To-End Visual Learning.  Learning with deep neural networks forms the
state-of-the-art in many tasks such as image classification,
image detection, speech recognition, text analysis. We here set
out to gain understanding in learning in an ??end-to-end?? manner
for an autonomous vehicle, which refers to directly learning the
decision which will result from the perception of the scene. For
example, we consider learning a binary ??stop??/??go?? decision, with
respect to pedestrians, given the input image. In this work we
propose to use additional information, referred to as ??proxy
supervision??, for improved learning and study its effects on the
overall performance. We show that the proxy labels significantly
improve the robustness of learning, while achieving as good, or
better, accuracy than in the original task of binary classification."
"Changing Model Behavior at Test-time using Reinforcement Learning.  Machine learning models are often used at test-time subject to constraints and trade-offs not present at training-time. For example, a computer vision model operating on an embedded device may need to perform real-time inference, or a translation model operating on a cell phone may wish to bound its average compute time in order to be power-efficient. In this work we describe a mixture-of-experts model and show how to change its test-time resource-usage on a per-input basis using reinforcement learning. We test our method on a small MNIST-based example."
"Evolving Ext4 for Shingled Disks.  Drive-Managed SMR (ShingledMagnetic Recording) disks offer a plug-compatible higher-capacity replacement for conventional disks. For non-sequential workloads, these disks show bimodal behavior: After a short period of high throughput they enter a continuous period of low throughput. We introduce ext4-lazy, a small change to the Linux ext4 file system that significantly improves the throughput in both modes. We present benchmarks on four different drive-managed SMR disks from two vendors, showing that ext4-lazy achieves 1.7-5.4x improvement over ext4 on a metadata-light file server benchmark. On metadata-heavy benchmarks it achieves 2-13x improvement over ext4 on drive-managed SMR disks as well as on conventional disks."
"Explaining the Learning Dynamics of Direct Feedback Alignment.  Two recently developed methods, Feedback Alignment (FA) and Direct Feedback
Alignment (DFA), have been shown to obtain surprising performance on vision
tasks by replacing the traditional backpropagation update with a random feedback
update. However, it is still not clear what mechanisms allow learning to happen
with these random updates. In this work we argue that DFA can be viewed as a
noisy variant of a layer-wise training method we call Linear Aligned Feedback
Systems (LAFS). We support this connection theoretically by comparing the update
rules for the two methods. We additionally empirically verify that the random
update matrices used in DFA work effectively as readout matrices, and that strong
correlations exist between the error vectors used in the DFA and LAFS updates.
With this new connection between DFA and LAFS we are able to explain why the
??alignment?? happens in DFA."
"ExtDict: Extensible Dictionaries for Data- and Platform-Aware Large-Scale Learning.  This paper proposes ExtDict, a novel data- and
platform-aware framework for iterative analysis/learning of massive
and dense datasets. Iterative execution is prohibitively costly
for distributed architectures where the cost of moving data
is continually growing compared with the cost of arithmetic
computing. ExtDict creates a performance model that quantifies
the computational cost of iterative analysis algorithms on a target
platform in terms of FLOPs, communication, and memory, which
characterize runtime, energy, and storage respectively. The core
of ExtDict is a novel parametric data projection algorithm, called
Extensible Dictionary, that enables versatile and sparse representations
of the data to minimize this computational cost. We
show that ExtDict can achieve the optimal performance objective,
according to our quantified cost model, by platform-aware tuning
of the Extensible Dictionary parameters. An accompanying API
ensures automated applicability of ExtDict to various algorithms,
datasets, and platforms. Proof-of-concept evaluations of massive
and dense data on different platforms demonstrate more than an
order of magnitude improvement in performance compared to the
state-of-the-art, within guaranteed user-defined error bounds."
"Particle Value Function.  The policy gradients of the expected return objective can react slowly to rare rewards.
Yet, in some cases agents may wish to emphasize the low or high returns
regardless of their probability. Borrowing from the economics and control literature,
we review the risk-sensitive value function that arises from an exponential
utility and illustrate its effects on an example. This risk-sensitive value function
is not always applicable to reinforcement learning problems, so we introduce
the particle value function defined by a particle filter over the distributions of an
agent??s experience, which bounds the risk-sensitive one. We illustrate the benefit
of the policy gradients of this objective in Cliffworld."
"Datacenter Optics: Requirement, Technology and Trend.  This paper reviews over decade of technology evolution and advancement of datacenter optical interconnect, which is mainly driven by the explosive bandwidth demand growth of web and cloud based services. Emerging trend and technical options to scale bandwidth well beyond 400Gb/s have also been discussed."
"A Neural Architecture for Dialectal Arabic Segmentation.  The automated processing of Arabic dialects is challenging due to the lack of spelling standards and the scarcity of annotated data and resources in general. Segmentation of words into their constituent tokens is an important processing step for natural language processing. In this paper, we show how a segmenter can be trained on only 350 annotated tweets using neural networks without any normalization or reliance on lexical features or linguistic resources. We deal with segmentation as a sequence labeling problem at the character level. We show experimentally that our model can rival state-of-the-art methods that heavily depend on additional resources."
"Introduction to the Aggregate Marketing System Simulator.  Advertising is becoming more and more complex, and there is a strong demand for measurement tools that are capable of keeping up. In tandem with new measurement problems and solutions, new capabilities for evaluating measurement methodologies are needed. Given the complex marketing environment and the multitude of analytical methods that are available, simulation has become an essential tool for evaluating and comparing analysis options.
This paper describes the Aggregate Marketing System Simulator (AMASS), a sim- ulation tool capable of generating aggregate-level time series data related to marketing measurement (e.g., channel-level marketing spend, website visits, competitor spend, pricing, sales volume, etc.). It is flexible enough to model a wide variety of marketing situations that include different mixes of advertising spend, levels of ad effectiveness, types of ad targeting, sales seasonality, competitor activity, and much more. A key feature of AMASS is that it generates ground truth for marketing performance met- rics, including return on ad spend and marginal return on ad spend. The capabilities provided by AMASS create a foundation for evaluating and improving measurement methods, including media mix models (MMMs), campaign optimization (Scott, 2015), and geo experiments (Vaver and Koehler, 2011), across complex modeling scenarios."
"Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering.  This paper presents a new baseline for visual question answering task. Given an image and a question in natural language, our model produces accurate answers according to the content of the image. Our model, while being architecturally simple and relatively small in terms of trainable parameters, sets a new state of the art on both unbalanced and balanced VQA benchmark. On VQA 1.0  open ended challenge, our model achieves 64.6% accuracy on the test-standard set without using additional data, an improvement of 0.4% over state of the art, and on newly released VQA 2.0, our model scores 59.7% on validation set outperforming best previously reported results by 4%. The results presented in this paper are especially interesting because very similar models have been tried before but significantly lower performance were reported. In light of the new results we hope to see more meaningful research on visual question answering in the future."
Challenges and Opportunities in Media Mix Modeling.  Advertisers have a need to understand the effectiveness of their media spend in driving sales in order to optimize budget allocations.  Media mix models are a common and widely used approach for doing so.  The paper outlines the various challenges such models encounter in consistently providing valid answers to the advertiser??s questions on media effectiveness.  The paper also discusses opportunities for improvements in media mix models that can produce better inference.
"A Hierarchical Bayesian Approach to Improve Media Mix Models Using Category Data.  One of the major problems in developing media mix models is that the data that is generally available to the modeler lacks sufficient quantity and information content to reliably estimate the parameters in a model of even moderate complexity. Pooling data from different brands within the same product category provides more observations and greater variability in media spend patterns. We either directly use the results from a hierarchical Bayesian model built on the category dataset, or pass the information learned from the category model to a brand-specific media mix model via informative priors within a Bayesian framework, depending on the data sharing restriction across brands. We demonstrate using both simulation and real case studies that our category analysis can improve parameter estimation and reduce uncertainty of model prediction and extrapolation."
"Geo-level Bayesian Hierarchical Media Mix Modeling.  Media mix modeling is a statistical analysis on historical data to measure the return on investment
(ROI) on advertising and other marketing activities. Current practice usually utilizes data aggregated
at a national level, which often suffers from small sample size and insufficient variation in
the media spend. When sub-national data is available, we propose a geo-level Bayesian hierarchical
media mix model (GBHMMM), and demonstrate that the method generally provides estimates
with tighter credible intervals compared to a model with national level data alone. This reduction
in error is due to having more observations and useful variability in media spend, which can protect
advertisers from unsound reallocation decisions. Under some weak conditions, the geo-level model
can reduce the ad targeting bias. When geo-level data is not available for all the media channels,
the geo-level model estimates generally deteriorate as more media variables are imputed using the
national level data"
"Bayesian Methods for Media Mix Modeling with Carryover and Shape Effects.  Media mix models are used by advertisers to measure the effectiveness of their advertising and provide insight in making future budget allocation decisions. Advertising usually has lag effects and diminishing returns, which are hard to capture using linear regression. In this paper, we propose a media mix model with flexible functional forms to model the carryover and shape effects of advertising. The model is estimated using a Bayesian approach in order to make use of prior knowledge accumulated in previous or related media mix models. We illustrate how to calculate attribution metrics such as ROAS and mROAS from posterior samples on simulated data sets. Simulation studies show that the model can be estimated very well for large size data sets, but prior distributions have a big impact on the posteriors when the sample size is small and may lead to biased estimates. We apply the model to data from a shampoo advertiser, and use Bayesian Information Criterion (BIC) to choose the appropriate specification of the functional forms for the carryover and shape effects. We further illustrate that the optimal media mix based on the model has a large variance due to the variance of the parameter estimates."
"A Neural Representation of Sketch Drawings.  We present sketch-rnn, a recurrent neural network (RNN) able to construct stroke-based drawings of common objects. The model is trained on thousands of crude human-drawn images representing hundreds of classes. We outline a framework for conditional and unconditional sketch generation, and describe new robust training methods for generating coherent sketch drawings in a vector format."
"Experiments in Handwriting with a Neural Network.  Neural networks are an extremely successful approach to machine learning, but it??s tricky to understand why they behave the way they do. This has sparked a lot of interest and effort around trying to understand and visualize them, which we think is so far just scratching the surface of what is possible. In this article we will try to push forward in this direction by taking a generative model of handwriting1 and visualizing it in a number of ways. In the end we don??t have some ultimate answer or visualization, but we do have some interesting ideas to share. Ultimately we hope they make it easier to divine some meaning from the internals of these model."
"Prediction errors of molecular machine learning models lower than hybrid DFT error.  We investigate the impact of choosing regressors and molecular representations for the construction of fast machine learning (ML) models of thirteen electronic ground-state properties of organic molecules. The performance of each regressor/representation/property combination is assessed with learning curves which report approximation errors as a function of training set size. Molecular structures and properties at hybrid density functional theory (DFT) level of theory used for training and testing come from the QM9 database [Ramakrishnan et al, Scientific Data 1 140022 (2014)] and include dipole moment, polarizability, HOMO/LUMO energies and gap, electronic spatial extent, zero point vibrational energy, enthalpies and free energies of atomization, heat capacity and the highest fundamental vibrational frequency. Various representations from the literature have been studied (Coulomb matrix, bag of bonds, BAML and ECFP4, molecular graphs (MG)), as well as newly developed distribution based variants including histograms of distances (HD), and angles (HDA/MARAD), and dihedrals (HDAD). Regressors include linear models (Bayesian ridge regression (BR) and linear regression with elastic net regularization (EN)), random forest (RF), kernel ridge regression (KRR) and two types of neural networks, graph convolutions (GC) and gated graph networks (GG). We present numerical evidence that ML model predictions for all properties can reach an approximation error to DFT which is on par with chemical accuracy. These findings indicate that ML models could be more accurate than DFT if explicitly electron correlated quantum (or experimental) data was provided."
"Language Modeling in the Era of Abundant Data.  Overview of N-gram language modeling on large amounts of data, anchored in the reality of the speech recognition team at Google."
"Indexing Public-Private Graphs.  We consider the reachability indexing problem for private-public directed graphs. In these graphs nodes come in three flavors: public??nodes visible to all users, private??nodes visible to a specific set of users, and protected??nodes visible to any user who can see at least one of the node??s parents.  We are interested in computing the set of nodes visible to a specific user online. There are two obvious algorithms: precompute the result for every user, or run a reachability algorithm at query time. This paper explores the trade-off between these two strategies. Our approach is to identify a set of additional visible seed nodes for each user. The online reachability algorithm explores the graph starting at these nodes. We first formulate the problem as asymmetric k-center with outliers, and then give an efficient and practical algorithm. We prove new theoretical guarantees for this problem and show empirically that it performs very well in practice."
"Robust speech recognition in reverberant environments using subband-based steady-state monaural and binaural suppression.  The precedence effect describes the ability of the auditory system to suppress the later-arriving components of sound in a reverberant environment, maintaining the perceived arrival azimuth of a sound in the direction of the actual source, even though later reverberant components may arrive from other directions. It is also widely believed that precedence-like processing can also improve speech intelligibility, as well as the accuracy of speech recognition systems, in reverberant environments. While the mechanisms underlying the precedence effect have traditionally been assumed to be binaural in nature, it is also possible that the suppression of later components may take place monaurally, and that the suppression of the later-arriving components of the spatial image may be a consequence of this more peripheral processing. This paper compares the potential contributions of onset enhancement (and consequent steady-state suppression) of the envelopes of subband components of speech at both the monaural and binaural levels. Experimental results indicate that substantial improvement in recognition accuracy can be obtained in reverberant environments if the feature extraction includes both onset enhancement and binaural interaction. Recognition accuracy appears to be relatively unaffected by the stage in the suppression processing at which the binaural interaction takes place."
"Enhancing Video Summarization via Vision-Language Embedding.  This paper addresses video summarization, or the problem
of distilling a raw video into a shorter form while still
capturing the original story. We show that visual representations
supervised by freeform language make a good
fit for this application by extending a recent submodular
summarization approach with representativeness and
interestingness objectives computed on features from a joint
vision-language embedding space. We perform an evaluation
on two diverse datasets, UT Egocentric and
TV Episodes, and show that our new objectives give
improved summarization ability compared to standard visual
features alone. Our experiments also show that the
vision-language embedding need not be trained on domainspecific
data, but can be learned from standard still image
vision-language datasets and transferred to video. A further
benefit of our model is the ability to guide a summary using
freeform text input at test time, allowing user customization."
"Unsupervised Learning of Depth and Ego-Motion from Video.  This paper addresses video summarization, or the problem
of distilling a raw video into a shorter form while still
capturing the original story. We show that visual representations
supervised by freeform language make a good
fit for this application by extending a recent submodular
summarization approach with representativeness and
interestingness objectives computed on features from a joint
vision-language embedding space. We perform an evaluation
on two diverse datasets, UT Egocentric and
TV Episodes, and show that our new objectives give
improved summarization ability compared to standard visual
features alone. Our experiments also show that the
vision-language embedding need not be trained on domainspecific
data, but can be learned from standard still image
vision-language datasets and transferred to video. A further
benefit of our model is the ability to guide a summary using
freeform text input at test time, allowing user customization."
"Instance-Level Label Propagation with Multi-Instance Learning.  Label propagation is a popular semi-supervised
learning technique that transfers information from
labeled examples to unlabeled examples through a
graph. Most label propagation methods construct a
graph based on example-to-example similarity, assuming
that the resulting graph connects examples
that share similar labels. Unfortunately, examplelevel
similarity is sometimes badly defined. For
instance, two images may contain two different
objects, but have similar overall appearance due to
large similar background. In this case, computing
similarities based on whole-image would fail propagating
information to the right labels. This paper
proposes a novel Instance-Level Label Propagation
(ILLP) approach that integrates label propagation
with multi-instance learning. Each example is
treated as containing multiple instances, as in the
case of an image consisting of multiple regions.
We first construct a graph based on instancelevel
similarity and then simultaneously identify
the instances carrying the labels and propagate the
labels across instances in the graph. Optimization
is based on an iterative Expectation Maximization
(EM) algorithm. Experimental results on two
benchmark datasets demonstrate the effectiveness
of the proposed approach over several state-of-theart
methods."
HyperNetworks (Blog Post).  Blog post for HyperNetworks.
Teaching Machines to Draw.  Google Research blog post for ??A Neural Representation of Sketch Drawings??.
"Time-Contrastive Networks: Self-Supervised Learning from Video.  We propose a self-supervised approach for learning representations and robotic behaviors entirely from unlabeled videos recorded from multiple viewpoints, and study how this representation can be used in two robotic imitation settings: imitating object interactions from videos of humans, and imitating human poses. Imitation of human behavior requires a viewpoint-invariant representation that captures the relationships between end-effectors (hands or robot grippers) and the environment, object attributes, and body pose. We train our representations using a triplet loss, where multiple simultaneous viewpoints of the same observation are attracted in the embedding space, while being repelled from temporal neighbors which are often visually similar but functionally different. This signal causes our model to discover attributes that do not change across viewpoint, but do change across time, while ignoring nuisance variables such as occlusions, motion blur, lighting and background. We demonstrate that this representation can be used by a robot to directly mimic human poses without an explicit correspondence, and that it can be used as a reward function within a reinforcement learning algorithm. While representations are learned from an unlabeled collection of task-related videos, robot behaviors such as pouring are learned by watching a single 3rd-person demonstration by a human. Reward functions obtained by following the human demonstrations under the learned representation enable efficient reinforcement learning that is practical for real-world robotic systems. Video results, open-source code and dataset are available at https://sermanet.github.io/imitate"
"A Study of Compact Reserve Pricing Languages.  Online advertising allows advertisers to implement fine-tuned targeting of users. While such precise targeting leads to more effective advertising,  it introduces challenging multidimensional pricing and bidding problems for publishers and advertisers. In this context, advertisers and publishers need to deal with an exponential number of possibilities. As a result, designing efficient and compact multidimensional bidding and pricing systems and algorithms are practically important for online advertisement.  Compact bidding languages have already been studied in the context of multiplicative bidding.  In this paper, we study the compact pricing problem."
"Scalable Feature Selection via Distributed Diversity Maximization.  Feature selection is a fundamental problem in machine learning
and data mining. The majority of feature selection algorithms
are designed for running on a single machine (centralized
setting) and they are less applicable to very large
datasets. Although there are some distributed methods to
tackle this problem, most of them are distributing the data
horizontally which are not suitable for datasets with a large
number of features and few number of instances. Thus, in this
paper, we introduce a novel vertically distributable feature selection
method in order to speed up this process and be able
to handle very large datasets in a scalable manner. In general,
feature selection methods aim at selecting relevant and
non-redundant features (Minimum Redundancy and Maximum
Relevance). It is much harder to consider redundancy
in a vertically distributed setting than a centralized setting
since there is no global access to the whole data. To the best
of our knowledge, this is the first attempt toward solving the
feature selection problem with a vertically distributed filter
method which handles the redundancy with consistently comparable
results with centralized methods. In this paper, we
formalize the feature selection problem as a diversity maximization
problem by introducing a mutual-information-based
metric distance on the features. We show the effectiveness of
our method by performing an extensive empirical study. In
particular, we show that our distributed method outperforms
state-of-the-art centralized feature selection algorithms on a
variety of datasets. From a theoretical point of view, we have
proved that the used greedy algorithm in our method achieves
an approximation factor of 1/4 for the diversity maximization
problem in a distributed setting with high probability. Furthermore,
we improve this to 8/25 expected approximation
using multiplicity in our distribution."
"Deals or No Deals: Contract Design for Online Advertising.  Billions of dollars worth of display advertising are sold via contracts and deals. This paper presents a formal study of preferred deals, a new generation of contracts for selling online advertisement, that generalize the traditional reservation contracts; these contracts are suitable for advertisers with advanced targeting capabilities. We propose a constant-factor approximation algorithm for maximizing the revenue that can be obtained from these deals. We show, both theoretically and via data analysis, that deals, with appropriately chosen minimum-purchase guarantees, can yield significantly higher revenue than auctions. We evaluate our algorithm using data from Google's ad exchange platform. Our algorithm obtains about 90% of the optimal revenue where the second-price auction, even with personalized reserve, obtains at most 52% of the benchmark."
"Budget Management Strategies in Repeated Auctions.  In online advertising, advertisers purchase ad placements by participating in a long sequence of repeated auctions. One of the most important features advertising platforms often provide, and advertisers often use, is budget management, which allows advertisers to control their cumulative expenditures. Advertisers typically declare the maximum daily amount they are willing to pay, and the platform adjusts allocations and payments to guarantee that cumulative expenditures do not exceed budgets. There are multiple ways to achieve this goal, and each one, when applied to all budget-constrained advertisers simultaneously, steers the system toward a different equilibrium. While previous research focused on online stochastic optimization techniques or game-theoretic equilibria of such settings, our goal in this paper is to compare the ``system equilibria'' of a range of budget management strategies in terms of the seller's profit and buyers' utility. In particular, we consider six different budget management strategies including probabilistic throttling, thresholding, bid shading, reserve pricing, and multiplicative boosting. We show these methods admit a system equilibrium in a rather general setting, and prove dominance relations between them in a simplified setting. Our study sheds light on the impact of budget management strategies on the tradeoff between the seller's profit and buyers' utility. Finally, we also empirically compare the system equilibria of these strategies using real ad auction data in sponsored search and randomly generated bids. The empirical study confirms our theoretical findings about the relative performances of budget management strategies."
"Bi-Objective Online Matching and Submodular Allocations.  Online allocation problems have been widely studied due to their numerous practical
applications (particularly to Internet advertising), as well as considerable
theoretical interest. The main challenge in such problems is making assignment
decisions in the face of uncertainty about future input; effective algorithms need to
predict which constraints are most likely to bind, and learn the balance between
short-term gain and the value of long-term resource availability.
In many important applications, the algorithm designer is faced with multiple
objectives to optimize. In particular, in online advertising it is fairly common to
optimize multiple metrics, such as clicks, conversions, and impressions, as well
as other metrics which may be largely uncorrelated such as ??share of voice??, and
??buyer surplus??. While there has been considerable work on multi-objective offline
optimization (when the entire input is known in advance), very little is known
about the online case, particularly in the case of adversarial input. In this paper,
we give the first results for bi-objective online submodular optimization, providing
almost matching upper and lower bounds for allocating items to agents with two
submodular value functions. We also study practically relevant special cases of
this problem related to Internet advertising, and obtain improved results. All our
algorithms are nearly best possible, as well as being efficient and easy to implement
in practice."
"Linear Relaxations for Finding Diverse Elements in Metric Spaces.  Choosing a diverse subset of a large collection of points in a metric space is a fundamental
problem, with applications in feature selection, recommender systems,
web search, data summarization, etc. Various notions of diversity have been proposed,
tailored to different applications. The general algorithmic goal is to find
a subset of points that maximize diversity, while obeying a cardinality (or more
generally, matroid) constraint. The goal of this paper is to develop a novel linear
programming (LP) framework that allows us to design approximation algorithms
for such problems. We study an objective known as sum-min diversity, which
is known to be effective in many applications, and give the first constant factor
approximation algorithm. Our LP framework allows us to easily incorporate additional
constraints, as well as secondary objectives. We also prove a hardness result
for two natural diversity objectives, under the so-called planted clique assumption.
Finally, we study the empirical performance of our algorithm on several standard
datasets. We first study the approximation quality of the algorithm by comparing
with the LP objective. Then, we compare the quality of the solutions produced by
our method with other popular diversity maximization algorithms."
"Fair Resource Allocation in A Volatile Marketplace.  We consider the setting where a seller must allocate a collection of goods to budgeted buyers, as exemplified by online advertising systems where platforms decide which impressions to serve to various advertisers. Such resource allocation problems are challenging for two reasons: (a) the seller must strike a balance between optimizing her own revenues and guaranteeing fairness to her (repeat) buyers and (b) the problem is inherently dynamic due to the uncertain, time-varying supply of goods available with the seller. We propose a stochastic approximation scheme akin to a dynamic market equilibrium. Our scheme relies on frequent re-solves of an Eisenberg-Gale convex program, and does not require the seller to have any knowledge about how goods arrival processes evolve over time. The scheme fully extracts buyer budgets (thus maximizing seller revenues), while at the same time provides a 0.47 approximation of the proportionally fair allocation of goods achievable in the offline case, as long as the supply of goods comes from a wide family of (possibly non-stationary) Gaussian processes. We then extend our results to a more general family of metrics called \alpha-fairness. Finally, we deal with a multi-objective problem where the seller is concerned with both the proportional fairness and efficiency of the allocation, and propose a hybrid algorithm which achieves a 0.27 bi-criteria guarantee against fairness and efficiency."
"Decentralized utilitarian mechanisms for scheduling games.  Game Theory and Mechanism Design are by now standard tools for studying and designing massive decentralized systems. Unfortunately, designing mechanisms that induce socially efficient outcomes often requires full information and prohibitively large computational resources. In this work we study simple mechanisms that require only local information. Specifically, in the setting of a classic scheduling problem, we demonstrate local mechanisms that induce outcomes with social cost close to that of the socially optimal solution. Somewhat counter-intuitively, we find that mechanisms yielding Pareto dominated outcomes may in fact enhance the overall performance of the system, and we provide a justification of these results by interpreting these inefficiencies as externalities being internalized. We also show how to employ randomization to obtain yet further improvements. Lastly, we use the game-theoretic insights gained to obtain a new combinatorial approximation algorithm for the underlying optimization problem. Decentralized utilitarian mechanisms for scheduling games. Available from: https://www.researchgate.net/publication/275166819_Decentralized_utilitarian_mechanisms_for_scheduling_games [accessed May 2, 2017]."
"A Segmental Framework for Fully-Unsupervised  Large-Vocabulary Speech Recognition.  Zero-resource speech technology is a growing research area that aims to develop methods for speech processing in the absence of transcriptions, lexicons, or language modelling text. Early systems focused on identifying isolated recurring terms in a corpus, while more recent full-coverage systems attempt to completely segment and cluster the audio into word-like units??effectively performing unsupervised speech recognition. To our knowledge, this article presents the first such system evaluated on large vocabulary multi-speaker data. The system uses a Bayesian modelling framework with segmental word representations: each word segment is represented as a fixed-dimensional acoustic embedding obtained by mapping the sequence of feature frames to a single embedding vector. We compare our system on English and Xitsonga datasets to state-of-the-art baselines, using a variety of measures including word error rate (obtained by mapping the unsupervised output to ground truth transcriptions). We show that by imposing a consistent top-down segmentation while also using bottom-up knowledge from detected syllable boundaries, both single-speaker and multi-speaker versions of our system outperform a purely bottom-up single-speaker syllable-based approach. We also show that the discovered clusters can be made less  speaker- and gender-specific by using an unsupervised autoencoder-like feature extractor to learn better frame-level features (prior to embedding). Our system??s discovered clusters are still less pure than those of two multi-speaker term discovery systems, but provide far greater coverage."
"The Spread of Physical Activity Through Social Networks.  We study the evolution of daily physical activity of 44.5K
users on the Fitbit social network over a period of eight
months. A time-aggregated analysis shows that average alter activity, gender, and body mass index (BMI) are significantly predictive of ego activity when controlling for ego
BMI, gender, and number of friends. The direction and
effect size of the associations surfaced vary when considering chronic conditions self-reported by a large portion of the
users including diabetes, dyslipidemia, hypertension and depression. When considering the co-evolution of activity and
friendship on a month by month basis in a within-subject
analysis, we show via fixed effects modeling that the fluctuations in average alter activity significantly predict fluctuations in ego activity. Finally, we investigate the causal
factors that may drive change of physical activity over time.
We leverage a class of novel non-parametric statistical tests
to rule out homophily as the sole source of dependence in activity, even in the presence of unobserved individual traits."
"Bicriteria Distributed Submodular Maximization in a Few Rounds.  We study the problem of efficiently optimizing submodular functions under 
cardinality constraints in distributed setting. Recently, several 
distributed algorithms for this problem have been introduced 
which either achieve a sub-optimal solution or they run in super-constant
number
of rounds of computation. Unlike previous work, we aim
to design distributed algorithms in multiple rounds
with almost optimal approximation guarantees
at the cost of outputting a larger number of elements. 
Toward this goal, we present a distributed algorithm that, for any \epsilon &gt; 
0
and any constant r, 
outputs a set 
S of O(rk/\epsilon^{1\over r}) items in r rounds, and achieves 
a (1-\epsilon)-approximation of the value of the optimum set with k items. 
This is the first 
distributed algorithm
that achieves an approximation factor of (1-\epsilon) running in less than
$\log {1\over \epsilon}$ number of rounds.
We also prove a hardness result showing that the output of any $1-\epsilon$ approximation distributed algorithm limited to one distributed round should have at least $\Omega(k/\epsilon)$ items. In light of this hardness result, our distributed algorithm in one round, $r=1$, is asymptotically tight in terms of the output size. We support the theoretical guarantees 
with an extensive empirical study of our 
algorithm showing that achieving almost optimum solutions is indeed possible in 
a few rounds for large-scale real datasets."
"PoS, Morphology and Dependencies Annotation Guidelines for Arabic.  The aim of this document is  to provide a list of dependency tags that are to be used for the Arabic dependency annotation task, with examples provided for each tag. The dependency representation is a simple description of the grammatical relationships in a sentence. It represents all sentence relations uniformly typed as dependency relations. The dependencies are all binary relations between a governor (also known the head) and a dependant (any complement of or modifier to the head)."
"Characterizing Online Discussion Using Coarse Discourse Sequences.  In this work, we present a novel method for classifying comments in online discussions into a set of coarse discourse acts towards the goal of better understanding discussions at scale. To facilitate this study, we devise a categorization of coarse discourse acts designed to encompass general online discussion and allow for easy annotation by crowd workers. We collect and release a corpus of over 9,000 threads comprising over 100,000 comments manually annotated via paid crowdsourcing with discourse acts and randomly sampled from the site Reddit. Using our corpus, we demonstrate how the analysis of discourse acts can characterize different types of discussions, including discourse sequences such as Q&amp;A pairs and chains of disagreement, as well as different communities. Finally, we conduct experiments to predict discourse acts using our corpus, finding that structured prediction models such as conditional random fields can achieve an F1 score of 75%. We also demonstrate how the broadening of discourse acts from simply question and answer to a richer set of categories
can improve the recall performance of Q&amp;A extraction."
"What Mobile Ads Know About Mobile Users.  We analyze the software stack of popular mobile
advertising libraries on Android and investigate how they protect
the users of advertising-supported apps from malicious advertising.
We find that, by and large, Android advertising libraries
properly separate the privileges of the ads from the host app by
confining ads to dedicated browser instances that correctly apply
the same origin policy. We then demonstrate how malicious ads can infer sensitive
information about users by accessing external storage, which is
essential for media-rich ads in order to cache video and images.
Even though the same origin policy prevents confined ads from
reading other apps?? external-storage files, it does not prevent
them from learning that a file with a particular name exists. We
show how, depending on the app, the mere existence of a file
can reveal sensitive information about the user. For example, if
the user has a pharmacy price-comparison app installed on the
device, the presence of external-storage files with certain names
reveals which drugs the user has looked for. We conclude with our recommendations for redesigning
mobile advertising software to better protect users from malicious
advertising."
"Efficient Natural Language Response Suggestion for Smart Reply.  This paper presents a computationally efficient machine-learned method for natural language response suggestion. Feed-forward neural networks using n-gram embedding features encode messages into vectors which are optimized to give message-response pairs a high dot-product value. An optimized search finds response suggestions. The method is evaluated in a large-scale commercial e-mail application, Inbox by Gmail. Compared to a sequence-to-sequence approach, the new system achieves the same quality at a small fraction of the computational requirements and latency."
"A No-Reference Video Quality Predictor for H.264 Compression and Scaling Artifacts.  No-Reference (NR) video quality assessment (VQA) models, as opposed to full-reference (FR) models, are gaining immense popularity as they offer scope for broader applicability to user-uploaded video-centric services such as YouTube and Facebook, where the pristine references are unavailable. However, there are very few, well-performing NR-VQA models owing to the difficulty of the problem. In this paper, we propose a novel <code>opinion unaware' NR video quality predictor that solely relies on the</code>quality-aware' natural statistical models in the space-time domain. The proposed quality predictor called \textbf{S}elf-reference based \textbf{LE}arning-free \textbf{E}valuator of \textbf{Q}uality (SLEEQ) consists of three components: feature extraction in the spatial and temporal domains, motion-based feature fusion, and spatial-temporal feature pooling to derive a single quality score for a given video. We demonstrate the competence of the proposed model, which significantly outperforms the existing NR VQA models and competes very well with a leading human judgment trained FR VQA model."
"Guetzli: Perceptually Guided JPEG Encoder.  Guetzli is a new JPEG encoder that aims to produce visually indistinguishable images at a lower bit-rate than other common JPEG encoders. It optimizes both the JPEG global quantization tables and the DCT coefficient values in each JPEG block using a closed-loop optimizer. Guetzli uses Butteraugli, our perceptual distance metric, as the source of feedback in its optimization process. We reach a 29-45% reduction in data size for a given perceptual distance, according to Butteraugli, in comparison to other compressors we tried. Guetzli's computation is currently extremely slow, which limits its applicability to compressing static content and serving as a proof- of-concept that we can achieve significant reductions in size by combining advanced psychovisual models with lossy compression techniques."
"Stories from survivors: Privacy &amp; security practices when coping with intimate partner abuse.  We present a qualitative study of the digital privacy and security motivations, practices, and challenges of survivors of intimate partner abuse (IPA). This paper provides a framework for organizing survivors' technology practices and challenges into three phases: physical control, escape, and life apart. This three-phase framework combines technology practices with three phases of abuse to provide an empirically sound method for technology creators to consider how survivors of IPA can leverage new and existing technologies. Overall, our results suggest that the usability of and control over privacy and security functions should be or continue to be high priorities for technology creators seeking ways to better support survivors of IPA."
"Exploiting cyclic symmetry in convolutional neural networks.  Many image modalities exhibit rotational symmetry. Convolutional neural networks are sometimes trained using data augmentation to exploit this, but the networks are still required to learn the rotation equivariance properties from the data. Encoding these properties into the network architecture, as we are already used to doing for translation equivariance by using convolutional layers, could result in a more efficient use of the parameter budget by relieving the model from learning them. We introduce four operations which can be inserted into neural network models as layers, and which can be combined to make these models partially equivariant to rotations. They also enable parameter sharing across different orientations. We compare models modified in this way with unmodified baselines and demonstrate improved performance and regularization effects on several datasets with different rotation equivariance properties."
"Efficient Hyperparameter Optimization and Infinitely Many Armed Bandits.  Performance of machine learning algorithms depends critically on identifying a
good set of hyperparameters. While recent approaches use Bayesian Optimiza-
tion to adaptively select configurations, we focus on speeding up random search
through adaptive resource allocation. We present H YPERBAND , a novel algorithm
for hyperparameter optimization that is simple, flexible, and theoretically sound.
H YPERBAND is a principled early-stoppping method that adaptively allocates a pre-
defined resource, e.g., iterations, data samples or number of features, to randomly
sampled configurations. We compare H YPERBAND with state-of-the-art Bayesian
Optimization methods on several hyperparameter optimization problems. We ob-
serve that H YPERBAND can provide over an order of magnitude speedups over
competitors on a variety of neural network and kernel-based learning problems."
"Signal Processing for Big Data.  The information explosion propelled by the advent of online social media, the Internet, and global-scale communications has rendered learning from data increasingly important. At any given time around the globe, large volumes of data are generated by today??s ubiquitous communication, imaging, and mobile devices such as cell phones, surveillance cameras, medical and e-commerce platforms, as well as social networking sites. While many find this intrusive and raise legitimately ??Big Brother?? concerns, there is no denying that tremendous economic growth and improvement in quality of life hinge upon harnessing the potential benefits of analyzing massive data."
"Introduction to the Issue on Signal Processing for Big Data.  With the Internet, social media, wireless mobile devices, and pervasive sensors continuously collecting massive amounts of data, we undoubtedly live in an era of ??data deluge.?? Learning from such huge volumes of data however, promises ground-breaking advances in science and engineering along with consequent improvements in quality of life. Indeed, mining information from big data could limit the spread of epidemics and diseases, identify trends in financial and e-markets, unveil topologies and dynamics of emergent social-computational systems, accelerate brain imaging, neuroscience and systems biology models, and also protect critical infrastructure including the power grid and the Internet's backbone network."
"Spanner: Becoming a SQL System.  Spanner is a globally-distributed data management system that backs hundreds of mission-critical services at Google. Spanner is built on ideas from both the systems and database communities. The first Spanner paper published at OSDI'12 focused on the systems aspects such as scalability, automatic sharding, fault tolerance, consistent replication, external consistency, and wide-area distribution. This paper highlights the database DNA of Spanner. We describe distributed query execution in the presence of resharding, query restarts upon transient failures, range extraction that drives query routing and index seeks, and the improved blockwise-columnar storage format. We touch upon migrating Spanner to the common SQL dialect shared with other systems at Google."
"Makalu: Fast Recoverable Allocation of Non-volatile Memory.  Byte addressable non-volatile memory (NVRAM) is likely to
supplement, and perhaps eventually replace, DRAM. Applications
can then persist data structures directly in memory
instead of serializing them and storing them onto a
durable block device. However, failures during execution
can leave data structures in NVRAM unreachable or corrupt. In
this paper, we address memory management of non-volatile
memory, offering an integrated allocator and garbage collector
that maintains internal consistency, minimizes memory
leaks, and is efficient in the face of failures. We show that a careful allocator design can both support a
less restrictive and much more familiar programming model.
By lazily persisting non-essential metadata and by employing
a post-failure recovery-time garbage collector, the per
allocation persistence overhead is greatly reduced. Experimental
results show that the resulting online speed and scalability
of our allocator are comparable to well-known transient
allocators and significantly better than state-of-the-art
persistent allocators."
"Rise of the Chatbots:  Finding A Place For Artificial Intelligence in India and US.  This research study explores how chatbots can broadly find a place in routine daily lives. Chatbot development has increased while in many cases its purpose still remains loosely defined. Due to its novelty and relatively new technology, there is an opportunity to create meaningful experiences with chatbots in a typical person??s life. Qualitative insights were collected from 54 participants in India and the US over the course of two weeks. To identify opportunities for chatbots, we must understand how these programs are perceived and what needs exist for people. The research objectives include: 1) anticipations for chatbots 2) preferred input modalities 3) finding a place for chatbots."
"Mobile as a Means to Electrification in Uganda.  This research study investigates a proposed model using mobile phones as an accelerator for solar adoption in Uganda. Sixty-five percent of Uganda is non-electrified, and 72% of people are unbanked. Due to the lack of financial infrastructure and banking history, banks are not able to predict default loan behaviors. In an attempt to scale solar adoption, we propose a USSD model for users to receive energy loan approvals based on mobile financial history. The research involves a mix-method qualitative approach with 36 users to evaluate the following objectives: 1) user??s ability to understand and consent to a ??mobile credit check?? for loan approval 2) perception of variable loan discounts for those with good credit and 3) how the USSD model will fit in the broader solar environment."
"Generation of large-scale simulated utterances in virtual rooms to train deep-neural networks for far-field speech recognition in Google Home.  We describe the structure and application of an acoustic room
simulator to generate large-scale simulated data for training
deep neural networks for far-field speech recognition. The system
simulates millions of different room dimensions, a wide
distribution of reverberation time and signal-to-noise ratios,
and a range of microphone and sound source locations. We
start with a relatively clean training set as the source and artificially
create simulated data by randomly sampling a noise
configuration for every new training example. As a result,
the acoustic model is trained using examples that are virtually
never repeated. We evaluate performance of this approach
based on room simulation using a factored complex Fast Fourier
Transform (CFFT) acoustic model introduced in our earlier
work, which uses CFFT layers and LSTM AMs for joint multichannel
processing and acoustic modeling. Results show that
the simulator-driven approach is quite effective in obtaining
large improvements not only in simulated test conditions, but
also in real / rerecorded conditions. This room simulation system
has been employed in training acoustic models including
the ones for the recently released Google Home."
"HyperLogLog Hyper Extended: Sketches for Concave Sublinear Frequency Statistics.  One of the most common statistics computed over data elements is the number of distinct keys.  A thread of research pioneered by Flajolet and Martin three decades ago culminated in the design of optimal approximate counting sketches, which have size that is double logarithmic in the number of distinct keys and provide estimates with a small relative error.  Moreover, the sketches are composable, and thus suitable for streamed,  parallel, or distributed computation. We consider here all statistics of the frequency distribution of keys, where a contribution of a key to the aggregate is concave and grows (sub)linearly with  its frequency.  These fundamental aggregations are very  common in text, graphs, and logs analysis and include logarithms, low frequency moments, and capping statistics. We design composable sketches of double-logarithmic size for all concave sublinear statistics. Our design combines theoretical optimality and practical simplicity. In a nutshell, we specify tailored  mapping functions of data elements to output elements so that our target statistics on the data elements is approximated by the (max-) distinct statistics of the output elements, which can be approximated using off-the-shelf sketches. Our key insight is relating these target statistics  to the {\em complement Laplace} transform of the input frequencies."
"Action Language Hybrid AL.  This paper introduces Hybrid AL, an extension of the action
language AL that allows to reason about actions and their consequences
for the domains where such consequences can be described practically
only as results of the computations by exetrnal algorithms. While the
semantics of the action language AL is defined using ASP, the semantics
of the action language Hybrid AL is defined using Hybrid ASP -
an extension of ASP allowing rules to control sequential processing of
arbitrary binary data by external algorithms."
"Online and Linear-Time Attention by Enforcing Monotonic Alignments.  Recurrent neural network models with an attention mechanism have proven to be extremely effective on a wide variety of sequence-to-sequence problems.
However, the fact that soft attention mechanisms perform a pass over the entire input sequence when producing each element in the output sequence precludes their use in online settings and results in a quadratic time complexity.
Based on the insight that the alignment between input and output sequence elements is monotonic in many problems of interest, we propose an end-to-end differentiable method for learning monotonic alignments which, at test time, enables computing attention online and in linear time.
We validate our approach on sentence summarization, machine translation, and online speech recognition problems and achieve results competitive with existing sequence-to-sequence models."
"Get To The Point: Summarization with Pointer-Generator Networks.  Neural sequence-to-sequence models have provided a new viable approach to ab- stractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the origi- nal text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we pro- pose a novel architecture that augments the standard sequence-to-sequence atten- tion model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate repro- duction of information while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail sum- marization task, outperforming the current abstractive state-of-the-art ROUGE scores with statistical significance."
"Sharp Minima Can Generalize For Deep Nets.  Despite their overwhelming capacity to overfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice. However, explaining why this is the case is still an open area of research. One standing hypothesis that is gaining popularity, e.g. Hochreiter &amp; Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization. This paper argues that most notions of flatness are problematic for deep models and can not be directly applied to explain generalization. Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper minima. Furthermore, if we allow to reparametrize a function, the geometry of its parameters can change drastically without affecting its generalization properties."
"Neural Optimizer Search with Reinforcement Learning.  We present an approach to automate the process of discovering optimization methods, with a focus on deep learning architectures. We train a Recurrent Neural Network controller to generate a string in a domain specific language that describes a mathematical update equation based on a list of primitive functions, such as the gradient, running average of the gradient, etc. The controller is trained with Reinforcement Learning to maximize the performance of a model after a few epochs. 
On CIFAR-10, our method discovers several update rules that are better than many commonly used optimizers, such as Adam, RMSProp, or SGD with and without Momentum on a ConvNet model. 
We introduce two new optimizers, named PowerSign and AddSign, which we show transfer well and improve training on a variety of different tasks and architectures, including ImageNet classification and Google's neural machine translation system."
"Device Placement Optimization with Reinforcement Learning.  The past few years have seen much success in applying neural networks to many practical problems. Together with this success is the growth in size and computational requirements for training and inference with neural networks. A common approach to address these requirements is to use a heterogeneous distributed environment with a mix of hardware devices such as CPUs, and GPUs. Importantly, the decision of placing parts of the neural models on devices is most often made by a human expert relying on heuristic approaches. In this paper, we propose a method which learns to optimize device placement. Key to our method is the employment of a recurrent neural network to predict a set of device placements for a target neural computation graph. The execution time according to the predicted placements is then used as the reward function to optimize the parameters of the recurrent neural network. Our main result is that on Inception for ImageNet classification, and on LSTM, for language modeling and neural translation, our model finds non-trivial device placements that significantly outperform handcrafted heuristics and traditional algorithmic methods."
"Learned Optimizers that Scale and Generalize.  Learning to learn has emerged as an important direction for achieving artificial intelligence. Two of the primary barriers to its adoption are an inability to scale to larger problems and a limited ability to generalize to new tasks. We introduce a learned gradient descent optimizer that generalizes well to new tasks, and which has significantly reduced memory and computation overhead. We achieve this by introducing a novel hierarchical RNN architecture, with minimal per-parameter overhead, augmented with additional architectural features that mirror the known structure of optimization tasks. We also develop a meta-training ensemble of small, diverse, optimization tasks capturing common properties of loss landscapes. The optimizer learns to out-perform RMSProp/ADAM on problems in this corpus. More importantly, it performs comparably or better when applied to small convolutional neural networks, despite seeing no neural networks in its meta-training set. Finally, it generalizes to train Inception V3 and ResNet V2 architectures on the ImageNet dataset, optimization problems that are of a vastly different scale than those it was trained on."
"Neural Message Passing for Quantum Chemistry.  Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation function to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark, results we believe are strong enough to justify retiring this benchmark."
"Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control.  This paper proposes a general method for improving the structure and quality of sequences generated by a recurrent neural network (RNN), while maintaining information originally learned from data, as well as sample diversity. An RNN is first pre-trained on data using maximum likelihood estimation (MLE), and the probability distribution over the next token in the sequence learned by this model is treated as a prior policy. Another RNN is then trained using reinforcement learning (RL) to generate higher-quality outputs that account for domain-specific incentives while retaining proximity to the prior policy of the MLE RNN. To formalize this objective, we derive novel off-policy RL methods for RNNs from KL-control. The effectiveness of the approach is demonstrated on two applications; 1) generating novel musical melodies, and 2) computational molecular generation. For both problems, we show that the proposed method improves the desired properties and structure of the generated sequences, while maintaining information learned from data."
"Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders.  Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive."
"Large-Scale Evolution of Image Classifiers.  Neural networks have proven effective at solving difficult problems but designing their architectures can be challenging, even for image classification problems alone. Evolutionary algorithms provide a technique to discover such networks automatically. Despite significant computational requirements, we show that evolving models that rival large, hand-designed architectures is possible today. We employ simple evolutionary techniques at unprecedented scales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting from trivial initial conditions. To do this, we use novel and intuitive mutation operators that navigate large search spaces. We stress that no human participation is required once evolution starts and that the output is a fully-trained model. Throughout this work, we place special emphasis on the repeatability of results, the variability in the outcomes and the computational requirements."
"Learning to Generate Long-term Future via Hierarchical Prediction.  We propose a hierarchical approach for making long-term predictions of future frames. To avoid inherent compounding errors in recursive pixel-level prediction, we propose to first estimate high-level structure in the input frames, then predict how that structure evolves in the future, and finally by observing a single frame from the past and the predicted high-level structure, we construct the future frames without having to observe any of the pixel-level predictions. Long-term video prediction is difficult to perform by recurrently observing the predicted frames because the small errors in pixel space exponentially amplify as predictions are made deeper into the future. Our approach prevents pixel-level error propagation from happening by removing the need to observe the predicted frames. Our model is built with a combination of LSTM and analogy based encoder-decoder convolutional neural networks, which independently predict the video structure and generate the future frames, respectively. In experiments, our model is evaluated on the Human3.6M and Penn Action datasets on the task of long-term pixel-level video prediction of humans performing actions and demonstrate significantly better results than the state-of-the-art."
"Deep Value Networks Learn to Evaluate and Iteratively Refine Structured Outputs.  We approach structured output prediction by learning a deep value network (DVN) that evaluates different output structures for a given input. For example, when applied to image segmentation, the value network takes an image and a segmentation mask as inputs and predicts a scalar score evaluating the mask quality and its correspondence with the image. Once the value network is optimized, at inference, it finds output structures that maximize the score of the value net via gradient descent on continuous relaxations of structured outputs. Thus DVN takes advantage of the joint modeling of the inputs and outputs. Our framework applies to a wide range of structured output prediction problems. We conduct experiments on multi-label classification based on text data and on image segmentation problems. DVN outperforms several strong baselines and the state-of-the-art results on these benchmarks. In addition, on image segmentation, the proposed deep value network learns complex shape priors and effectively combines image information with the prior to obtain competitive segmentation results."
"Robust Speech Recognition Based on Binaural Auditory Processing.  This paper discusses a combination of techniques for improving
speech recognition accuracy in the presence of reverberation
and spatially-separated interfering sound sources. Interaural
Time Delay (ITD), observed as a consequence of the difference
in arrival times of a sound to the two ears, is an important feature
used by the human auditory system to reliably localize and separate
sound sources. In addition, the ??precedence effect?? helps
the auditory system differentiate between the direct sound and
its subsequent reflections in reverberant environments. This paper
uses a cross-correlation-based measure across the two channels
of a binaural signal to isolate the target source by rejecting
portions of the signal corresponding to larger ITDs. To overcome
the effects of reverberation, the steady-state components
of speech are suppressed, effectively boosting the onsets, so as
to retain the direct sound and suppress the reflections. Experimental
results show a significant improvement in recognition
accuracy using both these techniques. Cross-correlation-based
processing and steady-state suppression are carried out separately,
and the order in which these techniques are applied produces
differences in the resulting recognition accuracy."
"Zero-Shot Task Generalization with  Multi-Task Deep Reinforcement Learning.  As a step towards developing zero-shot task generalization capabilities in reinforcement learning (RL), this paper introduces a new RL problem where the agent should learn to execute sequences of instructions after learning useful skills that solve subtasks. In this problem, we consider two types of generalizations: to previously unseen instructions and to longer sequences of instructions. For generalization over unseen instructions, we propose a new analogy-making objective which encourages learning correspondences between similar subtasks using neural networks. For generalization over sequential instructions, we present a hierarchical deep RL architecture where a meta controller learns to use the acquired skills while executing the instructions. To deal with delayed reward, we propose a new neural architecture in the meta controller that learns when to update the subtask, which makes learning more stable. Experimental results on a stochastic 3D visual domain show that analogy-making can be successfully applied to various generalization scenarios, and our hierarchical architecture generalizes well to longer instructions as well as unseen instructions."
"Glimmers: Resolving the Privacy/Trust Quagmire.  Users today enjoy access to a wealth of services that rely on user-contributed data, such as recommendation services, prediction services, and services that help classify and interpret raw data. The quality of such services inescapably relies on trustworthy contributions from users. However, validating the trustworthiness of contributions can often rely on supporting contextual data, which may contain privacy-sensitive information, such as a user's location or usage habits, creating a conflict between privacy and trust: users benefit from a higher-quality service that identifies and removes illegitimate user contributions, but, at the same time, they may be reluctant to let the service access their private information to achieve this high quality. We argue that this conflict can be resolved with a pragmatic Glimmer of Trust, which allows services to validate user contributions in a trustworthy way without forfeiting user privacy. We describe how trustworthy hardware such as Intel's SGX can be used on the client-side---in contrast to much recent work exploring SGX in cloud services---to realize the Glimmer architecture, and demonstrate how this realization is able to resolve the tension between privacy and trust in a variety of cases."
"LB3D: A parallel implementation of the Lattice-Boltzmann method for simulation of interacting amphiphilic fluids.  We introduce the lattice-Boltzmann code LB3D, version 7.1. Building on a parallel program and supporting tools which have enabled research utilising high performance computing resources for nearly two decades, LB3D version 7 provides a subset of the research code functionality as an open source project. Here, we describe the theoretical basis of the algorithm as well as computational aspects of the implementation. The software package is validated against simulations of meso-phases resulting from self-assembly in ternary fluid mixtures comprising immiscible and amphiphilic components such as water??oil??surfactant systems. The impact of the surfactant species on the dynamics of spinodal decomposition are tested and quantitative measurement of the permeability of a body centred cubic (BCC) model porous medium for a simple binary mixture is described. Single-core performance and scaling behaviour of the code are reported for simulations on current supercomputer architectures."
"Acoustic Modeling for Google Home.  This paper describes the technical and system building advances made to the Google Home multichannel speech recognition system, which was launched in November 2016. Technical advances include an adaptive dereverberation frontend, the use of neural network models that do multichannel processing jointly with acoustic modeling, and grid lstms to model frequency variations. On the system level, improvements include adapting the model using Google Home specific data. We present results on a variety of multichannel sets. The combination of technical and system advances result in a reduction of WER of over 18\% relative compared to the current production system."
"Next-Step Conditioned Deep Convolutional Neural Networks Improve Protein Secondary Structure Prediction.  Recently developed deep learning techniques have significantly improved the accuracy of various speech and image recognition systems. In this paper we show how to adapt some of these techniques to create a novel chained convolutional architecture with next-step conditioning for improving performance on protein sequence prediction problems. We explore its value by demonstrating its ability to improve performance on eight-class secondary structure prediction. We first establish a state-of-the-art baseline by adapting recent advances in convolutional neural networks which were developed for vision tasks. This model achieves 70.0% per amino acid accuracy on the CB513 benchmark dataset without use of standard performance-boosting techniques such as ensembling or multitask learning. We then improve upon this state-of-the-art result using a novel chained prediction approach which frames the secondary structure prediction as a next-step prediction problem. This sequential model achieves 70.3% Q8 accuracy on CB513 with a single model; an ensemble of these models produces 71.4% Q8 accuracy on the same test set, improving upon the previous overall state of the art for the eight-class secondary structure problem. Our models are implemented using TensorFlow, an open-source machine learning software library available at TensorFlow.org; we aim to release the code for these experiments as part of the TensorFlow repository."
"Precise Estimates of Single-Trial Dynamics in Motor Cortex using Deep Learning Techniques.  Neuroscience is experiencing a data revolution in which many hundreds or thousands of neurons are recorded
simultaneously. This often reveals structure in the population activity that is not apparent from single neuron
responses. However, understanding this structure on a single-trial basis is often challenging due to limited observations
of the neural population, trial-to-trial variability, and the inherent noise of action potential arrival times.
Here we introduce Latent Factor Analysis via Dynamical Systems (LFADS), a deep-learning method to infer latent
dynamics from simultaneously recorded, single-trial, high-dimensional neural spiking data. LFADS is a sequential
model based on a variational auto-encoder (Kingma &amp; Welling, 2013). By making a dynamical systems hypothesis
regarding the generation of the observed data, LFADS reduces observed spiking to a set of low-dimensional
temporal factors, per-trial initial conditions, and inferred inputs. Here we apply LFADS to a variety of datasets
from monkey motor cortex. We show that LFADS??s estimates of neural population state are more informative
about behavioral variables than population activity itself. In addition, LFADS uncovers multiple known dynamic
features of single-trial motor cortical firing rates, including slow oscillations (1-3Hz) that accompany the transition
from pre- to peri-movement activity (Churchland et al., Nature 2012), and high-frequency oscillations (15-45
Hz) that occur during the pre-movement period (Donoghue et al., J Neurophys 1998). In cases where the neural
data??s dynamics cannot be modeled by an initial state alone (e.g., unexpected perturbations), LFADS infers
time-varying external inputs that correlate with behavioral outcomes. Finally, we apply LFADS to an unstructured
dataset (no precise timing, free-paced reaching movements, no repeated conditions) and show that it uncovers
precise state estimates and inputs from unstructured activity. These results showcase the ability of LFADS to infer
precise estimates of single-trial dynamics on multiple timescales and uncover inputs that correlate with behavioral
choices."
"Who Said What: Modelling Individual Labels Improves Classification.  Data are often labeled by many different experts with each expert only labeling a small fraction of the data and each data point being labeled by several experts. This reduces the workload on individual experts and also gives a better estimate of the unobserved ground truth. When experts disagree, the standard approaches are to treat the majority opinion as the correct label or to model the correct label as a distribution. These approaches, however, do not make any use of potentially valuable information about which expert produced which label. To make use of this extra information, we propose modeling the experts individually and then learning averaging weights for combining them, possibly in sample-specific ways. This allows us to give more weight to more reliable experts and take advantage of the unique strengths of individual experts at classifying certain types of data. Here we show that our approach leads to improvements in computer-aided diagnosis of diabetic retinopathy. We also show that our method performs better than competing algorithms by Welinder and Perona, and by Mnih and Hinton. Our work offers an innovative approach for dealing with the myriad real-world settings that use expert opinions to define labels for training."
"Migrating to BeyondCorp: Maintaining Productivity While Improving Security.  If you've read the three previous installments in the series about Google's BeyondCorp network security model, you may be thinking: ??That all sounds good...but how does my organization move from where we are today to a similar model? What do I need to do? What's the potential impact on my company and my employees??? This article discusses how we moved from our legacy network to the BeyondCorp model--changing the fundamentals of network access--without breaking the company??s productivity."
"Sparse Non-negative Matrix Language Modeling: Maximum Entropy Flexibility on the Cheap.  We present a new method for estimating the sparse non-negative model (SNM) by
using a small amount of held-out data and the multinomial loss that is natural
for language modeling; we validate it experimentally against the previous
estimation method which uses leave-one-out on training data and a binary loss
function and show that it performs equally well. Being able to train on
held-out data is very important in practical situations where training data is
mismatched from held-out/test data. We find that fairly small amounts of
held-out data (on the order of 30-70 thousand words) are sufficient for
training the adjustment model, which is the only model component estimated
using gradient descent; the bulk of model parameters are relative frequencies
counted on training data. A second contribution is a comparison between SNM and the related class of
Maximum Entropy language models. While much cheaper computationally, we show
that SNM achieves slightly better perplexity results for the same feature set
and same speech recognition accuracy on voice search and short message
dictation."
"Predictive State Smoothing (PRESS): Scalable non-parametric regression for high-dimensional data with variable selection.  We introduce predictive state smoothing (PRESS), a novel semi-parametric regression technique for high-dimensional data using predictive state representations. PRESS is a fully probabilistic model for the optimal kernel smoothing matrix. We present efficient algorithms for the joint estimation of the state space as well as the non-linear mapping of observations to predictive states and as an alternative algorithms to minimize leave-one-out cross validation error. The proposed estimator is straightforward to implement using (stochastic) gradient descent and scales well for large N and large p. LASSO penalty parameters as well the optimal smoothness can be estimated as part of the optimization. Finally we show that out-of-sample predictions are on par with or better than alternative state-of-the-art regression methods on the abalone and MNIST benchmark datasets. Yet unlike alternative methods PRESS gives meaningful domain-specific insights and can be used for statistical inference via regression coefficients."
"Uniform Multilingual Multi-Speaker Acoustic Model for Statistical Parametric Speech Synthesis of Low-Resourced Languages.  Acquiring data for text-to-speech (TTS) systems is expensive. This typically requires large amounts of training data, which is not available for low-resourced languages. Sometimes small amounts of data can be collected, while often - no data may be available at all. This paper presents acoustic
modeling approach utilizing long short-term memory (LSTM) recurrent neural network (RNN) aimed at partially addressing the language data scarcity problem. Unlike speaker-adaption systems that aim to preserve speaker similarity across languages, the salient feature of the proposed approach is that, once constructed, the resulting system does not need retraining to cope with the previously unseen languages. This is due to language and speaker-agnostic model topology and universal linguistic feature set. Experiments on twelve languages show that the system is able to produce
intelligible and sometimes natural output when language is unseen. We also show that, when small amounts of training data are available, pooling the data sometimes improves the overall intelligibility and naturalness. Finally, we show that sometimes having a multilingual system with no prior exposure to the language is better than building single-speaker system from small amounts of
data for that language."
"Areal and Phylogenetic Features for Multilingual Speech Synthesis.  We introduce phylogenetic and areal language features to the domain of
  multilingual text-to-speech (TTS) synthesis. Intuitively, enriching the
  existing universal phonetic features with such cross-language shared representations
  should benefit the multilingual acoustic models and help to address issues like
  data scarcity for low-resource languages. We investigate these representations
  using the acoustic models based on long short-term memory (LSTM) recurrent
  neural networks (RNN). Subjective evaluations conducted on eight languages
  from diverse language families show that sometimes phylogenetic and areal
  representations lead to significant multilingual synthesis quality improvements."
"Taming Undefined Behavior in LLVM.  A central concern for an optimizing compiler is the design of its intermediate representation (IR) for code. The IR should make it easy to perform transformations, and should also afford efficient and precise static analysis. In this paper we study an aspect of IR design that has received little attention: the role of undefined behavior. The IR for every optimizing compiler we have looked at, including GCC, LLVM, Intel's, and Microsoft's, supports one or more forms of undefined behavior (UB), not only to reflect the semantics of UB-heavy programming languages such as C and C++, but also to model inherently unsafe low-level operations such as memory stores and to avoid over-constraining IR semantics to the point that desirable transformations become illegal. The current semantics of LLVM's IR fails to justify some cases of loop unswitching, global value numbering, and other important ``textbook'' optimizations, causing long-standing bugs. We present solutions to the problems we have identified in LLVM's IR and show that most optimizations currently in LLVM remain sound, and that some desirable new transformations become permissible. Our solutions do not degrade compile time or performance of generated code."
"Spatiotemporal atlas parameterization for evolving meshes.  We convert a sequence of unstructured textured meshes into a mesh with incrementally changing connectivity and atlas parameterization.  Like prior work on surface tracking, we seek temporally coherent mesh connectivity to enable efficient representation of surface geometry and texture.  Like recent work on evolving meshes, we pursue local remeshing to permit tracking over long sequences containing significant deformations or topological changes.  Our main contribution is to show that both goals are realizable within a common framework that simultaneously evolves both the set of mesh triangles and the parametric map.  Sparsifying the remeshing operations allows the formation of large spatiotemporal texture charts.  These charts are packed as prisms into a 3D atlas for a texture video.  Reducing tracking drift using mesh-based optical flow helps improve compression of the resulting video stream."
"Protein Word Detection using Text Segmentation Techniques.  Literature in Molecular Biology is abundant with linguistic metaphors. In particular, there have been works in the past that attempt to draw parallels between linguistics and biology, driven by the  fundamental premise that proteins have a language of their own. Since word detection is fundamental to the decipherment of any unknown language, we attempt to establish a problem mapping from natural language text to protein sequences at the level of words. Towards this end, we explore the use of an unsupervised text segmentation algorithm to the task of extracting ??biological words?? from protein sequences. We also demonstrate the effectiveness of using domain knowledge to
complement data driven approaches in the text segmentation task, as well as in its biological
counterpart."
"Cell-based screening: extracting meaning from complex data.  Unbiased discovery approaches have the potential to uncover neurobiological insights into CNS disease and lead to the development of therapies. Here, we review lessons learned from imaging-based screening approaches and recent advances in these areas, including powerful new computational tools to synthesize complex data into more useful knowledge that can reliably guide future research and development."
"Surprising properties of dropout in deep networks.  We analyze dropout in deep networks with rectified linear units and the quadratic loss. Our results
expose surprising differences between the behavior of dropout and more traditional regularizers like
weight decay. For example, on some simple data sets dropout training produces negative weights
even though the output is the sum of the inputs. This provides a counterpoint to the suggestion that
dropout discourages co-adaptation of weights. We also show that the dropout penalty can grow
exponentially in the depth of the network while the weight-decay penalty remains essentially linear,
and that dropout is insensitive to various re-scalings of the input features, outputs, and network
weights. This last insensitivity implies that there are no isolated local minima of the dropout training
criterion. Our work uncovers new properties of dropout,"
"Tacotron: Towards End-to-End Speech Synthesis.  We analyze dropout in deep networks with rectified linear units and the quadratic loss. Our results
expose surprising differences between the behavior of dropout and more traditional regularizers like
weight decay. For example, on some simple data sets dropout training produces negative weights
even though the output is the sum of the inputs. This provides a counterpoint to the suggestion that
dropout discourages co-adaptation of weights. We also show that the dropout penalty can grow
exponentially in the depth of the network while the weight-decay penalty remains essentially linear,
and that dropout is insensitive to various re-scalings of the input features, outputs, and network
weights. This last insensitivity implies that there are no isolated local minima of the dropout training
criterion. Our work uncovers new properties of dropout,"
"Sequence-to-Sequence Models Can Directly Translate Foreign Speech.  We present a recurrent encoder-decoder deep neural network architecture that directly translates speech in one language into text in another. The model does not explicitly transcribe the speech into text in the source language, nor does it require supervision from the ground truth source language transcription during training. We apply a slightly modified sequence-to-sequence with attention architecture that has previously been used for speech recognition and show that it can be repurposed for this more complex task, illustrating the power of attention-based models. A single model trained end-to-end obtains state-of-the-art performance on the Fisher Callhome Spanish-English speech translation task, outperforming a cascade of independently trained sequence-to-sequence speech recognition and machine translation models by 1.8 BLEU points on the Fisher test set. In addition, we find that making use of the training data in both languages by multi-task training sequence-to-sequence speech translation and recognition models with a shared encoder network can improve performance by a further 1.4 BLEU points."
"Random Features for Compositional Kernels.  We describe and analyze a simple random feature scheme (RFS) from prescribed compositional kernels. The compositional kernels we use are inspired by the structure of convolutional neural networks and kernels. The resulting scheme yields sparse and efficiently computable features. Each random feature can be represented as an algebraic expression over a small number of (random) paths in a composition tree. Thus, compositional random features can be stored compactly. The discrete nature of the generation process enables de-duplication of repeated features, further compacting the representation and increasing the diversity of the embeddings. Our approach complements and can be combined with previous random feature schemes."
"The Space of Transferable Adversarial Examples.  Adversarial examples are maliciously perturbed inputs designed to mislead machine learning (ML) models at test-time. They often transfer: the same adversarial example fools more than one model. 
In this work, we propose novel methods for estimating the previously unknown dimensionality of the space of adversarial inputs. We find that adversarial examples span a contiguous subspace of large (~25) dimensionality. Adversarial subspaces with higher dimensionality are more likely to intersect. We find that for two different models, a significant fraction of their subspaces is shared, thus enabling transferability. 
In the first quantitative analysis of the similarity of different models' decision boundaries, we show that these boundaries are actually close in arbitrary directions, whether adversarial or benign. We conclude by formally studying the limits of transferability. We derive (1) sufficient conditions on the data distribution that imply transferability for simple model classes and (2) examples of scenarios in which transfer does not occur. These findings indicate that it may be possible to design defenses against transfer-based attacks, even for models that are vulnerable to direct attacks."
"Adversarial Attacks on Neural Network Policies.  Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at this http URL"
"Bridging the Gap Between Value and Policy Based Reinforcement Learning.  We formulate a new notion of softmax temporal consistency that generalizes the standard hard-max Bellman consistency usually considered in value based reinforcement learning (RL). In particular, we show how softmax consistent action values correspond to optimal policies that maximize entropy regularized expected reward. More importantly, we establish that softmax consistent action values and the optimal policy must satisfy a mutual compatibility property that holds across any state-action subsequence. Based on this observation, we develop a new RL algorithm, Path Consistency Learning (PCL), that minimizes the total inconsistency measured along multi-step subsequences extracted from both both on and off policy traces. An experimental evaluation demonstrates that PCL significantly outperforms strong actor-critic and Q-learning baselines across several benchmark tasks."
"Learning Hard Alignments with Variational Inference.  There has recently been significant interest in hard attention models for tasks such as object recognition, visual captioning and speech recognition. Hard attention can offer benefits over soft attention such as decreased computational cost, but training hard attention models can be difficult because of the discrete latent variables they introduce. Previous work has used REINFORCE and Q-learning to approach these issues, but those methods can provide high-variance gradient estimates and be slow to train. In this paper, we tackle the problem of learning hard attention for a 1-d temporal task using variational inference methods, specifically the recently introduced VIMCO and NVIL. Furthermore, we propose novel baselines that adapt VIMCO to this setting. We demonstrate our method on a phoneme recognition task in clean and noisy environments and show that our method outperforms REINFORCE with the difference being greater for a more complicated task."
"Discrete Sequential Prediction of Continuous Actions for Deep RL.  It has long been assumed that high dimensional continuous control problems cannot be solved effectively by discretizing individual dimensions of the action space due to the exponentially large number of bins over which policies would have to be learned. In this paper, we draw inspiration from the recent success of sequence-to-sequence models for structured prediction problems to develop policies over discretized spaces. Central to this method is the realization that complex functions over high dimensional spaces can be modeled by neural networks that use next step prediction. Specifically, we show how Q-values and policies over continuous spaces can be modeled using a next step prediction model over discretized dimensions. With this parameterization, it is possible to both leverage the compositional structure of action spaces during learning, as well as compute maxima over action spaces (approximately). On a simple example task we demonstrate empirically that our method can perform global search, which effectively gets around the local optimization issues that plague DDPG and NAF. We apply the technique to off-policy (Q-learning) methods and show that our method can achieve the state-of-the-art for off-policy methods on several continuous control tasks."
"Improved end-of-query detection for streaming speech recognition.  In many streaming speech recognition applications such as voice search it is important to determine quickly and accurately when the user has finished speaking their query. A conventional approach to this task is to declare end-of-query whenever a fixed interval of silence is detected by a voice activity detector (VAD) trained to classify each frame as speech or silence. However silence detection and end-of-query detection are fundamentally different tasks, and the criterion used during VAD training may not be optimal. In particular the conventional approach ignores potential acoustic cues such as filler sounds and past speaking rate which may indicate whether a given pause is temporary or query-final. In this paper we present a simple modification to make the conventional VAD training criterion more closely related to end-of-query detection. A unidirectional long short-term memory architecture allows the system to remember past acoustic events, and the training criterion incentivizes the system to learn to use any acoustic cues relevant to predicting future user intent. We show experimentally that this approach improves latency at a given accuracy for end-of-query detection for voice search."
"Exploring the structure of a real-time, arbitrary neural artistic stylization network.  In this paper, we present a method which combines the flexibility of the neural algorithm of artistic style with the speed of fast style transfer networks to allow real-time stylization using any content/style image pair. We build upon recent work leveraging conditional instance normalization for multi-style transfer networks by learning to predict the conditional instance normalization parameters directly from a style image. The model is successfully trained on a corpus of roughly 80,000 paintings and is able to generalize to paintings previously unobserved. We demonstrate that the learned embedding space is smooth and contains a rich structure and organizes semantic information associated with paintings in an entirely unsupervised manner."
"Learning to Skim Text.  Recurrent Neural Networks are showing much promise in many sub-areas of natural language processing, ranging from document classification to machine translation to automatic question answering. Despite their promise, many recurrent models have to read the whole text word by word, making it slow to handle long documents. For example, it is difficult to use a recurrent network to read a book and answer questions about it. In this paper, we present an approach of reading text while skipping irrelevant information if needed. The underlying model is a recurrent network that learns how far to jump after reading a few words of the input text. We employ a standard policy gradient method to train the model to make discrete jumping decisions. In our benchmarks on four different tasks, including number prediction, sentiment analysis, news article classification and automatic Q\&amp;A, our proposed model, a modified LSTM with jumping, is up to 6 times faster than the standard sequential LSTM, while maintaining the same or even better accuracy."
"PixColor: Pixel Recursive Colorization.  We propose a novel approach to automatically produce multiple colorized versions of a grayscale image. Our method results from the observation that the task of automated colorization is relatively easy given a low-resolution version of the color image. We first train a conditional PixelCNN to generate a low resolution color for a given grayscale image. Then, given the generated low-resolution color image and the original grayscale image as inputs, we train a second CNN to generate a high-resolution colorization of an image. We demonstrate that our approach produces more diverse and plausible colorizations than existing methods, as judged by human raters in a ""Visual Turing Test""."
"A Comparison of Sequence-to-Sequence Models for Speech Recognition.  In this work, we conduct a detailed evaluation of various all-neural, end-to-end trained, sequence-to-sequence models applied to the task of speech recognition. Notably, each of these systems directly predicts graphemes in the written domain, without using an external pronunciation lexicon, or a separate language model. We examine several sequence-to-sequence models including connectionist temporal classification (CTC), the recurrent neural network (RNN) transducer, an attention-based model, and a model which augments the RNN-transducer with an attention mechanism. We find that end-to-end models are capable of learning all components of the speech recognition process: acoustic, pronunciation, and language models, directly outputting words in the written form (e.g., ??one hundred dollars?? to ??$100??), in a single jointly-optimized neural network. Furthermore, the sequence-to-sequence models are competitive with traditional state-of-the-art approaches on dictation test sets, although the baseline outperforms these models on voice-search test sets."
"Highway-LSTM and Recurrent Highway Networks for Speech Recognition.  Recently, very deep networks, with as many as hundreds of
layers, have shown great success in image classification tasks.
One key component that has enabled such deep models is the
use of ??skip connections??, including either residual or highway
connections, to alleviate the vanishing and exploding gradient
problems. While these connections have been explored
for speech, they have mainly been explored for feed-forward
networks. Since recurrent structures, such as LSTMs, have produced
state-of-the-art results on many of our Voice Search tasks,
the goal of this work is to thoroughly investigate different approaches
to adding depth to recurrent structures. Specifically,
we experiment with novel Highway-LSTM models with bottlenecks
skip connections and show that a 10 layer model can outperform
a state-of-the-art 5 layer LSTM model with the same
number of parameters by 2% relative WER. In addition, we experiment
with Recurrent Highway layers and find these to be on
par with Highway-LSTM models, when given sufficient depth."
"Headset Removal for Virtual and Mixed Reality.  Virtual Reality (VR) has advanced significantly in recent years and allows users to explore novel environments (both real and imaginary), play games, and engage with media in a way that is unprecedentedly immersive. However, compared to physical reality, sharing these experiences is difficult because the user's virtual environment is not easily observable from the outside and the user's face is partly occluded by the VR headset. Mixed Reality (MR) is a medium that alleviates some of this disconnect by sharing the virtual context of a VR user in a flat video format that can be consumed by an audience to get a feel for the user's experience. Even though MR allows audiences to connect actions of the VR user with their virtual environment, empathizing with them is difficult because their face is hidden by the headset. We present a solution to address this problem by virtually removing the headset and revealing the face underneath it using a combination of 3D vision, machine learning and graphics techniques. We have integrated our headset removal approach with Mixed Reality, and demonstrate results on several VR games and experiences."
"The Coherence and Flexibility of the Institutional Order: The Role of Abstraction and Modularity.  Ludwig Lachmann, throughout his career, wrestled with the problem of intertemporal coordination in a world of heterogeneous expectations. He emphasized the unknowability of the future, yet also recognized that coordination of plans still occurs. Lachmann pointed to institutions as providing the key link between expectations and coordination. But this poses the additional problem of the coherence and flexibility of the institutional order. While Lachmann provides many tantalizing clues on how to reconcile the contrasting needs for coherence to support coordination and flexibility to accommodate unforeseen change, he fails to provide a unified theory upon which to build. We recast Lachmann??s explanation of the institutional order in terms of the concepts of abstraction and modularity borrowed from computer programming. Economists, largely under the influence of Hebert Simon, have examined the role of modularity, but they have ignored the twin concept of abstraction. Simon emphasizes the role of modularity in decomposing complex systems, but programmers also emphasize the role of abstraction in composing them. The programmers?? challenge of composition can be viewed as analogous to economists?? problem of plan coordination. We argue that not only do the concepts of abstraction and modularity provides a better foundation for Lachmann??s theory of the institutional order, but they provide a natural link to Hayek??s work on abstract orders."
"Google's next-generation real-time unit-selection synthesizer using sequence-to-sequence LSTM-based autoencoders.  A neural network model that significant improves unit-selection-based Text-To-Speech synthesis is presented. The model employs a sequence-to-sequence LSTM-based autoencoder that compresses the acoustic and linguistic features of each unit to a fixed-size vector referred to as an embedding. Unit-selection is facilitated by formulating the target cost as an L2 distance in the embedding space. In open-domain speech synthesis the method achieves a 0.2 improvement in the MOS, while for limited-domain it reaches the cap of 4.5 MOS. Furthermore, the new TTS system halves the gap 
 between the previous unit-selection system and WaveNet in terms of quality while retaining low computational cost and latency."
"Data Management Challenges in Production Machine Learning.  This tutorial discusses data-management issues that
arise in the context of production ML pipelines. Informed
by our own experience with such large-scale pipelines, we
focus on issues related to validating, debugging, cleaning,
understanding, and enriching training data. The goal of the
tutorial is to bring forth these issues, draw connections to
prior work in the database literature, and outline the open
research questions that are not addressed by prior art. We
believe that the data management community is well positioned
to address these issues and we hope to motivate the
audience to look more closely in this area."
"Google Vizier: A Service for Black-Box Optimization.  This tutorial discusses data-management issues that
arise in the context of production ML pipelines. Informed
by our own experience with such large-scale pipelines, we
focus on issues related to validating, debugging, cleaning,
understanding, and enriching training data. The goal of the
tutorial is to bring forth these issues, draw connections to
prior work in the database literature, and outline the open
research questions that are not addressed by prior art. We
believe that the data management community is well positioned
to address these issues and we hope to motivate the
audience to look more closely in this area."
"Thinking about Availability in Large Service Infrastructures.  We increasingly depend on the availability of online services, either directly as users, or indirectly, when cloud-provider services support directly-accessed services.  The availability of these ""visible services""  depends in complex ways on the availability of a complex underlying set of invisible infrastructure services. In our experience, most software engineers lack useful frameworks to create and evaluate designs for individual services that support end-to-end availability in these infrastructures, especially given cost, performance, and other constraints on viable commercial services. Even given the extensive research literature on techniques for replicated state machines and other fault-tolerance mechanisms, we found little help in this literature for addressing infrastructure-wide availability. Past research has often focused on point solutions, rather than end-to-end ones. In particular, it seems quite difficult to define useful targets for infrastructure-level availability, and then to translate these to design requirements for individual services. We argue that, in many but not all ways, one can think about availability with the mindset that we have learned to use for security, and we discuss some general techniques that appear useful for implementing and operating high-availability
infrastructures. We encourage a shift in emphasis for academic research into availability."
"Towards Understanding the Invertibility of Convolutional Neural Networks.  Several recent works have empirically observed that Convolutional Neural Nets (CNNs) are (approximately) invertible. To understand this approximate invertibility phenomenon and how to leverage it more effectively, we focus on a theoretical explanation and develop a mathematical model of sparse signal recovery that is consistent with CNNs with random weights. We give an exact connection to a particular model of model-based compressive sensing (and its recovery algorithms) and random-weight CNNs. We show empirically that several learned networks are consistent with our mathematical analysis and then demonstrate that with such a simple theoretical framework, we can obtain reasonable re- construction results on real images. We also discuss gaps between our model assumptions and the CNN trained for classification in practical scenarios."
"N-gram Language Modeling using Recurrent Neural Network Estimation.  We investigate the effective memory depth of RNN models by using them for $n$-gram language model (LM) smoothing. Experiments on a small corpus (UPenn Treebank, one million words of training data and 10k vocabulary) have found the LSTM cell with dropout to be the best model for encoding the $n$-gram state when compared with feed-forward and vanilla RNN models. When preserving the sentence independence assumption the LSTM $n$-gram matches the LSTM LM performance for $n=9$ and slightly outperforms it for $n=13$. When allowing dependencies across sentence boundaries, the LSTM $13$-gram almost matches the perplexity of the unlimited history LSTM LM. LSTM $n$-gram smoothing also has the desirable property of improving with increasing $n$-gram order, unlike the Katz or Kneser-Ney back-off estimators. Using multinomial distributions as targets in training instead of the usual one-hot target is only slightly beneficial for low $n$-gram orders. Experiments on the One Billion Words benchmark show that the results hold at larger scale. Building LSTM $n$-gram LMs may be appealing for some practical situations: the state in a $n$-gram LM can be succinctly represented with $(n-1)*4$ bytes storing the identity of the words in the context and batches of $n$-gram contexts can be processed in parallel. On the downside, the $n$-gram context encoding computed by the LSTM is discarded, making the model more expensive than a regular recurrent LSTM LM."
"Quick Access: Building a Smart Experience for Google Drive.  Google Drive is a cloud storage and collaboration service used by hundreds of millions of users around the world. Quick Access is a new feature in Google Drive that surfaces the relevant documents to the user on the home page. We describe the development of a machine-learned service behind this feature. Our metrics show that this feature cuts the time it takes for users to locate their documents in half. The development of this product feature is an illustration of a number of more general challenges and constraints associated with machine learning product deployment such as dealing with private corpora and protecting user privacy, working with data services that are not designed with machine-learning in mind and may be owned and operated by different teams with different constraints, and evolving product definitions which inform the metric being optimized. We believe that the lessons learned from this experience will be useful to practitioners tackling a wide range of applied machine-learning problems."
"The Moving Context Kit: Designing for Context Shifts in Multi-Device Experiences.  Multi-device product designers need tools to better address ecologically valid constraints in naturalistic settings early in their design process. To address this need, we created a reusable design kit of scenarios, ??hint?? cards, and a framework that codifies insights from prior work and our own field study. We named the kit the Moving Context Kit, or McKit for short, because it helps designers focus on context shifts that we found to be highly influential in everyday multi-device use. Specifically, we distilled the following findings from our field study in the McKit: (1) devices are typically specialized into one of six roles during parallel use??notifier, broadcaster, collector, gamer, remote, and hub, and (2) device roles are influenced by context shifts between private and shared situations. Through a workshop, we validated that the McKit enables designers to engage with complex user needs, situations, and relationships when incorporating novel multi-device techniques into the products they envision."
"Deconvolution and Checkerboard Artifacts.  When we look very closely at images generated by neural networks, we often see a strange checkerboard pattern of artifacts. These artifacts appear to be caused by deconvolutions. We demonstrate that replacing deconvolution with a ""resize-convolution"" causes these artifacts to disappear in a variety of contexts."
"Emerging TV Experiences: How VR, Voice, and Young Audiences Have Changed the Landscape of TV Experiences.  At TVX 2015, we led an interactive workshop to explore how people, contexts, and multi-device experiences contribute to a changed landscape of TV watching behaviors. Since then, innovation in the television and video industry has taken leaps forward through the development of new technologies and an ever-growing audience. This one-day, interactive workshop at TVX 2017 will bring together academics and professionals to update our previously formed framework, and to understand the design challenges and opportunities that have arisen from development in three new areas: the VR/360 video space, voice interactions, and engagement of younger audiences. Let??s come together and update the framework we built in 2015."
"Contracting a Planar Graph Efficiently.  We show a data structure that can maintain a simple planar graph under edge contractions in linear total time. The data structure supports adjacency queries and provides access to neighbor lists in $O(1)$ time. Moreover, it can report all the arising loops and parallel edges. By applying the data structure, we can improve the running times of algorithms for several planar graph problems, including decremental 2-edge, 2-vertex and 3-edge connectivity. We also show that using our data structure in a black-box manner, one obtains very simple optimal algorithms for computing MST and 5-coloring in planar graphs."
"A Computational Model for TensorFlow (An Introduction).  TensorFlow is a powerful, programmable system for machine learning.
This paper aims to provide the basics of a conceptual framework for
understanding the behavior of TensorFlow models during training and inference:
it describes an operational semantics, of the kind common in
the literature on programming languages. More broadly, the paper
suggests that a programming-language perspective is fruitful in
designing and in explaining systems such as TensorFlow."
"Measuring HTTPS adoption on the web.  HTTPS ensures that the Web has a base level of privacy and integrity. Security engineers, researchers, and browser vendors have long worked to spread HTTPS to as much of the Web as possible via outreach efforts, developer tools, and browser changes. How much progress have we made toward this goal of widespread HTTPS adoption? We gather metrics to benchmark the status and progress of HTTPS adoption on the Web in 2017. To evaluate HTTPS adoption from a user perspective, we collect large-scale, aggregate user metrics from two major browsers (Google Chrome and Mozilla Firefox). To measure HTTPS adoption from a Web developer perspective, we survey server support for HTTPS among top and long-tail websites. We draw on these metrics to gain insight into the current state of the HTTPS ecosystem."
"SHRec: Scalable Holistic Recommendation.  HTTPS ensures that the Web has a base level of privacy and integrity. Security engineers, researchers, and browser vendors have long worked to spread HTTPS to as much of the Web as possible via outreach efforts, developer tools, and browser changes. How much progress have we made toward this goal of widespread HTTPS adoption? We gather metrics to benchmark the status and progress of HTTPS adoption on the Web in 2017. To evaluate HTTPS adoption from a user perspective, we collect large-scale, aggregate user metrics from two major browsers (Google Chrome and Mozilla Firefox). To measure HTTPS adoption from a Web developer perspective, we survey server support for HTTPS among top and long-tail websites. We draw on these metrics to gain insight into the current state of the HTTPS ecosystem."
"Self-Supervised Learning of Structure and Motion from Video.  We propose SfM-Net, a geometry-aware neural network
for motion estimation in videos that decomposes frame-toframe
pixel motion in terms of scene and object depth, camera
motion and 3D object rotations and translations. Given
a sequence of frames, SfM-Net predicts depth, segmentation,
camera and rigid object motions, converts those into
a dense frame-to-frame motion field (optical flow), differentiably
warps frames in time to match pixels and backpropagates.
The model can be trained with various degrees
of supervision: 1) completely unsupervised, 2) supervised
by ego-motion (camera motion), 3) supervised by
depth (e.g., as provided by RGBD sensors), 4) supervised
by ground-truth optical flow. We show that SfM-Net successfully
estimates segmentation of the objects in the scene,
even though such supervision is never provided. It extracts
meaningful depth estimates or infills depth of RGBD sensors
and successfully estimates frame-to-frame camera displacements.
SfM-Net achieves state-of-the-art optical flow
performance. Our work is inspired by the long history of
research in geometry-aware motion estimation, Simultaneous
Localization and Mapping (SLAM) and Structure from
Motion (SfM). SfM-Net is an important first step towards
providing a learning-based approach for such tasks. A major
benefit over the existing optimization approaches is that
our proposed method can improve itself by processing more
videos, and by learning to explicitly model moving objects
in dynamic scenes."
"Filtering Variational Objectives.  When used as a surrogate objective for maximum likelihood estimation in latent variable models, the evidence lower bound (ELBO) produces state-of-the-art results. Inspired by this, we consider the extension of the ELBO to a family of lower bounds defined by a particle filter's estimator of the marginal likelihood, the filtering variational objectives (FIVOs). FIVOs take the same arguments as the ELBO, but can exploit a model's sequential structure to form tighter bounds. We present results that relate the tightness of FIVO's bound to the variance of the particle filter's estimator by considering the generic case of bounds defined as log-transformed likelihood estimators. Experimentally, we show that training with FIVO results in substantial improvements over training with ELBO on sequential data."
"Attention is All You Need.  The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
"Depthwise Separable Convolutions for Neural Machine Translation.  Depthwise separable convolutions reduce the number of parameters and computation used in convolutional operations while increasing representational efficiency. They have been shown to be successful in image classification models, both in obtaining better models than previously possible for a given parameter count (the Xception architecture) and considerably reducing the number of parameters required to perform at a given level (the MobileNets family of architectures). Recently, convolutional sequence-to-sequence networks have been applied to machine translation tasks with good results. In this work, we study how depthwise separable convolutions can be applied to neural machine translation. We introduce a new architecture inspired by Xception and ByteNet, called SliceNet, which enables a significant reduction of the parameter count and amount of computation needed to obtain results like ByteNet, and, with a similar parameter count, achieves new state-of-the-art results. In addition to showing that depthwise separable convolutions perform well for machine translation, we investigate the architectural changes that they enable: we observe that thanks to depthwise separability, we can increase the length of convolution windows, removing the need for filter dilation. We also introduce a new ""super-separable"" convolution operation that further reduces the number of parameters and computational cost for obtaining state-of-the-art results."
"One Model To Learn Them All.  Depthwise separable convolutions reduce the number of parameters and computation used in convolutional operations while increasing representational efficiency. They have been shown to be successful in image classification models, both in obtaining better models than previously possible for a given parameter count (the Xception architecture) and considerably reducing the number of parameters required to perform at a given level (the MobileNets family of architectures). Recently, convolutional sequence-to-sequence networks have been applied to machine translation tasks with good results. In this work, we study how depthwise separable convolutions can be applied to neural machine translation. We introduce a new architecture inspired by Xception and ByteNet, called SliceNet, which enables a significant reduction of the parameter count and amount of computation needed to obtain results like ByteNet, and, with a similar parameter count, achieves new state-of-the-art results. In addition to showing that depthwise separable convolutions perform well for machine translation, we investigate the architectural changes that they enable: we observe that thanks to depthwise separability, we can increase the length of convolution windows, removing the need for filter dilation. We also introduce a new ""super-separable"" convolution operation that further reduces the number of parameters and computational cost for obtaining state-of-the-art results."
"Avoiding Discrimination through Causal Reasoning.  Recent work on fairness in machine learning has focused on various statistical discrimination criteria and how they trade off. Most of these criteria are observational: They depend only on the joint distribution of predictor, protected attribute, features, and outcome. While convenient to work with, observational criteria have severe inherent limitations that prevent them from resolving matters of fairness conclusively. 
Going beyond observational criteria, we frame the problem of discrimination based on protected attributes in the language of causal reasoning. This viewpoint shifts attention from ""What is the right fairness criterion?"" to ""What do we want to assume about the causal data generating process?"" Through the lens of causality, we make several contributions. First, we crisply articulate why and when observational criteria fail, thus formalizing what was before a matter of opinion. Second, our approach exposes previously ignored subtleties and why they are fundamental to the problem. Finally, we put forward natural causal non-discrimination criteria and develop algorithms that satisfy them."
"Climbing a shaky ladder: Better adaptive risk estimation.  We revisit the \emph{leaderboard problem} introduced by Blum and Hardt (2015) in an effort to reduce overfitting in machine learning benchmarks. We show that a randomized version of their Ladder algorithm achieves leaderboard error O(1/n^{0.4}) compared with the previous best rate of O(1/n^{1/3}). 
Short of proving that our algorithm is optimal, we point out a major obstacle toward further progress. Specifically, any improvement to our upper bound would lead to asymptotic improvements in the general adaptive estimation setting as have remained elusive in recent years. This connection also directly leads to lower bounds for specific classes of algorithms. In particular, we exhibit a new attack on the leaderboard algorithm that both theoretically and empirically distinguishes between our algorithm and previous leaderboard algorithms."
"From optimal transport to generative modeling: the VEGAN cookbook.  We study unsupervised generative modeling in terms of the optimal transport (OT) problem between true (but unknown) data distribution PX and the latent variable model distribution PG. We show that the OT problem can be equivalently written in terms of probabilistic encoders, which are constrained to match the posterior and prior distributions over the latent space. When relaxed, this constrained optimization problem leads to a penalized optimal transport (POT) objective, which can be efficiently minimized using stochastic gradient descent by sampling from PX and PG. We show that POT for the 2-Wasserstein distance coincides with the objective heuristically employed in adversarial auto-encoders (AAE) (Makhzani et al., 2016), which provides the first theoretical justification for AAEs known to the authors. We also compare POT to other popular techniques like variational auto-encoders (VAE) (Kingma and Welling, 2014). Our theoretical results include (a) a better understanding of the commonly observed blurriness of images generated by VAEs, and (b) establishing duality between Wasserstein GAN (Arjovsky and Bottou, 2017) and POT for the 1-Wasserstein distance."
"Better Text Understanding Through Image-To-Text Transfer.  Generic text embeddings are successfully used in a variety of tasks. However, they are often learnt by capturing the co-occurrence structure from pure text corpora, resulting in limitations of their ability to generalize. In this paper, we explore models that incorporate visual information into the text representation. Based on comprehensive ablation studies, we propose a conceptually simple, yet well performing architecture. It outperforms previous multimodal approaches on a set of well established benchmarks. We also improve the state-of-the-art results for image-related text datasets, using orders of magnitude less data."
"Critical Hyper-Parameters: No Random, No Cry.  The selection of hyper-parameters is critical in Deep Learning. Because of the long training time of complex models and the availability of compute resources in the cloud, ""one-shot"" optimization schemes - where the sets of hyper-parameters are selected in advance (e.g. on a grid or in a random manner) and the training is executed in parallel - are commonly used. It is known that grid search is sub-optimal, especially when only a few critical parameters matter, and suggest to use random search instead. Yet, random search can be ""unlucky"" and produce sets of values that leave some part of the domain unexplored. Quasi-random methods, such as Low Discrepancy Sequences (LDS) avoid these issues. We show that such methods have theoretical properties that make them appealing for performing hyperparameter search, and demonstrate that, when applied to the selection of hyperparameters of complex Deep Learning models (such as state-of-the-art LSTM language models and image classification models), they yield suitable hyperparameters values with much fewer runs than random search. We propose a particularly simple LDS method which can be used as a drop-in replacement for grid or random search in any Deep Learning pipeline, both as a fully one-shot hyperparameter search or as an initializer in iterative batch optimization."
"Approximation and Convergence Properties of Generative Adversarial Learning.  Generative adversarial networks (GAN) approximate a target data distribution by jointly optimizing an objective function through a ""two-player game"" between a generator and a discriminator. Despite their empirical success, however, two very basic questions on how well they can approximate the target distribution remain unanswered. First, it is not known how restricting the discriminator family affects the approximation quality. Second, while a number of different objective functions have been proposed, we do not understand when convergence to the global minima of the objective function leads to convergence to the target distribution under various notions of distributional convergence. 
In this paper, we address these questions in a broad and unified setting by defining a notion of adversarial divergences that includes a number of recently proposed objective functions. We show that if the objective function is an adversarial divergence with some additional conditions, then using a restricted discriminator family has a moment-matching effect. Additionally, we show that for objective functions that are strict adversarial divergences, convergence in the objective function implies weak convergence, thus generalizing previous results."
"Toward Optimal Run Racing: Application to Deep Learning Calibration.  This paper aims at one-shot learning of deep neural nets, where a highly parallel setting is considered to address the algorithm calibration problem - selecting the best neural architecture and learning hyper-parameter values depending on the dataset at hand. The notoriously expensive calibration problem is optimally reduced by detecting and early stopping non-optimal runs. The theoretical contribution regards the optimality guarantees within the multiple hypothesis testing framework. Experimentations on the Cifar10, PTB and Wiki benchmarks demonstrate the relevance of the approach with a principled and consistent improvement on the state of the art with no extra hyper-parameter."
"A comparative study of counterfactual estimators.  We provide a comparative study of several widely used off-policy estimators (Empirical Average, Basic Importance Sampling and Normalized Importance Sampling), detailing the different regimes where they are individually suboptimal. We then exhibit properties optimal estimators should possess. In the case where examples have been gathered using multiple policies, we show that fused estimators dominate basic ones but can still be improved."
"A Unified Approach to Adaptive Regularization in Online and Stochastic Optimization.  We describe a framework for deriving and analyzing online optimization algorithms that incorporate adaptive, data-dependent regularization, also termed preconditioning. Such algorithms have been proven useful in stochastic optimization by reshaping the gradients according to the geometry of the data. Our framework captures and unifies much of the existing literature on adaptive online methods, including the AdaGrad and Online Newton Step algorithms as well as their diagonal versions. As a result, we obtain new convergence proofs for these algorithms that are substantially simpler than previous analyses. Our framework also exposes the rationale for the different preconditioned updates used in common stochastic optimization methods."
"Learning by Association - A versatile semi-supervised training method for neural networks.  In many real-world scenarios, labeled data for a specific training task is costly to obtain. Semi-supervised methods make use of abundantly available unlabeled data and a smaller number of labeled examples.
We propose a new framework for semi-supervised training of deep neural networks that is inspired by learning in humans. ""Associations"" are made from embeddings of labeled samples to those of unlabeled ones and back. The optimization schedule encourages correct association cycles that end up at the same class where the association was started from and penalizes wrong associations that end at a different class.
The implementation is easy to use and can be added to any existing end-to-end training setup.
We demonstrate the capabilities of our approach on several data sets and show that it can improve performance on classification tasks up to state of the art, making use of additionally available unlabeled data. We also show how to apply this to the task of domain adaptation, surpassing current state-of-the-art results."
"Training object class detectors with click supervision.  Training object class detectors typically requires a large
set of images with objects annotated by bounding boxes.
However, manually drawing bounding boxes is very time
consuming. In this paper we greatly reduce annotation
time by proposing center-click annotations: we ask anno-
tators to click on the center of an imaginary bounding box
which tightly encloses the object instance. We then incor-
porate these clicks into existing Multiple Instance Learn-
ing techniques for weakly supervised object localization, to
jointly localize object bounding boxes over all training im-
ages. Extensive experiments on PASCAL VOC 2007 and
MS COCO show that: (1) our scheme delivers high-quality
detectors, performing substantially better than those pro-
duced by weakly supervised techniques, with a modest ex-
tra annotation effort; (2) these detectors in fact perform in a
range close to those trained from manually drawn bounding
boxes; (3) as the center-click task is very fast, our scheme
reduces total annotation time by 11?? to 22??."
"Efficient Attention using a Fixed-Size Memory Representation.  The standard content-based attention mechanism typically used in sequence-to-sequence models is computationally expensive as it requires the comparison of large encoder and decoder states at each time step. In this work, we propose an alternative attention mechanism based on a fixed size memory representation that is more efficient. Our technique predicts a compact set of K attention contexts during encoding and lets the decoder compute an efficient lookup that does not need to consult the memory. We show that our approach performs on-par with the standard attention mechanism while yielding inference speedups of 20% for real-world translation tasks and more for tasks with longer sequences. By visualizing attention scores we demonstrate that our models learn distinct, meaningful alignments."
"Oscar: A Practical Page-Permissions-Based Scheme for Thwarting Dangling Pointers.  Using memory after it has been freed opens programs up
to both data and control-flow exploits. Recent work on
temporal memory safety has focused on using explicit
lock-and-key mechanisms (objects are assigned a new
lock upon allocation, and pointers must have the correct
key to be dereferenced) or corrupting the pointer values
upon free(). Placing objects on separate pages and using
page permissions to enforce safety is an older, well-known technique that has been maligned as too slow,
without comprehensive analysis. We show that both old
and new techniques are conceptually instances of lock-and-key, and argue that, in principle, page permissions
should be the most desirable approach. We then validate
this insight experimentally by designing, implementing,
and evaluating Oscar, a new protection scheme based on
page permissions. Unlike prior attempts, Oscar does not
require source code, is compatible with standard and custom memory allocators, and works correctly with programs that fork. Also, Oscar performs favorably ?? often
by more than an order of magnitude ?? compared to recent proposals: overall, it has similar or lower runtime
overhead, and lower memory overhead than competing
systems."
"Bounding the Costs of Quantum Simulation of Many-Body Physics in Real Space.  We present a quantum algorithm for simulating the dynamics of a first-quantized Hamiltonian in real space based on the truncated Taylor series algorithm. We avoid the possibility of singularities by applying various cutoffs to the system and using a high-order finite difference approximation to the kinetic energy operator. We find that our algorithm can simulate ?? interacting particles using a number of calculations of the pairwise interactions that scales, for a fixed spatial grid spacing, as O(??^2), versus the O(??^5)  time required by previous methods (assuming the number of orbitals is proportional to ??), and scales super-polynomially better with the error tolerance than algorithms based on the Lie-Trotter-Suzuki product formula. Finally, we analyze discretization errors that arise from the spatial grid and show that under some circumstances these errors can remove the exponential speedups typically afforded by quantum simulation."
"Incoherent idempotent ambisonics rendering.  We describe a family of ambisonics rendering methods that is based
on optimizing the soundfield component that lies in the null space
of the operator that maps the loudspeaker signals onto the given
ambisonics representation. In contrast to traditional rendering ap-
proaches, the new method avoids the coherent addition of loud-
speaker contributions to the sound field. As a result, it provides
a space-invariant timbre and good spatial directionality outside the
spatial region where the ambisonics soundfield description is ac-
curate. The new method is idempotent at all frequencies and has
relatively low computational complexity. Our experimental results
confirm the effectiveness of the method."
"Joint Wideband Source Localization and Acquisition Based on a Grid-Shift Approach.  This paper addresses the problem of joint wideband localization
and acquisition of acoustic sources. The source locations as well
as acquisition of the original source signals are obtained in a joint
fashion by solving a sparse recovery problem. Spatial sparsity is
enforced by discretizing the acoustic scene into a grid of predefined
dimensions. In practice, energy leakage from the source location to
the neighboring grid points is expected to produce spurious location
estimates, since the source location will not coincide with one of the
grid points. To alleviate this problem we introduce the concept of
grid-shift. A particular source is then near a point on the grid in at
least one of a set of shifted grids. For the selected grid, other sources
will generally not be on a grid point, but their energy is distributed
over many points. A large number of experiments on real speech
signals show the localization and acquisition effectiveness of the
proposed approach under clean, noisy and reverberant conditions"
"Inferring Software Component Interaction Dependency for Adaptation Support.  A self-managing software system should be able to monitor and analyze its runtime behavior and make adaptation decisions accordingly to meet certain desirable objectives. Traditional software adaptation techniques and recent ""models@runtime"" approaches usually require an a priori model for a system's dynamic behavior. Oftentimes the model is difficult to define and labor-intensive to maintain, and tends to get out of date due to adaptation and architecture decay. We propose an alternative approach that does not require defining the system's behavior model beforehand, but instead involves mining software component interactions from system execution traces to build a probabilistic usage model, which is in turn used to analyze, plan, and execute adaptations. In this paper, we demonstrate how such an approach can be realized and effectively used to address a variety of adaptation concerns. In particular, we describe the details of one application of this approach for safely applying dynamic changes to a running software system without creating inconsistencies. We also provide an overview of two other applications of the approach,  identifying potentially malicious (abnormal) behavior for self-protection, and improving deployment of software components in a distributed setting for performance self-optimization. Finally, we report on our experiments with engineering self-management features in an emergency deployment system using the proposed mining approach."
"Towards Zero Shot Frame Semantic Parsing for Domain Scaling.  State-of-the-art slot filling models for goal-oriented human/machine conversational language understanding systems rely on deep learning methods. Multi-task training of such models alleviate the need for in-domain annotated datasets, as they benefit from shared wording, meanings and schema elements across different tasks and domains. However, bootstrapping a semantic parsing model for a new domain using only the semantic frame, such as the back-end API or knowledge graph schema, is still one of the holy grail tasks of language understanding. This paper proposes a deep learning based approach that can utilize only the slot label descriptions in context without the need of any labeled or unlabeled in-domain examples, to quickly bootstrap a new domain. The main idea is using the encoding of the slot names and descriptions within a multi-task deep learning slot filling model, resulting in soft alignments across domains by leveraging implicit transfer learning. Such an approach is promising for solving the domain scaling problem of language understanding models and eliminates dependency on large amounts manually annotated training data sets. Furthermore, our controlled experiments using a multitude of domains show that this approach results in significantly better semantic parsing performance when compared to using only in-domain data."
"Sequential Dialogue Context Modeling for Spoken Language  Understanding.  Spoken Language Understanding (SLU) is a key component of goal oriented dialogue systems that would parse user utterances into semantic frame representations. Traditionally SLU does not utilize the dialogue history beyond the previous system turn and contextual ambiguities are resolved by the downstream components. In this paper, we explore novel approaches for modeling dialogue context in a recurrent neural network (RNN) based language understanding system. We propose the Sequential Dialogue Encoder Network, that allows encoding context from the dialogue history in chronological order. We compare the performance of our proposed architecture with two context models, one that uses just the previous turn context and another that encodes dialogue context in a memory network, but loses the order of utterances in the dialogue history. Experiments with a multi-domain dialogue dataset demonstrate that the proposed architecture results in reduced semantic frame error rates."
"Low-Depth Quantum Simulation of Materials.  Quantum simulation of the electronic structure problem is one of the most researched applications of quantum computing. The majority of quantum algorithms for this problem encode the wavefunction using $N$ molecular orbitals, leading to Hamiltonians with ${\cal O}(N^4)$ second-quantized terms. To avoid this overhead, we introduce basis functions which diagonalize the periodized Coulomb operator, providing Hamiltonians for condensed phase systems with $N^2$ second-quantized terms. Using this representation we can implement single Trotter steps of the Hamiltonians with gate depth of ${\cal O}(N)$ on a planar lattice of qubits -- a quartic improvement over prior methods. Special properties of our basis allow us to apply Trotter based simulations with planar circuit depth in $\widetilde{\cal O}(N^{7/2} / \epsilon^{1/2})$ and Taylor series methods with circuit size $\widetilde{\cal O}(N^{11/3})$, where $\epsilon$ is target precision. Variational algorithms also require significantly fewer measurements to find the mean energy using our representation, ameliorating a primary challenge of that approach. We conclude with a proposal to simulate the uniform electron gas (jellium) using a linear depth variational ansatz realizable on near-term quantum devices with planar connectivity. From these results we identify simulation of low-density jellium as an ideal first target for demonstrating quantum supremacy in electronic structure."
"Strategies for Quantum Computing Molecular Energies Using the Unitary Coupled Cluster Ansatz.  The variational quantum eigensolver (VQE) algorithm combines the ability of quantum computers to efficiently compute expectations values with a classical optimization routine in order to approximate ground state energies of quantum systems. In this paper, we study the application of VQE to the simulation of molecular energies using the unitary coupled cluster (UCC) ansatz. We introduce new strategies to reduce the circuit depth for the implementation of UCC and improve the optimization of the wavefunction based on efficient classical approximations of the cluster amplitudes. Additionally, we propose a method to compute the energy gradient within the VQE approach. We illustrate our methodology with numerical simulations for a system of four hydrogen atoms that exhibit strong correlation and show that the cost of the circuit depth and execution time of VQE using a UCC ansatz can be reduced without introducing significant loss of accuracy in the final wavefunctions and energies."
"Synthesizing Normalized Faces from Facial Identity Features.  We present a method for synthesizing a frontal, neutral-expression image of a person's face given an input face photograph. This is achieved by learning to generate facial landmarks and textures from features extracted from a facial-recognition network. Unlike previous approaches, our encoding feature vector is largely invariant to lighting, pose, and facial expression. Exploiting this invariance, we train our decoder network using only frontal, neutral-expression photographs. Since these photographs are well aligned, we can decompose them into a sparse set of landmark points and aligned texture maps. The decoder then predicts landmarks and textures independently and combines them using a differentiable image warping operation. The resulting images can be used for a number of applications, such as analyzing facial attributes, exposure and white balance adjustment, or creating a 3-D avatar."
"Learning Hierarchical Information Flow with Recurrent Neural Modules.  We propose a deep learning model inspired by neocortical communication via the thalamus. Our model consists of recurrent neural modules that send features via a routing center, endowing the modules with the flexibility to share features over multiple time steps. We show that our model learns to route information hierarchically, processing input data by a chain of modules. We observe common architectures, such as feed forward neural networks and skip connections, emerging as special cases of our architecture, while novel connectivity patterns are learned for the text8 compression task. We demonstrate that our model outperforms standard recurrent neural networks on three sequential benchmarks."
"SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation.  Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 teams, with 17 participating in <em>all  language tracks</em>. We summarize performance and review a selection of well performing methods. Analysis highlights common errors, providing insight into the limitations of existing models. To support ongoing work on semantic representations, the <em>STS Benchmark</em> is introduced as a new shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012-2017)."
"Effectively Building Tera Scale MaxEnt Language Models Incorporating Non-Linguistic Signals.  Maximum Entropy (MaxEnt) Language Models (LMs) are powerful models
that can incorporate linguistic and non-linguistic contextual signals
in a unified framework, by optimizing a convex loss function.
In addition to their flexibility, a key advantage is their scalability,
in terms of model size and the amount of data that can be used during
training. We present the following two contributions to
MaxEnt training: (1) By leveraging smaller amounts of transcribed
data, we demonstrate that a MaxEnt LM trained on various
types of corpora can be easily adapted to better match the test
distribution of speech recognition; (2) A novel adaptive-training approach that efficiently
models multiple types of non-linguistic features in a
universal model. We test the impact of these approaches on Google's state-of-the-art
speech recognizer for the task of voice-search transcription and
dictation. Training 10B parameter models utilizing a corpus
of up to 1T words, we show large reductions in word error
rate from adaptation across multiple languages. Also, human evaluations
show strong significant improvements on a wide range of domains from
using non-linguistic signals. For example, adapting to geographical
domains (e.g., US States and cities) affects about 4% of test
utterances, with 2:1 wins to loss ratio."
"Structural Analysis and Optimal Design of Distributed System Throttlers.  In this paper, we investigate the performance analysis and synthesis of distributed system throttlers (DST). A throttler is a mechanism that limits the flow rate of incoming metrics, e.g., byte per second, network bandwidth usage, capacity, traffic, etc. This can be used to protect a service's backend/clients from getting overloaded, or to reduce the effects of uncertainties in demand for shared services. We study performance deterioration of DSTs subject to demand uncertainty. We then consider network synthesis problems that aim to improve the performance of noisy DSTs via communication link modifications as well as server update cycle modifications."
"A Brief Study of In-Domain Transfer and Learning from Fewer Samples using A Few Simple Priors.  Domain knowledge can often be encoded in the structure of a network, such as convolutional layers for vision, which has been shown to increase generalization and decrease sample complexity, or the number of samples required for successful learning. In this study, we ask whether sample complexity can be reduced for systems where the structure of the domain is unknown beforehand, and the structure and parameters must both be learned from the data. We show that sample complexity reduction through learning structure is possible for at least two simple cases. In studying these cases, we also gain insight into how this might be done for more complex domains."
"Homeless Young People, Jobs, and a Future Vision: Community Members?? Perceptions of the Job Co-op.  We report on an empirical study where neighborhood stakeholders?? views of the Job Co-op future vision were investigated. Taking on the values of a grassroots service agency, while also drawing on emerging practices of peer-to-peer sharing systems, the Job Co-op matches homeless young people, up to age 30, to suitable jobs and job sponsors. Community members were invited to engage the future vison at a streetfair exhibition, comprising a carnival wheel of barriers and solutions to homelessness for beginning conversations; a storyboard for introducing the design vision; and a questionnaire for eliciting feedback. Qualitative analysis of 71 collected questionnaires explicated the socio-technical design space for addressing the problem of youth, homelessness, and jobs. The method demonstrates the use of exhibition design for constructing a design-oriented social context for community engagement, for educational outreach, for disseminating research and design possibilities, for conducting research, and for cooperative design"
"Consistent k-clustering.  We introduce the {\em consistent $k$-clustering} problem, which asks to minimize the number of re-clusterings necessary to maintain a constant approximate solution as points arrive in an online manner. We prove lower bounds of $\Omega(k \log n)$ on the number of clustering changes necessary, and give an algorithm that needs only $O(k^3 \log^5 n)$ changes to maintain a constant competitive solution. This is an exponential improvement on the naive solution of reclustering at every time step. Finally, we show experimentally that our approach performs much better than the theoretical bound, with the number of changes growing approximately as $O(\log n)$"
"Ego-splitting Framework: from Non-Overlapping to Overlapping Clusters.  We propose a new framework called Ego-splitting for detecting
clusters in complex networks which leverage the local structures
known as ego-nets (i.e. the subgraph induced by the neighborhood
of each node) to de-couple overlapping clusters. Ego-splitting is
highly scalable and flexible framework, with provable theoretical
guarantees, that reduce the complex overlapping clustering problem
to a simpler and more amenable non-overlapping (partitioning)
problem. We can solve community detection in graphs with tens
of billions of edges and outperform previous solutions based on
ego-nets analysis.
More precisely, our framework works in two steps: a local ego-
net analysis, and a global graph partitioning. In the local step, we
first partition the nodes?? ego-nets using non-overlapping clustering.
We then use these clusters to split each node of the graph into
its persona nodes that represents the instantiation of the node in
its communities. Then, in the global step, we partition these new
persona nodes to obtain an overlapping clustering of the original
graph."
"TV Impact on Online Searches.  We study the impact of TV advertising on viewers' online search behaviors. In particular, we develop methodologies to estimate the incremental searches that can be causally attributed to TV ad spots, based on Bayesian Structural Time Series (BSTS) models. Simulation studies show that the TV induced incremental search volumes can be accurately estimated in most cases. Our work provides a way of comparing incremental searches from TV v.s. digital. We demonstrate our methods with a case study, and present the insights that can be generated by such analyses."
"A Practical Algorithm for Solving the Incoherence Problem of Topic Models In Industrial Applications.  Topic models are often applied in industrial settings to discover
user profiles from activity logs where documents correspond
to users and words to complex objects such as
web sites and installed apps. Standard topic models ignore
the content-based similarity structure between these objects
largely because of the inability of the Dirichlet prior to capture
such side information of word-word correlation. Several
approaches were proposed to replace the Dirichlet prior
with more expressive alternatives. However, this added expressivity
comes with a heavy premium: inference becomes
intractable and sparsity is lost which renders these alternatives
not suitable for industrial scale applications. In this
paper we take a radically different approach to incorporating
word-word correlation in topic models by applying this
side information at the posterior level rather than at the
prior level. We show that this choice preserves sparsity and
results in a graph-based sampler for LDA whose computational
complexity is asymptotically on bar with the state of
the art Alias base sampler for LDA. We illustrate the
efficacy of our approach over real industrial datasets that
span up to billion of users, tens of millions of words and
thousands of topics. To the best of our knowledge, our approach
provides the first practical and scalable solution to
this important problem"
"Playing together: The importance of joint engagement in the design of technology for children.  In the design of technology for children, many products focus on providing content that is both engaging and appropriate for children at a given age and developmental stage. However, less attention is paid to the context in which children tend to engage with digital products. When focusing on children 13 and younger, context often includes social interaction with parents, caregivers and friends, which provides many opportunities to design digital experiences that support co-play and joint media engagement (JME). In this workshop, we will be discussing real world case studies, as well as theoretical approaches used by researchers, designers, and academics to design technology for children that includes and fosters co-play and joint media engagement experiences. Our discussion will be centered not only on understanding what are co-engagement experiences, but also how these are produced and why these are important for the child end user. The expected outcome of the workshop will be a set of principles, examples, and guidelines for digital media developers to consider when designing rich digital experiences that take into consideration not only the child, but the context for engagement as well."
"Quantum Approach to the Unique Sink Orientation Problem.  We consider quantum algorithms for the unique sink orientation problem on cubes. This problem is widely considered to be of intermediate computational complexity. This is because there is no known polynomial algorithm (classical or quantum) for the problem and yet it arises as part of a series of problems for which it being intractable would imply complexity-theoretic collapses. We give a reduction which proves that if one can efficiently evaluate the kth power of the unique sink orientation outmap, then there exists a polynomial time quantum algorithm for the unique sink orientation problem on cubes."
"How Google Surveys Works.  Google Surveys is a market research platform that surveys internet and smartphone users.
Since its launch in 2012, Google Surveys has evolved in several ways: the maximum questions
per survey has increased from two to 10, the online panel has expanded to tens of
millions of unique daily users, and a new mobile app panel has 4M active users and additional
targeting capabilities. This paper will explain how Google Surveys works as of May 2017,
while also discussing its advantages and limitations for mitigating different kinds of biases."
"Achievement of Sustained Net Plasma Heating in a Fusion Experiment with the Optometrist Algorithm.  Many fields of basic and applied science require efficiently exploring complex systems with high dimensionality. An example of such a challenge is optimising the performance of plasma fusion experiments. The highly-nonlinear and temporally-varying interaction between the plasma, its environment and external controls presents a considerable complexity in these experiments. A further difficulty arises from the fact that there is no single objective metric that fully captures both plasma quality and equipment constraints. To efficiently optimise the system, we develop the Optometrist Algorithm, a stochastic perturbation method combined with human choice. Analogous to getting an eyeglass prescription, the Optometrist Algorithm confronts a human operator with two alternative experimental settings and associated outcomes. A human operator then chooses which experiment produces subjectively better results. This innovative technique led to the discovery of an unexpected record confinement regime with positive net heating power in a field-reversed configuration plasma, characterised by a &gt;50% reduction in the energy loss rate and concomitant increase in ion temperature and total plasma energy."
"Endpoint detection using grid long short-term memory networks for streaming speech recognition.  The task of endpointing is to determine when the user has finished speaking, which is important for interactive speech applications such as voice search and Google Home. In this paper, we propose a GLDNN-based (grid long short-term memory, deep neural network) endpointer model and show that it provides significant improvements over a state-of-the-art CLDNN (convolutional, long short-term memory, deep neural networks) model. Specifically, we replace the convolution layer with a grid LSTM layer that models both spectral and temporal variations through recurrent connections. Results show that the GLDNN achieves 39% relative improvement in false alarm rate at a fixed false reject rate of 2%, and reduces median latency by 11%. We also include detailed experiments investigating why grid LSTMs offer better performance than CLDNNs. Analysis reveals that the recurrent connection along the frequency axis is an important factor that greatly contributes to the performance of grid LSTMs, especially in the presence of background noise. Finally, we also show that multichannel input further increases robustness to background speech. Overall, we achieved 16% (100 ms) endpointer latency improvement relative to our previous best model."
"Modeling DNA Nanodevices Using Graph Rewrite Systems.  DNA based nanostructures and devices are becoming ubiquitous in nanotechnology with rapid advancements in theory and experiments in DNA self-assembly which have led to a myriad of DNA nanodevices. However, the modeling methods used by researchers in the field for design and analysis of DNA nanostructures and nanodevices have not progressed at the same rate. Specifically, there does not exist a formal system that can capture the spectrum of the most frequently intended chemical reactions on DNA nanostructures and nanodevices which have branched and pseudo-knotted structures. In this paper we introduce a graph rewriting system for modeling DNA nanodevices. We define pseudo-DNA nanostructures (PDNs), which describe the sequence information and secondary structure of DNA nanostructures, but exclude modeling of tertiary structures. We define a class of labeled graphs called DNA graphs, that provide a graph theoretic representation of PDNs. We introduce a set of graph rewrite rules that operate on DNA graphs. Our DNA graphs and graph rewrite rules provide a powerful and expressive way to model DNA nanostructures and their reactions. These rewrite rules model most conventional reactions on DNA nanostructures, which include hybridization, dehybridization, base-stacking, and a large family of enzymatic reactions. A subset of these rewrite rules would likely be used for a basic graph rewrite system modeling most DNA devices, which use just DNA hybridization reactions, whereas other of our rewrite rules could be incorporated as needed for DNA devices for example enzymic reactions. To ensure consistency of our systems, we define a subset of DNA graphs which we call well-formed DNA graphs, whose strands have consistent 5' to 3' polarity. We show that if we start with an input set of well-formed DNA graphs, our rewrite rules produce only well-formed DNA graphs."
"TAPAS: Two-pass Approximate Adaptive Sampling for Softmax.  TAPAS is a novel adaptive sampling method for the softmax model. It uses a two pass sampling strategy where the examples used to approximate the gradient of the partition function are first sampled according to a squashed population distribution and then resampled adaptively using the context and current model. We describe an efficient distributed implementation of TAPAS. We show, on both synthetic data and a large real dataset, that TAPAS has low computational overhead and works well for minimizing the rank loss for multi-class classification problems with a very large label space."
"U-District Job Co-op: constructing a future vision for homeless young people and employment.  Purpose ?? Addressing the question, how might socio-technical systems help homeless young people to succeed broadly in employment, the purpose of this paper is to present a future vision, the U-District Job Co-op, where youth take on ??mini-jobs?? offered by neighborhood stakeholders.
Design/methodology/approach ?? Drawing on value sensitive design, design-based, and qualitative research methods, the Job Co-op is explicated by reporting on three linked studies.
Findings ?? First, based on empirical research with varied neighborhood stakeholders, barriers and possible solutions to employment for homeless young people are presented. Second, three design insights for shaping a solution space of socio-technical systems for job search are presented and used analytically to examine six existing systems. Third, findings from a co-design study in which homeless young people expressed their understandings for web-based job services explicate the vision of the Job Co-op.
Social implications ?? This study offers a socio-technical approach, grounded in the neighborhood context, for supporting homeless young people in job search and related activities.
Originality/value ?? The studies reported in this paper demonstrate how methods for information system
design can be used to generate and clarify opportunities for human benefit and for the development of
socio-technical systems that account for human values"
"Analysis of the gift exchange problem.  In the gift exchange game there are n players and n wrapped gifts. When a player??s number is called, that person can either choose one of the remaining wrapped gifts, or can ??steal?? a gift from someone who has already unwrapped it, subject to the restriction that no gift can be stolen more than a total of ?? times.  The problem is to determine the number of ways that the game can be played out, for given values of ?? and n. Formulas and asymptotic expansions are given for these numbers. This work was inspired in part by a 2005 remark by Robert A. Proctor in the On-Line Encyclopedia of Integer Sequences."
"Reliable Launches at Scale.  How do you perform up to 70 product and feature launches per week safely, reliably and reproducibly? Google staffed a dedicated team of Site Reliability Engineers to solve this question: Launch Coordination Engineers work across Google's service space to audit new products and features for reliability, act as liaisons between teams involved in a launch, and be gatekeepers."
"The Many Ways Your Monitoring Is Lying To You.  This talk looks at various failure modes of monitoring systems, with a goal of making readers more aware of the difference between the monitoring system's view of the world and the system itself."
"Building Blocks for  Site Reliability.  How does Google run reliable systems? At the heart of Site Reliability Engineering is the idea of treating reliability as a software problem and and asking software engineers to design an operations function. This talk will examine the organizational, conceptual and technological building blocks that together comprise the concept of site reliability engineering at Google."
"Semantics of Asynchronous JavaScript.  The Node.js runtime has become a major platform for developers 
building cloud, mobile, or IoT applications using JavaScript. Since
the JavaScript language is single threaded, Node.js programs
must make use of asynchronous callbacks and event loops managed
by the runtime to ensure applications remain responsive. While
conceptually simple, this programming model contains numerous
subtleties and behaviors that are defined implicitly by the current
Node.js implementation. This paper presents the first comprehensive
formalization of the Node.js asynchronous execution model
and defines a high-level notion of async-contexts to formalize
fundamental relationships between asynchronous events in
an application. These formalizations provide a foundation
for the construction of static or dynamic program analysis
tools, support the exploration of alternative Node.js event
loop implementations, and provide a high-level conceptual
framework for reasoning about relationships between the
execution of asynchronous callbacks in a Node.js application."
"New bounds on the price of bandit feedback for mistake-bounded online multiclass learning.  We study two generalizations of the
mistake bound model to online multiclass classification.
In the standard model, the learner receives the
correct classification at the end of each round, and
in the bandit model, the learner only finds out whether
its prediction was correct or not.  For a set F of multiclass
classifiers, let opts(F) and optb(F) be the optimal bounds
for learning F according to these two models.  We show that an
optb(F) &lt; (1 + o(1)) (|Y| ln |Y|) opts(F) bound is the best possible 
up to the leading constant, closing a Theta(log |Y|) factor gap."
"A Polynomial-Time Dynamic Programming Algorithm for Phrase-Based Decoding with a Fixed Distortion Limit.  Decoding of phrase-based translation models
in the general case is known to be NP complete,
by a reduction from the traveling
salesman problem (Knight, 1999). In practice,
phrase-based systems often impose a hard
distortion limit that limits the movement of
phrases during translation. However, the impact
on complexity after imposing such a constraint
is not well studied. In this paper, we
describe a dynamic programming algorithm
for phrase-based decoding with a fixed distortion
limit. The runtime of the algorithm is
O(n d! l h^{d+1}) where n is the sentence length,
d is the distortion limit, l is a bound on the
number of phrases starting at any position in
the sentence, and h is related to the maximum
number of target language translations for any
source word. The algorithm makes use of a
novel representation that gives a new perspective
on decoding of phrase-based models."
"Speed and accuracy trade-offs for modern convolutional object detectors.  The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems.  A number of successful systems have been proposed in recent years, but apples-to-apples comparisons are difficult due to different base feature extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms.  We present a unified implementation of the Faster R-CNN~\cite{ren2015faster}, R-FCN~\cite{dai2016r} and SSD~\cite{liu2015ssd} systems, which we view as ``meta-architectures'' and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures.  On one extreme end of this spectrum where speed and memory are critical, we present a detector that runs at over 50 frames per second and can be deployed on a mobile device.  On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the COCO detection task."
"The Applied Pi Calculus: Mobile Values, New Names, and Secure Communication.  We study the interaction of the programming construct ``new'', which generates statically scoped names, with communication via messages on channels. This interaction is crucial in security protocols, which are the main motivating examples for our work; it also appears in other programming-language contexts. We define the applied pi calculus, a simple, general extension of the pi calculus in which values can be formed from names via the application of built-in functions, subject to equations, and be sent as messages.  (In contrast, the pure pi calculus lacks built-in functions; its only messages are atomic names.) We develop semantics and proof techniques for this extended language and apply them in reasoning about security protocols. This paper essentially subsumes the conference paper that introduced the applied pi calculus in 2001.  It fills gaps, incorporates improvements, and further explains and studies the applied pi calculus.  Since 2001, the applied pi calculus has been the basis for much further work, described in many research publications and sometimes embodied in useful software, such as the tool ProVerif, which relies on the applied pi calculus to support the specification and automatic analysis of security protocols. Although this paper does not aim to be a complete review of the subject, it benefits from that further work and provides better foundations for some of it. In particular, the applied pi calculus has evolved through its implementation in ProVerif, and the present definition reflects that evolution."
"Exploring decision making with Android's runtime permission dialogs using in-context surveys.  A great deal of research on the management of user data on smartphones via permission systems has revealed significant levels of user discomfort, lack of understanding, and lack of attention. The majority of these studies were conducted on Android devices before runtime permission dialogs were widely deployed. In this paper we explore how users make decisions with runtime dialogs on smartphones with Android 6.0 or higher. We employ an experience sampling methodology in order to ask users the reasons influencing their decisions immediately after they decide. We conducted a longitudinal survey with 157 participants over a 6 week period. We explore the grant and denial rates of permissions, overall and on a per permission type basis. Overall, our participants accepted 84% of the permission requests. We observe differences in the denial rates across permissions types; these vary from 23% (for microphone) to 10% (calendar). We find that one of the main reasons for granting or denying a permission request depends on users?? expectation on whether or not an app should need a permission. A common reason for denying permissions is because users know they can change them later. Among the permissions granted, our participants said they were comfortable with 90% of those decisions - indicating that for 10% of grant decisions users may be consenting reluctantly. Interestingly, we found that women deny permissions twice as often as men."
"Local Topic Discovery via Boosted Ensemble of Nonnegative Matrix Factorization.  Nonnegative matrix factorization (NMF) has been increasingly  popular  for  topic  modeling  of  large-scale documents. However, the resulting topics often represent only general, thus redundant information  about  the  data  rather  than  minor,  but  potentially  meaningful information  to users.   To tackle this problem, we propose a novel ensemble model of nonnegative matrix factorization for discovering high-quality local topics. Our method leverages the idea of an ensemble model to successively perform NMF given a residual matrix obtained from previous stages and generates a sequence of topic sets. The novelty of our method lies in the fact that it utilizes the residual matrix inspired by a state-of-the-art gradient boosting model and applies a sophisticated local weighting scheme on the given matrix to enhance the locality of topics, which in turn delivers high-quality, focused topics of interest to users."
"Content-based Related Video Recommendations.  This is a demo of related video recommendations, seeded from random YouTube videos, and based purely on video content signals. Traditional recommendation systems using collaborative filtering (CF) approaches suggest related videos for a given seed based on how many users have watched a particular candidate video right after watching the seed video. This does not take the video content into account but relies on aggregate user behavior. Traditional CF approaches work very well when the seed and the candidate videos are relatively popular ?? they must be watched in a sequence by many users in order for them to be identified as related by the CF system.
In this demo, we focus on the cold-start problem, where either the seed and/or the candidate video are freshly uploaded (or undiscovered) so the CF system cannot identify any related videos for them.  Being able to recommend freshly uploaded videos as well as recommend good related videos for fresh video seeds are important for improving freshness and user engagement. We model this as a video content-based similarity learning  problem,  and  learn  deep  video  embeddings  trained  to  predict  ground-truth  video  relationships (identified by a CF co-watch-based system) but using only visual content. The system does not depend on availability on video metadata or any click information, and can generalize to both popular and tail content, as well as new video uploads. It embeds any new video into a 1024-dimensional representation based on its content and pairwise video similarity is computed simply as a dot product in the embedding space. We show that the learned video embeddings generalize beyond simple visual similarity and are able to capture complex semantic relationships."
"No Fuss Distance Metric Learning using Proxies.  We address the problem of distance metric learning (DML), defined as learning a distance consistent with a notion of semantic similarity. Traditionally, for this problem supervision is expressed in the form of sets of points that follow an ordinal relationship -- an anchor point x is similar to a set of positive points Y, and dissimilar to a set of negative points Z, and a loss defined over these distances is minimized.
While the specifics of the optimization differ, in this work we collectively call this type of supervision Triplets and all methods that follow this pattern Triplet-Based methods. These methods are challenging to optimize. A main issue is the need for finding informative triplets, which is usually achieved by a variety of tricks such as increasing the batch size, hard or semi-hard triplet mining, etc. Even with these tricks, the convergence rate of such methods is slow. In this paper we propose to optimize the triplet loss on a different space of triplets, consisting of an anchor data point and similar and dissimilar proxy points which are learned as well. These proxies approximate the original data points, so that a triplet loss over the proxies is a tight upper bound of the original loss. This proxy-based loss is empirically better behaved. As a result, the proxy-loss improves on state-of-art results for three standard zero-shot learning datasets, by up to 15% points, while converging three times as fast as other triplet-based losses."
"Latent LSTM Allocation: Joint clustering and non-linear dynamic modeling of sequence data.  Recurrent neural network, such as Long-short term memory
(LSTM), are powerful tools for modeling sequential data,
however, they lack interpretability and requires large num-
ber of parameters. On the other hand, topic models, such
as Latent Dirichlet Allocation (LDA), are powerful tools for
uncovering the hidden structure in a document collection,
however, they lack the same strong predictive power as deep
models. In this paper we bridge the gap between such mod-
els and propose Latent LSTM Allocation (LLA). In LLA
each document is modeled as a sequence of words, and the
model jointly groups words into topics and learns the tempo-
ral dynamics over the sequence. Our model is interpretable,
concise and can capture intricate dynamics. We give an ef-
ficient MCMC-EM inference algorithm for our model that
scales to millions of documents. Our experimental evalu-
ations shows that the proposed model compares favorably
with several state-of-the-art baselines."
"Canopy --- Fast Sampling with Cover Trees.  Hierarchical Bayesian models often capture distributions over a very large number of distinct atoms.
The need for this arises when organizing huge amount of unsupervised data, for instance, features extracted using deep convnets can be exploited to organize abundant unlabeled images.
Inference for hierarchical Bayesian models in such cases can be rather nontrivial, leading to approximate approaches.
In this work, we propose a sampler based on Cover Trees that is exact and that has guaranteed runtime logarithmic in the number of atoms and is polynomial in the inherent dimensionality of the underlying parameter space.
In other words, the algorithm is as fast as search over a hierarchical data structure and we demonstrate the effectiveness on both synthetic and real datasets, consisting of over 100 million images."
"Associative Domain Adaptation.  Hierarchical Bayesian models often capture distributions over a very large number of distinct atoms.
The need for this arises when organizing huge amount of unsupervised data, for instance, features extracted using deep convnets can be exploited to organize abundant unlabeled images.
Inference for hierarchical Bayesian models in such cases can be rather nontrivial, leading to approximate approaches.
In this work, we propose a sampler based on Cover Trees that is exact and that has guaranteed runtime logarithmic in the number of atoms and is polynomial in the inherent dimensionality of the underlying parameter space.
In other words, the algorithm is as fast as search over a hierarchical data structure and we demonstrate the effectiveness on both synthetic and real datasets, consisting of over 100 million images."
"Content Sniffing with Comma Chameleon.  MIME type sniffing or content sniffing has led to a new class of web security problems closely related to polyglots: if one partially controls the server response in, e.g., an API call response or a returned document and convinces the browser to treat this response as HTML, then it??s straightforward
XSS. The attacker would be able to impersonate the user in the context of the given domain: if it is hosting a web application, an exploit would be able to read user data and perform arbitrary actions
in the name of the user in the given web application. In other cases, user content might be interpreted
as other (non-HTML) types, and then, instead of XSS, content-sniffing vulnerabilities would be permitted for the exfiltration of cross-domain data?? just as bad. We focus on PDF-based content-sniffing attacks. Our goal is to construct a payload that turns a harmless content injection into passive file formats (e.g., JSON or CSV) into an XSS-equivalent content sniffing vulnerability."
"The power of sparsity in convolutional neural networks.  Deep convolutional networks are well-known for their high computational and memory demands. Given limited resources, how does one design a network that balances its size, training time, and prediction accuracy? A surprisingly effective approach to trade accuracy for size and speed is to simply reduce the number of channels in each convolutional layer by a fixed fraction and retrain the network. In many cases this leads to significantly smaller networks with only minimal changes to accuracy. In this paper, we take a step further by empirically examining a strategy for deactivating connections between filters in convolutional layers in a way that allows us to harvest savings both in run-time and memory for many network architectures. More specifically, we generalize 2D convolution to use a channel-wise sparse connection structure and show that this leads to significantly better results than the baseline approach for large networks including VGG and Inception V3."
"Transliterated mobile keyboard input via weighted finite-state transducers.  We present an extension to a mobile keyboard input decoder based on finite-state transducers that provides general transliteration support, and demonstrate its use for input of South Asian languages using a QWERTY keyboard.  On-device keyboard decoders must operate under strict latency and memory constraints, and we present several transducer optimizations that allow for high accuracy decoding under such constraints.  Our methods yield substantial accuracy improvements and latency reductions over an existing baseline transliteration keyboard approach.  The resulting system was launched for 22 languages in Google Gboard in the first half of 2017."
"Learning from Relatives: Unified Dialectal Arabic Segmentation.  Arabic dialects do not just share a common koine, but there are shared pan-dialectal linguistic phenomena that allow computational models for dialects to learn from each other. In this paper we build a unified segmentation model where the training data for different dialects are combined and a single model is trained. The model yields higher accuracies than dialect-specific models, eliminating the need for dialect identification before segmentation. We also measure the degree of relatedness between four major Arabic dialects by testing how a segmentation model trained on one dialect performs on the other dialects. We found that linguistic relatedness is contingent with geographical proximity. In our experiments we use SVM-based ranking and bi-LSTM-CRF sequence labeling."
"Approximate Linear Programming for Logistic Markov Decision Processes.  Online and mobile interactions with users, in areas such as advertising and product or content
recommendation, have been transformed by machine learning techniques. However, such methods have largely focused on myopic prediction, i.e., predicting immediate user response to system
actions (e.g., ads or recommendations), without explicitly accounting for the long-term impact
on user behavior, nor the potential need for planning action sequences. In this work, we propose
the use of Markov decision processes (MDPs) to formulate the long-term decision problem and
address two key questions that emerge in their application to user interaction. The first focuses on model formulation, specifically, how best to construct MDP models of
user interaction in a way that exploits the great successes of myopic prediction models. To this
end, we propose a new model called logistic MDPs, an MDP formulation that allows the concise specification of transition dynamics. It does so by augmenting the natural factored form of
dynamic Bayesian networks (DBNs) with user response variables that are captured by a logistic
regression model (the latter being precisely the model used for myopic user interaction). The second question we address is how best to solve large logistic MDPs of this type. A
variety of methods have been proposed for solving MDPs that exploit the conditional independence reflected in the DBN representations, including approximate linear programming (ALP).
Despite their compact form, logistic MDPs do not admit the same conditional independence as
DBNs, nor do they satisfy the linearity requirements for standard ALP. We propose a constraint
generation approach to ALP for logistic MDPs that circumvents these problems by: (a) recovering
compactness by conditioning on the logistic response variable; and (b) devising two procedures,
one exact and one approximate, that linearize the search for violated constraints in the master LP.
For the approximation procedure, we also derive error bounds on the quality of the induced policy. We demonstrate the effectiveness of our approach on advertising problems with up to several
thousand sparse binarized features (up to 2^54 and 2^39 actions)."
"A Predictive Model for User Motivation and Utility Implications of Privacy Protection Mechanisms in Location Check-Ins.  Location check-ins contain both geographical and semantic information about the visited venues. Semantic information is usually represented by means of tags (e.g., ??restaurant??). Such data can reveal some personal information about users beyond what they actually expect to disclose, hence their privacy is threatened. To mitigate such threats, several privacy protection techniques based on location generalization have been proposed. Although the privacy implications of such techniques have been extensively studied, the utility implications are mostly unknown. In this paper, we propose a predictive model for quantifying the effect of a privacy-preserving technique (i.e., generalization) on the perceived utility of check-ins. We first study the users?? motivations behind their location check ins, based on a study targeted at Foursquare users (N = 77). We propose a machine-learning method for determining the motivation behind each check-in, and we design a motivation-based predictive model for the utility implications of generalization. Based on the survey data, our results show that the model accurately predicts the fine-grained motivation behind a check-in in 43% of the cases and in 63% of the cases for the coarse-grained motivation. It also predicts, with a mean error of 0.52 (on a scale from 1 to 5), the loss of utility caused by semantic and geographical generalization. This model makes it possible to design of utility-aware, privacy-enhancing mechanisms in location-based online social networks. It also enables service providers to implement location-sharing mechanisms that preserve both the utility and privacy for their users."
"MapReduce for Integer Factorization.  Integer factorization is a very hard computational problem. Currently no efficient algorithm for integer factorization is publicly known. However, this is an important problem on which it relies the security of many real world cryptographic systems. I present an implementation of a fast factorization algorithm on MapReduce. MapReduce is a programming model for high performance applications developed originally at Google. The quadratic sieve algorithm is split into the different MapReduce phases and compared against a standard implementation."
"Distilling Knowledge from Ensembles of Neural Networks for Speech Recognition.  Speech recognition systems that combine multiple types of acoustic models have been shown to outperform single-model systems. However, such systems can be complex to implement and too resource-intensive to use in production. This paper describes how to use knowledge distillation to combine acoustic models in a way that has the best of all worlds: It improves recognition accuracy significantly, can be implemented with standard training tools, and requires no additional complexity during recognition. First, we identify a simple but particularly strong type of ensemble: a late combination of recurrent neural networks with different architectures and training objectives. To harness such an ensemble, we use a variant of standard cross-entropy training to distill it into a single model and then discriminatively fine-tune the result. An evaluation on 2,000-hour large vocabulary tasks in 5 languages shows that the distilled models provide up to 8.9% WER improvement over conventionally-trained baselines, despite having an identical number of parameters."
"Time Series Anomaly Detection: Detection of Anomalous Drops with Limited Features and Sparse Examples in Noisy Periodic Data.  Google uses continuous streams of data from industry partners to deliver accurate results for various products to end users. Unexpected drops in incoming traffic can be an indication of an underlying issue and may be an early warning that remedial action may be necessary. Detecting such drops is non-trivial because streams are variable and noisy, with roughly regular spikes (in many different shapes) in traffic rate. We investigated the question of whether or not we can predict anomalies in these data streams. Our goal is to utilize Machine Learning and statistical approaches to classify anomalous drops in periodic, but noisy, traffic patterns. Since we do not have a large body of labeled examples to directly apply supervised learning for anomaly classification, we approached the problem in two parts. First we used TensorFlow to train our various models including DNNs, RNNs, and LSTMs to perform regression and predict the expected value in the time series. Second we created anomaly detection rules that compared the actual values to predicted values. Since the problem requires finding sustained anomalies, rather than just short delays or momentary inactivity in the data, our two detection methods focused on continuous sections of activity rather than individual data points. We tried multiple combinations of our models and rules and found that using the intersection of our two anomaly detection methods proved to be an effective method of detecting anomalies on almost all of our models. In the process we also found that not all data fell within our experimental assumptions, as one data stream had no periodicity, and therefore no time based model could predict it."
The Calculus of Service Availability.  You're only as available as the sum of your dependencies.
"Jupiter Rising: A Decade of Clos Topologies and Centralized Control in Google's Datacenter Network.  We present our approach for overcoming the cost, operational complexity, and limited scale endemic to datacenter networks a decade ago. Three themes unify the five generations of datacenter networks detailed in this paper. First, multi-stage Clos topologies built from commodity switch silicon can support cost-effective deployment of building-scale networks. Second, much of the general, but complex, decentralized network routing and management protocols supporting arbitrary deployment scenarios were overkill for single-operator, pre-planned datacenter networks. We built a centralized control mechanism based on a global configuration pushed to all datacenter switches. Third, modular hardware design coupled with simple, robust software allowed our design to also support inter-cluster and wide-area networks. Our datacenter networks run at dozens of sites across the planet, scaling in capacity by 100x over 10 years to more than 1 Pbps of bisection bandwidth."
"Datacenter Interconnect and Networking: From Evolution to Holistic Revolution.  In this presentation, we will review the evolution of Google??s intra-datacenter interconnects and networking over the past decade, then outline future technology directions which, along with a more holistic design approach, will be needed to keep pace with the requirements and growth of the datacenter."
"Learning Discriminative and Transformation Covariant Local Feature Detectors.  Robust covariant local feature detectors are important for detecting local features that are (1) discriminative of the image content and (2) can be repeatably detected at consistent locations when the image undergoes diverse transformations. Such detectors are critical for applications such as image search and scene reconstruction. Many learning-based local feature detectors address one of these two problems while overlooking the other. In this work, we propose a novel learning-based method to simultaneously address both issues. Specifically, we extend the previously proposed covariant constraint by defining the concepts of ??standard patch?? and ??canonical feature?? and leverage these to train a novel robust covariant detector. We show that the introduction of these concepts greatly simplifies the learning stage of the covariant detector, and also makes the detector much more robust. Extensive experiments show that our method outperforms previous handcrafted and learning-based detectors by large margins in terms of repeatability."
"Analyzing energy technologies and policies using DOSCOE.  Low-carbon electricity technologies are often evaluated by their Levelized
Cost of Energy (LCOE). However, LCOE cannot model the impact of one
electricity source on the value of others. In previous work, System LCOE
was proposed to estimate the costs of integrating an intermittent source
into a grid consisting of multiple dispatchable electricity sources. Using a new DOSCOE (Dispatch-optimized system cost of electricity) model,
we generalize System LCOE. DOSCOE can handle any mixture of dispatchable
and non-dispatchable sources. It can analyze systems which contain storage,
have legacy infrastructure, or have imposed policies. DOSCOE thus updates
System LCOE to be applicable to more realistic electricity grid models.
DOSCOE uses a linear program to find the capacity and generation mix which
yields minimum LCOE. Running this linear program multiple times yields
System LCOE curves. DOSCOE shows that to cost-effectively remove the last 10-20% of fossil
fuels requires a moderate price on carbon and either low-cost nuclear power
or carbon capture and sequestration. Alternatively, a hypothetical zero-carbon
source needs to have a net present cost less than $2200/kW to displace existing
fossil-fuel plants."
"Cyber, Nano, and AGI Risks: Decentralized Approaches to Reducing Risks.  The aim of this paper, rather than attempting to present one coherent strategy for reducing existential risks, is to introduce a variety of possible options with the goal of broadening the discussion and inviting further investigation. Two themes appear throughout: (1) the proposed approaches for risk reduction attempt to avoid the dangers of centralized ??solutions,?? and (2) cybersecurity is not treated as a separate risk. Instead, trustworthy cybersecurity is a prerequisite for the success of our proposed approaches to risk reduction."
"Eyemotion: Classifying facial expressions in VR using eye-tracking cameras.  One of the main challenges of social interaction in virtual reality settings is that head-mounted displays occlude a large portion of the face, blocking facial expressions and thereby restricting social engagement cues among users. Hence, auxiliary means of sensing and conveying these expressions are needed. We present an algorithm to automatically infer expressions by analyzing only a partially occluded face while the user is engaged in a virtual reality experience. Specifically, we show that images of the user's eyes captured from an IR gaze-tracking camera within a VR headset are sufficient to infer a select subset of facial expressions without the use of any fixed external camera. Using these inferences, we can generate dynamic avatars in real-time which function as an expressive surrogate for the user. We propose a novel data collection pipeline as well as a novel approach for increasing CNN accuracy via personalization. Our results show a mean accuracy of 74% (F1 of 0.73) among 5 `emotive' expressions and a mean accuracy of 70% (F1 of 0.68) among 10 distinct facial action units, outperforming human raters."
"Monotonic Calibrated Interpolated Look-Up Tables.  Real-world machine learning applications may require functions to be interpretable and fast to evaluate, in addition to accurate. In particular, guaranteed monotonicity of the learned function can be critical to user trust.   We propose meeting these three goals for low-dimensional machine learning problems by learning flexible, monotonic functions using calibrated interpolated look-up tables.  We extend the structural risk minimization framework
of lattice regression to  train  monotonic look-up tables by solving a convex prob-
lem with appropriate linear inequality constraints. In addition, we propose jointly learning interpretable calibrations of each feature to normalize continuous features and handle categorical or missing data, though this changes the optimization problem to be non-convex.  We address large-scale learning through parallelization, mini-batching, and propose random sampling of additive regularizer terms.  Experiments on seven real-world problems with five to sixteen features and thousands to millions of training samples show the proposed monotonic functions can achieve state-of-the-art accuracy on practical problems while providing greater transparency to users."
"Improving Phenotypic Measurements in High-Content Imaging Screens.  Image-based screening is a powerful technique to reveal how chemical, genetic, and environmental perturbations affect cellular state. Its potential is restricted by the current analysis algorithms that target a small number of cellular phenotypes and rely on expert-engineered image features. Newer algorithms that learn how to represent an image are limited by the small amount of labeled data for ground-truth, a common problem for scientific projects. We demonstrate a sensitive and robust method for distinguishing cellular phenotypes that requires no additional ground-truth data or training. It achieves state-of-the-art performance classifying drugs by similar molecular mechanism, using a Deep Metric Network that has been pre-trained on consumer images and a transformation that improves sensitivity to biological variation. However, our method is not limited to classification into predefined categories. It provides a continuous measure of the similarity between cellular phenotypes that can also detect subtle differences such as from increasing dose. The rich, biologically-meaningful image representation that our method provides can help therapy development by supporting high-throughput investigations, even exploratory ones, with more sophisticated and disease-relevant models."
"End-to-End Training of Acoustic Models for Large Vocabulary Continuous Speech Recognition with TensorFlow.  This article discusses strategies for end-to-end training of state-
of-the-art acoustic models for Large Vocabulary Continuous
Speech Recognition (LVCSR), with the goal of leveraging Ten-
sorFlow components so as to make efficient use of large-scale
training sets, large model sizes, and high-speed computation
units such as Graphical Processing Units (GPUs). Benchmarks
are presented that evaluate the efficiency of different approaches
to batching of training data, unrolling of recurrent acoustic
models, and device placement of TensorFlow variables and op-
erations. An overall training architecture developed in light of
those findings is then described. The approach makes it possi-
ble to take advantage of both data parallelism and high speed
computation on GPU for state-of-the-art sequence training of
acoustic models. The effectiveness of the design is evaluated
for different training schemes and model sizes, on a 20, 000
hour Voice Search task."
"Data Decisions and Theoretical Implications when Adversarially Learning Fair Representations.  How can we learn classifier that is ``fair'' for a protected or sensitive group, when we do not know if the input to the classifier affects the protected group?  How can we train such a classifier when data on the protected group is difficult to attain?  In many settings, finding out the sensitive input attribute can be prohibitively expensive even during model training, and possibly impossible during model serving.  For example, in recommender systems, if we want to predict if a user will click on a given recommendation, we often do not know many attributes of the user, e.g., race or age, and many attributes of the content are hard to determine, e.g., the language or topic.  Thus, it is not feasible to use a different classifier calibrated based on knowledge of the sensitive attribute. Here, we use an adversarial training procedure to remove information about the sensitive attribute from the latent representation learned by a neural network.  In particular, we study how the choice of data for the adversarial training effects the resulting fairness properties.  We find two interesting results: a remarkably small amount of data is needed to train these models, and there is still a gap between the theoretical implications and the empirical results."
"Context-aware Captions from Context-agnostic Supervision.  We describe a model to induce discriminative image captions based only on generative ground-truth training data. For example, given images and descriptions of ??zebras?? and ??horses??, our system can generate discriminative language that describes the zebra images while capturing the differences  with the ??horse?? images . Producing discriminative language is a foundational problem in the study of pragmatic behavior: Humans can effortlessly repurpose language for being persuasive and effective in communication. We first propose a novel inference procedure based on a reflex speaker and an introspector to induce discrimination between concepts. Intuitively, the reflex speaker models a good utterance for some concept (??zebra??), while the introspector models how discriminative the sentence is between the concepts (??zebra?? and ??horse??). Unlike previous approaches, the form of our listener has the attractive property of being amenable to joint approximate inference to select utterances that satisfy both the speaker and the introspector, yielding an introspective speaker. We apply our introspective speaker to the CUB-Text dataset to describe why an image contains a particular bird category as opposed to some other closely related bird category and to the MS COCO dataset to generate language that points to one out two semantically similar images. Evaluations with discriminative ground truth collected on CUB and with humans on MSCOCO reveal that our approach outperforms baseline approaches for discrimination. We then draw qualitative insights from our model outputs which suggest that in some cases one may interpret the introspective speaker outputs to be lies in service of the higher goal of discrimination."
"On expression patterns and developmental origin of human brain regions.  Anatomical substructures of the human brain have characteristic cell-types, connectivity and local circuitry, which are reflected in area-specific transcriptome signatures, but the principles governing area-specific transcription and their relation to brain development are still being studied. In adult rodents, areal transcriptome patterns agree with the embryonic origin of brain regions, but the processes and genes that preserve an embryonic signature in regional expression profiles were not quantified. Furthermore, it is not clear how embryonic-origin signatures of adult-brain expression interplay with changes in expression patterns during development. Here we first quantify which genes have regional expression-patterns related to the developmental origin of brain regions, using genome-wide mRNA expression from post-mortem adult human brains. We find that almost all human genes (92%) exhibit an expression pattern that agrees with developmental brain-region ontology, but that this agreement changes at multiple phases during development. Agreement is particularly strong in neuron-specific genes, but also in genes that are not spatially correlated with neuron-specific or glia-specific markers. Surprisingly, agreement is also stronger in early-evolved genes. We further find that pairs of similar genes having high agreement to developmental region ontology tend to be more strongly correlated or anti-correlated, and that the strength of spatial correlation changes more strongly in gene pairs with stronger embryonic signatures. These results suggest that transcription regulation of most genes in the adult human brain is spatially tuned in a way that changes through life, but in agreement with development-determined brain regions."
"Group online adaptive learning.  Sharing information among multiple learning agents can accelerate learning. It
could be particularly useful if learners operate in continuously changing environments, because
a learner could benefit from previous experience of another learner to adapt to their
new environment. Such group-adaptive learning has numerous applications, from predicting
financial time-series, through content recommendation systems, to visual understanding for
adaptive autonomous agents. Here we address the problem in the context of online adaptive learning. We formally
define the learning settings of group online adaptive learning (GOAL) and derive an algorithm
(SOAL) to address it. SOAL avoids modeling explicitly or their dynamics, and instead
shares information continuously. The key idea is that learners share a common small pool of
experts, which they can use in a weighted adaptive way.We define group adaptive regret and
prove that SOAL maintains known bounds on the adaptive regret obtained for single adaptive
learners. Furthermore, it quickly adapts when presented with scenes that are related to
scenes shared by other learners. We demonstrate the benefits of the approach for two domains: vision and text. First,
in the visual domain, we study a visual navigation task where a robot learns to navigate
based on outdoor video scenes. We show how navigation can improve when knowledge
from other robots in related scenes is available. Second, in the text domain, we create a new
dataset for the task of assigning submitted papers to relevant editors. This is, inherently,
an adaptive learning task due to the dynamic nature of research fields evolving in time.
We show how learning to assign editors improves when knowledge from other editors is
available. Together, these results demonstrate the benefits for sharing information across
learners in concurrently changing environments."
"Learning From Noisy Large-Scale Datasets With Minimal Supervision.  We present an approach to effectively utilize small sets of reliable labels in conjunction with massive datasets of noisy labels to learn powerful image representations. A common approach is to pre-train a network using the large set of noisy labels and fine-tune it using the clean labels. We present an alternative: we use the clean labels to captures the structure in the label space and learn a mapping
between noisy and clean labels. This allows to ??clean the dataset??, and fine-tune the network using both the clean labels and the full dataset with reduced noise. The approach comprises a multi-task network that jointly learns to clean noisy labels and to annotate images with accurate labels. We evaluate our approach using the recently released Open Images dataset, containing ??? 9 million images with multiple annotations per image. Our results demonstrate that the proposed approach outperforms fine-tuning across all major groups of labels in the Open Image dataset. The approach is particularly effective on the large number of labels with 20-80% label noise."
"A Generic Coordinate Descent Framework for Learning from Implicit Feedback.  In recent years, interest in recommender research has shifted from explicit feedback towards implicit feedback data. A diversity of complex models has been proposed for a wide variety of applications. Despite this, learning from implicit feedback is still computationally challenging. So far, most work relies on stochastic gradient descent (SGD) solvers which are easy to derive, but in practice challenging to apply, especially for tasks with many items. For the simple matrix factorization model, an efficient coordinate descent (CD) solver has been previously proposed. However, efficient CD approaches have not been derived for more complex models. In this paper, we provide a new framework for deriving efficient CD algorithms for complex recommender models. We identify and introduce the property of k-separable models. We show that k-separability is a sufficient property to allow efficient optimization of implicit recommender problems with CD. We illustrate this framework on a variety of state-of-the-art models including factorization machines and Tucker decomposition. To summarize, our work provides the theory and building blocks to derive efficient implicit CD algorithms for complex recommender models."
"Understanding the Mirai Botnet.  The Mirai botnet, composed primarily of embedded
and IoT devices, took the Internet by storm in late 2016
when it overwhelmed several high-profile targets with
massive distributed denial-of-service (DDoS) attacks. In
this paper, we provide a seven-month retrospective analysis
of Mirai??s growth to a peak of 600k infections and
a history of its DDoS victims. By combining a variety
of measurement perspectives, we analyze how the botnet
emerged, what classes of devices were affected, and
how Mirai variants evolved and competed for vulnerable
hosts. Our measurements serve as a lens into the fragile
ecosystem of IoT devices. We argue that Mirai may represent
a sea change in the evolutionary development of
botnets??the simplicity through which devices were infected
and its precipitous growth, demonstrate that novice
malicious techniques can compromise enough low-end
devices to threaten even some of the best-defended targets.
To address this risk, we recommend technical and nontechnical
interventions, as well as propose future research
directions."
"Do-It-Yourself Lighting Design for Product Videography.  The growth of online marketplaces for selling goods has increased
the need for product photography by novice users and consumers.
Additionally, the increased use of online media and large-screen
billboards has increased the adoption of videos and animated gifs
for advertising, going beyond just using still imagery. Product
videography is a fledgling field that is gaining traction in online
media. The key distinction between professional product videography and
photography and regular home-brewed consumer efforts is often the
lighting. Professional photographers use specialized hardware and
studio setups, and bring expert knowledge and skills to create good
lighting that shows off the product??s shape and material, while also
producing aesthetically pleasing results. In this paper, we introduce a new do-it-yourself (DIY) approach
to lighting design to enable novice users to produce studio quality
lighting for product photography and videography. By studying
state-of-the-art product videography, we identify design principles
used to light products through emphasizing highlights, rim lighting,
and contours. We devise a set of computational metrics to achieve
these design goals. Our workflow is: a user acquires video of the
product by simply mounting a video camera on a tripod, and using
a tablet to light objects by waving the tablet around the object. We
automatically analyze and split this acquired video into snippets that
match our design principles. Finally, we present an interface that
lets users easily select snippets with specific characteristics and then
assembles them to produce a final pleasing video of the product."
"Deep Bilateral Learning for Real-Time Image Enhancement.  Performance is a critical challenge in mobile image processing. Given a reference imaging pipeline, or even human-adjusted pairs of images, we seek to reproduce the enhancements and enable real-time evaluation. For this, we introduce a new neural network architecture inspired by bilateral grid processing and local affine color transforms. Using pairs of input/output images, we train a convolutional neural network to predict the coefficients of a locally-affine model in bilateral space. Our architecture learns to make local, global, and content-dependent decisions to approximate the desired image transformation. At runtime, the neural network consumes a low-resolution version of the input image, produces a set of affine transformations in bilateral space, upsamples those transformations in an edge-preserving fashion using a new slicing node, and then applies those upsampled transformations to the full-resolution image. Our algorithm processes high-resolution images on a smartphone in milliseconds, provides a real-time viewfinder at 1080p resolution, and matches the quality of state-of-the-art approximation techniques on a large class of image operators. Unlike previous work, our model is trained off-line from data and therefore does not require access to the original operator at runtime. This allows our model to learn complex, scene-dependent transformations for which no reference implementation is available, such as the photographic edits of a human retoucher."
"The modeling, simulation, and operational control of aerospace communication networks.  A paradigm shift is taking place in aerospace communications. Traditionally, aerospace systems have relied upon circuit switched communications; geostationary communications satellites act as bent-pipe transponders and are not burdened with packet processing and the complexity of mobility in a network topology. But factors such as growing mission complexity (platforms with multiple payloads, multi-hop communication relay requirements, etc.) and NewSpace development practices are driving the rapid adoption of packet-based network protocols in aerospace networks. Meanwhile, several new aerospace networks are being designed to provide either low latency, high-resolution imaging or low-latency Internet access while operating in non-geostationary orbits or even in the upper atmosphere. The need for high data-rate communications in these networks is simultaneously driving greater reliance on beamforming, directionality, and narrow beamwidths in RF communications and free-space optical communications. This dissertation explores the challenges and offers novel solutions in the modeling, simulation, and operational control of these new aerospace networks. In the concept, design, and development phases of such networks, the dissertation motivates the use of network simulators to model network protocols and network application traffic instead of relying solely on static, offline link budget calculations. It also contributes a new approach to network simulation that can integrate with spatial temporal information systems for high-fidelity modeling of time-dynamic geometry, antenna gain patterns, and wireless signal propagation in the physical layer. And towards the operational control of such networks, the dissertation introduces Temporospatial Software Defined Networking (TS-SDN), a new approach that leverages predictability in the propagated motion of platforms and high-fidelity wireless link modeling to build a holistic, predictive view of the accessible network topology and provides SDN applications with the ability to optimize the network topology and routing through the direct expression of network behavior and requirements. A high-level overview of an implementation of Temporospatial SDN at Alphabet is included. The dissertation also describes and demonstrates the benefits of the application of TS-SDN in Low Earth Orbiting (LEO) satellite constellations and High Altitude Platform Systems (HAPS)."
"Inverting Face Embeddings with Convolutional Neural Networks.  Deep neural networks have dramatically advanced the state of the art for many areas of machine learning. Recently they have been shown to have a remarkable ability to generate highly complex visual artifacts such as images and text rather than simply recognize them. In this work we use neural networks to effectively invert low-dimensional face embeddings while producing realistically looking consistent images. Our contribution is twofold, first we show that a gradient ascent style approaches can be used to reproduce consistent images, with a help of a guiding image. Second, we demonstrate that we can train a separate neural network to effectively solve the minimization problem in one pass, and generate images in real-time. We then evaluate the loss imposed by using a neural network instead of the gradient descent by comparing the final values of the minimized loss function."
"152 Simple Steps to Stay Safe Online: Security Advice for Non-Tech-Savvy Users.  Users often don??t follow expert advice for staying secure online, but the reasons for users?? noncompliance are only partly understood. More than 200 security experts were asked for the top three pieces of advice they would give non-tech-savvy users. The results suggest that, although individual experts give thoughtful, reasonable answers, the expert community as a whole lacks consensus."
"On the Protection of Private Information in Machine Learning Systems: Two Recent Approaches.  The recent, remarkable growth of machine learning has led to intense interest in the privacy of the data on which machine learning relies, and to new techniques for preserving privacy. However, older ideas about privacy may well remain valid and useful.  This note reviews two recent works on privacy in the light of the wisdom of some of the early literature, in particular the principles distilled by Saltzer and Schroeder in the 1970s."
"High-temperature high-pressure calorimeter for studying gram-scale heterogeneous chemical reactions.  We present an instrument for measuring pressure changes and heat flows of physical and chemical processes occurring in gram-scale solid samples under high pressures of reactive gases. Operation is demonstrated at 1232 ??C under 33 bars of pure hydrogen. Calorimetric heat flow is inferred using a grey-box non-linear lumped-element heat transfer model of the instrument. Using an electrical calibration heater to deliver 900 J/1 W pulses at the sample position, we demonstrate a dynamic calorimetric power resolution of 50 mW when an 80-s moving average is applied to the signal. Integration of the power signal showed that the 900 J pulse energy could be measured with an average accuracy of 6.35% or better over the temperature range 150-1100 ??C. This instrument is appropriate for the study of high-temperature metal hydride materials for thermochemical energy storage."
"Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.  The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between enormous data and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets."
"TURN TAP: Temporal Unit Regression Networks for Temporal Action Proposals.  Temporal Action Proposal (TAP) generation is an important problem, as fast and accurate extraction of semantically important (e.g. human actions) segments from untrimmed videos is an important step for large-scale video analysis. We propose a novel Temporal Unit Regression Network (TURN) model. There are two salient aspects of TURN: (1) TURN jointly predicts action proposals and refines the temporal boundaries by temporal coordinate regression; (2) Fast computation is enabled by unit feature reuse: a long untrimmed video is decomposed into video units, which are reused as basic building blocks of temporal proposals. TURN outperforms the state-of-the-art methods under average recall (AR) by a large margin on THUMOS-14 and ActivityNet datasets, and runs at over 880 frames per second (FPS) on a TITAN X GPU. We further apply TURN as a proposal generation stage for existing temporal action localization pipelines, it outperforms state-of-the-art performance on THUMOS-14 and ActivityNet."
"TALL: Temporal Activity Localization via Language Query.  This paper focuses on temporal localization of actions in untrimmed videos. Existing methods typically train classifiers for a pre-defined list of actions and apply them in a sliding window fashion. However, activities in the wild consist of a wide combination of actors, actions and objects; it is difficult to design a proper activity list that meets users' needs. We propose to localize activities by natural language queries. Temporal Activity Localization via Language (TALL) is challenging as it requires: (1) suitable design of text and video representations to allow cross-modal matching of actions and language queries; (2) ability to locate actions accurately given features from sliding windows of limited granularity. We propose a novel Cross-modal Temporal Regression Localizer (CTRL) to jointly model text query and video clips, output alignment scores and action boundary regression results for candidate clips. For evaluation, we adopt TaCoS dataset, and build a new dataset for this task on top of Charades by adding sentence temporal annotations, called Charades-STA. We also build complex sentence queries in Charades-STA for test. Experimental results show that CTRL outperforms previous methods significantly on both datasets."
"Detecting argument selection defects.  Identifier names are often used by developers to convey additional information about the meaning of a program over and above the semantics of the programming language itself. We present an algorithm that uses this information to detect argument selection defects, in which the programmer has chosen the wrong argument to a method call in Java programs. We evaluate our algorithm at Google on 200 million lines of internal code and 10 million lines of predominantly open-source external code and find defects even in large, mature projects such as OpenJDK, ASM, and the MySQL JDBC. The precision and recall of the algorithm vary depending on a sensitivity threshold. Higher thresholds increase precision, giving a true positive rate of 85%, reporting 459 true positives and 78 false positives. Lower thresholds increase recall but lower the true positive rate, reporting 2,060 true positives and 1,207 false positives. We show that this is an order of magnitude improvement on previous approaches. By analyzing the defects found, we are able to quantify best practice advice for API design and show that the probability of an argument selection defect increases markedly when methods have more than five arguments."
"Credit-Scheduled Delay-Bounded Congestion Control for Datacenters.  Small RTTs (???10s of usec), bursty flow arrivals, and a large number of concurrent flows (1,000s) in datacenters bring fundamental challenges to congestion control as they either force a flow to send at most one packet per RTT or induce a large queue build-up. The widespread use of shallow buffered switches also makes the problem more challenging with hosts generating many flows in bursts. In addition, as link speeds increase, algorithms that gradually probe for bandwidth take a long time to reach the fair-share. An ideal datacenter congestion control must provide 1) zero data loss, 2) fast convergence, 3) low buffer occupancy, and 4) high utilization. However, these requirements present conflicting goals. This paper presents a new radical approach, called ExpressPass, an end-to-end credit-scheduled, delay-bounded congestion control for datacenters. ExpressPass uses credit packets to control congestion even before sending data packets, which enables us to achieve bounded delay and fast convergence. It gracefully handles bursty flow arrivals. We implement ExpressPass using commodity switches and provide evaluations using testbed experiments and simulations. ExpressPass converges up to 80 times faster than DCTCP in 10 Gbps links, and the gap increases as link speeds become faster. It greatly improves performance under heavy incast workloads and significantly reduces the flow completion times, especially, for small and medium size flows compared to DCQCN, DCTCP, HULL, and DX under realistic workloads."
"Feature Learning with Raw-Waveform CLDNNs for Voice Activity Detection.  Voice Activity Detection (VAD) is an important preprocessing
step in any state-of-the-art speech recognition system.
Choosing the right set of features and model architecture can
be challenging and is an active area of research. In this paper
we propose a novel approach to VAD to tackle both feature
and model selection jointly. The proposed method is based
on a CLDNN (Convolutional, Long Short-Term Memory, Deep
Neural Networks) architecture fed directly with the raw waveform.
We show that using the raw waveform allows the neural
network to learn features directly for the task at hand, which is
more powerful than using log-mel features, specially for noisy
environments. In addition, using a CLDNN, which takes advantage
of both frequency modeling with the CNN and temporal
modeling with LSTM, is a much better model for VAD compared
to the DNN. The proposed system achieves over 78% relative
improvement in False Alarms (FA) at the operating point
of 2% False Rejects (FR) on both clean and noisy conditions
compared to a DNN of comparable size trained with log-mel
features. In addition, we study the impact of the model size
and the learned features to provide a better understanding of the
proposed architecture"
"Spatially Adaptive Computation Time for Residual Networks.  This paper proposes a deep learning architecture based on Residual Network that dynamically adjusts the number of executed layers for the regions of the image. This architecture is end-to-end trainable, deterministic and problem-agnostic. It is therefore applicable without any modifications to a wide range of computer vision problems such as image classification, object detection and image segmentation. We present experimental results showing that this model improves the computational efficiency of ResNet on the challenging ImageNet classification and COCO object detection datasets. Additionally, we evaluate the computation time maps on the image saliency dataset cat2000 and find that they correlate surprisingly well with human eye fixation positions."
"A Light Touch for Heavily Constrained SGD.  Minimizing empirical risk subject to a set of constraints can be a useful strategy for learning restricted
classes of functions, such as monotonic functions, submodular functions, classifiers that
guarantee a certain class label for some subset of examples, etc. However, these restrictions may
result in a very large number of constraints. Projected stochastic gradient descent (SGD) is often
the default choice for large-scale optimization in machine learning, but requires a projection after
each update. For heavily-constrained objectives, we propose an efficient extension of SGD that
stays close to the feasible region while only applying constraints probabilistically at each iteration.
Theoretical analysis shows a compelling trade-off between per-iteration work and the number of
iterations needed on problems with a large number of constraints."
"Fast and Flexible Monotonic Functions with Ensembles of Lattices.  In many real-world machine learning problems, there are some inputs that are known should be positively (or negatively) related to the output, and in such cases constraining the trained model to respect that monotonic relationship can provide regularization, and makes the model more interpretable. However, flexible monotonic functions are computationally challenging to learn beyond a few features. We break through this barrier by learning ensembles of monotonic calibrated look-up tables (lattices). A key contribution is an automated algorithm for selecting feature subsets for the ensemble base models.  We demonstrate that compared to random forests, these ensembles produce similar or better accuracy, while providing guaranteed monotonicity consistent with prior knowledge, smaller model size and faster evaluation."
"Satisfying Real-world Goals with Dataset Constraints.  The goal of minimizing misclassification error on a training set is often just one of several real-world goals that might be defined on different datasets. For example, one may require a classifier to also make positive predictions at some specified rate for some subpopulation (fairness), or to achieve a specified empirical recall. Other real-world goals include reducing churn with respect to a previously deployed model, or stabilizing online training. In this paper we propose handling multiple goals on multiple datasets by training with dataset constraints, using the ramp penalty to accurately quantify costs, and present an efficient algorithm to approximately optimize the resulting non-convex constrained optimization problem. Experiments on both benchmark and real-world industry datasets demonstrate the effectiveness of our approach."
"Launch and Iterate: Reducing Prediction Churn.  Practical applications of machine learning often involve successive training iterations with ever improving features and increasing training examples. Ideally, changes in the output of any new model should only be improvements (wins) over the previous iteration, but in practice the predictions may change neutrally for many examples, resulting in extra net-zero wins and losses, that we refer to as churn. These changes in the predictions are problematic for usability for some applications, and make it harder to measure if a change is statistically significant positive. In this paper, we formulate the problem and present a stabilization operator to regularize a classifier towards a previous classifier. We use a Markov chain Monte Carlo stabilization operator to produce a model with more consistent predictions but without degrading accuracy. We investigate the properties of the proposal with theoretical analysis.  Experiments on benchmark datasets for three different classification algorithms demonstrate the method and the range of churn reduction it can provide."
"Deep Lattice Networks and Partial Monotonic Functions.  We propose learning deep models that are monotonic with respect to a user
specified set of inputs by alternating layers of linear embeddings, ensembles of
lattices, and calibrators (piecewise linear functions), with appropriate constraints
for monotonicity, and jointly training the resulting network. We implement the
layers and projections with new computational graph nodes in TensorFlow and use
the ADAM optimizer and batched stochastic gradients. Experiments on benchmark
and real-world datasets show that six-layer monotonic deep lattice networks achieve
state-of-the art performance for classification and regression with monotonicity
guarantees."
"End-to-End Learning of Semantic Grasping.  We consider the task of semantic robotic grasping, in which a robot picks up an object of a user-specified class using only monocular images. Inspired by the two-stream hypothesis of visual reasoning, we present a semantic grasping framework that learns object detection, classification, and grasp planning in an end-to-end fashion. A <code>ventral stream'' recognizes object class while a</code>dorsal stream'' simultaneously interprets the geometric relationships necessary to execute successful grasps. We leverage the autonomous data collection capabilities of robots to obtain a large self-supervised dataset for training the dorsal stream, and use semi-supervised label propagation to train the ventral stream with only a modest amount of human supervision. We experimentally show that our approach exhibits an improvement in accuracy over grasping systems whose components are not learned end-to-end, including a baseline method that uses bounding box detection. Furthermore, we show that jointly training our model with auxiliary data consisting of non-semantic grasping data, as well as semantically labeled images without grasp actions, has the potential to substantially improve semantic grasping performance."
"The Kinetics Human Action Video Dataset.  We describe the DeepMind Kinetics human action video dataset.   The  dataset  contains  400  human  action  classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions  are  human  focussed  and  cover  a  broad  range  of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands.  We describe the statistics of the dataset, how it was collected, and give some baseline performance figures for neural network architectures trained and tested for human action classification on this dataset."
"AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions.  This paper introduces a video dataset of spatio-temporally localized Atomic Visual Actions (AVA). The AVA dataset densely annotates 80 atomic visual actions in 192 15-minute video clips, where actions are localized in space and time, resulting in 740k action labels with multiple labels per person occurring frequently. The key characteristics of our dataset are: (1) the definition of atomic visual actions, rather than composite actions; (2) precise spatio-temporal annotations with possibly multiple annotations for each person; (3) exhaustive annotation of these atomic actions over 15-minute video clips; (4) people temporally linked across consecutive segments; and (5) using movies to gather a varied set of action representations. This departs from existing datasets for spatio-temporal action recognition, which typically provide sparse annotations for composite actions in short video clips. We will release the dataset publicly. AVA, with its realistic scene and action complexity, exposes the intrinsic difficulty of action recognition. To benchmark this, we present a novel approach for action localization that builds upon the current state-of-the-art methods, and demonstrates better performance on JHMDB and UCF101-24 categories. While setting a new state of the art on existing datasets, the overall results on AVA are low at 16.2% mAP, underscoring the need for developing new approaches for video understanding."
"Extreme clicking for efficient object annotation.  Manually annotating object bounding boxes is central to building computer vision datasets, and it is very time consuming (annotating ILSVRC [53] took 35s for one high-quality box [62]). It involves clicking on imaginary corners of a tight box around the object. This is difficult as these corners are often outside the actual object and several adjustments are required to obtain a tight box. We propose extreme clicking instead: we ask the annotator to click on four physical points on the object: the top, bottom, left- and right-most points. This task is more natural and these points are easy to find. We crowd-source extreme point annotations for PASCAL VOC 2007 and 2012 and show that (1) annotation time is only 7s per box, 5x faster than the traditional way of drawing boxes [62]; (2) the quality of the boxes is as good as the original ground-truth drawn the traditional way; (3) detectors trained on our annotations are as accurate as those trained on the original ground-truth. Moreover, our extreme clicking strategy not only yields box coordinates, but also four accurate boundary points. We show (4) how to incorporate them into GrabCut to obtain more accurate segmentations than those delivered when initializing it from bounding boxes; (5) semantic segmentations models trained on these segmentations outperform those trained on segmentations derived from bounding boxes."
"Discovering the physical parts of an articulated object class from multiple videos.  We propose a method to discover the physical parts of an articulated object class (e.g. tiger, horse) from multiple videos. Since the individual parts of an object can move independently of one another, we discover them as object regions that consistently move relatively with respect to the rest of the object  across videos. We then learn a location model of the parts and segment them accurately in the individual videos using an energy function that also enforces temporal and spatial consistency in the motion of the parts. Traditional methods for motion segmentation or non-rigid structure from motion cannot discover parts unless they display independent motion, since they operate on one video at a time. Our method overcomes this problem by discovering the parts across videos, which allows to discover them in videos where they move to videos where they do not.
We evaluate our method on a new dataset of 32 videos of tigers and horses, where we significantly outperform state-of-the art motion segmentation on the task of part discovery (roughly twice the accuracy)."
"The Devil is in the Decoders.  Many machine vision applications require predictions for every pixel of the input image (for example semantic segmentation, boundary detection). Models for such problems usually consist of encoders which decreases spatial resolution while learning a high-dimensional representation, followed by decoders who recover the original input resolution and result in low-dimensional predictions. While encoders have been studied rigorously, relatively few studies address the decoder side. Therefore this paper presents an extensive comparison of a variety of decoders for a variety of pixel-wise prediction tasks. Our contributions are: (1) Decoders matter: we observe significant variance in results between different types of decoders on various problems. (2) We introduce a novel decoder: bilinear additive upsampling. (3) We introduce new residual-like connections for decoders. (4) We identify two decoder types which give a consistently high performance."
A more general method for pronunciation learning.  Automatic speech recognition relies on pronunciation dictionaries for accurate results and previous work used pronunciation learning algorithms to build them. Efficient algorithms must balance having the ability to learn varied pronunciations while being constrained enough to be robust. Our approach extends one of such algorithms \cite{Kou2015} by replacing a finite state transducer (FST) built from a limited-size candidate list with a general and flexible FST building mechanism. This architecture can accommodate a wide variety of pronunciation predictions and can also learn pronunciations without having the written form. It can also use an FST built from a recursive neural network (RNN) and tune the importance given to the written form. The new approach reduces the number of incorrect pronunciations learned by up to 25% (relative) on a random sampling of Google voice traffic
"SVCCA: Singular Vector Canonical Correlation Analysis for Deep Understanding and Improvement.  With the continuing empirical successes of deep networks, it becomes increasingly
important to develop better methods for understanding training of models
and the representations learned within. In this paper we propose Singular Vector
Canonical Correlation Analysis (SVCCA), a tool for quickly comparing two representations
in a way that is both invariant to affine transform (allowing comparison
between different layers and networks) and fast to compute (allowing more
comparisons to be calculated than with previous methods). We deploy this tool
to measure the intrinsic dimensionality of layers, showing in some cases needless
over-parameterization; to probe learning dynamics throughout training, finding
that networks converge to final representations from the bottom up; to show
where class-specific information in networks is formed; and to suggest new training
regimes that simultaneously save computation and overfit less."
"The Unreasonable Effectiveness of Random Orthogonal Embeddings.  We examine a class of embeddings based on structured random matrices with orthogonal rows which can be applied in many machine learning applications including dimensionality reduction and kernel approximation. For both the Johnson-Lindenstrauss transform and the angular kernel, we show that we can select matrices yielding guaranteed improved performance in accuracy and/or speed compared to earlier methods. We introduce matrices with complex entries which give significant further accuracy improvement. We provide geometric and Markov chain-based perspectives to help understand the benefits, and empirical results which suggest that the approach is helpful in a wider range of applications."
"Modulating early visual processing by language.  It is commonly assumed that language refers to high-level visual concepts while
leaving low-level visual processing unaffected. This view dominates the current
literature in computational models for language-vision tasks, where visual and
linguistic input are mostly processed independently before being fused into a single
representation. In this paper, we deviate from this classic pipeline and propose to
modulate the entire visual processing by linguistic input. Specifically, we condition
the batch normalization parameters of a pretrained residual network (ResNet) on a
language embedding. This approach, which we call MOdulated RESnet (MORES),
significantly improves strong baselines on two visual question answering tasks. Our
ablation study shows that modulating from the early stages of the visual processing
is beneficial. We finally show that ResNet image features are effectively grounded."
"Value Prediction Network.  This paper proposes a novel deep reinforcement learning (RL) architecture, called Value Prediction Network (VPN), which integrates model-free and model-based RL methods into a single neural network. In contrast to typical model-based RL methods, VPN learns a dynamics model whose abstract states are trained to make option-conditional predictions of future values (discounted sum of rewards) rather than of future observations. Our experimental results show that VPN has several advantages over both model-free and model-based baselines in a stochastic environment where careful planning is required but building an accurate observation-prediction model is difficult. Furthermore, VPN outperforms Deep Q-Network (DQN) on several Atari games even with short-lookahead planning, demonstrating its potential as a new way of learning a good state representation."
"End-to-End Learning of Semantic Grasping.  We consider the task of semantic robotic grasping, in which a robot picks up an object of a user-specified class using only monocular images. Inspired by the two-stream hypothesis of visual reasoning, we present a semantic grasping framework that learns object detection, classification, and grasp planning in an end-to-end fashion. A ""ventral stream"" recognizes object class while a ""dorsal stream"" simultaneously interprets the geometric relationships necessary to execute successful grasps. We leverage the autonomous data collection capabilities of robots to obtain a large self-supervised dataset for training the dorsal stream, and use semi-supervised label propagation to train the ventral stream with only a modest amount of human supervision. We experimentally show that our approach improves upon grasping systems whose components are not learned end-to-end, including a baseline method that uses bounding box detection. Furthermore, we show that jointly training our model with auxiliary data consisting of non-semantic grasping data, as well as semantically labeled images without grasp actions, has the potential to substantially improve semantic grasping performance."
"Trust-PCL: An Off-Policy Trust Region Method for Continuous Control.  Trust region methods, such as TRPO, are often used to stabilize policy optimization algorithms in reinforcement learning (RL). While current trust region strategies are effective for continuous control, they typically require a prohibitively large amount of on-policy interaction with the environment. To address this problem, we propose an off-policy trust region method, Trust-PCL. The algorithm is the result of observing that the optimal policy and state values of a maximum reward objective with a relative-entropy regularizer satisfy a set of multi-step pathwise consistencies along any path. Thus, Trust-PCL is able to maintain optimization stability while exploiting off-policy data to improve sample efficiency. When evaluated on a number of continuous control tasks, Trust-PCL improves the solution quality and sample efficiency of TRPO."
"A discriminative view of MRF pre-processing algorithms.  While Markov Random Fields (MRFs) are widely used in computer vision, they present a quite challenging inference problem. MRF inference can be accelerated by pre-processing techniques like Dead End Elimination (DEE) or QPBO-based approaches which compute the optimal labeling of a subset of variables. These techniques are guaranteed to never wrongly label a variable but they often leave a large number of variables unlabeled. We address this shortcoming by interpreting pre-processing as a classification problem, which allows us to trade off false positives (i.e., giving a variable an incorrect label) versus false negatives (i.e., failing to label a variable). We describe an efficient discriminative rule that finds optimal solutions for a subset of variables. Our technique provides both per-instance and worst-case guarantees concerning the quality of the solution. Empirical studies were conducted over several benchmark datasets. We obtain a speedup factor of 2 to 12 over expansion moves  without preprocessing, and on difficult non-submodular energy functions produce slightly lower energy."
"A Meta-Learning Perspective on Cold-Start Recommendations for Items.  Matrix factorization (MF) is one of the most popular techniques for product recommendation, but is known to suffer from serious cold-start problems. Item cold-start problems are particularly acute in settings such as Tweet recommendation where new items arrive continuously. In this paper, we present a {\it meta-learning} strategy to address item cold-start when new items arrive continuously. We propose two deep neural network architectures that implement our meta-learning strategy. The first architecture learns a linear classifier whose weights are determined by the item history while the second architecture learns a neural network whose biases are instead adapted based on item history. We evaluate our techniques on the real-world problem of Tweet recommendation. On production data at Twitter, we demonstrate that our proposed techniques significantly beat the MF baseline with lookup table based user embeddings and also outperform the state-of-the-art production model for Tweet recommendation."
"On Blackbox Backpropagation and Jacobian Sensing.  From a small number of calls to a given ??blackbox"""" on random input perturbations,
we show how to efficiently recover its unknown Jacobian, or estimate the left action
of its Jacobian on a given vector. Our methods are based on a novel combination of
compressed sensing and graph coloring techniques, and provably exploit structural
prior knowledge about the Jacobian such as sparsity and symmetry while being
noise robust. We demonstrate efficient backpropagation through noisy blackbox
layers in a deep neural net, improved data-efficiency in the task of linearizing the
dynamics of a rigid body system, and the generic ability to handle a rich class of
input-output dependency structures in Jacobian estimation problems."
"Manifold Regularization for Kernelized LSTD.  Policy evaluation or value/Q-function approximation is a key procedure in reinforcement learning (RL). It is a necessary component of policy iteration and can be used for variance reduction in policy gradient methods. Therefore its quality has a significant impact on most RL algorithms. Motivated by manifold regularized learning, we propose a novel kernelized policy evaluation method that takes advantage of the intrinsic geometry of the state space learned from data, in order to achieve better sample efficiency and higher accuracy in Q-function approximation. Applying the proposed method in the Least-Squares Policy Iteration (LSPI) framework, we observe superior performance compared to widely used parametric basis functions on two standard benchmarks in terms of policy quality."
"Dynamic Routing between Capsules.  A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation paramters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules.  When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule. The final version of the paper is under revision to encorporate reviewers comments."
"A Vendor-Agnostic Root of Trust for Measurement.  We report the success of a project that Google performed as a proof-of-concept for increasing confidence in first-instruction integrity across a variety of server and peripheral environments. We begin by motivating the problem of first-instruction integrity and share the lessons learned from our proof-of-concept implementation. Our goal in sharing this information is to increase industry support and engagement for similar designs. Notable features include a vendor-agnostic capability to interpose on the SPI peripheral bus (from which bootstrap firmware is loaded upon power-on in a wide variety of devices today) without negatively impacting the efficacy of any existing vendor- or device-specific integrity mechanisms, thereby providing additional defense-in-depth."
"New Modeling Method and Design Optimization for a Soft-Switched DC-DC Converter.  High performance cloud computing enables many key future technologies such as artificial intelligence (AI), self-driving vehicle, big data analysis, and internet of things (IoT), using clustered CPU and GPU servers in the datacenter. To improve the power efficiency and the infrastructure flexibility, the computing industry is adopting 54VDC to power the servers in the open compute racks. In this paper, a new modeling technique for a soft-switched DC-DC converter is presented and can be used to guide optimal design in different applications, for example, 54V to point-of-load (PoL) for the new open compute rack. To improve the model accuracy and reduce the complexity, this paper proposes a reduced order linear differential equation (LDE) based modeling technique to discover 1) the tank resonance involving the output inductor; 2) output current ripple and its impact on power efficiency; 3) the proper on-time control for soft switching; 4) unique bleeding mode under the heavy load; 5) output power capability of the converter; 6) the inherent output droop of the converter for phase current sharing and 7) component tolerance analysis and impact on the performance of the converter. With the power loss estimation, design guideline is provided for a reference design and design improvement based on this new modeling technique. Using the proposed method, great accuracy can be expected in the efficiency estimation. Simulation and experimental results are provided to verify the modeling technique in a 54V-1.2V 25A DC-DC converter prototype."
"Large-Scale 3D Chips: Challenges and Solutions for Design Automation, Testing, and Trustworthy Integration.  Three-dimensional (3D) integration of electronic chips has been advocated by both industry and academia
for many years. It is acknowledged as one of the most promising approaches to meet ever-increasing demands on
performance, functionality, and power consumption. Furthermore, 3D integration has been shown to be most effective
and efficient once large-scale integration is targeted for. However, a multitude of challenges has thus far obstructed
the mainstream transition from ??classical 2D chips?? to such large-scale 3D chips. In this paper, we survey all popular
3D integration options available and advocate that using an interposer as system-level integration backbone would
be the most practical for large-scale industrial applications and design reuse. We review major design (automation)
challenges and related promising solutions for interposer-based 3D chips in particular, among the other 3D options.
Thereby we outline (i) the need for a unified workflow, especially once full-custom design is considered, (ii) the current
design-automation solutions and future prospects for both classical (digital) and advanced (heterogeneous) interposer
stacks, (iii) the state-of-art and open challenges for testing of 3D chips, and (iv) the challenges of securing hardware in
general and the prospects for large-scale and trustworthy 3D chips in particular."
"Fast and Highly Scalable Bayesian MDP on a GPU Platform.  By employing Optimal Bayesian Robust (OBR), Bayesian Markov
Decision Process (BMDP) can be a power optimization method to
solve large problems. However, due to the ??curse of dimensionality??,
the data storage limitation hinders the practical application of
BMDP. To overcome this impediment, we propose a novel Improved
Compressed Sparse Row (ICSR) data structure in this paper, and developed
the implementation of BMDP solver with ICSR technique
on a heterogeneous platform with GPU. The simulation results
demonstrate that our techniques achieve about a 5?? reduction in
memory utilization over using full matrix, and an average speedup
of 4.1?? over using full matrix. Additionally, we present a study of
the tradeoff between the runtime and the trends of result difference
between our ICSR techniques and using full matrix."
"Where the Wild Warnings Are: Root Causes of Chrome Certificate Errors.  HTTPS error warnings are supposed to alert browser users to network attacks. Unfortunately, a wide range of non-attack circumstances trigger hundreds of millions of spurious browser warnings per month. Spurious warnings frustrate users, hinder the widespread adoption of HTTPS, and undermine trust in browser warnings. We investigate the root causes of HTTPS error warnings in the field, with the goal of resolving benign errors. We study a sample of over 300 million errors that Google Chrome users encountered in the course of normal browsing. After manually reviewing more than 2,000 error reports, we developed automated rules to classify the top causes of HTTPS error warnings. We are able to automatically diagnose the root causes of two-thirds of error reports. To our surprise, we find that more than half of errors are caused by client-side or network issues instead of server misconfigurations. Based on these findings, we implemented more actionable warnings and other browser changes to address client-side error causes. We further propose solutions for other classes of root causes."
"Neural Symbolic Machines:  Learning Semantic Parsers on Freebase  with Weak Supervision.  Modern semantic parsers, which map natural language utterances to executable logical forms, have been successfully trained over large knowledge bases from weak supervision, but require hand-crafted rules and substantial feature engineering. Recent attempts to train an end-to-end neural network for semantic parsing have either used strong supervision (full logical forms), or have employed synthetic datasets and differentiable operations. In this work, we propose the Boss-Programmer-Computer framework to integrate neural network models with symbolic operations. Within this framework, we introduce Neural Symbolic Machines, in which a sequence-to-sequence neural network ""programmer"" controls a non-differentiable ""computer"" that executes Lisp programs (equivalent to logical forms) and provides code assistance. The interaction between the ""programmer"" and ""computer"" dramatically reduces the search space and effectively learns the semantic parser from weak supervision over a large knowledge base, such as Freebase. Our model obtained new state-of-the-art performance on \textsc{WebQuestionsSP}, a challenging semantic parsing dataset."
"BeyondCorp: The User Experience.  Modern semantic parsers, which map natural language utterances to executable logical forms, have been successfully trained over large knowledge bases from weak supervision, but require hand-crafted rules and substantial feature engineering. Recent attempts to train an end-to-end neural network for semantic parsing have either used strong supervision (full logical forms), or have employed synthetic datasets and differentiable operations. In this work, we propose the Boss-Programmer-Computer framework to integrate neural network models with symbolic operations. Within this framework, we introduce Neural Symbolic Machines, in which a sequence-to-sequence neural network ""programmer"" controls a non-differentiable ""computer"" that executes Lisp programs (equivalent to logical forms) and provides code assistance. The interaction between the ""programmer"" and ""computer"" dramatically reduces the search space and effectively learns the semantic parser from weak supervision over a large knowledge base, such as Freebase. Our model obtained new state-of-the-art performance on \textsc{WebQuestionsSP}, a challenging semantic parsing dataset."
"Binaural processing for robust speech recognition of degraded speech.  This paper discusses a new combination of techniques that help in improving the accuracy of speech recognition in adverse conditions using two microphones. Classic approaches toward binaural speech processing use some form of cross-correlation over time across the two sensors to effectively iso-
late target speech from interferers. Several additional techniques using temporal and spatial masking have been proposed in the past to improve recognition accuracy in the presence of reverberation and interfering talkers. In this paper, we consider the use of cross-correlation across frequency over
some limited range of frequency channels in addition to the existing methods of monaural and binaural processing. This has the effect of locating and reinforcing coincident peaks across frequency over the representation of binaural interaction and provides local smoothing over the specified range of frequencies. Combined with the temporal and spatial masking techniques mentioned above, this leads to significant improvements in binaural speech recognition."
"TensorFlow Estimators: Managing Simplicity vs. Flexibility in High-Level Machine Learning Frameworks.  We present a framework for specifying, training, evaluating, and deploying machine learning models. Our focus is to simplify writing cutting edge machine learning models in a way that enables bringing those models into production. Recognizing the fast evolution of the field of deep learning, we make no attempt to capture the design space of all possible model architectures in a DSL or similar configuration. We allow users to write code to define their models, but provide abstractions that guide developers to write models in ways conducive to productionization, as well as providing a unifying Estimator interface, a unified interface making it possible to write downstream infrastructure (distributed training, hyperparameter tuning, ??) independent of the model implementation. We balance the competing demands for flexibility and simplicity by offering APIs at different levels of abstraction, making common model architectures available ??out of the box??, while providing a library of utilities designed to speed up experimentation with model architectures. To make out of the box models flexible and usable across a wide range of problems, these canned Estimators are parameterized not only over traditional hyperparameters, but also using feature columns, a declarative specification describing how to interpret input data. We discuss our experience in using this framework in research and production environments, and show the impact on code health, maintainability, and development speed."
"Scalable evacuation routing in a dynamic environment.  In emergency management, tools are needed so we can take the appropriate action at different stages of an evacuation. Recent wildfires in California showed how quickly a natural disaster can affect a large geographical area. Natural disasters can create unpredicted traffic congestion or can temporarily block urban or rural roads. Evacuating a large area in an emergency situation is not possible without prior knowledge of the road network and the ability to generate an efficient evacuation plan. An ideal evacuation routing algorithm should be able to generate realistic and efficient routes for each evacuee from the source to the closest shelter. It should also be able to quickly update routes as the road network changes during the evacuation. For example, if a main road is blocked during a flood, the evacuation routing algorithm should update the plan based on this change in the road network. In this article major works in evacuation routing have been studied and a new algorithm is developed that is faster and can generate better evacuation routes. Additionally, it can quickly adjust the routes if changes in the road network are detected. The new algorithm's performance and running time are reported."
"Arabic Multi-Dialect Segmentation: bi-LSTM-CRF vs. SVM.  Arabic word segmentation is essential for a variety of NLP applications such machine translation and information retrieval. Segmentation entails breaking words into their constituent stems, affixes and clitics. In this paper, we compare two approaches for segmenting four major Arabic dialects using only several thousand training examples for each dialect. The two approaches involve posing the problem as a ranking problem, where an SVM ranker picks the best segmentation, and as a sequence labeling problem, where a bi-LSTM RNN coupled with CRF determines where best to segment words. We are able to achieve solid segmentation results for all dialects using rather limited training data. We also show that employing Modern Standard Arabic data for domain adaptation and assuming context independence improve overall results."
"Source-Side Left-to-Right or Target-Side Left-to-Right? An Empirical Comparison of Two Phrase-Based Decoding Algorithms.  This paper describes an empirical study of the phrase-based decoding algorithm proposed
by Chang and Collins (2017). The algorithm produces a translation by processing the source-language sentence in strictly left-to-right order, differing from commonly used approaches that build the target-language sentence in left-to-right order. Our results show that the new algorithm is competitive with Moses (Koehn et al., 2007) in terms of both speed and BLEU scores."
"Learning Edge Representations via Low-Rank Asymmetric Projections.  We propose a new method for embedding graphs while preserving directed edge information. Learning such continuous-space vector representations (or embeddings) of nodes in a graph is an important first step for using network information (from social networks, user-item graphs, knowledge bases, etc.) in many machine learning tasks. 
Unlike previous work, we (1) explicitly model an edge as a function of node embeddings, and we (2) propose a novel objective, the ""graph likelihood"", which contrasts information from sampled random walks with non-existent edges. Individually, both of these contributions improve the learned representations, especially when there are memory constraints on the total size of the embeddings. When combined, our contributions enable us to significantly improve the state-of-the-art by learning more concise representations that better preserve the graph structure. 
We evaluate our method on a variety of link-prediction task including social networks, collaboration networks, and protein interactions, showing that our proposed method learn representations with error reductions of up to 76% and 55%, on directed and undirected graphs. In addition, we show that the representations learned by our method are quite space efficient, producing embeddings which have higher structure-preserving accuracy but are 10 times smaller."
"Profit Sharing and Efficiency in Utility Games.  We study utility games (Vetta, FOCS 2002) where a set of players join teams to produce social utility, and receive individual utility in the form of payments in return. These games have many natural applications in competitive settings such as labor markets, crowdsourcing, etc. The efficiency of such a game depends on the profit sharing mechanism -- the rule that maps utility produced by the players to their individual payments. We study three natural and widely used profit sharing mechanisms -- egalitarian or equal sharing, marginal gain or value addition when a player joins, and marginal loss or value depletion when a player leaves. For these settings, we give tight bounds on the price of anarchy, thereby allowing comparison between these popular mechanisms from a (worst case) social welfare perspective."
"Scalable Multi-Domain Dialogue State Tracking.  Dialogue state tracking (belief tracking) is a key component of task-oriented dialogue systems and aims to estimate the user's goal at each user turn. State of the art approaches for state tracking rely on deep learning methods. These approaches represent dialogue state as a distribution over all possible value  s for a slot, for each slot present in the ontology. Such a representation is not scalable for slots for which the set of possible values is unbounded (e.g.,   date, time or location) or dynamic (e.g., movies, usernames). We introduce a novel framework for state tracking which is independent of the value-set for a slot. The key idea is to obtain a set of values of interest (candidate set) which is bounded in size for each slot and represent the state as a distribution over the candidate set. Such an approach solves the problem of slot-scalability by making the state representation independent of the value set. Furthermore, by leveraging the slot-independent architecture and transfer learning, our model scales well and can be quickly bootstrapped to unseen domains with just a few training examples."
"An Analysis of ""Attention"" in Sequence-to-Sequence Models.  In this paper, we conduct a detailed investigation of attention-based models for automatic speech recognition (ASR). First, we explore different types of attention, including online and full-sequence attention. Second, we explore different sub-word units to see how much of the end-to-end ASR process can reasonably be captured by an attention model. In experimental evaluations, we find that although attention is typically focussed over a small region of the acoustics during each step of next label prediction, full sequence attention outperforms ??online?? attention, although this gap can be significantly reduced by increasing the length of the segments over which attention is computed. Furthermore, we find that content-independent phonemes are a reasonable sub-word unit for attention models; when used in the second-pass to rescore N-best hypotheses these models provide over a 10% relative improvement in word error rate."
Constellation Shaping: Can It be Useful for Datacenter Reach Communication ?.  In this workshop talk we will discuss the potential and value of flexible modulation formats and constellation shaping for datacenter interconnection bandwidth scaling
"Soft 3D Reconstruction for View Synthesis.  We present a novel algorithm for view synthesis that utilizes a soft 3D reconstruction to improve quality, continuity and robustness. Our main contribution is the formulation of a soft 3D representation that preserves depth uncertainty through each stage of 3D reconstruction and rendering. We show that this representation is beneficial throughout the view synthesis pipeline. During view synthesis, it provides a soft model of scene geometry that provides continuity across synthesized views and robustness to depth uncertainty. During 3D reconstruction, the same robust estimates of scene visibility can be applied iteratively to improve depth estimation around object edges. Our algorithm is based entirely on O(1) filters, making it conducive to acceleration and it works with structured or unstructured sets of input views. We compare with recent classical and learning-based algorithms on plenoptic lightfields, wide baseline captures, and lightfield videos produced from camera arrays."
"Encoding Bitrate Optimization Using Playback Statistics for HTTP-based Adaptive Video Streaming.  HTTP-based video streaming techniques have now been widely deployed to deliver video streams over communication networks. With HTTP-based adaptive streaming protocols, a video player can dynamically select a video stream from a set of pre-encoded representations of the video source based on its available bandwidth and viewport size. The bitrates of the encoded representations thus determine the video quality presented to viewers. We propose to minimize the average delivered bitrate on a per-chunk basis by modeling the probability that a player observes a particular representation. We evaluate the method through both extensive numerical simulation and real-world experiments. The proposed method with the investigated experiment settings demonstrates an overall bandwidth savings of 9.6% comparing with state of the art without loss of average delivered video quality."
"Cold-Start Reinforcement Learning with Softmax Policy Gradients.  Policy-gradient approaches to reinforcement learning have two common and
undesirable overhead procedures, namely warm-start training and sample variance
reduction. In this paper, we describe a reinforcement learning method based on
a softmax policy that requires neither of these procedures. Our method combines
the advantages of policy-gradient methods with the efficiency and simplicity of
maximum-likelihood approaches. We apply this new cold-start reinforcement
learning method in training sequence generation models for structured output
prediction problems. Empirical evidence validates this method on automatic
summarization and image captioning tasks."
"The Anatomy of Smartphone Unlocking - Why and How Android Users Around the World Lock their Phones.  With the growth in smartphone adoption around the world, threats to the personal information they contain are also increasing. To protect devices and their contents from unauthorized physical access, manufacturers offer locking mechanisms, such as PINs, passwords, and biometrics. However, from a security perspective, PINs and patterns are susceptible to guessing attacks [1, 4, 12] and shoulder-surfing [14]. Patterns are also vulnerable to smudge attacks [2].
Because of the limitations of existing locking mechanisms, a variety of novel techniques have been introduced in the academic literature. These include additional biometric security layers for PINs [15] and Android patterns [5], external hardware [3], and improving security by visual methods like indirect input [9, 11, 13]. However, for any alternative method to be successfully adopted, a detailed understanding of how real users interact with existing smartphone authentication mechanisms is needed.
As a result, the motivation for our research is twofold. First, we sought to understand the adoption and usage of current locking mechanisms: which ones are used, and what motivates people to use them. Second, we wanted to establish benchmarks for the current authentication mechanisms, against which future research can be compared: users are unlikely to switch to a mechanism that requires more time or effort than their current one.
To this end, we conducted two studies: an international survey [8] and a measurement-based in situ study [7]."
"Natural Language Processing with Small Feed-Forward Networks.  We show that small and shallow feedforward neural networks can achieve near state-of-the-art results on a range of unstructured and structured language processing tasks while being considerably cheaper in memory and computational requirements than deep recurrent models. Motivated by resource-constrained environments like mobile phones, we showcase simple techniques for obtaining such small neural network models, and investigate different tradeoffs when deciding how to allocate a small memory budget."
"Creating a universal SNP and small indel variant caller with deep neural networks.  Next-generation sequencing (NGS) is a rapidly evolving set of technologies that can be used to determine the sequence of an individual's genome by calling genetic variants present in an individual using billions of short, errorful sequence reads. Despite more than a decade of effort and thousands of dedicated researchers, the hand-crafted and parameterized statistical models used for variant calling still produce thousands of errors and missed variants in each genome. Here we show that a deep convolutional neural network can call genetic variation in aligned next-generation sequencing read data by learning statistical relationships (likelihoods) between images of read pileups around putative variant sites and ground-truth genotype calls. This approach, called DeepVariant, outperforms existing tools, even winning the ""highest performance"" award for SNPs in a FDA-administered variant calling challenge. The learned model generalizes across genome builds and even to other species, allowing non-human sequencing projects to benefit from the wealth of human ground truth data. We further show that, unlike existing tools which perform well on only a specific technology, DeepVariant can learn to call variants in a variety of sequencing technologies and experimental designs, from deep whole genomes from 10X Genomics to Ion Ampliseq exomes. DeepVariant represents a significant step from expert-driven statistical modeling towards more automatic deep learning approaches for developing software to interpret biological instrumentation data."
"CrowdVariant: a crowdsourcing approach to classify copy number variants.  Copy number variants (CNVs) are an important type of genetic variation and play a causal role in many diseases. However, they are also notoriously difficult to identify accurately from next-generation sequencing (NGS) data. For larger CNVs, genotyping arrays provide reasonable benchmark data, but NGS allows us to assay a far larger number of small (&lt; 10kbp) CNVs that are poorly captured by array-based methods. The lack of high quality benchmark callsets of small-scale CNVs has limited our ability to assess and improve CNV calling algorithms for NGS data. To address this issue we developed a crowdsourcing framework, called CrowdVariant, that leverages Google's high-throughput crowdsourcing platform to create a high confidence set of copy number variants for NA24385 (NIST HG002/RM 8391), an Ashkenazim reference sample developed in partnership with the Genome In A Bottle Consortium. In a pilot study we show that crowdsourced classifications, even from non-experts, can be used to accurately assign copy number status to putative CNV calls and thereby identify a high-quality subset of these calls. We then scale our framework genome-wide to identify 1,781 high confidence CNVs, which multiple lines of evidence suggest are a substantial improvement over existing CNV callsets, and are likely to prove useful in benchmarking and improving CNV calling algorithms. Our crowdsourcing methodology may be a useful guide for other genomics applications."
"Prochlo: Strong Privacy for Analytics in the Crowd.  The large-scale monitoring of computer users?? software activities has become commonplace, e.g., for application telemetry, error reporting, or demographic profiling. This paper describes a principled systems architecture??Encode, Shuffle, Analyze (ESA)??for performing such monitoring with high utility while also protecting user privacy. The ESA design, and its Prochlo implementation, are informed by our practical experiences with an existing, large deployment of privacy-preserving software monitoring. With ESA, the privacy of monitored users?? data is guaranteed by its processing in a three-step pipeline. First, the data is encoded to control scope, granularity, and randomness. Second, the encoded data is collected in batches subject to a randomized threshold, and blindly shuffled, to break linkability and to ensure that individual data items get ??lost in the crowd?? of the batch. Third, the anonymous, shuffled data is analyzed by a specific analysis engine that further prevents statistical inference attacks on analysis results. ESA extends existing best-practice methods for sensitive-data analytics, by using cryptography and statistical techniques to make explicit how data is elided and reduced in precision, how only common-enough, anonymous data is analyzed, and how this is done for only specific, permitted purposes. As a result, ESA remains compatible with the established workflows of traditional database analysis. Strong privacy guarantees, including differential privacy, can be established at each processing step to defend against malice or compromise at one or more of those steps. Prochlo develops new techniques to harden those steps, including the Stash Shuffle, a novel scalable and efficient oblivious-shuffling algorithm based on Intel??s SGX, and new applications of cryptographic secret sharing and blinding. We describe ESA and Prochlo, as well as experiments that validate their ability to balance utility and privacy."
"The Skype paradox: Homelessness and selective intimacy in the use of communications technology.  Digital technologies are likely to be appropriated by the homeless just as they are by other segments of society. However, these appropriations will reflect the particularities of their circumstances. What are these appropriations? Are they beneficial or effective? Can Skype, as a case in point, assuage the social disconnection that must be, for many, the experience of being homeless? This paper analyses some evidence about these questions and, in particular, the ways communications media are selected, oriented to and accounted for by the homeless young. Using data from a small corpus of interviews, it examines the specific ways in which choice of communication (face-to-face, social media, or video, etc.), are described by these individuals as elected for tactical and strategic reasons having to do with managing their family relations. These relations are massively important both in terms of how communications media are deployed, and in terms of being one of the sources of the homeless state the young find themselves in. The paper examines some of the methodical ways these issues are articulated and the type of ??causal facticity?? thereby constituted in interview talk. The paper also remarks on the paradoxical problem that technologies like Skype provide: at once allowing people in the general to communicate but in ways that the homeless young want to resist in the particular. The consequences of this for the shaping of communications technology in the future are remarked upon."
"ConceptVector: Text Visual Analytics via Interactive Lexicon Building using Word Embedding.  Central to many text analysis methods is the notion of a concept: a set of semantically related keywords characterizing a specific object, phenomenon, or theme. Advances in word embedding allow building such concepts from a small set of seed terms. However, naive application of such techniques may result in false positive errors because of the polysemy of human language. To mitigate this problem, we present a visual analytics system called ConceptVector that guides the user in building such concepts and then using them to analyze documents. Document-analysis case studies with real-world datasets demonstrate the fine-grained analysis provided by ConceptVector. To support the elaborate modeling of concepts using user seed terms, we introduce a bipolar concept model and support for irrelevant words. We validate the interactive lexicon building interface via a user study and expert reviews. The quantitative evaluation shows that the bipolar lexicon generated with our methods is comparable to human-generated ones."
"Raw Multichannel Processing Using Deep Neural Networks.  Multichannel ASR systems commonly separate speech enhancement, including localization, beamforming and postfiltering, from acoustic modeling. In this chapter, we perform multi-channel enhancement jointly with acoustic modeling in a deep neural network framework. Inspired by beamforming, which leverages differences in the fine time structure of the signal at different microphones to filter energy arriving from different directions, we explore modeling the raw time-domain waveform directly. We introduce a neural network architecture which performs multichannel filtering in the first layer of the network and show that this network learns to be robust to varying target speaker direction of arrival, performing as well as a model that is given oracle knowledge of the true target speaker direction. Next, we show how performance can be improved by factoring the first layer to separate the multichannel spatial filtering operation from a single channel filterbank which computes a frequency decomposition. We also introduce an adaptive variant, which updates the spatial filter coefficients at each time frame based on the previous inputs. Finally we demonstrate that these approaches can be implemented more efficiently in the frequency domain. Overall, we find that such multichannel neural networks give a relative word error rate improvement of more than 5% compared to a traditional beamforming-based multichannel ASR system and more than 10% compared to a single channel waveform model."
"Privacy and security experiences and practices of survivors of intimate partner abuse.  Recognizing how intimate partner abuse??s three phases??physical control, escape from abuser, and life apart??affect survivors?? technology use can help technology creators better understand and support this population??s digital security and privacy needs."
"Near Impressions for Observational Causal Ad Impact.  Advertisers often estimate the performance of their online advertising by either running randomized experiments, or applying models to observational data.  While randomized experiments are the gold standard of measurement, their cost and complexity often lead advertisers to rely instead on observational methods, such as attribution models.  A previous paper demonstrated the limitations of attribution models, as well as information issues that limit their performance.  This paper introduces ""near impressions"", an additional source of observational data that can be used to estimate causal ad impact without experiments.  We use both simulated and real experiments to demonstrate that near impressions greatly improve our ability to accurately measure the true value generated by ads."
"Video Frame Synthesis Using Deep Voxel Flow.  We address the problem of synthesizing new video frames in an existing video, either in-between existing frames (interpolation), or subsequent to them (extrapolation). This problem is challenging because video appearance and motion can be highly complex. Traditional optical-flow-based solutions often fail where flow estimation is challenging, while newer neural-network-based methods that hallucinate pixel values directly often produce blurry results. We combine the advantages of these two methods by training a deep network that learns to synthesize video frames by flowing pixel values from existing ones, which we call deep voxel flow. Our method requires no human supervision, and any video can be used as training data by dropping, and then learning to predict, existing frames. The technique is efficient, and can be applied at any video resolution. We demonstrate that our method produces results that both quantitatively and qualitatively improve upon the state-of-the-art."
"To Smiley, Or Not To Smiley? Considerations and Experimentation to Optimize Data Quality and User Experience for Contextual Product Satisfaction Measurement?.  Happiness Tracking Surveys (HaTS) at Google are designed to measure satisfaction with a product or feature in context of actual usage. Smiley faces have been added to a fully-labeled satisfaction scale, to increase discoverability of the survey and response rates. Sensitive to the potential variety of effects from images and visual presentation in online surveys (Tourangeau, Conrad &amp; Couper, 2013), this presentation will describe research designed to inform and optimize Google's use of smileys in Happiness Tracking Surveys across products and platforms: 1) We explore construct alignment by capturing users' interpretations of the various smiley faces, via open-ended responses. This data shows meaningful variation across potential smiley images, which informed design decisions.
2) We assess scaling properties of smileys by measuring each smiley independently on a 0-100 scale, to calculate semantic distance between smileys in order to achieve equally-spaced intervals between scale points (Klockars &amp; Yamagishi, 1988).
3) We describe considerations and evaluative metrics for a smiley-based scale with endpoint text labels, to be used with mobile apps and devices."
"User Experience Considerations for Contextual Product Surveys on Smartphones.  As use of smartphone-based tools &amp; services broadens and deepens, such products should be continuously evaluated and optimized to meet users' needs. In-product surveys are one way to gather attitudinal and user experience data at scale, in context of actual experiences. However, given space constraints, OS variance, product design differences, and app vs mobile web options, launching contextual surveys on smartphones requires considerable attention to several user experience aspects that can also impact data quality. In this talk, we will discuss a variety of practical UX considerations for in-context surveys on smartphones, drawing on real-world implementation and experimentation for several of Google's most-used mobile products. In particular, we'll discuss issues such as when to trigger a survey, sampling across platforms (mobile vs desktop), invitations vs inline questions, survey length, question types, device size, screen orientation, survey interaction with the host product/app. We will also explore the effect of design and text variants on smartphone survey response rates and response distributions."
"Effects of Pagination on Short Online Surveys.  When designing online surveys, researchers must choose from a variety of pagination options. Respondents' expectations, experiences, and behaviors may vary depending on a survey's pagination, affecting both breakoffs and responses themselves. Surprisingly little formal experimentation has been conducted on the effects of survey pagination, with initial evidence focused on a long survey of university
students (Peytchev, Couper, McCabe, Crawford 2006). This experiment is intended to further inform the effects on pagination in online surveys. In a split-ballot experiment, we served respondents one of three versions of a short online questionnaire (~15 questions) on attitudes and experiences toward an online product. questionnaire are randomly served to respondents constructed with a) one question per page, b) logical groupings of questions over several pages, and c) as few pages as possible. Effects of pagination are evaluated on breakoff rates, response time, item and unit nonresponse, interitem correlations, and perceived length/difficulty. We hypothesize that the questionnaire with the fewest (longest) pages will cause greater initial breakoff, and the one with most pages will suffer increased breakoff during the survey."
"Using SSDs to scale up Google Fusion Tables, a Database-in-the-Cloud.  Flash memory solid state drives (SSDs) have increasingly been advocated and adopted as a means of speeding up and scaling up data-driven applications. SSDs are becoming more widely available as an option in the cloud. However, when an application considers SSDs in the cloud, the best option for the application may not be immediate, among a number of choices for placing SSDs in the layers of the cloud. Although there have been many studies on SSDs, they often concern a specific setting, and how different SSD options in the cloud compare with each other is less well understood. In this paper, we describe how Google Fusion Tables (GFT) used SSDs and what optimizations were implemented to scale up its in-memory processing, clearly showing opportunities and limitations of SSDs in the cloud with quantitative analyses. We first discuss various SSD placement strategies and compare them with low-level measurements, and propose SSD-placement guidelines for a variety of cloud data services. We then present internals of our column engine and optimizations to better use the performance characteristics of SSDs. We empirically demonstrate that the optimizations enable us to scale our application to much larger datasets while retaining the low-latency and simple query processing architecture."
"Predicting Cardiovascular Risk Factors in Retinal Fundus Photographs using Deep Learning.  Traditionally, medical discoveries have been made by observing associations, designing experiments to test these hypotheses, and subsequently, applying more advanced modeling techniques to better quantify these associations. However, this process can fail for complex associations, particularly for images and signals. For example, hand-engineering features to describe a ??normal blood vessel?? in an image of a retina is challenging because of the wide variety of pixel values and shapes in real patient images. By contrast, deep learning, a machine learning technique that learns its own features, has been tremendously successful in identifying objects in natural scenes, such as cats in a variety of backgrounds and postures. In this paper, we present a case study of discovering new knowledge in retina images using deep learning models trained on retina images from over 280,000 patients, and validated on two independent datasets with 12,026 and 999 patients respectively. We show that retina images alone contain sufficient information to predict previously known associations to an unprecedented accuracy, such as age to within 3.3 years. In addition, our models are able to predict previously unknown associations, such as gender with an AUC of 0.97, smoking status with an AUC of 0.72, ethnicity with a kappa exceeding 0.6, systolic blood pressure within 11.23 mmHg, and HbA1c within 1.39%. We further show that our models used distinct aspects of the anatomy to generate each prediction, such as the optic disc or blood vessels, opening avenues of further research."
"Learning to count mosquitoes for the Sterile Insect Technique.  Mosquito-borne illnesses such as dengue, chikungunya, and Zika
are major global health problems, which are not yet addressable
with vaccines and must be countered by reducing mosquito popula-
tions. The Sterile Insect Technique (SIT) is a promising alternative
to pesticides; however, effective SIT relies on minimal releases of
female insects. This paper describes a multi-objective convolutional
neural net to significantly streamline the process of counting male
and female mosquitoes released from a SIT factory and provides a
statistical basis for verifying strict contamination rate limits from
these counts despite measurement noise. These results are a promis-
ing indication that such methods may dramatically reduce the cost
of effective SIT methods in practice."
"ESOMAR/GRBN Guideline on Mobile Research.  This new Guideline on Mobile Research aligns global policies with developing regulations and technology and the latest international developments for best practice in this area. Mobile research is a rapidly evolving field and a growing market which accounts for $1.8bn global annual turnover and is widely used in advanced as well as developing economies. Mobile research ranges from calling or texting respondents to ask them questions, to participants videoing how they perform daily tasks such as cooking and more recently, to collecting data generated by mobile devices such as geo-location data, all to provide researchers with richer insights about attitudes and behaviour. This new guideline is designed to help researchers address legal, ethical and practical considerations in using new technologies when conducting mobile research. The text has been drafted by a team of international experts to ensure that it incorporates the latest practices of mobile research, so that the new Guideline takes into account the continuing innovation in technology that has created information sources that are relevant to research. These include: -Passive data collection including biometric data, photos and recordings and instore tracking
-Mystery shopping through camera and video
-Data that may have been collected for a non-research purpose which is used in research including geolocation data from mobile providers, or usage data from app providers"
"Multimodal Storytelling via Generative Adversarial Imitation Learning.  Deriving event storylines is an effective summarization method to succinctly organize extensive information, which can significantly alleviate the pain of information overload. The critical challenge is the lack of widely recognized definition of storyline metric. Prior studies have developed various approaches based on different assumptions about users?? interests. These works can extract interesting patterns, but their assumptions do not guarantee that the derived patterns will match users?? preference. On the other hand, their exclusiveness of single modality source misses cross-modality information. This paper proposes a method, multimodal imitation learning via Generative Adversarial Networks(MIL-GAN), to directly model users?? interests as reflected by various data. In particular, the proposed model addresses the critical challenge by imitating users?? demonstrated storylines. Our proposed model is designed to learn the reward patterns given user-provided storylines and then applies the learned policy to unseen data. The proposed approach is demonstrated to be capable of acquiring the user??s implicit intent and outperforming competing methods by a substantial margin with a user study."
"I'm Sorry, Dave, I'm Afraid I Can't Do That: Chatbot Perception and Expectations.  Artificial intelligence continues to grow in popularity on mobile platforms, increasing exposure to chatbot apps. Chatbot technology has evolved over time, yet the purpose and added value that chatbots offer has not been clearly defined. In order to design a chatbot that provides a meaningful experience, we must first understand what expectations people have for this technology, and what opportunities are there for chatbots based on user needs. This study includes qualitative data from 54 participants in the US and India, sharing their expectations and experiences with a chatbot. The research objectives include: 1) understand user perception and expectations of chatbots 2) surface preferences for input modality and 3) identify domains where chatbots can add meaningful purpose."
"Geo-Distribution of Actor-Based Services.  Many service applications use actors as a programming model for the middle tier, to simplify synchronization,
fault-tolerance, and scalability. However, efficient operation of such actors in multiple, geographically distant
datacenters is challenging, due to the very high communication latency. Caching and replication are essential
to hide latency and exploit locality; but it is not a priori clear how to combine these techniques with the actor
programming model.
We present Geo, an open-source geo-distributed actor system that improves performance by caching
actor states in one or more datacenters, yet guarantees the existence of a single latest version by virtue of
a distributed cache coherence protocol. Geo??s programming model supports both volatile and persistent
actors, and supports updates with a choice of linearizable and eventual consistency. Our evaluation on several
workloads shows substantial performance benefits, and confirms the advantage of supporting both replicated
and single-instance coherence protocols as configuration choices. For example, replication can provide fast,
always-available reads and updates globally, while batching of linearizable storage accesses at a single location
can boost the throughput of an order processing workload by 7x."
"SLING: A framework for frame semantic parsing.  We describe SLING, a framework for parsing natural language into semantic frames. SLING supports general transition-based, neural-network parsing with bidirectional LSTM input encoding and a Transition Based Recurrent Unit (TBRU) for output decoding. The parsing model is trained end-to-end using only the text tokens as input. The transition system has been designed to output frame graphs directly without any intervening symbolic representation. The SLING framework includes an efficient and scalable frame store implementation as well as a neural network JIT compiler for fast inference during parsing. SLING is implemented in C++ and it is available for download on GitHub."
"Learning Differentially Private Recurrent Language Models.  We demonstrate that it is possible to train large recurrent language models with user-level differential privacy guarantees with only a negligible cost in predictive accuracy. Our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent. In particular, we add user-level privacy protection to the federated averaging algorithm, which makes ""large step"" updates from user-level data. Our work demonstrates that given a dataset with a sufficiently large number of users (a requirement easily met by even small internet-scale datasets), achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work. We find that our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a large dataset."
"High-Speed Channel Modeling with Deep Neural Network for Signal Integrity Analysis.  In this work, deep neural networks (DNNs) are
trained and used to model high-speed channels for signal integrity
analysis. The DNN models predict eye-diagram metrics by taking
advantage of the large amount of simulation results made
available in a previous design or at an earlier design stage. The
proposed DNN models characterize high-speed channels through
extrapolation with saved coefficients, which requires no complex
simulations and can be achieved in a highly efficient manner. It
is demonstrated through numerical examples that the proposed
DNN models achieve good accuracy in predicting eye-diagram
metrics from input design parameters. In the DNN models, no
assumptions are made on the distributions of and the interactions
among individual design parameters."
"Modeling Capacitor Derating in Power Integrity Simulation.  In this work, we propose a simulation methodology
that incorporates derating models of decoupling capacitors
for power integrity analysis. The construction of the derating
models of decoupling capacitors is based on the impedance
measurement and curve fitting method. Three approaches of
impedance measurement are compared and the most accurate
one is selected to build the derating models. The curve fitting
method converts the measured impedance into circuit models. A
library file containing the derating models is generated such that
it can be repeatedly used for different products at various design
cycles. The derating library takes into account the operation
conditions such as temperature and DC bias as well as the vendor
information. The proposed simulation methodology with the
derating library achieves high accuracy, which is demonstrated
through correlations with measurements."
"TensorFlow Agents: Efficient Batched Reinforcement Learning in TensorFlow.  We introduce TensorFlow Agents, an efficient infrastructure paradigm for building parallel reinforcement learning algorithms in TensorFlow. We simulate multiple environments in parallel, and group them to perform the neural network computation on a batch rather than individual observations. This allows the TensorFlow executing engine to parallelize computation, without the need for manual synchronization. Environments are stepped in separate Python processes to progress them in parallel without interference of the global interpreter lock. As part of this project, we introduce BatchPPO, an efficient implementation of the proximal policy optimization algorithm. By open sourcing TensorFlow Agents, we hope to provide a flexible starting point for future projects that accelerates future research in the field."
"Data breaches, phishing, or malware?  Understanding the risks of stolen credentials.  In this paper, we present the first longitudinal measurement study of the underground ecosystem fueling credential theft and assess the risk it poses to millions of users. Over the course of March, 2016--March, 2017, we identify 788,000 potential victims of off-the-shelf keyloggers; 12.4 million potential victims of phishing kits; and 1.9 billion usernames and passwords exposed via data breaches and traded on blackmarket forums. Using this dataset, we explore to what degree the stolen passwords---which originate from thousands of online services---enable an attacker to obtain a victim's valid email credentials---and thus complete control of their online identity due to transitive trust. Drawing upon Google as a case study, we find 7--25\% of exposed passwords match a victim's Google account. For these accounts, we show how hardening authentication mechanisms to include additional risk signals such as a user's historical geolocations and device profiles helps to mitigate the risk of hijacking. Beyond these risk metrics, we delve into the global reach of the miscreants involved in credential theft and the blackhat tools they rely on. We observe a remarkable lack of external pressure on bad actors, with phishing kit playbooks and keylogger capabilities remaining largely unchanged since the mid-2000s."
"An assessment of the causes of the errors in the 2015 UK general election opinion polls.  The opinion polls that were undertaken before the 2015 UK general election underestimated the Conservative lead over Labour by an average of 7 percentage points. This collective failure led politicians and commentators to question the validity and utility of political polling and raised concerns regarding a broader public loss of confidence in survey research. We assess the likely causes of the 2015 polling errors. We begin by setting out a formal account of the statistical methodology and assumptions that are required for valid estimation of party vote shares by using quota sampling. We then describe the current approach of polling organizations for estimating sampling variability and suggest a new method based on bootstrap resampling. Next, we use poll microdata to assess the plausibility of different explanations of the polling errors. Our conclusion is that the primary cause of the polling errors in 2015 was unrepresentative sampling."
"Approaches for Neural-Network Language Model Adaptation.  Language Models (LMs) for Automatic Speech Recognition
(ASR) are typically trained on large text corpora from news
articles, books and web documents. These types of corpora,
however, are unlikely to match the test distribution of ASR systems,
which expect spoken utterances. Therefore, the LM is
typically adapted to a smaller held-out in-domain dataset that is
drawn from the test distribution. We present three LM adaptation
approaches for Deep NN and Long Short-Term Memory
(LSTM): (1) Adapting the softmax layer in the NN; (2)
Adding a non-linear adaptation layer before the softmax layer
that is trained only in the adaptation phase; (3) Training the
extra non-linear adaptation layer in pre-training and adaptation
phases. Aiming to improve upon a hierarchical Maximum Entropy
(MaxEnt) second-pass LM baseline, which factors the
model into word-cluster and word models, we build an NN
LM that predicts only word clusters. Adapting the LSTM LM
by training the adaptation layer in both training and adaptation
phases (Approach 3), we reduce the cluster perplexity by
30% compared to an unadapted LSTM model. Initial experiments
using a state-of-the-art ASR system show a 2.3% relative
reduction in WER on top of an adapted MaxEnt LM."
"Fast Fourier Color Constancy.  We present Fast Fourier Color Constancy (FFCC), a novel color constancy algorithm which works by reformulating the problem of illuminant estimation into a spatial localization task on a torus. On standard benchmarks, our model produces lower error rates than the previous state-of-the-art by $10-12\%$, while also being $250-3000\times$ faster. This speed and accuracy is primarily due to how FFCC primarily operates in the frequency domain, though this approach also introduces a set of new difficulties regarding aliasing, directional statistics and preconditioning, which we address. Unlike past work, our model produces a complete posterior distribution over illuminants instead of a single illuminant estimate, which allows for a richer analysis and enables a novel temporal smoothing technique. FFCC is capable of running at $\sim 700$ frames per second on a mobile phone, making it a viable solution to the problem of constructing an effective, real-time, temporally-coherent automatic white balance algorithm."
"Scaling Large Data Center Interconnects: Challenges and Solutions.  Increasing demands for web and cloud-based services have been driving exponential growth of datacenter bandwidth. This paper discusses, from Google??s perspective, emerging challenges and possible technical solutions to scale intra-datacenter and intra-campus interconnection network bandwidth."
"Learning Spread-out Local Feature Descriptors.  We propose a simple, yet powerful regularization technique that can be used to significantly improve both the pairwise and triplet losses in learning local feature descriptors. The idea is that in order to fully utilize the expressive power of the descriptor space, good local feature descriptors should be sufficiently ??spread-out?? over the space. In this work, we propose a regularization term to maximize the spread in feature descriptor inspired by the property of uniform distribution. We show that the proposed regularization with triplet loss outperforms existing Euclidean distance based descriptor learning techniques by a large margin. As an extension, the proposed regularization technique can also be used to improve image-level deep feature embedding."
"Crafting a lexicon of referential expressions for NLG applications.  To be perfectly conversational, an agent needs to produce grammatically correct and eloquent sentences. To reach this goal, we use templatic systems with linguistically-aware specifications to generate idiomatic utterances, coupled with annotated lexical entities. The morphosyntactic features of the lexical entities are crucial to render grammatical and natural sounding sentences. Existing electronic resources, like dictionaries or thesauri, lack wide-scale information about referential expressions (i.e. proper names). In this work, we focus on the creation of a large-scale lexicon of such referential expressions, relying on n-gram models, morpho-syntactic parsing, and non-linguistic knowledge. We describe the linguistic information we collect and the techniques we use to automatically extract this from large text corpora in a way that scales across languages and over millions of entities."
"Grading the Grids: What Works and What Doesn??t Using Paradata to Assess Response Quality and Usability.  Grids (or matrix, table) are commonly used on self-administered surveys. In order to optimize online surveys for smartphones, grid designs aiming for small-screen devices are emerging. In this study we investigate four research questions regarding the effectiveness and drawbacks of different grid designs, more specifically do the grid design effect:
Data quality, as indicated by breakoffs, satisfying behaviors and response errors
Response time
Response distributions
Inter-relationships among questions
We conducted two experiments.
The first experiment was conducted in April 2016 in Brazil, US and Germany. We tested a progressive grid, a responsive and a collapsable grid. Results were analyzed for desktop/laptops only due to the small number of respondents who took the study via smartphones. 
We found the collapsable grid eliciting the highest amount of error prompts for item nonresponse.
The second experiment was fielded in August 2016 testing grid designs on three types of answer scales: a 7-point fully-labeled rating scale, a 5-point fully-labeled rating scale, and a 6-point fully-labeled frequency scale. Respondents from the US and Japan to an online survey were randomly assigned to one of three conditions: (a) no grid, where each question was presented on a separate screen; (b) responsive grid, where a grid is shown on large screens and as single-column vertical table on small screens (with question stem fixed as header); (c) progressive grid, where grouped questions were presented screen-by-screen with question stem and sub-questions (stubs) fixed on top. Quotas were enforced so that half of the respondents completed the survey on large-screen devices (desktop/tablet computers) and the other half on smartphones. Respondents were 600 per grid condition per screen size per country.
Findings showed that progressive grid had less straightlining and response errors whereas responsive grid had less break-offs. Differences were also found between grid designs in terms of response time and response distributions; however patterns varied by country, screen size and answer scales. Further analysis will explore the effect of grid design on question inter-relationships.
While visual and interactive features impact the utility of grid designs, we found that the effects might vary by question types, screen sizes, and countries. More experiments are needed to explore designs truly optimized for online surveys."
"Large-Scale Content-Only Video Recommendation.  Traditional recommendation systems using collaborative filtering (CF) approaches work relatively well when the candidate videos are sufficiently popular. With the increase of user-created videos, however, recommending fresh videos gets more and more important, but pure CF-based systems may not perform well in such cold-start situation. In this paper, we model recommendation as a video content-based similarity learning problem, and learn deep video embeddings trained to predict video relationships identified by a co-watch-based system but using only visual and audial content. The system does not depend on availability on video meta-data, and can generalize to both popular and tail content, including new video uploads. We demonstrate performance of the proposed method in large-scale datasets, both quantitatively and qualitatively."
DIY research.  An interview with Tim Macer about what DIY research means for Market Research
"Boosted Second-price Auctions for Heterogeneous Bidders.  Due to its simplicity and desirable incentive aspects, the second-price auction is the most prevalent auction format used by advertising exchanges. However, even with the optimized choice of the reserve prices, this auction is not optimal when the bidders are heterogeneous, i.e., when the bidder valuation distributions differ significantly. We propose a new auction format called the boosted second-price auction, which favors bidders with lower inverse hazard rates (IHRs), roughly speaking, bidders with more stable bidding behavior. Based on our analysis of auction data from Google??s advertising exchange, we found bidders to be heterogeneous and can be ordered based on their IHRs. In this paper, we theoretically analyze and describe how our proposed boosted second-price auctions increase revenue over that of the widely used second-price auctions by favoring bidders with lower IHRs. We also provide practical guidelines for determining boost values and validate these guidelines both theoretically and empirically. Our counterfactuals, based on actual transaction data, show that boosted second-price auctions that follow our guidelines perform almost optimally and obtain up to 3% more revenue than second-price auctions."
"Dynamic Pricing for Heterogeneous Time-Sensitive Customers.  A core problem in the area of revenue management is pricing goods in the presence of strategic customers. We study this problem when customers are heterogeneous with respect to their initial valuations for the item and their time sensitivities, i.e., the customers differ in both their initial valuations and the rates at which their initial valuation decreases with delay in purchase. We characterize the optimal mechanism for selling durable goods in such environments and show that delayed allocation and dynamic pricing can be effective screening tools for maximizing firm profit. We also investigate the impact of production and holding costs on the optimal mechanism."
"Code-reuse attacks for the Web: Breaking Cross-Site Scripting Mitigations via Script Gadgets.  Cross-Site Scripting (XSS) is a constant problem of the Web platform. Since its initial public documentation in the year 2000 until the present day, XSS is continuously on top of the vulnerability statistics. Even though a considerable amount of research and developer education has been conducted to address XSS on the source code level, the overall number of discovered XSS problems remains high. For this reason various approaches to mitigate XSS have been proposed as a second line of defense, with HTML sanitizers, Web Application Firewalls, browser-based XSS filters, and the Content Security Policy being only some prominent examples. Thereby, most of these mechanisms focus on
script tags and event handlers, by either removing them from user-provided content or by preventing their script code from executing. In this paper, we demonstrate that this approach is no longer sufficient for modern applications: We describe a novel Web attack that is capable to circumvent all currently existing XSS mitigation
techniques. In this attack, the attacker abuses so called script gadgets to execute JavaScript. Script gadgets are legitimate JavaScript fragments within an application??s legitimate code base. In most cases, these gadgets utilize DOM selectors to interact with elements in the Web document. Through an initial injection point, the attacker can inject benign-looking HTML elements, which are ignored by potential mitigation technique but match the selector of the gadget.
This way, the attacker can hijack the input of a gadget and, thus, cause processing of his input, which in turn leads to code execution of attacker-controlled values. We demonstrate that these gadgets are omnipresent in almost all modern JavaScript frameworks and present an empirical study showing the prevalence of script gadgets in productive code. As a result, we assume most mitigation techniques in web applications written today can be bypassed."
"A Taste of Android Oreo (v8.0) Device Manufacturer.  In 2017, over two billion Android devices developed by more than a thousand device manufacturers (DMs) around the world are actively in use. Historically, silicon vendors (SVs), DMs, and telecom carriers extended the Android Open Source Project (AOSP) platform source code and used the customized code in final production devices. Forking, on the other hand, makes it hard to accept upstream patches (e.g., security fixes). In order to reduce such software update costs, starting from Android v8.0, the new Vendor Test Suite (VTS) splits hardware-independent framework and hardware-dependent vendor implementation by using versioned, stable APIs (namely, vendor interface). Android v8.0 thus opens the possibility of a fast upgrade of the Android framework as long as the underlying vendor implementation passes VTS. This tutorial teaches how to develop, test, and certify a compatible Android vendor interface implementation running below the framework. We use an Android Virtual Device (AVD) emulating an Android smartphone device to implement a user-space device driver which uses formalized interfaces and RPCs, develop VTS tests for that component, execute the extended tests, and certify the extended vendor implementation."
"Strategies for Foveated Compression and Transmission.  This paper address three different strategies (F-JPEG, F-DSC and F-YUV) for foveated image compression plus methods for the transmission of foveated image data. In place of simple subsampling for foveation, the proposed foveated compression strategies use varying compression ratios--- a low compression ratio centrally, and progressively increasing compression ratios towards the periphery."
"Speaker Diarization with LSTM.  For many years, i-vector based speaker embedding techniques were the dominant approach for speaker verification and speaker diarization applications. However, mirroring the rise of deep learning in various domains, neural network based speaker embeddings, also known as d-vectors, have consistently demonstrated superior speaker verification performance. In this paper, we build on the success of d-vector based speaker verification systems to develop a new d-vector based approach to speaker diarization. Specifically, we combine LSTM-based d-vector audio embeddings with recent work in non-parametric clustering to obtain a state-of-the-art speaker diarization system. Our experiments on CALLHOME American English and 2003 NIST Rich Transcription conversational telephone speech (CTS) corpus suggest that d-vector based diarization systems offer significant advantages over traditional i-vector based systems."
"Generalized End-to-End Loss for Speaker Verification.  In this paper, we propose a new loss function called generalized end-to-end (GE2E) loss, which makes the training of speaker verification models more efficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike TE2E, the GE2E loss function updates the network in a way that emphasizes examples that are difficult to verify at each step of the training process. Additionally, the GE2E loss does not require an initial stage of example selection. With these properties, the model with new loss function learns a better model, by decreasing EER by more than 10%, in shorter period of time, by reducing the training time by &gt;60%. We also introduce the MultiReader technique, which allow us do domain adaptation - training more accurate model that supports multiple keywords (i.e. ""OK Google"" and ""Hey Google"") as well as multiple dialects."
"Attention-Based Models for Text-Dependent Speaker Verification.  Attention-based models have recently shown great performance on a range of tasks, such as speech recognition, machine translation, and image captioning due to their ability to summarize relevant information that expands through the entire length of an input sequence. In this paper, we analyze the usage of attention mechanisms to the problem of sequence summarization in our end-to-end text-dependent speaker recognition system. We explore different topologies and their variants of the attention layer, and compare different pooling methods on the attention weights. Ultimately, we show that attention-based models can improves the Equal Error Rate (EER) of our speaker verification system by relatively 14% compared to our non-attention LSTM baseline model."
"Uncovering Latent Style Factors for Expressive Speech Synthesis.  Prosodic modeling is a core problem in speech synthesis. The key challenge is producing desirable prosody from textual input containing only phonetic information. In this preliminary study, we introduce the concept of ""style tokens"" in Tacotron, a recently proposed end-to-end neural speech synthesis model. Using style tokens, we aim to extract independent prosodic styles from training data. We show that without annotation data or an explicit supervision signal, our approach can automatically learn a variety of prosodic variations in a purely data-driven way. Importantly, each style token corresponds to a fixed style factor regardless of the given text sequence. As a result, we can control the prosodic style of synthetic speech in a somewhat predictable and globally consistent way."
"Carousel: Scalable Traffic Shaping at End-Hosts.  Optimal utilization of network resources and isolation among traffic in modern datacenters and across backbone links, where thousands of different applications and millions of connections are competing for high bandwidth and low latency, often requires the use of various mechanisms at end hosts to shape, rate limit, pace, prioritize, or delay some packets over others. These mechanisms involve one or more classifiers, responsible for matching packets and moving them into different queues based on policy, a shaping algorithm attached to each queue, responsible for delaying, dropping, or remarking packets as necessary, and one or more scheduling algorithms, consuming packets from the various queues and generally responsible for providing fairness and prioritization across different queues. Scaling these architectures at end hosts with thousands of traffic classes while maintaining performance and isolation is challenging, especially in Cloud environments or when trying to offload functionality to hardware. In this paper we present Carousel, a rate limiter that scales to thousands of policies and million of flows, does not require multiple queues or the cooperation of VMs or OS drivers in classifying packets, and is easily implemented on modern hardware and software. Production experience at a Cloud service provider demonstrate that existing traffic shapers consume 10% of server??s overall CPU resources while Carousel can accomplish the same work with negligible cost. Rate conformance of Carousel is within 0.5% of the target rate as compared with 6% achieved by existing systems."
"Towards Learning Semantic Audio Representations from Unlabeled Data.  Our goal is to learn semantically structured audio representations without relying on categorically labeled data. We consider several class-agnostic semantic constraints that are inherent to non-speech audio: (i) sound categories are invariant to additive noise and translations in time, (ii) mixtures of two sound events inherit the categories of the constituents, and (iii) the categories of events in close temporal proximity in a single recording are likely to be the same or related.  We apply these invariants in the service of sampling training data for triplet-loss embedding models using a large unlabeled dataset of YouTube soundtracks. The resulting low-dimensional representations provide both greatly improved query-by-example retrieval performance and reduced labeled data and model complexity requirements for supervised sound classification."
"A Decomposition of Forecast Error in Prediction Markets.  We analyze sources of error in prediction market forecasts in order to bound the difference between a security's price and the ground truth it estimates. We consider cost-function-based prediction markets in which an automated market maker adjusts security prices according to the history of trade. We decompose the forecasting error into three components: sampling error, arising because traders only possess noisy estimates of ground truth; market-maker bias, resulting from the use of a particular market maker (i.e., cost function) to facilitate trade; and convergence error, arising because, at any point in time, market prices may still be in flux. Our goal is to make explicit the tradeoffs between these error components, influenced by design decisions such as the functional form of the cost function and the amount of liquidity in the market. We consider a specific model in which traders have exponential utility and exponential-family beliefs representing noisy estimates of ground truth. In this setting, sampling error vanishes as the number of traders grows, but there is a tradeoff between the other two components. We provide both upper and lower bounds on market-maker bias and convergence error, and demonstrate via numerical simulations that these bounds are tight. Our results yield new insights into the question of how to set the market's liquidity parameter and into the forecasting benefits of enforcing coherent prices across securities."
"Souper: A Synthesizing Superoptimizer.  If we can automatically derive compiler optimizations, we might be able to sidestep some of the substantial engineering challenges involved in creating and maintaining a high-quality compiler. We developed Souper, a synthesizing superoptimizer, to see how far these ideas might be pushed in the context of LLVM. Along the way, we discovered that Souper's intermediate representation was sufficiently similar to the one in Microsoft Visual C++ that we applied Souper to that compiler as well. Shipping, or about-to-ship, versions of both compilers contain optimizations suggested by Souper but implemented by hand. Alternately, when Souper is used as a fully automated optimization pass it compiles a Clang compiler binary that is about 3 MB (4.4%) smaller than the one compiled by LLVM."
"Asynchronous Parallel Coordinate Minimization for MAP Inference.  Finding the maximum a-posteriori (MAP) assignment is a central task in graphical models. Since modern applications give rise to very large problem instances, there is increasing need for efficient solvers. In this work we propose to improve the efficiency of coordinate-minimization-based dual-decomposition solvers by running them asynchronously in parallel. In this case message-passing inference is performed by multiple processing units simultaneously, all reading and writing to shared memory, without coordination. We analyze the convergence properties of the resulting algorithms and identify settings where speedup gains can be expected. Our numerical evaluations show that this approach indeed achieves significant speedups in common computer vision tasks."
"A Comparison of Questionnaire Biases Across Sample Providers.  Survey research, like all methods, is fraught with potential sources of error that can significantly affect the validity and reliability of results. There are four major types of error common to surveys as a data collection method: (1) coverage error arising from certain segments of a target population being excluded, (2) nonresponse error where not all those selected for a sample respond, (3) sampling error which results from the fact that surveys only collect data from a subset of the population being measured, and (4) measurement error. Measurement error can arise from the wording and design of survey questions (i.e., instrument error), as well as the variability in respondent ability and motivation (i.e., respondent error) [17]. This paper focuses primarily on measurement error as a source of bias in surveys. It is well established that instrument error [34, 40] and respondent error (e.g., [21]) can yield meaningful differences in results. For example, variations in response order, response scales, descriptive text, or images used in a survey can lead to instrument error which can result in skewed response distributions. Certain types of questions can trigger other instrument error biases, such as the tendency to agree with statements presented in an agree/disagree format (acquiescence bias) or the hesitancy to admit undesirable behaviors or overreport desirable behaviors (social desirability bias). Respondent error is largely related to the amount of cognitive effort required to answer a survey and arises when respondents are either unable or unwilling to exert the required effort [21]. Such measurement error has been compared across survey modes, such as face-to-face, telephone, and Internet (e.g., [9, 18]), but little work has compared different Internet samples, such as crowdsourcing task platforms (e.g., Amazon??s Mechanical Turk), paywall surveys (e.g., Google Consumer Surveys), opt-in panels (e.g., Survey Sampling International), and probability based panels (e.g., the Gfk KnowledgePanel). Because these samples differ in recruiting, context, and incentives, respondents may be more or less motivated to effortfully respond to questions, leading to different degrees of bias in different samples. The specific instruments deployed to respondents in these different modes can also exacerbate the situation by requiring more or less cognitive effort to answer satisfactorily. 
The present study has two goals: 
Investigate the impact of question wording on response distributions in order to measure the strength of common survey biases arising from instrument and respondent error
Compare the variance in the degree of these biases across Internet survey samples with differing characteristics in order to determine whether certain types of samples are more susceptible to certain biases than others."
"Optimizing Binary Translation for Dynamically Generated Code.  Dynamic binary translation serves as a core technology that enables a wide range of important tools such as profiling, bug detection, program analysis, and security. Many of the target applications often include large amounts of dynamically generated code, which poses a special performance challenge in maintaining consistency between the source application and the translated application. This paper
introduces two approaches for optimizing binary translation of JITs and other dynamic code generators. First we present a system of efficient source code annotations that allow developers to demarcate dynamic code regions and identify code changes within those regions. The second technique avoids the annotation and source code requirements by automatically inferring the presence of a JIT and instrumenting its write instructions with translation consistency operations. We implemented these techniques in DynamoRIO and demonstrate performance improvements over the state-of-the-art DBT systems on JIT applications as high as 7.3x over base DynamoRIO and Pin."
"Orienteering Algorithms for Generating Travel Itineraries.  We study the problem of automatically and efficiently generating itineraries
for users who are on vacation.  We focus on the common case, wherein the trip
duration is more than a single day.  Previous efficient algorithms based on
greedy heuristics suffer from two problems.  First, the itineraries are often
unbalanced, with excellent days visiting top attractions followed by days of
exclusively lower-quality alternatives.  Second, the trips often re-visit
neighborhoods repeatedly in order to cover increasingly low-tier points of
interest.  Our primary technical contribution is an algorithm that addresses
both these problems by maximizing the quality of the worst day.  We give
theoretical results showing that this algorithm's competitive factor is within
a factor two of the guarantee of the best available algorithm for a single day,
across many variations of the problem.  We also give detailed empirical
evaluations using two distinct datasets: (a) anonymized Google historical visit
data and (b) Foursquare public check-in data.  We show first that the overall
utility of our itineraries is almost identical to that of algorithms
specifically designed to maximize total utility, while the utility of the worst
day of our itineraries is roughly twice that obtained from other approaches.
We then turn to evaluation based on human raters who score our itineraries only
slightly below the itineraries created by human travel experts with deep
knowledge of the area."
"Visualizing Dataflow Graphs of Deep Learning Models in TensorFlow.  We present a design study of the TensorFlow Graph Visualizer, part of the TensorFlow machine intelligence platform. This
tool helps users understand complex machine learning architectures by visualizing their underlying dataflow graphs. The tool works
by applying a series of graph transformations that enable standard layout techniques to produce a legible interactive diagram. To
declutter the graph, we decouple non-critical nodes from the layout. To provide an overview, we build a clustered graph using the
hierarchical structure annotated in the source code. To support exploration of nested structure on demand, we perform edge bundling
to enable stable and responsive cluster expansion. Finally, we detect and highlight repeated structures to emphasize a model??s
modular composition. To demonstrate the utility of the visualizer, we describe example usage scenarios and report user feedback.
Overall, users find the visualizer useful for understanding, debugging, and sharing the structures of their models."
"Multiscale Quantization for Fast Similarity Search.  We propose a multiscale quantization approach for fast similarity search on large, high-dimensional datasets. The key insight of the approach is that quantization methods, in particular product quantization, perform poorly when there is large variance in the norms of the data points. This is a common scenario for real-world datasets, especially when doing product quantization of residuals obtained from coarse vector quantization. To address this issue, we propose a multiscale formulation where we learn a separate scalar quantizer of the residual norm scales. All parameters are learned jointly in a stochastic gradient descent framework to minimize the overall quantization error. We provide theoretical motivation for the proposed technique and conduct comprehensive experiments on two large-scale public datasets, demonstrating substantial improvements in recall over existing state-of-the-art methods."
"Position Bias Estimation for Unbiased Learning to Rank in Personal Search.  A well-known challenge in learning from click data is its inherent bias and most notably position bias. Traditional click models aim to extract the (query, document) relevance and the estimated bias is usually discarded after relevance is extracted. In contrast, the most recent work on unbiased learning-to-rank can effectively leverage the bias and thus focuses on estimating bias rather than relevance. Existing approaches use search result randomization over a small percentage of production traffic to estimate the position bias. This is not desired because result randomization can negatively impact users' search experience. In this paper, we compare different schemes for result randomization (i.e., RandTopN and RandPair) and show their negative effect in personal search. Then we study how to infer such bias from regular click data without relying on randomization. We propose a regression-based Expectation-Maximization (EM) algorithm that is based on a position bias click model and that can handle highly sparse clicks in personal search. We evaluate our EM algorithm and the extracted bias in the learning-to-rank setting. Our results show that it is promising to extract position bias from regular clicks without result randomization. The extracted bias can improve the learning-to-rank algorithms significantly. In addition, we compare the pointwise and pairwise learning-to-rank models. Our results show that pairwise models are more effective in leveraging the estimated bias."
"Robust and low-complexity blind source separation for meeting rooms.  The aim of this work is to provide robust, low-complexity demixing of sound sources from a set of microphone signals for a typical meeting scenario where the source mixture is relatively sparse in time.
We define a similarity matrix that characterizes the similarity of the spatial signature of the observations at different time instants within a frequency band. Each entry of the similarity matrix is the sum of a set of kernelized similarity measures, each operating on single frequency bin. The kernelization leads to high robustness as it reduces the importance of outliers. Clustering by means of affinity propagation provides the separation of talkers without the need to specify the talker number in advance. The clusters can be used directly for separation, or they can be used as a global pre-processing method that identifies sources for an adaptive demixing procedure. Our experimental results confirm the that the approach performs significantly better than two reference methods."
"Black Box Optimization via a Bayesian-Optimized Genetic Algorithm.  We present a simple and robust optimization algorithm related to genetic algorithms, and with analogies to the popular CMA-ES search algorithm, that serves as a cheap alternative to Bayesian Optimization. The algorithm is robust against both monotonic transforms of the objective function value and affine transformations of the feasible region.  It is fast and easy to implement, and has performance comparable to CMA-ES on a suite of benchmarks while spending less CPU in the optimization algorithm, and can exhibit better overall performance than Bayesian Optimization when the objective function is cheap."
"Latent Cross: Making Use of Context in Recurrent Recommender Systems.  The success of recommender systems often depends on their ability to understand and make use of the context of the recommendation request.  Significant research has focused on how time, location, interfaces, and a plethora of other contextual features affect recommendations.  However, in using deep neural networks for recommender systems, researchers often ignore these contexts or incorporate them as ordinary features in the model. In this paper, we study how to effectively treat contextual data in neural recommender systems.  We begin with an empirical analysis of the conventional approach to context as features in feed-forward recommenders and demonstrate that this approach is inefficient in capturing common feature crosses.  We apply this insight to design a state-of-the-art RNN recommender system.  We first describe our RNN-based recommender system in use at YouTube.  Next, we offer ""Latent Cross,"" an easy-to-use technique to incorporate contextual data in the RNN by embedding the context feature first and then performing an element-wise product of the context embedding with model's hidden states. We demonstrate the improvement in performance by using this Latent Cross technique in multiple experimental settings."
"Neural Paraphrase Identification of Questions with Noisy Pretraining.  We present a solution to the problem of paraphrase identification of questions. We 
focus on a recent dataset of question pairs annotated with binary paraphrase labels and 
show that a variant of the decomposable attention model (Parikh et al., 2016) results in 
accurate performance on this task, while being far simpler than many competing neural 
architectures. Furthermore, when the model is pretrained on a noisy dataset of automatically 
collected question paraphrases, it obtains the best reported performance on the dataset."
"Learning Recurrent Span Representations for Extractive Question Answering.  The reading comprehension task, that asks questions about a given evidence document,
is a central problem in natural language understanding. Recent formulations
of this task have typically focused on answer selection from a set of candidates
pre-defined manually or through the use of an external NLP pipeline. However,
Rajpurkar et al. (2016) recently released the SQUAD dataset in which the answers
can be arbitrary strings from the supplied text. In this paper, we focus on
this answer extraction task, presenting a novel model architecture that efficiently
builds fixed length representations of all spans in the evidence document with a recurrent
network. We show that scoring explicit span representations significantly
improves performance over other approaches that factor the prediction into separate
predictions about words or start and end markers. Our approach improves
upon the best published results of Wang &amp; Jiang (2016) by 5% and decreases the
error of Rajpurkar et al.??s baseline by &gt; 50%."
"An RNN Model of Text Normalization.  This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And when we open-source our data, we will be providing a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions."
"Tangent: automatic differentiation using source code transformation in Python.  Automatic differentiation (AD) is an essential primitive for machine learning programming systems. Tangent is a Python package that performs AD using source code transformation (SCT) in Python. It takes numeric functions written in a syntactic subset of Python and NumPy as input, and transforms them into new Python functions which calculate a derivative. This approach to automatic differentiation is different from existing packages popular in machine learning, such as TensorFlow and Autograd. Advantages are that Tangent generates gradient code in Python which is readable by the user and easy to understand and debug. Tangent also introduces a new syntax for easily injecting code into the generated gradient code, further improving usability."
"The (Un)reliability of Saliency methods.  Saliency methods aim to explain the predictions of deep neural networks. These methods lack reliability when the explanation is sensitive to factors that do not contribute to the model prediction. We use a simple and common pre-processing step ---adding a constant shift to the input data--- to show that a transformation with no effect on the model can cause numerous methods to incorrectly attribute. In order to guarantee reliability, we posit that methods should fulfill input invariance, the requirement that a saliency method mirror the sensitivity of the model with respect to transformations of the input. We show, through several examples, that saliency methods that do not satisfy input invariance result in misleading attribution."
"Distilling a Neural Network Into a Soft Decision Tree.  Deep neural networks have proved to be a very effective way to perform classification tasks. They excel when the input data is high dimensional, the relationship between the input and the output is complicated, and the number of labeled training examples is large. But it is hard to explain why a learned network makes a particular classification decision on a particular test case. This is due to their reliance on distributed hierarchical representations. If we could take the knowledge acquired by the neural net and express the same knowledge in a model that relies on hierarchical decisions instead, explaining a particular decision would be much easier. We describe a way of using a trained neural net to create a type of soft decision tree that generalizes better than one learned directly from the training data."
"Who said what: Modeling individual labelers improves classification.  Data are often labelled by many different experts with each expert only labeling a small fraction of the data and each data point being labelled by several experts. This reduces the workload on individual experts and also gives a better estimate of the unobserved ground truth. When experts disagree, the standard approaches are to treat the majority opinion as the correct label or to model the correct label as a distribution. These approaches, however, do not make any use of potentially valuable information about which expert produced which label. To make use of this extra information, we propose modeling the experts individually and then learning mixing proportions for combining them in sample-specific ways. This allows us to give more weight to more reliable experts and makes it possible to take advantage of the unique strengths of individual experts at classifying certain types of data. Here we show that our approach leads to improved computer-aided diagnosis of diabetic retinopathy, where the experts are human doctors and the data are retinal images. We compare our method against those of Welinder and Perona, and Mnih and Hinton. Our work offers an innovative approach for dealing with the myriad real-world settings that lack ground truth labels."
"Wasserstein Auto-Encoders.  We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE). This regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality."
"Stochastic Variational Video Prediction.  Predicting the future in real-world settings, particularly from raw sensory observations such as images, is exceptionally challenging. Real-world events can be stochastic and unpredictable, and the high dimensionality and complexity of natural images requires the predictive model to build an intricate understanding of the natural world. Many existing methods tackle this problem by making simplifying assumptions about the environment. One common assumption is that the outcome is deterministic and there is only one plausible future which leads to low-quality predictions in real-world settings with stochastic dynamics. In contrast, we developed a variational stochastic method for video prediction that predicts a different possible future for each sample of its latent random variables. To the best of our knowledge, our model is the first to provide effective stochastic multi-frame prediction for real-world video. We demonstrate the capability of the proposed method in predicting detailed future frames of videos on multiple real-world datasets, both action-free and action-conditioned. We find that our proposed method produces substantially improved video predictions when compared to the same model without stochasticity, and to other stochastic video prediction methods. The TensorFlow-based implementation of our method will be open sourced upon publication."
"Improving image generative models with human interactions.  GANs provide a framework for training generative models which mimic a data distribution. However, in many cases we wish to train a generative model to optimize some auxiliary objective function within the data it generates, such as making more aesthetically pleasing images. In some cases, these objective functions are difficult to evaluate, e.g. they may require human interaction. Here, we develop a system for efficiently training a GAN to increase a generic rate of positive user interactions, which could represent aesthetic ratings or any other objective. To do this, we build a model of human behavior in the targeted domain from a relatively small set of interactions, and then use this behavioral model as an auxiliary loss function to improve the generative model. As a proof of concept, we demonstrate that this system is successful at improving positive interaction rates simulated from a variety of objectives, and characterize some factors that affect its performance."
"Fidelity-Weighted Learning.  Learning meaningful and useful task-dependent data representations requires many training instances -- but training labels are expensive to obtain, and may be of varying quality. This creates a fundamental quality-versus-quantity trade-off in the learning process. Do we learn from the small amount of high-quality data or the potentially large amount of weakly-labeled data (obtained from heuristics or crowd-sourcing, etc.)? We argue that if we could somehow know and take the label-quality into account when learning the data representation, we could get the best of both worlds. 
To this end, we propose ``fidelity-weighted learning'' (\fwl), a semi-supervised student-teacher approach for training deep neural networks using weakly-labeled data. \fwl modulates the parameter updates to a \emph{student} network (trained on the task we care about) on a per-sample basis according to the posterior confidence of the label-quality estimated by a \emph{teacher}.  Both student and teacher are learned from the data. We evaluate \fwl on two real-world tasks in information retrieval and natural language processing where we outperform state-of-the-art alternative semi-supervised methods, indicating that our approach makes better use of the label information and results in better task-dependent data representations."
"XGAN: Unsupervised Image-to-Image Translation for many-to-many Mappings.  Style transfer usually refers to the task of applying color and texture information from a specific style image to a given content image while preserving the structure of the latter. Here we tackle the more generic problem of semantic style transfer: given two unpaired collections of images, we aim to learn a mapping between the corpus-level style of each collection, while preserving semantic content shared across the two domains. We introduce XGAN (""Cross-GAN""), a dual adversarial autoencoder, which captures a shared representation of the common domain semantic content in an unsupervised way, while jointly learning the domain-to-domain image translations in both directions. We exploit ideas from the domain adaptation literature and define a semantic consistency loss which encourages the model to preserve semantics in the learned embedding space. We report promising qualitative results for the task of face-to-cartoon translation. The cartoon dataset we collected for this purpose is in the process of being released as a new benchmark for semantic style transfer."
"MorphNet: Fast &amp; Simple Resource-Constrained Structure Learning of Deep Networks.  We present MorphNet, an approach to automate the design of neural network structures. MorphNet iteratively shrinks and expands a network, shrinking via a resource-weighted sparsifying regularizer on activations and expanding via a uniform multiplicative factor on all layers. In contrast to previous approaches, our method is scalable to large networks, adaptable to specific resource constraints (e.g. the number of floating-point operations per inference), and capable of increasing the network's performance. When applied to standard network architectures on a wide variety of datasets, our approach discovers novel structures in each domain, obtaining higher performance while respecting the resource constraint."
"Are GANs Created Equal? A Large-Scale Study.  Generative adversarial networks (GAN) are a powerful subclass of generative models. Despite a very rich research activity leading to numerous interesting new GAN algorithms, it is still very hard to assess which algorithm(s) perform better than others. We conduct a neutral, multi-faceted large-scale empirical study encompassing the state-of-the art models and evaluation measures. We find that most models can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that improvements can come from computational budget and tuning more than fundamental algorithmic changes. To overcome some limitations of the current metrics, we also propose several data sets on which precision and recall can be computed. Our experimental results suggest that future GAN research should be based on more systematic and objective evaluation procedures. Also, we did not find evidence that any of the tested algorithms consistently outperform the original one."
"Bayesian Optimization for a Better Dessert.  We present a case study on applying Bayesian Optimization to a complex real-world system; our challenge was to optimize chocolate chip cookies.  The process was a mixed-initiative system where both human chefs, human raters, and a machine optimizer participated in 144 experiments.  This process resulted in highly rated cookies that deviated from expectations in some surprising ways -- much less sugar in California, and cayenne in Pittsburgh.  Our experience highlights the importance of incorporating domain expertise and the value of transfer learning approaches."
"Geometry-driven quantization for omnidirectional image coding.  In this paper we propose a method to adapt the quantization tables of typical block-based transform codecs when the input to the encoder is a panoramic image resulting from equirectangular projection of a spherical image. When the visual content is projected from the panorama to the viewport, a frequency shift is occurring. The quantization can be adapted accordingly: the quantization step sizes that would be optimal to quantize the transform coefficients of the viewport image block, can be used to quantize the coefficients of the panoramic block. As a proof of concept, the proposed quantization strategy has been used in JPEG compression. Results show that a rate reduction up to 2.99% can be achieved for the same perceptual quality of the spherical signal with respect to a standard quantization."
"Deformable block based motion estimation in omnidirectional image sequences.  This paper presents an extension of block-based motion estimation for omnidirectional videos, based on a camera and translational object motion model that accounts for the spherical geometry of the imaging system. We use this model to design a new algorithm to perform block matching in sequences of panoramic frames that are the result of the equirectangular projection. Experimental results demonstrate that significant gains can be achieved with respect to the classical exhaustive block matching algorithm (EBMA) in terms of accuracy of motion prediction. In particular, average quality improvements up to approximately 6dB in terms of Peak Signal to Noise Ratio (PSNR), 0.043 in terms of Structural SIMilarity index (SSIM), and 2dB in terms of spherical PSNR, can be achieved on the predicted frames."
"Quantitative evaluation of omnidirectional video quality.  Omnidirectional video encoding and delivery are rapidly evolving fields, where choosing an efficient representation for storage and transmission of pixel data is critical. Given that there are a number of projections (pixel representations), a projection independent measure is needed to evaluate the merits of different options. We present a technique to evaluate projection quality by rendering virtual views and use this to evaluate three projections in common use: Equirectangular, Cubemap, and Equi-Angular Cubemap. Through evaluation on dozens of videos, our metrics rank the projection types consistently with pixel density computations and small scale user studies."
"BranchOut: Regularization for Online Ensemble Tracking with CNNs.  We propose a simple but effective online ensemble tracking algorithm based on convolutional neural networks (CNNs). The proposed algorithm employs a CNN for target representation, which has a common convolutional layers but has multiple branches of fully connected layers. For better regularization, a subset of branches in the CNN are selected randomly for online learning whenever target appearance models need to be updated. We call this technique BranchOut. In addition, each branch may have a different number of layers conceptually to maintain variable abstraction
levels of target appearances. BranchOut with multilevel target representation allows us to learn robust target appearance models with great diversity and makes it possible to handle various challenges related to target appearance modeling effectively. Our algorithm is tested standard tracking benchmarks and shows the state-of-the-art performance even without pretraining on tracking sequences."
"To prune, or not to prune: exploring the efficacy of pruning for model compression.  Model pruning seeks to induce sparsity into the deep network's various connection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports Han et al. (2015) Narang, et al. (2017) prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model's dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy."
"Improving Smiling Detection with Race and Gender Diversity.  Recent progress in deep learning has been accompanied by a growing concern for whether models are fair for users, with equally good performance across different demographics. In computer vision research, such questions are relevant to face detection and the related task of face attribute detection, among others. We measure race and gender inclusion in the context of smiling detection, and introduce a method for improving smiling detection across demographic groups. Our method introduces several modifications over existing detection methods, leveraging twofold transfer learning to better model facial diversity. Results show that this technique improves accuracy against strong baselines for most demographic groups as well as overall. Our best-performing model defines a new state-of-the-art for smiling detection, reaching 91% on the Faces of the World dataset. The accompanying multi-head diversity classifier also defines a new state-of-the-art for gender classification, reaching 93.87% on the Faces of the World dataset. This research demonstrates the utility of modeling race and gender to improve a face attribute detection task, using a twofold transfer learning framework that allows for privacy towards individuals in a target dataset."
"Deformable Shape Completion with Graph Convolutional Autoencoders.  The availability of affordable and portable depth sensors has made scanning objects and people simpler than ever. However, dealing with occlusions and missing parts is still a significant challenge. The problem of reconstructing a (possibly non-rigidly moving) 3D object from a single or multiple partial scans has received increasing attention in recent years. In this work, we propose a novel learning-based method for the completion of partial shapes. Unlike the majority of existing approaches, our method focuses on objects that can undergo non-rigid deformations. The core of our method is a variational autoencoder with graph convolutional operations that learns a latent space for complete realistic shapes. At inference, we optimize to find the representation in this latent space that best fits the generated shape to the known partial input. The completed shape exhibits a realistic appearance on the unknown part. We show promising results towards the completion of synthetic and real scans of human body and face meshes exhibiting different styles of articulation and partiality."
"On Lattice Generation for Large Vocabulary Speech Recognition.  Lattice generation is an essential feature of the decoder for many
speech recognition applications. In this paper, we first review
lattice generation methods for WFST-based decoding and describe in a
uniform formalism two established approaches for state-of-the-art
speech recognition systems: the phone pair and the N-best histories
approaches. We then present a novel optimization method,
pruned determinization followed by minimization, that produces a
deterministic minimal lattice that retains all paths within specified
weight and lattice size thresholds.  Experimentally, we show that
before optimization, the phone-pair and the N-best histories
approaches each have conditions where they perform better when
evaluated on video transcription and mixed voice search and dictation
tasks. However, once this lattice optimization procedure is applied,
the phone pair approach has the lowest oracle WER for a given lattice
density by a significant margin. We further show that the pruned
determinization presented here is efficient to use during decoding
unlike classical weighted determinization from which it is derived.
Finally, we consider on-the-fly lattice rescoring in which the
lattice generation and combination with the secondary LM are done
in one step.  We compare the phone pair and N-best histories
approaches for this scenario and find the former superior in our
experiments."
"The Case for Learned Index Structures.  Indexes are models: a BTree-Index can be seen as a model to map a key to the position of a record within a sorted array, a Hash-Index as a model to map a key to a position of a record within an unsorted array, and a BitMap-Index as a model to indicate if a data record exists or not.  In this exploratory research paper, we start from this premise and posit that all existing index structures can be replaced with other types of models, including deep-learning models, which we term learned indexes.  The key idea is that a model can learn the sort order or structure of lookup keys and use this signal to effectively predict the position or existence of  records.  We theoretically analyze under which conditions learned indexes outperform traditional index structures and describe the main challenges in designing learned index structures.  Our initial results show, that by using neural nets we are able to outperform cache-optimized B-Trees by up to 70% in speed while saving an order-of-magnitude in memory over several real-world data sets.  More importantly though, we believe that the idea of replacing core components of a data management system through learned models has far reaching implications for future systems designs and that this work just provides a glimpse of what might be possible."
"HARP: Hierarchical Representation Learning for Networks.  We present HARP, a novel method for learning low dimensional embeddings of a graph's nodes which preserves higher-order structural features. Our proposed method achieves this by compressing the input graph prior to embedding it, effectively avoiding troublesome embedding configurations (i.e. local minima) which can pose problems to non-convex optimization. HARP works by finding a smaller graph which approximates the global structure of its input. This simplified graph is used to learn a set of initial representations, which serve as good initializations for learning representations in the original, detailed graph. We inductively extend this idea, by decomposing a graph in a series of levels, and then embed the hierarchy of graphs from the coarsest one to the original graph. HARP is a general meta-strategy to improve all of the state-of-the-art neural algorithms for embedding graphs, including DeepWalk, LINE, and Node2vec. Indeed, we demonstrate that applying HARP's hierarchical paradigm yields improved implementations for all three of these methods, as evaluated on both classification tasks on real-world graphs such as DBLP, BlogCatalog, CiteSeer, and Arxiv, where we achieve a performance gain over the original implementations by up to 14% Macro F1."
"Choice Models for Product Optimization and Pricing.  Presented at Enterprise Applications of the R Language (EARL), London, 2017. This presentation introduces the topic of discrete choice models, also known as conjoint analysis, and how to model them in R. Choice models estimate the importance of product (or project) features and how customers or other stakeholders value tradeoffs with respect to cost. They are used to determine optimal pricing, product demand, and differences among groups of consumers. We describe the concept in the context of survey research and discuss why a choice model is preferable to other models. Next, we will see how to estimate such models in R, with an overview of available R packages. This includes a brief walkthrough of code (made publicly available) with an end-to-end demonstration. Finally, we describe a real-world application at Google, applying the model to understand civic behavior. The technical content is approachable for any R user; the talk will be of interest to R users who are engaged in business strategy, product planning, customer insight, forecasting, or market research."
"3D object classification and retrieval with Spherical CNNs.  3D object classification and retrieval presents many challenges that are not present in the traditional (planar) image setting. First, there is the question of shape representation. Face-vertex meshes, for instance, are widely used in computer graphics, but their irregular structure complicate their use as inputs to learning models. Previous works have converted meshes to more structured representations, such a collections of rendered views or volumetric grids, in order to feed them to 2D or 3D CNNs. These representations, however, are redundant and wasteful, requiring large amounts of storage, pre-processing time, and large networks with millions of parameters to handle them. Another challenge is how to treat object orientations. Orientation-invariance is a desired property for any classification engine, yet most current models do not address this explicitly, rather requiring increased model and sample complexity to handle arbitrary input orientations. We present a model that aims to be efficient in both the number of learnable parameters and input size. We leverage the group convolution equivariance properties; more specifically, the spherical convolution, to build a network that learns feature maps equivariant to SO(3) actions by design. By mapping a 3D input to the surface of a sphere, we also end up with a small input size."
Now Playing: Continuous low-power music recognition.  Existing music recognition applications require both user activation and a connection to a server that performs the actual recognition. In this paper we present a low power music recognizer that runs entirely on a mobile phone and automatically recognizes music without requiring any user activation. A small music detector runs continuously on the mobile phone??s DSP (digital signal processor) chip and only wakes main the processor when it is confident that music is present. Once woken the detector on the main processor is provided with an 8s buffer of audio which is then fingerprinted and compared to the stored fingerprints in the on-device fingerprint database of over 70000 songs.
"Embedding methods for fine-grained entity type classification.  We propose a new approach to the task of fine grained entity type classifications based on label embeddings that allows for information sharing among related labels. Specifically, we learn an embedding for each label and each feature such that labels which frequently co-occur are close in
the embedded space. We show that it outperforms state-of-the-art methods on two fine grained entity-classification benchmarks and that the model can exploit the finer-grained labels to improve classification of standard coarse types."
"Wavenet based low rate speech coding.  Traditional parametric coding of speech facilitates low rate but provides poor reconstruction quality because of the inadequacy of the model used. We describe how a WaveNet generative speech model can be used to generate high quality speech from the bit stream of a standard parametric coder operating at 2.4 kb/s. We compare this parametric coder with a waveform coder based on the same generative model and show that approximating the signal waveform incurs a large rate penalty. Our experiments confirm the high performance of the WaveNet based coder and show that the speech produced by the system is able to additionally perform implicit bandwidth extension and does not significantly impair recognition of the original speaker for the human listener, even when that speaker has not been used during the training of the generative model."
"A Bayesian Clearing Mechanism for Combinatorial Auctions.  In this paper, we cast the problem of combinatorial auction design in a Bayesian framework in order to incorporate prior information into the auction process and minimize the number of rounds. We develop a generative model of bidder valuations and market prices such that clearing prices become maximum a posteriori estimates given observed bidder valuations. The model then forms the basis of an auction process which alternates between refining estimates of bidder valuations, and computing candidate clearing prices. We provide an implementation of the auction using assumed density filtering to estimate valuations and expectation maximization to compute prices. An empirical evaluation over a range of valuation domains demonstrates that our Bayesian auction mechanism is very competitive against a conventional combinatorial clock auction, even under the most favorable choices of price increment for this baseline."
"Hiding Images in Plain Sight:  Deep Steganography.  Steganography is the practice of concealing a secret message within another,
ordinary, message. Commonly, steganography is used to unobtrusively hide a small
message within the noisy regions of a larger image. In this study, we attempt
to place a full size color image within another image of the same size. Deep
neural networks are simultaneously trained to create the hiding and revealing
processes and are designed to specifically work as a pair. The system is trained on
images drawn randomly from the ImageNet database, and works well on natural
images from a wide variety of sources. Beyond demonstrating the successful
application of deep learning to hiding images, we carefully examine how the result
is achieved and explore extensions. Unlike many popular steganographic methods
that encode the secret message within the least significant bits of the carrier image,
our approach compresses and distributes the secret image??s representation across
all of the available bits."
"Learning to Attack: Adversarial Transformation Networks.  With the rapidly increasing popularity of deep neural networks
for image recognition tasks, a parallel interest in generating
adversarial examples to attack the trained models has
arisen. To date, these approaches have involved either directly
computing gradients with respect to the image pixels or directly
solving an optimization on the image pixels. We generalize
this pursuit in a novel direction: can a separate network
be trained to efficiently attack another fully trained network?
We demonstrate that it is possible, and that the generated
attacks yield startling insights into the weaknesses of
the target network. We call such a network an Adversarial
Transformation Network (ATN). ATNs transform any input
into an adversarial attack on the target network, while being
minimally perturbing to the original inputs and the target network??s
outputs. Further, we show that ATNs are capable of
not only causing the target network to make an error, but can
be constructed to explicitly control the type of misclassification
made. We demonstrate ATNs on both simple MNIST digit
classifiers and state-of-the-art ImageNet classifiers deployed
by Google, Inc.: Inception ResNet-v2."
"Learning typographic style: from discrimination to synthesis.  Typography is a ubiquitous art form that affects our understanding, perception and trust in what we read. Thousands of different font-faces have been created with enormous variations in the characters. In this paper, we learn the style of a font by analyzing a small subset of only four letters. From these four letters, we learn two tasks. The first is a discrimination task: given the four letters and a new candidate letter, does the new letter belong to the same font? Second, given the four basis letters, can we generate all of the other letters with the same characteristics as those in the basis set? We use deep neural networks to address both tasks, quantitatively and qualitatively measure the results in a variety of novel manners, and present a thorough investigation of the weaknesses and strengths of the approach. All of the experiments are conducted with publicly available font sets."
"Learning Deep Models of Optimization Landscapes.  In all but the most trivial optimization problems,
the structure of the solutions exhibit complex interdependencies
between the input parameters. Decades of research with
stochastic search techniques has shown the benefit of explicitly
modeling the interactions between sets of parameters and the
overall quality of the solutions discovered. We demonstrate a
novel method, based on learning deep networks, to model the
global landscapes of optimization problems. To represent the
search space concisely and accurately, the deep networks must
encode information about the underlying parameter interactions
and their contributions to the quality of the solution. Once
the networks are trained, the networks are probed to reveal
parameter combinations with high expected performance with
respect to the optimization task. These estimates are used to
initialize fast, randomized, local-search algorithms, which in
turn expose more information about the search space that is
subsequently used to refine the models. We demonstrate the
technique on multiple problems that have arisen in a variety of
real-world domains, including: packing, graphics, job scheduling,
layout and compression. Strengths, limitations, and extensions of
the approach are extensively discussed and demonstrated."
"A Cascade Architecture for Keyword Spotting on Mobile Devices.  We present a cascade architecture for keyword spotting with speaker verification on mobile devices. By pairing a small computational footprint with specialized digital signal processing (DSP) chips, we are able to achieve low power consumption while continuously listening for a keyword."
"Efficient Implementation of the Room Simulator for Training Deep Neural Network Acoustic Models.  In this paper, we describe how to efficiently implement an acoustic room simulator to generate large-scale simulated data for training deep neural networks. Even though Google Room Simulator in [1] was shown to be quite effective in reducing the Word Error Rates (WERs) for far-field applications by generating simulated far-field training sets, it requires a very large number of Fast Fourier Transforms (FFTs) of large size. Room Simulator in [1] used approximately 80 percent of Central Processing Unit (CPU) usage in our CPU + Graphics Processing Unit (GPU) training architecture [2]. In this work, we implement an efficient OverLap Addition (OLA) based filtering using the open-source FFTW3 library. Further, we investigate the effects of the Room Impulse Response (RIR) lengths. Experimentally, we conclude that we can cut the tail portions of RIRs whose power is less than 20 dB below the maximum power without sacrificing the speech recognition accuracy. However, we observe that cutting RIR tail more than this threshold harms the speech recognition accuracy for rerecorded test sets. Using these approaches, we were able to reduce CPU usage for the room simulator portion down to 9.69 percent in CPU/GPU training architecture. Profiling result shows that we obtain 22.4 times speed-up on a single machine and 37.3 times speed up on Google's distributed training infrastructure."
"The Unexpected Entry and Exodus of Women in Computing and HCI in India.  In India, women represent 45% of total computer science enrollment in universities, almost three times the rate in the United States, where it is 17%. At the same time, women make up an estimated 25-30\% of the HCI community in India, almost half the rate in the U.S. We investigate the complexities of these surprising phenomena through qualitative research of Indian computer science and human-computer interaction researchers and  professionals at various life stages, from undergraduates to senior scientists. We find that cultural norms exert a powerful force on the representation of women in the tech sector, which is expressed in India as a societal whiplash in which women are encouraged to go into computing as students, but then expected to exit soon after they enter the tech workforce. Specifically, we find among other things that Indian familial norms play a significant role in pressuring young women into computing as a field; that familial pressures and workplace discrimination then cause a precipitous exit of women from computing at the onset of marriage; and that HCI occupies an interstitial space between art and technology that affects women's careers. Our findings underscore the societal influence on women's representation in the tech sector and invite further participation by the HCI community in related questions."
"Learning Unified Embedding for Apparel Recognition.  In apparel recognition, specialized models (e.g. models
trained for a particular vertical like dresses) can signifi-
cantly outperform general models (i.e. models that cover
a wide range of verticals). Therefore, deep neural network
models are often trained separately for different verticals
(e.g. [7]). However, using specialized models for different
verticals is not scalable and expensive to deploy. This paper
addresses the problem of learning one unified embedding
model for multiple object verticals (e.g. all apparel classes)
without sacrificing accuracy. The problem is tackled from
two aspects: training data and training difficulty. On the
training data aspect, we figure out that for a single model
trained with triplet loss, there is an accuracy sweet spot in
terms of how many verticals are trained together. To ease
the training difficulty, a novel learning scheme is proposed
by using the output from specialized models as learning targets
so that L2 loss can be used instead of triplet loss. This
new loss makes the training easier and make it possible for
more efficient use of the feature space. The end result is
a unified model which can achieve the same retrieval accuracy
as a number of separate specialized models, while
having the model complexity as one. The effectiveness of
our approach is shown in experiments."
"The Good, the Bad and the Ugly: A Study of Security Decisions in a Cyber-Physical Systems Game.  Stakeholders?? security decisions play a fundamental role in determining security requirements, yet, little is currently understood about how different stakeholder groups within an organisation approach security and the drivers and tacit biases underpinning their decisions. We studied and contrasted the security decisions of three demographics ?? security experts, computer scientists and managers ?? when playing a tabletop game that we designed and developed. The game tasks players with managing the security of a cyber-physical environment while facing various threats. Analysis of 12 groups of players (4 groups in each of our demographics) reveals strategies that repeat in particular demographics, e.g., managers and security experts generally favoring technological solutions over personnel training, which computer scientists preferred. Surprisingly, security experts were not ipso facto better players ?? in some cases, they made very questionable decisions ?? yet they showed a higher level of confidence in themselves. We classified players?? decision-making processes, i.e., procedure-, experience-, scenario- or intuition-driven. We identified decision patterns, both good practices and typical errors and pitfalls. Our game provides a requirements sandbox in which players can experiment with security risks, learn about decision-making and its consequences, and reflect on their own perception of security."
"CycleGAN, a Master of Steganography.  CycleGAN is one of the latest successful approaches to learning a correspondence between two distinct probability distributions. However, it may not always be possible or easy to find a natural one-to-one mapping between two domains. We demonstrate that in such cases CycleGAN model tends to ""hide"" at least some information about the input sample in the indistinguishable noise added to the output. This makes the network output look ""realistic"", while also allowing the complementary transformation to recover the original sample and thus satisfy cycle consistency requirement."
"AdaNet: Adaptive structural learning of artificial neural networks.  We present new algorithms for adaptively learning
artificial neural networks. Our algorithms
(ADANET) adaptively learn both the structure
of the network and its weights. They are
based on a solid theoretical analysis, including
data-dependent generalization guarantees that we
prove and discuss in detail. We report the results
of large-scale experiments with one of our
algorithms on several binary classification tasks
extracted from the CIFAR-10 dataset and on the
Criteo dataset. The results demonstrate that our
algorithm can automatically learn network structures
with very competitive performance accuracies
when compared with those achieved by neural
networks found by standard approaches."
"Structured prediction theory based on factor graph complexity.  We present a general theoretical analysis of structured prediction with a series
of new results. We give new data-dependent margin guarantees for structured
prediction for a very wide family of loss functions and a general family of hypotheses,
with an arbitrary factor graph decomposition. These are the tightest margin
bounds known for both standard multi-class and general structured prediction
problems. Our guarantees are expressed in terms of a data-dependent complexity
measure, factor graph complexity, which we show can be estimated from data and
bounded in terms of familiar quantities for several commonly used hypothesis sets
along with a sparsity measure for features and graphs. Our proof techniques include
generalizations of Talagrand??s contraction lemma that can be of independent
interest.
We further extend our theory by leveraging the principle of Voted Risk Minimization
(VRM) and show that learning is possible even with complex factor graphs. We
present new learning bounds for this advanced setting, which we use to design two
new algorithms, Voted Conditional Random Field (VCRF) and Voted Structured
Boosting (StructBoost). These algorithms can make use of complex features and
factor graphs and yet benefit from favorable learning guarantees. We also report
the results of experiments with VCRF on several datasets to validate our theory."
"Boosting with abstention.  We present a new boosting algorithm for the key scenario of binary classification
with abstention where the algorithm can abstain from predicting the label of a point,
at the price of a fixed cost. At each round, our algorithm selects a pair of functions,
a base predictor and a base abstention function. We define convex upper bounds
for the natural loss function associated to this problem, which we prove to be
calibrated with respect to the Bayes solution. Our algorithm benefits from general
margin-based learning guarantees which we derive for ensembles of pairs of base
predictor and abstention functions, in terms of the Rademacher complexities of the
corresponding function classes. We give convergence guarantees for our algorithm
along with a linear-time weak-learning algorithm for abstention stumps. We also
report the results of several experiments suggesting that our algorithm provides a
significant improvement in practice over two confidence-based algorithms."
"Learning with rejection.  We introduce a novel framework for classification with a rejection
option that consists of simultaneously learning two functions:
a classifier along with a rejection function. We present a full theoretical
analysis of this framework including new data-dependent learning
bounds in terms of the Rademacher complexities of the classifier and
rejection families as well as consistency and calibration results. These
theoretical guarantees guide us in designing new algorithms that can
exploit different kernel-based hypothesis sets for the classifier and rejection
functions. We compare and contrast our general framework with the
special case of confidence-based rejection for which we devise alternative
loss functions and algorithms as well. We report the results of several experiments
showing that our kernel-based algorithms can yield a notable
improvement over the best existing confidence-based rejection algorithm."
"Mobile Technologies for Conducting, Augmenting and Potentially Replacing Surveys.  Public opinion research is entering a new era, one in which traditional survey research may play a less dominant role. The proliferation of new technologies, such as mobile devices and social media platforms, are changing the societal landscape across which public opinion researchers operate. The ways in which people both access and share information about opinions, attitudes, and behaviors have gone through perhaps a greater transformation in the last decade than in any previous point in history and this trend appears likely to continue.  The rapid adoption of smartphones and ubiquity of social media are interconnected trends which may provide researchers with new data collection tools and alternative sources of information to augment or, in some cases, provide alternatives to more traditional data collection methods. However, this brave new world is not without its share of issues and pitfalls ?? technological, statistical, methodological, and ethical. As the leading association of public opinion research professionals, AAPOR is uniquely situated to examine and assess the potential impact of these ??emerging technologies?? on the broader discipline and industry of opinion research. In September 2012, AAPOR Council approved the formation of the Emerging Technologies Task Force with the goal of focusing on two critical areas: smartphones as data collection vehicles and social media as platform and information source. The purposes of the task force are to:
define and delineate the scope and landscape of each area;
describe the potential impact in terms of quality, efficiency, timeliness and analytic reach;
discuss opportunities and challenges based on available research;
delineate some of the key legal and ethical considerations; and
detail the gaps in our understanding and propose avenues of future research.
The report here examines the potential impact of mobile technologies on public opinion research ?? as a vehicle for facilitating some aspect of the survey research process (i.e., recruitment, questionnaire administration, reducing burden, etc.) and/or augmenting or replacing traditional survey research methods (i.e., location data, visual data, and the like)."
"Graph sketching-based Space-efficient Data Clustering.  In this paper, we address the problem of recovering arbitrary-shaped data clusters from datasets while facing high space constraints, as this is for instance the case in many real-world applications when analysis algorithms are directly deployed on resources-limited mobile devices collecting the data. We present DBMSTClu a new space-efficient density-based non-parametric method working on a Minimum Spanning Tree (MST) recovered from a limited number of linear measurements i.e. a sketched version of the dissimilarity graph G between the N objects to cluster. Unlike k-means, k-medians or k-medoids algorithms, it does not fail at distinguishing clusters with particular forms thanks to the property of the MST for expressing the underlying structure of a graph. No input parameter is needed contrarily to DBSCAN or the Spectral Clustering method. An approximate MST is retrieved by following the dynamic semi-streaming model in handling the dissimilarity graph G as a stream of edge weight updates which is sketched in one pass over the data into a compact structure requiring O(N polylog(N)) space, far better than the theoretical memory cost O(N^{2}) of G. The recovered approximate MST T as input, DBMSTClu
then successfully detects the right number of non-convex clusters by performing relevant cuts on T in a time linear in N. We provide theoretical guarantees on the quality of the clustering partition and also demonstrate its advantage over the existing state-of-the-art on several datasets."
"The Geometry of Random Features.  We present an in-depth examination of the effectiveness of radial basis function kernel (beyond
Gaussian) estimators based on orthogonal random feature maps. We show that orthogonal estimators
outperform state-of-the-art mechanisms that use iid sampling under weak conditions for tails of the
associated Fourier distributions. We prove that for the case of many dimensions, the superiority of the orthogonal transform over iid methods can be accurately measured by a property we define called the charm of the kernel, and that orthogonal random features provide optimal kernel estimators.
Furthermore, we provide the first theoretical results which explain why orthogonal random features outperform unstructured on downstream tasks such as kernel ridge regression by showing that orthogonal random features provide kernel algorithms with better spectral properties than the previous state-of-the-art. Our results enable practitioners more generally to estimate the benefits from applying orthogonal transforms."
"The Role of Surveys in the Era of ??Big Data??.  Survey data have recently been compared and contrasted with so-called ??Big Data?? and some observers have speculated about how Big Data may eliminate the need for survey research. While both Big Data and survey research have a lot to offer, very little work has examined the ways that they may best be used together to provide richer datasets. This chapter offers a broad definition of Big Data and proposes a framework for understanding how the benefits and error properties of Big Data and surveys may be leveraged in ways that are complementary. This chapter presents several of the opportunities and challenges that may be faced by those attempting to bring these different sources of data together."
"Large-Scale 3D Scene Classification With Multi-View Volumetric CNN.  We introduce a method to classify imagery using a convo- lutional neural network (CNN) on multi-view image pro- jections. The power of our method comes from using pro- jections of multiple images at multiple depth planes near the reconstructed surface. This enables classification of categories whose salient aspect is appearance change un- der different viewpoints, such as water, trees, and other materials with complex reflection/light response proper- ties. Our method does not require boundary labelling in images and works on pixel-level classification with a small (few pixels) context, which simplifies the cre- ation of a training set. We demonstrate this application on large-scale aerial imagery collections, and extend the per-pixel classification to robustly create a consistent 2D classification which can be used to fill the gaps in non- reconstructible water regions. We also apply our method to classify tree regions. In both cases, the training data can quickly be generated using a small number of manually- created polygons on a map. We show that even with a very simple and standard network our CNN outperforms the state-of-the-art image classification, the Inception-V3 model retrained from a large collection of aerial images."
"Federated Control with Hierarchical Multi-Agent Deep Reinforcement Learning.  We present a framework combining hierarchical and multi-agent deep reinforcement learning approaches to solve coordination problems among a multitude of agents using a semi-decentralized model. The framework extends the multi-agent learning setup by introducing a meta-controller that guides the communication between agent pairs, enabling agents to focus on communicating with only one other agent at any step. This hierarchical decomposition of the task allows for efficient exploration to learn policies that identify globally optimal solutions even as number of collaborating agents increases. We show initial experimental results on a simulated multi-task dialogue problem."
"Endogenous Budget Constraints.  This paper considers a model in which a single buyer seeks to buy a continuum of objects from a continuum of sellers. The sellers specify prices at which they are willing to sell their objects to the buyer, and the buyer then decides which objects to purchase. I illustrate that the buyer has an incentive to commit to a binding budget constraint before seeking offers from the sellers."
"Loss Functions for Predicted Click-Through Rates in Auctions for Online Advertising.  We characterize the optimal loss functions for predicted click-through rates in auctions for online advertising. Whereas standard loss functions such as mean squared error or log likelihood severely penalize large mispredictions while imposing little penalty on smaller mistakes, a loss function reflecting the true economic loss from mispredictions imposes significant penalties for small mispredictions and only slightly larger penalties on large mispredictions. We illustrate that when the model is misspecified using such a loss function can improve economic efficiency, but the efficiency gain is likely to be small."
"No Classification without Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing World.  Modern machine learning systems such as image classifers rely heavily on
  large scale data sets for training.  Such data sets are costly to create,
  thus in practice a small number of freely available, open source data sets
  are widely used.  Such strategies may be particularly important for ML
  applications in the developing world, where resources may be constrained
  and the cost of creating suitable large scale data sets may be a
  blocking factor.  However, we suggest that examining the {\em geo-diversity}
  of open data sets is critical before adopting a data set for such use
  cases.  In particular, we analyze two large, publicly available image
  data sets to assess geo-diversity and find that these data sets appear
  to exhibit a observable amerocentric and eurocentric representation bias.
  Further, we perform targeted analysis on classifiers that use these data
  sets as training data to assess the impact of these training distributions,
  and find strong differences in the relative performance on images from
  different locales.  These results emphasize the need to ensure
  geo-representation when constructing data sets for use in the developing
  world."
"Keyword Spotting for Google Assistant Using Contextual Speech Recognition.  Modern machine learning systems such as image classifers rely heavily on
  large scale data sets for training.  Such data sets are costly to create,
  thus in practice a small number of freely available, open source data sets
  are widely used.  Such strategies may be particularly important for ML
  applications in the developing world, where resources may be constrained
  and the cost of creating suitable large scale data sets may be a
  blocking factor.  However, we suggest that examining the {\em geo-diversity}
  of open data sets is critical before adopting a data set for such use
  cases.  In particular, we analyze two large, publicly available image
  data sets to assess geo-diversity and find that these data sets appear
  to exhibit a observable amerocentric and eurocentric representation bias.
  Further, we perform targeted analysis on classifiers that use these data
  sets as training data to assess the impact of these training distributions,
  and find strong differences in the relative performance on images from
  different locales.  These results emphasize the need to ensure
  geo-representation when constructing data sets for use in the developing
  world."
"The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction.  Creating reliable, production-level machine learning systems brings on a host of concerns not found in small toy examples or even large offline research experiments. Testing and monitoring are key considerations for ensuring the production-readiness of an ML system, and for reducing technical debt of ML systems. But it can be difficult to formulate specific tests, given that the actual prediction behavior of any given model is difficult to specify a priori. In this paper, we present 28 specific tests and monitoring needs, drawn from experience with a wide range of production ML systems to help quantify these issues and present an easy to follow road-map to improve production readiness and pay down ML technical debt."
"SemEval-2016 Task 1: Semantic Textual Similarity, Monolingual and Cross-Lingual Evaluation.  Semantic Textual Similarity (STS) is the degree of semantic equivalence between two snippets of text. Similarity is expressed on an ordinal scale that spans from semantic equivalence to the two texts being completely dissimilar to each other with intermediate values capturing specifically defined levels of incompletely overlapping similarity. While prior evaluations constrained themselves to just monolingual snippets of text, the 2016 shared task includes a pilot sub-task on computing semantic similarity on cross-lingual text snippets. This year's traditional monolingual sub-task includes the evaluation of English text snippets from the following four domains: Plagiarism Detection, Post-Edited Machine Translations, Question-Answering, and News Article Headlines. From the question-answering domain we included both question-question and answer-answer pairs. The cross-lingual task provides paired English-Spanish text snippets drawn from the same sources as the monolingual data as well as independently sampled news data. The monolingual task attracted 42 participating teams producing 118 system submissions, while the cross-lingual pilot task attracted 24 teams submitting 26 systems."
"On the move: Mixed methods for research in Mobile HCI.  Mixed methods are commonly used within HCI, reflecting the field's many contributing disciplines. While there is a rich literature on mixed methodology in the social sciences, particularly education, health, and evaluation, HCI researchers have engaged little with this literature or its resulting frameworks for mixed methods research design. The goal of this workshop is to bring together researchers in industry and academia who practice mixed methods for mobile technology design in order to formalize the processes that we have used with success, to document our challenges, and to make recommendations for designing mixed methods research in the subdiscipline. We will accomplish these goals with a number of hands on activities designed to share participants?? individual expertise, challenge pre-conceived notions, and later converge on a set of frameworks."
"Building Empathy: Scaling User Research for Organizational Impact.  Building user empathy in a tech organization is crucial to ensure that products are designed with an eye toward user needs and experiences. The Pokerface program is a Google internal user empathy campaign with 26 researchers that helped more than 1500 employees??including engineers, product managers, designers, analysts, and program managers across more than 15 sites??have first-hand experiences with their users. Here, we discuss the goals of the Pokerface program, some challenges that we have faced during execution, and the impact we have measured thus far."
"Gender Equity in Technologies: Considerations for Design in the Global South.  Here we share our reflections on creating an inclusive design process, based on our research on gender and technology in India, Pakistan, Bangladesh, and Nigeria. We are currently expanding our research to Mexico and Brazil. We have interviewed nearly 200 women, including new mothers, rural farm workers, call-center workers, CS Ph.D. students, and bankers. We suggest four considerations to help designers and researchers navigate the contours of gender inclusion."
"Adversarial Patch.  We present a method to create universal, robust, targeted adversarial image patches in the real world. The patches are universal because they can be used to attack any scene, robust because they work under a wide variety of transformations, and targeted because they can cause a classifier to output any target class. These adversarial patches can be printed, added to any scene, photographed, and presented to image classifiers; even when the patches are small, they cause the classifiers to ignore the other items in the scene and report a chosen target class."
"Hidden in Plain Sight: Classifying Emails Using Embedded Image Contents.  A vast majority of the emails received by people today are machine-generated by businesses communicating with consumers. While some emails originate as a result of a transaction (e.g., hotel or restaurant reservation confirmations, online purchase receipts, shipping notifications, etc.), a large fraction are commercial emails promoting an offer (a special sale, free shipping, available for a limited time, etc.). The sheer number of these promotional emails makes it difficult for users to read all these emails and decide which ones are actually interesting and actionable. In this paper, we tackle the problem of extracting information from commercial emails promoting an offer to the user. This information enables an email platform to build several new experiences that can unlock the value in these emails without the user having to navigate and read all of them. For instance, we can highlight offers that are expiring soon, or display a notification when there's an unexpired offer from a merchant if your phone recognizes that you are at that merchant's store. A key challenge in extracting information from such commercial emails is that they are often image-rich and contain very little text. Training a machine learning (ML) model on a rendered image-rich email and applying it to each incoming email can be prohibitively expensive. In this paper, we describe a cost-effective approach for extracting signals from both the text and image content of commercial emails in the context of a free email platform that serves over a billion users around the world. The key insight is to leverage the template structure of emails, and use off-the-shelf OCR techniques to obtain the text from images to augment the existing text features offline. Compared to a text-only approach, we show that we are able to identify 9.12% more email templates corresponding to ~5% more emails being identified as offers. Interestingly, our analysis shows that this 5% improvement in coverage is across the board, irrespective of whether the emails were sent by large merchants or small local merchants, allowing us to deliver an improved experience for everyone."
"Training ultra-deep CNNs with critical initialization.  In recent years, state-of-the-art methods in computer vision have utilized increasingly deep convolutional neural network architectures (CNNs), with some of the most successful models employing 1000 layers or more. Optimizing networks of such depth is extremely challenging and has up until now been possible only when the architectures incorporate special residual connections and batch normalization. In this work, we demonstrate that it is possible to train vanilla CNNs of depth 1500 or more simply by careful choice of initialization. We derive this initialization scheme theoretically, by developing a mean field theory for the dynamics of signal propagation in random CNNs with circular boundary conditions.  We show that the order-to-chaos phase transition of such CNNs is similar to that of fully-connected networks, and we provide empirical evidence that ultra-deep vanilla CNNs are trainable if the weights and biases are initialized near the order-to-chaos transition."
"M3 Gesture Menu: Design and Experimental Analyses of Marking Menus for Touchscreen Mobile Interaction.  Despite their learning advantages in theory, marking menus have faced adoption challenges in practice, even on today's touchscreen-based mobile devices. We address these challenges by designing, implementing, and evaluating multiple versions of M3 Gesture Menu (M3), a reimagination of marking menus targeted at mobile interfaces. M3 is defined on a grid rather than in a radial space, relies on gestural shapes rather than directional marks, and has constant and stationary space use. Our first controlled experiment on expert performance showed M3 was faster and less error-prone by a factor of two than traditional marking menus. A second experiment on learning demonstrated for the first time that users could successfully transition to recall-based execution of a dozen commands after three ten-minute practice sessions with both M3 and Multi-Stroke Marking Menu. Together, M3, with its demonstrated resolution, learning, and space use benefits, contributes to the design and understanding of menu selection in the mobile-first era of end-user computing."
"Neural Graph Learning: Training Neural Networks Using Graphs.  Label propagation is a powerful and flexible semi-supervised learning technique on graphs. Neural networks, on the other hand, have proven track records in many supervised learning tasks. In this work, we propose a training framework with a graph-regularized objective, namely Neural Graph Machines, that can combine the power of neural networks and label propagation. This work generalizes previous literature on graph-augmented training of neural networks, enabling it to be applied to multiple neural architectures (Feed-forward NNs, CNNs and LSTM RNNs) and a wide range of graphs. The new objective allows the neural networks to harness both labeled and unlabeled data by: (a) allowing the network to train using labeled data as in the supervised setting, (b) biasing the network to learn similar hidden representations for neighboring nodes on a graph, in the same vein as label propagation. Such architectures with the proposed objective can be trained efficiently using stochastic gradient descent and scaled to large graphs; its runtime is linear in the number of edges. The proposed joint training approach convincingly outperforms many existing methods on a wide range of tasks (multi-label classification on social graphs, news categorization, document classification and semantic intent classification), with multiple forms of graph inputs (including graphs with and without node-level features) and using different types of neural networks."
"ProjectionNet: Learning Efficient On-Device Deep Networks Using Neural Projections.  Deep neural networks have become ubiquitous for applications related to visual recognition and language understanding tasks. However, it is often prohibitive to use typical neural netwok models on devices like mobile phones or smart watches since the model sizes are huge and cannot fit in the limited memory available on such devices. While these devices could make use of machine learning models running on high-performance data centers with CPUs or GPUs, this is not feasible for many applications because data can be privacy sensitive and inference needs to be performed directly ``on'' device."
"PRM-RL: Long-range Robotic Navigation Tasks by Combining Reinforcement Learning and Sampling-based Planning.  We present PRM-RL, a hierarchical method for long-range navigation task completion that combines sampling-based path planning with reinforcement learning (RL) agents. The RL agents learn short-range, point-to-point navigation policies that capture robot dynamics and task constraints with-
out knowledge of the large-scale topology, while the sampling-based planners provide an approximate map of the space of possible configurations of the robot from which collision-
free trajectories feasible for the RL agents can be identified. The same RL agents are used to control the robot under the direction of the planning, enabling long-range navigation. We use the Probabilistic Roadmaps (PRMs) for the sampling-based planner. The RL agents are constructed using feature-based and deep neural net policies in continuous state and action spaces. We evaluate PRM-RL on two navigation tasks with non-trivial robot dynamics: end-to-end differential drive indoor
navigation in office environments, and aerial cargo delivery in urban environments with load displacement constraints. These evaluations included both simulated environments and
on-robot tests. Our results show improvement in navigation task completion over both RL agents on their own and traditional sampling-based planners. In the indoor navigation task, PRM-
RL successfully completes up to 215 m long trajectories under noisy sensor conditions, and the aerial cargo delivery completes flights over 1000 m without violating the task constraints in an
environment 63 million times larger than used in training."
"Value of Sharing Data.  This paper analyzes whether advertisers would be better off using data that would enable them to target users more accurately if the only way they could use this data is by sharing the data with all advertisers. I present a wide range of theoretical results that illustrate general circumstances under which an advertiser can be assured that sharing data will make the advertiser better off. I then empirically analyze how sharing data would affect Google??s top 1000 advertisers on mobile apps, and find that over 98% of these advertisers would always be better off sharing data, even if the data can also help their competitors bid more accurately."
"Learning to Attend, Copy, and Generate for Session-Based Query Suggestion.  Users try to articulate their complex information needs during search sessions by reformulating their queries. In order to make this process more effective, search engines provide related queries to help users to specify the information need in their search process.
In this paper, we propose a customized sequence-to-sequence model for session-based query suggestion.In our model, we employ a query-aware attention mechanism to capture the structure of the session context.  This enables us to control the scope of the session from which we infer the suggested next query, which helps not only handle the noisy data but also automatically detect session boundaries. Furthermore, we observe that based on user query reformulation behavior, a large portion of terms of a query in a session is retained from the previously submitted queries in the same session and consists of mostly infrequent or unseen terms that are usually not included in the vocabulary.  We therefore empower the decoder of our model to access the source words from the session context during decoding by incorporating a copy mechanism. Moreover, we propose evaluation metrics to assess the quality of the generative models for query suggestion. We conduct an extensive set of experiments and analysis. The results suggest that our model outperforms the baselines both in terms of the generating queries and scoring candidate queries for the task of query suggestion."
"Avoiding Your Teacher's Mistakes: Training Neural Networks with Controlled Weak Supervision.  Users try to articulate their complex information needs during search sessions by reformulating their queries. In order to make this process more effective, search engines provide related queries to help users to specify the information need in their search process.
In this paper, we propose a customized sequence-to-sequence model for session-based query suggestion.In our model, we employ a query-aware attention mechanism to capture the structure of the session context.  This enables us to control the scope of the session from which we infer the suggested next query, which helps not only handle the noisy data but also automatically detect session boundaries. Furthermore, we observe that based on user query reformulation behavior, a large portion of terms of a query in a session is retained from the previously submitted queries in the same session and consists of mostly infrequent or unseen terms that are usually not included in the vocabulary.  We therefore empower the decoder of our model to access the source words from the session context during decoding by incorporating a copy mechanism. Moreover, we propose evaluation metrics to assess the quality of the generative models for query suggestion. We conduct an extensive set of experiments and analysis. The results suggest that our model outperforms the baselines both in terms of the generating queries and scoring candidate queries for the task of query suggestion."
"Overcoming Pitfalls in Behavior Tree Design.  Behavior trees are increasingly used to allow large software teams to collaborate on complex agent behaviors, but there are a few key choices in their implementation which can either make them much harder to architect than they need to be - or much more flexible, easy to extend, and easy to reuse. Anthony Francis got his start in AI implementing task architectures much like behavior trees, execpt before they were cool, and in this article he describes some of the tricks needed to apply behavior trees to robotic systems, including a few C++ template tricks that made the system much more extensible."
"Lessons from Building Static Analysis Tools at Google.  In this article, we describe how we have applied the lessons
from Google??s previous experience with FindBugs Java analysis,
as well as lessons from the academic literature, to build
a successful static analysis infrastructure that is used daily
by the majority of engineers at Google. Our tooling detects
thousands of issues per day that are fixed by engineers, by
their own choice, before the problematic code is checked into
the codebase."
"When Not to Comment: Questions and Tradeoffs with API Documentation for C++ Projects.  Without usable and accurate documentation of how to use an API,
programmers can find themselves deterred from reusing relevant
code. In C++, one place developers can find documentation is in
a header file, but when information is missing, they may look at
the corresponding implementation (the ??.cc?? file). To understand
what??s missing from C++ API documentation and whether it should
be fixed, we conducted a mixed-methods study. This involved three
experience sampling studies with hundreds of developers at the
moment they visited implementation code, interviews with 18 of
those developers, and interviews with 8 API maintainers. We found
that in many cases, updating documentation may provide only
limited value for developers, while requiring effort maintainers
don??t want to invest. This helps frame future tools and processes
designed to fill in missing low-level API documentation."
"A Cross-Tool Study on Program Analysis Tool Notification Communication.  Program analysis tools use notifications to communicate
with developers, but previous research suggests that developers
encounter challenges that impede this communication.
This paper describes a qualitative study that identifies 12
kinds of challenges that cause notifications to miscommunicate
with developers. Our resulting theory reveals that
many challenges span multiple tools and multiple levels of
developer experience. Our results suggest that, for example,
future tools that model developer experience could improve
communication and help developers build more accurate
mental models."
"Beyond word importance: using contextual decompositions to extract interactions from LSTMs..  The driving force behind the recent success of LSTMs has been their ability to
learn complex and non-linear relationships. Consequently, our inability to de-
scribe these relationships has led to LSTMs being characterized as black boxes.
To this end, we introduce contextual decomposition (CD), a novel algorithm for
capturing the contributions of combinations of words or variables in terms of CD
scores. On the task of sentiment analysis with the Yelp and SST data sets, we
show that CD is able to reliably identify words and phrases of contrasting senti-
ment, and how they are combined to yield the LSTM??s final prediction. Using the
phrase-level labels in SST, we also demonstrate that CD is able to successfully
extract positive and negative negations from an LSTM, something which has not
previously been done."
"One-shot Coresets: The Case of k-Clustering.  Scaling clustering algorithms to massive data sets is a challenging task. Recently, several successful approaches based on data summarization methods, such as coresets and sketches, were proposed. 
While these techniques provide provably good and small summaries, they are inherently problem dependent --- the practitioner has to commit to a fixed clustering objective before even exploring the data. 
However, can one construct small data summaries for a wide range of clustering problems simultaneously? In this work, we affirmatively answer this question by proposing an efficient algorithm that constructs such one-shot summaries for $k$-clustering problems while retaining strong theoretical guarantees."
"Horizontally Scalable Submodular Maximization.  A variety of large-scale machine learning problems can be cast as instances of constrained submodular maximization. Existing approaches for distributed submodular maximization have a critical drawback: The capacity ?? the number of instances that can fit in memory of each machine ?? must grow with the data set size. In practice, while one can provision many machines, the capacity of each machine is limited by physical constraints. We propose a truly scalable approach for distributed submodular maximization when the capacity of each machine is fixed. The proposed framework applies to a broad class of algorithms and a variety of constraints. We provide theoretical guarantees on the approximation factor for any available capacity. We empirically evaluate the proposed algorithm on a variety of data sets and demonstrate that the algorithm achieves performance competitive with the centralized greedy solution."
"State of Mutation Testing at Google.  Mutation testing assesses test suite efficacy by inserting small faults into programs and measuring the ability of the test suite to detect them. It is widely considered the strongest test criterion in terms of finding the most faults and it subsumes a number of other coverage criteria. Traditional mutation analysis is computationally prohibitive which hinders its adoption as an industry standard. In order to alleviate the computational issues, we present a diff-based probabilistic approach to mutation analysis that drastically reduces the number of mutants by omitting lines of code without statement coverage and lines that are determined to be uninteresting - we dub these arid lines. Furthermore, by reducing the number of mutants and carefully selecting only the most interesting ones we make it easier for humans to understand and evaluate the result of mutation analysis. We propose a heuristic for judging whether a node is arid or not, conditioned on the programming language. We focus on a code-review based approach and consider the effects of surfacing mutation results on developer attention. The described system is used by 6,000 engineers in Google on all code changes they author or review, affecting in total more than 14,000 code authors as part of the mandatory code review process. The system processes about 30% of all diffs across Google that have statement coverage
calculated."
"Fleet management at scale: How Google manages a quarter million computers securely and efficiently.  Providing Google employees with the tools they need to be productive requires our engineering teams to support a diverse fleet of nearly a quarter-million computers across four operating systems. Operating at this scale requires an efficient approach to imaging, managing, and securing our fleet without hampering user productivity or shifting the maintenance burden to our support staff. This paper discusses how Google has developed reviewable, repeatable, and automated backend processes using flexible open-source systems management software."
"Hierarchical Mixtures of GLMs for Combining Multiple Ground Truths.  In real-world machine learning problems it is often the case that the gold-standard for a particular learning problem is not accurately reflected by any one particular data set. For example, when modeling the landing-page quality associated with a search result, labels from human evaluators are often biased towards ??brandname?? sites, whereas labels derived from conversions can potentially confound search abandonment and successful conversion. In this paper we propose a class of models for characterizing and isolating the relative bias of a prediction problem across multiple data sets. These models can be used either as tools for data analysis, with the goal of calculating the divergence to the hypothetical gold-standard, or as smoothing procedures aimed at capturing as much shared structure between the domains as possible."
"PLUMS: Predicting Links Using Multiple Sources..  Link prediction is an important problem in online social and collaboration networks, for recommending friends and future collaborators. Most of the existing approaches for link prediction are focused on building unsupervised or supervised classification models based on the availability of accepts and rejects of the past recommendations. Several of these methods are feature-based and they construct a large number of network-level features to make the prediction more effective. A more flexible approach is to allow the model to learn the required features from the network for a specific task, rather than explicit feature engineering. In addition, most of the social and collaboration relationships do not happen instantly and rather build slowly over time through several low cost interactions, such as email and chat. The existing approaches often ignore the availability of such auxiliary networks to make link prediction more robust and effective. The main focus of this work is to build a robust and effective classifier for link prediction using multiple auxiliary networks. We develop a supervised random walk model, that does not require any explicit feature construction, and can be personalized to each user based on the past accept and reject behavior. Our approach consistently outperforms several popular baselines in terms of precision and recall in multiple real-life data sets. Also, our approach is robust to noise and sparsity in auxiliary networks, while several popular baselines, specifically feature-based ones, are inconsistent in their performance under such conditions."
"Graphical RNN Models.  Many time series are generated by a set of entities that interact with one another over time. This paper introduces a broad, flexible framework to learn from multiple inter-dependent time series generated by such entities. Our framework explicitly models the entities and their interactions through time. It achieves this by building on the capabilities of Recurrent Neural Networks, while also offering several ways to incorporate domain knowledge/constraints into the model architecture. The capabilities of our approach are showcased through an application to weather prediction, which shows gains over strong baselines."
"MapReduce and Its Application to Massively Parallel Learning of Decision Tree Ensembles.  In this chapter we look at leveraging the MapReduce distributed computing framework
(Dean and Ghemawat, 2004) for parallelizing machine learning methods of wide
interest, with a specific focus on learning ensembles of classification or regression
trees. Building a production-ready implementation of a distributed learning algorithm
can be a complex task. With the wide and growing availability of MapReduce-capable
computing infrastructures, it is natural to ask whether such infrastructures may be of
use in parallelizing common data mining tasks such as tree learning. For many data
mining applications, MapReduce may offer scalability as well as ease of deployment
in a production setting (for reasons explained later).
We initially give an overview of MapReduce and outline its application in a classic
clustering algorithm, k-means. Subsequently, we focus on PLANET: a scalable distributed
framework for learning tree models over large datasets. PLANET defines tree
learning as a series of distributed computations and implements each one using the
MapReduce model. We show how this framework supports scalable construction of
classification and regression trees, as well as ensembles of such models. We discuss
the benefits and challenges of using a MapReduce compute cluster for tree learning
and demonstrate the scalability of this approach by applying it to a real-world learning
task from the domain of computational advertising.
MapReduce is a simple model for distributed computing that abstracts away many of
the difficulties in parallelizing data management operations across a cluster of commodity
machines. By using MapReduce, one can alleviate, if not eliminate, many complexities
such as data partitioning, scheduling tasks across many machines, handling machine
failures, and performing inter-machine communication. These properties have motivated
many companies to run MapReduce frameworks on their compute clusters for data
analysis and other data management tasks. MapReduce has become in some sense an
industry standard. For example, there are open-source implementations such as Hadoop
that can be run either in-house or on cloud computing services such as Amazon EC2."
"Generating Wikipedia by Summarizing Long Sequences.  We show that generating English Wikipedia articles can be approached as a multi-
document summarization of source documents. We use extractive summarization
to coarsely identify salient information and a neural abstractive model to generate
the article. For the abstractive model, we introduce a decoder-only architecture
that can scalably attend to very long sequences, much longer than typical encoder-
decoder architectures used in sequence transduction. We show that this model can
generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia
articles. When given reference documents, we show it can extract relevant factual
information as reflected in perplexity, ROUGE scores and human evaluations."
"Unsupervised deep clustering for semantic object retrieval.  Learning a set of diverse and representative features from a large set of unlabeled
data has long been an area of active research. We present a method that separates
proposals of potential objects into semantic classes in an unsupervised manner.
Our preliminary results show that different object categories emerge and can later
be retrieved from test images. We propose a differentiable clustering approach
which can be integrated with Deep Neural Networks to learn semantic classes in
end-to-fashion without manual class labeling."
"Three-dimensional models visual differential.  Methods and systems for rendering a three-dimensional (3D) data model of an object are provided. An example method may include receiving information associated with a first 3D data model and a second 3D data model of an object. The method may also include rendering a first set of images of the first 3D data model, and rendering a second set of images of the second 3D data model. The method may also include comparing respective images of the first set of images to images of the second set of images to determine a plurality of image difference scores between the respective images of the first set of images and the images of the second set of images. The method may also include determining an object difference score based on the determined plurality of image difference scores."
Feature agnostic geometric alignment.  Methods and apparatus for aligning objects are provided. A computing device can receive first and second object representations that are respectively associated with first and second surface representations. The computing device can apply an object transformation to the respective first and second object representations to modify geometric features of the respective first and second object representations based on one or more values of one or more characteristics of the respective first and second surface representation. The computing device can align the first object representation and the second object representation using an alignment of the transformed first object representation and the transformed second object representation. The computing device can provide an output based on the aligned first object representation and second object representation.
Seamless texturing of 3D meshes of objects from multiple views.  Methods and systems for texturing of three-dimensional (3D) object data models are provided. An example method may include receiving information indicating a geometry of an object receiving a plurality of images of the object. The method may also include assigning images of the plurality of images that have a resolution above a threshold to a plurality of polygons that approximate the geometric surface of the object. The method may also include determining adjacent polygons that are assigned to different images of the plurality of images so as to identify boundaries of images and minimizing such boundaries. The method may also include determining a mismatch factor for boundaries of the modified boundaries of images and reassigning images in boundaries having a mismatch factor above a threshold so as to reduce a gradient variation between the images in the modified boundaries.
"Spectral distortion model for training phase-sensitive deep-neural networks for far-field speech recognition.  In this paper, we present an algorithm which introduces phaseperturbation
to the training database when training phase-sensitive
deep neural-network models. Traditional features such as log-mel or
cepstral features do not have have any phase-relevant information.
However more recent features such as raw-waveform or complex
spectra features contain phase-relevant information. Phase-sensitive
features have the advantage of being able to detect differences in
time of arrival across different microphone channels or frequency
bands. However, compared to magnitude-based features, phase
information is more sensitive to various kinds of distortions such
as variations in microphone characteristics, reverberation, and so
on. For traditional magnitude-based features, it is widely known
that adding noise or reverberation, often called Multistyle-TRaining
(MTR) , improves robustness. In a similar spirit, we propose an algorithm
which introduces spectral distortion to make the deep-learning
model more robust against phase-distortion. We call this approach
Spectral-Distortion TRaining (SDTR) and Phase-Distortion TRaining
(PDTR). In our experiments using a training set consisting of
22-million utterances, this approach has proved to be quite successful
in reducing Word Error Rates in test sets obtained with real
microphones on Google Home"
"Sound source separation using phase difference and reliable mask selection.  In this paper, we present an algorithm called Reliable Mask Selection-
Phase Difference Channel Weighting (RMS-PDCW) which selects
the target source masked by a noise source using the Angle of Arrival
(AoA) information calculated using the phase difference informa-
tion. The RMS-PDCW algorithm selects masks to apply using the
information about the localized sound source and the onset detec-
tion of speech. We demonstrate that this algorithm shows relatively
5.3 percent improvement over the baseline acoustic model, which
was multistyle-trained using 22 million utterances on the simulated
test set consisting of real-world and interfering-speaker noise with
reverberation time distribution between 0 ms and 900 ms and SNR
distribution between 0 dB up to clean."
"Challenges on the Journey to Co-Watching YouTube.  In order to better understand social aspects of the short-form video watching experience, we investigated the journey to co-watching, from searching and discovering content, to choosing and experiencing videos with others. After identifying, through a large-scale survey, some of the most typical situations that bring people to YouTube, we deployed a one week-long diary study with 12 participants in which they performed a set of frequent video tasks at their leisure, half by themselves, and half with someone else. Following the diary study, we had participants reenact the diary study tasks remotely with the experimenter. We observed that users face multiple challenges on the journey to co-watching a video. They must share a device designed for an individual, use different methods for selecting videos than when by themselves, negotiate or turn-take in order to make a decision, and potentially watch a video that they do not enjoy. Along this journey, users must engage in impression management to consider how their choices might make them appear to others. We present design recommendations for remote and collocated co-watching to improve the social video watching experience."
"What can be Found on the Web and How:  A Characterization of Web Browsing Patterns.  In this paper, we suggest a novel approach to studying user
browsing behavior, i.e., the ways users get to different pages
on the Web. Namely, we classified all user browsing paths
leading to web pages into several types or browsing patterns.
In order to define browsing patterns, we consider several im-
portant points of the browsing path: its origin, the last page
before the user gets to the domain of the target page, and
the target page referrer. Each point can be of several types,
which leads to 56 possible patterns. The distribution of the
browsing paths over these patterns forms the navigational
profile of a web page. We conducted a comprehensive large-scale study of naviga-
tional profiles of different web pages. First, we demonstrated
that the navigational profile of a web page carry crucial in-
formation about the properties of this page (e.g., its pop-
ularity and age). Second, we found that the Web consists
of several typical non-overlapping clusters formed by pages
of similar ranges of incoming traffic. These clusters can be
characterized by the functionality of their pages."
"(Almost) Zero-Shot Cross-Lingual Spoken Language Understanding.  Spoken language understanding (SLU) is a component of
goal-oriented dialogue systems that aims to interpret user's natural language queries in system's semantic representation format. While current state-of-the-art SLU
approaches achieve high performance for English domains, the same is
not true for other languages. Approaches in the literature for
extending SLU models and grammars to new languages rely primarily on machine
translation. This poses a challenge in scaling to new languages, as
machine translation systems may not be reliable for several
(especially low resource) languages. In this work, we examine different
approaches to train a SLU component with little
supervision for two new languages -- Hindi and Turkish, and show that with
only a few hundred labeled examples we can surpass the approaches
proposed in the literature.  Our experiments show that training a
model bilingually (i.e., jointly with English), enables faster
learning, in that the model requires fewer labeled instances in the
target language to generalize. Qualitative analysis shows that rare
slot types benefit the most from the bilingual training."
"A Novel Class of Robust Covert Channels Using Out-of-order Packets.  Covert channels are usually used to circumvent security policies and allow information leakage without being observed. In this paper, we propose a novel covert channel technique using the packet reordering phenomenon as a host for carrying secret communications. Packet reordering is a common phenomenon on the Internet. Moreover, it is handled transparently from the user and application-level processes. This makes it an attractive medium to exploit for sending hidden signals to receivers by dynamically manipulating packet order in a network flow. In our approach, specific permutations of successive packets are selected to enhance the reliability of the channel, while the frequency distribution of their usage is tuned to increase stealthiness by imitating real Internet traffic. It is very expensive for the adversary to discover the covert channel due to the tremendous overhead to buffer and sort the packets among huge amount of background traffic. A simple tool is implemented to demonstrate this new channel. We studied extensively the robustness and capabilities of our proposed channel using both simulation and experimentation over large varieties of traffic characteristics. The reliability and capacity of this technique have shown promising results. We also investigated a practical mechanism for distorting and potentially preventing similar novel channels."
"On the importance of single directions for generalization.  Despite their ability to memorize large datasets, deep neural networks often achieve good generalization performance. However, the differences between the learned solutions of networks which generalize and those which do not remain unclear. Additionally, the tuning properties of single directions (defined as the activation of a single unit or some linear combination of units in response to some input) have been highlighted, but their importance has not been evaluated. Here, we connect these lines of inquiry to demonstrate that a network??s reliance on single directions is a good predictor of its generalization performance, across networks trained on datasets with different fractions of corrupted labels, across ensembles of networks trained on datasets with unmodified labels, across different hyper- parameters, and over the course of training. While dropout only regularizes this quantity up to a point, batch normalization implicitly discourages single direction reliance, in part by decreasing the class selectivity of individual units. Finally, we find that class selectivity is a poor predictor of task importance, suggesting not only that networks which generalize well minimize their dependence on individual units by reducing their selectivity, but also that individually selective units may not be necessary for strong network performance."
"Ambisonics soundfield navigation using  directional decomposition and path distance estimation.  We propose a method of ambisonic soundfield navigation based on optimized directional decomposition that extracts ordered directional signals and estimates signal path distances using a correlation matrix. The method allows for room-scale soundfield navigation using a single ambisonic microphone signal. In comparison to other source-based navigation approaches, our approach more accurately handles differences between direct sources and reflections, which results in more plausible behavior when translating in spaces with many observable reflection points. Further, we show our method is efficient in computation and memory and integrates easily with existing ambisonic binaural rendering solutions. We evaluate the quality and accuracy of our translation method using synthetic signals convolved using image-source-method generated B-format room impulse responses."
"Novel inter and intra prediction tools under consideration for the emerging AV1 video codec.  Google started the WebM Project in 2010 to develop open source, royalty-free video codecs designed specifically for media on the Web. The second generation codec released by the WebM project, VP9, is currently served by YouTube, and enjoys billions of views per day. Realizing the need for even greater compression efficiency to cope with the growing demand for video on the web, the WebM team embarked on an ambitious project to develop a next edition codec AV1, in a consortium of major tech companies called the Alliance for Open Media, that achieves at least a generational improvement in coding efficiency over VP9. In this paper, we focus primarily on new tools in AV1 that improve the prediction of pixel blocks before transforms, quantization and entropy coding are invoked. Specifically, we describe tools and coding modes that improve intra, inter and combined inter-intra prediction. Results are presented on standard test sets."
"Novel modes and adaptive block scanning order for intra prediction in AV1.  The demand for streaming video content is on the rise and growing exponentially. Networks bandwidth is very costly and therefore there is a constant effort to improve video compression rates and enable the sending of reduced data volumes while retaining quality of experience (QoE). One basic feature that utilizes the spatial correlation of pixels for video compression is Intra-Prediction, which determines the codec??s compression efficiency. Intra prediction enables significant reduction of the Intra-Frame (I frame) size and, therefore, contributes to efficient exploitation of bandwidth. In this presentation, we propose new Intra-Prediction algorithms that improve the AV1 prediction model and provide better compression ratios. Two (2) types of methods are considered: )1( New scanning order method that maximizes spatial correlation in order to reduce prediction error; and )2( New Intra-Prediction modes implementation in AVI. Modern video coding standards, including AVI codec, utilize fixed scan orders in processing blocks during intra coding. The fixed scan orders typically result in residual blocks with high prediction error mainly in blocks with edges. This means that the fixed scan orders cannot fully exploit the content-adaptive spatial correlations between adjacent blocks, thus the bitrate after compression tends to be large. To reduce the bitrate induced by inaccurate intra prediction, the proposed approach adaptively chooses the scanning order of blocks according to criteria of firstly predicting blocks with maximum number of surrounding, already Inter-Predicted blocks. Using the modified scanning order method and the new modes has reduced the MSE by up to five (5) times when compared to conventional TM mode / Raster scan and up to two (2) times when compared to conventional CALIC mode / Raster scan, depending on the image characteristics (which determines the percentage of blocks predicted with Inter-Prediction, which in turn impacts the efficiency of the new scanning method). For the same cases, the PSNR was shown to improve by up to 7.4dB and up to 4 dB, respectively. The new modes have yielded 5% improvement in BD-Rate over traditionally used modes, when run on K-Frame, which is expected to yield ~1% of overall improvement."
"Video coding mode decision as a classification problem.  In this paper, we show that it is possible to reduce the complexity of Intra MB coding in H. 264/AVC based on a novel chance constrained classifier. Using the pairs of simple mean-variances values, our technique is able to reduce the complexity of Intra MB coding process with a negligible loss in PSNR. We present an alternate approach to address the classification problem which is equivalent to machine learning. Implementation results show that the proposed method reduces encoding time to about 20% of the reference implementation with average loss of 0.05 dB in PSNR."
"Scalable Private Learning with PATE.  he rapid adoption of machine learning has increased concerns about the privacy implications of machine learning models trained on sensitive data, such as medical records or other personal information. To address those concerns, one promising approach is Private Aggregation of Teacher Ensembles, or PATE, which transfers to a ""student"" model the knowledge of an ensemble of ""teacher"" models, with intuitive privacy provided by training teachers on disjoint data and strong privacy guaranteed by noisy aggregation of teachers?? answers. However, PATE has so far been evaluated only on simple classification tasks like MNIST, leaving unclear its utility when applied to larger-scale learning tasks and real-world datasets. In this work, we show how PATE can scale to learning tasks with large numbers of output classes and uncurated, imbalanced training data with errors. For this, we introduce new noisy aggregation mechanisms for teacher ensembles that are more selective and add less noise, and prove their tighter differential-privacy guarantees. Our new mechanisms build on two insights: the chance of teacher consensus is increased by using more concentrated noise and, lacking consensus, no answer need be given to a student. The consensus answers used are more likely to be correct, offer better intuitive privacy, and incur lower-differential privacy cost. Our evaluation shows our mechanisms improve on the original PATE on all measures, and scale to larger tasks with both high utility and very strong privacy (?? &lt; 1.0)."
"Object category learning and retrieval with  weak supervision.  We consider the problem of retrieving objects from image data and learning to
classify them into meaningful semantic categories with minimal supervision. To
that end, we propose a fully differentiable unsupervised deep clustering approach
to learn semantic classes in an end-to-end fashion without individual class labeling
using only unlabeled object proposals. The key contributions of our work are 1)
a kmeans clustering objective where the clusters are learned as parameters of the
network and are represented as memory units, and 2) simultaneously building a
feature representation, or embedding, while learning to cluster it. This approach
shows promising results on two popular computer vision datasets: on CIFAR10 for
clustering objects, and on the more complex and challenging Cityscapes dataset
for semantically discovering classes which visually correspond to cars, people, and
bicycles. Currently, the only supervision provided is segmentation objectness masks,
but this method can be extended to use an unsupervised objectness-based object
generation mechanism which will make the approach completely unsupervised."
"Designing A/B tests in a collaboration network.  In this article, we discuss an approach to the design of experiments in a network. In particular, we describe a method to prevent potential contamination (or inconsistent treatment exposure) of samples due to network effects. We present data from Google Cloud Platform (GCP) as an example of how we use A/B testing when users are connected. Our methodology can be extended to other areas where the network is observed and when avoiding contamination is of primary concern in experiment design. We first describe the unique challenges in designing experiments on developers working on GCP. We then use simulation to show how proper selection of the randomization unit can avoid estimation bias. This simulation is based on the actual user network of GCP."
"Multilingual Speech Recognition with a Single End-to-End Model.  Training a conventional automatic speech recognition (ASR) system to support multiple languages is challenging because the sub-word unit, lexicon and word inventories are typically language specific. In contrast, sequence-to-sequence models are well suited for multilingual ASR because they encapsulate an acoustic, pronunciation and language model jointly in a single network. In this work we present a single sequence-to-sequence ASR model trained on 9 different Indian languages, which have very little overlap in their scripts. Specifically, we take a union of language-specific grapheme sets and train a grapheme-based sequence-to-sequence model jointly on data from all languages. We find that this model, which is not explicitly given any information about language identity, improves recognition performance by 21% relative compared to analogous sequence-to-sequence models trained on each language individually. By modifying the model to accept a language identifier as an additional input feature, we further improve performance by an additional 7% relative and eliminate confusion between different languages."
"An Analysis of Incorporating an External Language Model into a Sequence-to-Sequence Model.  Attention-based sequence-to-sequence models for automatic
speech recognition jointly train an acoustic model, language model,
and alignment mechanism. Thus, the language model component is
only trained on transcribed audio-text pairs. This leads to the use of
shallow fusion with an external language model at inference time.
Shallow fusion refers to log-linear interpolation with a separately
trained language model at each step of the beam search. In this
work, we investigate the behavior of shallow fusion across a range of
conditions: different types of language models, different decoding
units, and different tasks. On Google Voice Search, we demonstrate
that the use of shallow fusion with an neural LM with wordpieces
yields a 9.1% relative word error rate reduction (WERR) over our
competitive attention-based sequence-to-sequence model, obviating
the need for second-pass rescoring."
"Natural TTS Synthesis By Conditioning WaveNet On Mel Spectrogram Predictions.  Attention-based sequence-to-sequence models for automatic
speech recognition jointly train an acoustic model, language model,
and alignment mechanism. Thus, the language model component is
only trained on transcribed audio-text pairs. This leads to the use of
shallow fusion with an external language model at inference time.
Shallow fusion refers to log-linear interpolation with a separately
trained language model at each step of the beam search. In this
work, we investigate the behavior of shallow fusion across a range of
conditions: different types of language models, different decoding
units, and different tasks. On Google Voice Search, we demonstrate
that the use of shallow fusion with an neural LM with wordpieces
yields a 9.1% relative word error rate reduction (WERR) over our
competitive attention-based sequence-to-sequence model, obviating
the need for second-pass rescoring."
"Intriguing Properties of Adversarial Examples.  It is becoming increasingly clear that many machine learning classifiers are vulnerable
to adversarial examples. In attempting to explain the origin of adversarial
examples, previous studies have typically focused on the fact that neural networks
operate on high dimensional data, they overfit, or they are too linear. Here we
argue that the origin of adversarial examples is primarily due to an inherent uncertainty
that neural networks have about their predictions. We show that the functional
form of this uncertainty is independent of architecture, dataset, and training
protocol; and depends only on the statistics of the logit differences of the network,
which do not change significantly during training. This leads to adversarial error
having a universal scaling, as a power-law, with respect to the size of the adversarial
perturbation. We show that this universality holds for a broad range of datasets
(MNIST, CIFAR10, ImageNet, and random data), models (including state-of-theart
deep networks, linear models, adversarially trained networks, and networks
trained on randomly shuffled labels), and attacks (FGSM, step l.l., PGD). Motivated
by these results, we study the effects of reducing prediction entropy on
adversarial robustness. Finally, we study the effect of network architectures on
adversarial sensitivity. To do this, we use neural architecture search with reinforcement
learning to find adversarially robust architectures on CIFAR10. Our
resulting architecture is more robust to white and black box attacks compared to
previous attempts."
"Generalizing Hamiltonian Monte Carlo with Neural Networks.  We present a general-purpose method to train Markov Chain Monte Carlo kernels (parameterized by deep neural networks) that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jump distance, a proxy for mixing speed. We demonstrate significant empirical gains (up to $124\times$ greater effective sample size) on a collection of simple but challenging distributions. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. Python source code is included as supplemental material, and will be open-sourced with the camera-ready paper."
"Adversarial Spheres.  State of the art computer vision models have been shown to be vulnerable to small adversarial perturbations of the input. In other words, most images in the data distribution are both correctly classified by the model and are very close to a visually similar misclassified image. Despite substantial research interest, the cause of the phenomenon is still poorly understood and remains unsolved. We hypothesize that this counter intuitive behavior is a naturally occurring result of the high dimensional geometry of the data manifold. As a first step towards exploring this hypothesis, we study a simple synthetic dataset of classifying between two concentric high dimensional spheres. For this dataset we show a fundamental tradeoff between the amount of test error and the average distance to nearest error. In particular, we prove that any model which misclassifies a small constant fraction of a sphere will be vulnerable to adversarial perturbations of size O(1/sqrt(d)). Surprisingly, when we train several different architectures on this dataset, all of their error sets naturally approach this theoretical bound. As a result of the theory, the vulnerability of neural networks to small adversarial perturbations is a logical consequence of the amount of test error observed. We hope that our theoretical analysis of this very simple case will point the way forward to explore how the geometry of complex real-world data sets leads to adversarial examples."
"COCO-Stuff: Thing and Stuff Classes in Context.  Semantic classes can be either things (objects with a
well-defined shape, e.g. car, person) or stuff (amorphous
background regions, e.g. grass, sky). While lots of classifi-
cation and detection works focus on thing classes, less at-
tention has been given to stuff classes. Nonetheless, stuff
classes are important as they allow to explain important
aspects of an image, including (1) scene type; (2) which
thing classes are likely to be present and their location
(through contextual reasoning); (3) physical attributes, ma-
terial types and geometric properties of the scene. To un-
derstand stuff and things in context we introduce COCO-
Stuff, which augments 120,000 images of the COCO dataset
with pixel-wise annotations for 91 stuff classes. We introduce an efficient stuff
annotation protocol based on superpixels which leverages
the original thing annotations. We quantify the speed versus
quality trade-off of our protocol and explore the relation be-
tween annotation time and boundary complexity. Further-
more, we use COCO-Stuff to analyze: (a) the importance of
stuff and thing classes in terms of their surface cover and
how frequently they are mentioned in image captions; (b)
the spatial relations between stuff and things, highlighting
the rich contextual relations that make our dataset unique;
(c) the performance of a modern semantic segmentation
method on stuff and thing classes, and whether stuff is 
easier to segment than things."
"Learning Intelligent Dialogs for Bounding-Box Annotation.  We introduce Intelligent Annotation Dialogs for bound-
ing box annotation. We train an agent to automatically
choose a sequence of actions for a human annotator to pro-
duce a bounding box in a minimal amount of time. Specifi-
cally, we consider two actions: box verification [34], where
the annotator verifies a box generated by an object detector,
and manual box drawing. We explore two kinds of agents,
one based on predicting the probability that a box will be
positively verified, and the other based on reinforcement
learning. We demonstrate that (1) our agents are able to
learn efficient annotation strategies in several scenarios,
automatically adapting to the difficulty of an input image,
the desired quality of the boxes, the strenght of the detector,
and other factors; (2) in all scenarios the resulting annota-
tion dialogs speed up annotation compated to manual box
drawing alone and box verification alone, while also out-
performing any fixed combination of verification and draw-
ing in most scenarios; (3) in a realistic scenario where the
detector is iteratively re-trained, our agents evolve a series
of strategies that reflect the shifting trade-off between veri-
fication and drawing as the detector grows stronger."
"Diabetic Retinopathy and the Cascade into Vision Loss.  Vision loss from diabetic retinopathy should be unnecessary for patients with access to diabetic retinopathy screening, yet it still occurs at high rates and in varied contexts. Precisely because vision loss is only one of many late-stage complications of diabetes, interfering with the management of diabetes and making self-care more difficult, Vision Threatening Diabetic Retinopathy (VTDR) is considered a ??high stakes?? diagnosis. Our mixed-methods research addressed the contexts of care and treatment seeking in a sample of people with VTDR using safety-net clinic services and eye specialist referrals. We point to conceptual weaknesses in the single disease framework of health care by diagnosis, and we use the framework of ??cascades?? to clarify why and how certain non-clinical factors come to bear on long-term experiences of complex chronic diseases."
"Spatially adaptive image compression using a tiled deep network.  Deep neural networks represent a powerful class of function approximators that
can learn to compress and reconstruct images. Existing image compression
algorithms based on neural networks learn quantized representations with a
constant spatial bit rate across each image. While entropy coding introduces
some spatial variation, traditional codecs have benefited significantly by
explicitly adapting the bit rate based on local image complexity and visual
saliency. This paper introduces an algorithm that combines deep neural
networks with quality-sensitive bit rate adaptation using a tiled network. We
demonstrate the importance of spatial context prediction and show improved
quantitative (PSNR) and qualitative (subjective rater assessment) results
compared to a non-adaptive baseline and a recently published image compression
model based on fully-convolutional neural networks."
"Interface for Exploring Videos.  Searching and browsing for videos is a common task. Search engines attempt to provide a quality user experience by showing the most relevant results at the top of a results page. However, efficiency and relevancy can sometimes be less of a concern, e.g., when browsing for interesting content, such as videos, e.g., without entering a search query. When users browse or search for videos, showing a hierarchical list of search results does not enable easy discovery of other content that they may enjoy. This disclosure describes techniques to improve user enjoyment when browsing videos, e.g., directly, or after performing a video search. Clusters of videos from various sources are displayed, e.g., in a virtual reality user interface. The distance between the clusters represents the overlap of viewer audience from respective video sources."
"Optimizing  Simulations  with  Noise-Tolerant  Structured  Exploration.  We propose a simple drop-in noise-tolerant replacement for the standard finite difference procedure used ubiquitously in blackbox optimization. In our approach, parameter perturbation directions are defined by a family of deterministic or randomized structured matrices. We show that at the small cost of computing a Fast Fourier Transform (FFT), such structured finite differences consistently give higher quality approximation of gradients and Jacobians in comparison to vanilla approaches that use coordinate directions or random Gaussian perturbations. We show that 
linearization of noisy, blackbox dynamics using our methods leads to improved performance of trajectory optimizers like Iterative LQR and Differential Dynamic Programming on several classic continuous control tasks. By embedding structured exploration in implicit filtering methods, we are able to learn agile walking and turning policies for quadruped locomotion, that successfully transfer from simulation to actual hardware.
We give a theoretical justification of our methods in terms of bounds on the quality of gradient reconstruction in the presence of noise."
"VisualBackProp: efficient visualization of CNNs.  This paper proposes a new method, that we call VisualBackProp, for visualizing which sets of pixels of the input image contribute most to the predictions made by the convolutional neural network (CNN). The method heavily hinges on exploring the intuition that the feature maps contain less and less irrelevant information to the prediction decision when moving deeper in the network. The technique
we propose was developed as a debugging tool for CNN-based systems for steering self-driving cars and is therefore required to run in real-time, i.e. the proposed method was designed to require less computations then single forward propagation per image. This makes the presented visualization method a valuable debugging tool which can be easily used during training or inference. We furthermore justify our approach with theoretical argument and theoretically confirm that the proposed method identifies sets of input pixels, rather than individual pixels, that collaboratively con-
tribute to the prediction. Our theoretical findings stand in agreement with experimental results. The empirical evaluation shows the plausibility of the proposed approach on
road data."
"An Experience Sampling Study of User Reactions to Browser Warnings in the Field.  Web browser warnings should help protect people from malware, phishing, and network attacks. Adhering to warnings keeps people safer online. Recent improvements in warning design have raised adherence rates, but they could still be higher. And prior work suggests many people still do not understand them. Thus, two challenges remain: increasing both comprehension and adherence rates. To dig deeper into user decision making and comprehension of warnings, we performed an experience sampling study of web browser security warnings, which involved surveying over 6,000 Chrome and Firefox users in situ to gather reasons for adhering or not to real warnings. We find these reasons are many and vary with context. Contrary to older prior work, we do not find a single dominant failure in modern warning design---like habituation---that prevents effective decisions. We conclude that further improvements to warnings will require solving a range of smaller contextual misunderstandings."
"WSMeter: A Fast, Accurate, and Low-Cost Performance Evaluation for  Warehouse-Scale Computers.  A warehouse-scale computer (WSC) is a vast collection
of tightly networked computers providing modern internet
services, that is becoming increasingly popular as the most
cost-effective approach to serve users at global scale. It is
however extremely difficult to accurately measure the holistic
performance of WSC. The existing load-testing benchmarks
are tailored towards a dedicated machine model and do not
address shared infrastructure environments. Evaluating the
performance of a live shared production WSC environment
presents many challenges due to the lack of holistic performance
metrics, high evaluation costs, and potential service
disruptions they may cause. WSC providers and customers
are in need of a cost effective methodology to accurately evaluate
the holistic performance of their platforms and hosted
services.
To address these challenges, we propose WSMeter, a cost
effective framework and methodology to accurately evaluate
the holistic performance of WSC in a live production environment.
We define a new performance metric to accurately reflect
the holistic performance of a WSC running a wide variety of
unevenly distributed jobs. We propose a model to statistically
embrace the performance variances amplified by co-located
jobs, to evaluate holistic performance with minimum costs.
For validation of our approach, we analyze two real-world
use cases and show that WSMeter accurately discerns 7% and
1% performance improvements, using only 0.9% and 6.6% of
the machines in the WSC, respectively. We show through a
Cloud customer case study, where WSMeter helped quantify
the performance benefits of service software optimization with
minimal costs."
"Syllable-Based Acoustic Modeling with CTC-SMBR-LSTM.  We explore the feasibility of training long short-term memory (LSTM) recurrent neural networks (RNNs) with syllables, rather than phonemes, as outputs. Syllables are a natural choice of linguistic unit for modeling the acoustics of languages such as Mandarin Chinese, due to the inherent nature of the syllable as an elemental pronunciation construct and the limited size of the syllable set for such languages (around 1400 syllables for Mandarin). Our models are trained with Connectionist Temporal Classification (CTC) and sMBR loss using asynchronous stochastic gradient descent (ASGD) utilizing a parallel computation infrastructure for large-scale training. With feature frames computed every 30ms, our acoustic models are well suited to syllable-level modeling as compared to phonemes which can have a shorter duration. Additionally, when compared to word-level modeling, syllables have the advantage of avoiding out-of-vocabulary (OOV) model outputs. Our experiments on a Mandarin voice search task show that syllable-output models can perform as well as context-independent (CI) phone-output models, and, under certain circumstances can beat the performance of our state-of-the-art context-dependent (CD) models. Additionally, decoding with syllable-output models is substantially faster than that with CI models, and vastly faster than with CD models. We demonstrate that these improvements are maintained when the model is trained to recognize both Mandarin syllables and English phonemes."
"Faster Discovery of Neural Architectures by Searching for Paths in a Large Model.  We propose an approach for automatic model designing,  which is signifi-cantly faster and less expensive than previous methods.  The method, whichwe nameEfficient  Neural  Architecture  Search(ENAS), learns to discoverneural architectures by searching for an optimal path within a larger pre-determined  model.   ENAS  generates  a  discrete  mask  at  each  layer,  andis  trained  by  a  policy  gradient  method  to  improve  training  accuracy.   Inour  experiments,  ENAS  achieves  comparable  test  accuracies  while  being10x faster and requiring 100x less resources than NAS. On the CIFAR-10dataset, ENAS can design novel architectures that achieve the test error of3.86%, compared to 3.41% by standard NAS (Zoph et al., 2017).  On thePenn  Treebank  dataset,  ENAS also  discovers  a  novel  architecture,  whichachieves the test perplexity of 64.6 compared to 62.4 by standard NAS."
"Ensemble Adversarial Training: Attacks and Defenses.  Adversarial examples are perturbed inputs designed to fool machine learning models.
Adversarial training injects such examples into training data to increase robustness.
To scale this technique to large datasets, perturbations are crafted using
fast single-step methods that maximize a linear approximation of the model??s loss.
We show that this form of adversarial training converges to a degenerate global
minimum, wherein small curvature artifacts near the data points obfuscate a linear
approximation of the loss. The model thus learns to generate weak perturbations,
rather than defend against strong ones. As a result, we find that adversarial
training remains vulnerable to black-box attacks, where we transfer perturbations
computed on undefended models, as well as to a powerful novel single-step attack
that escapes the non-smooth vicinity of the input data via a small random step.
We further introduce Ensemble Adversarial Training, a technique that augments
training data with perturbations transferred from other models. We use ensemble
adversarial training to train ImageNet models with strong robustness to black-box
attacks. In particular, our most robust model won the first round of the NIPS 2017
competition on Defenses against Adversarial Attacks"
"Monotonic Chunkwise Attention.  Sequence-to-sequence models with an attention have been successfully applied to a wide variety of problems. Standard soft attention makes a pass over the entire input sequence when producing each element of the output sequence, which unfortunately results in a quadratic time complexity and prevents its use in online/??real- time?? settings. To address these issues, we propose Monotonic Chunkwise Attention (MoChA), which adaptively splits the input sequence into small chunks over which soft attention is computed. We show that models utilizing MoChA can be trained efficiently with standard backpropagation while allowing online and linear-time decoding at test time. When applied to online speech recognition, we obtain state-of-the-art results and match the performance of an offline soft attention decoder. In document summarization experiments where we do not expect monotonic alignments, we show significantly improved performance compared to a baseline monotonic attention model."
"Meta-Learning for Semi-Supervised Few-Shot Classification.  In few-shot classification, we are interested in learning algorithms that train a classifier from only a handful of labeled examples. Recent progress made in few-shot classification has featured meta-learning, in which a parameterized model for a learning algorithm is defined and trained on episodes representing different classification problems, each with a small labeled training set and its corresponding test set. In this work, we advance this few-shot classification paradigm towards a scenario where unlabeled examples are also available within each episode. We consider two situations: one where all unlabeled examples are assumed to belong to the same set of classes as the labeled examples of the episode, as well as the more realistic situation where examples from other {\it distractor} classes are also provided. To address this paradigm, we propose novel extensions of prototypical networks (Snell et al. 2017) that are augmented with the ability to use unlabeled examples when producing prototypes. These models are trained in an end-to-end way on episodes, to learn to leverage the unlabeled examples successfully. 
We evaluate these methods on versions of the Omniglot and mini-ImageNet benchmarks, adapted to this new framework augmented with unlabeled examples. We also propose a new split of ImageNet. Our experiments confirm that our prototypical networks can learn to improve their predictions due to unlabeled examples, much like a semi-supervised algorithm would."
"Thermometer Encoding: One Hot Way To Resist Adversarial Examples.  It is well known that for neural networks, it is possible to construct
inputs which are misclassified by the network yet indistinguishable from
true data points, known as ``adversarial examples''. We propose a simple
modification to standard neural network architectures, \emph{thermometer
encoding}, which significantly increases the robustness of the network to
adversarial examples. We demonstrate this robustness with experiments
on the MNIST, CIFAR-10, CIFAR-100, and SVHN datasets, and show that
models with thermometer-encoded inputs consistently have higher accuracy
on adversarial examples, while also maintaining the same accuracy on
non-adversarial examples and training more quickly."
"Large scale distributed neural network training through online distillation.  While techniques such as ensembling and distillation promise model quality improvements when paired with almost any base model they are seldom used as the multi-stage training setups they require are cumbersome and the extra hyperparameters introduced make the process of tuning even more expensive. In this paper we explore a variant of distillation which is relatively straightforward to use as it does not require a complicated multi-stage setup. We also show that distillation can be used as a meaningful distributed learning algorithm: instead of independent workers exchanging gradients, which requires worrying about delays and synchronization, independent workers can exchange full model checkpoints. This can be done far less frequently than exchanging gradients, breaking one of the scalability barriers of stochastic gradient descent. We have experiments on Criteo clickthrough rate, and the largest to-date dataset used for neural language modeling, based on Common Crawl and containing $6\times 10^{11}$ tokens. In these experiments we show we can scale at least $2\times$ as well as the maximum limit of distributed stochastic gradient descent. Finally, we also show that online distillation can dramatically reduce the churn in the predictions between different versions of a model."
"Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning.  Deep reinforcement learning algorithms can learn complex behavioral skills, but real-world application of these methods requires a considerable amount of experience to be collected by the agent. In practical settings, such as robotics, this involves repeatedly attempting a task, resetting the environment between each attempt. However, not all tasks are easily or automatically reversible. In practice, this learning process requires considerable human intervention. In this work, we propose an autonomous method for safe and efficient reinforcement learning that simultaneously learns a forward and backward policy, with the backward policy resetting the environment for a subsequent attempt. By learning a value function for the backward policy, we can automatically determine when the forward policy is about to enter a non-reversible state, providing for uncertainty-aware safety aborts. Our experiments illustrate that proper use of the backward policy can greatly reduce the number of manual resets required to learn a task and can reduce the number of unsafe actions that lead to non-reversible states."
"Don't decay the learning rate, increase the batch size.  It is common practice to decay the learning rate. Here we show one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size during training. This procedure is successful for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, and Adam. It reaches equivalent test accuracies after the same number of training epochs, but with fewer parameter updates, leading to greater parallelism and shorter training times. We can further reduce the number of parameter updates by increasing the learning rate $\epsilon$ and scaling the batch size $B \propto \epsilon$. Finally, one can increase the momentum coefficient $m$ and scale $B \propto 1/(1-m)$, although this tends to slightly reduce the test accuracy. Crucially, our techniques allow us to repurpose existing training schedules for large batch training with no hyper-parameter tuning. We train Inception-ResNet-V2 on ImageNet to $77\%$ validation accuracy in under 2500 parameter updates, efficiently utilizing training batches of 65536 images."
"Learning Permutations with gradient descent and the sinkhorn operator.  Gradient descent methods have greatly facilitated the practice of machine learning, as the learning problem can be usually represented as the minimization of a differentiable function over some parameters. However, in cases where some dependencies between parameters and variables are discrete, gradient descent cannot be applied, unless those discrete nodes are relaxed to continued values ones,  where derivatives can be defined. Nonetheless, no clear solution exists in cases of structured discrete objects defined by a certain combinatorial structure; for example, in permutations, which underlie the notions of ordering, ranking and matching of objects. Here we show how to extend the relaxation method to enable gradient descent in computational graphs containing permutations as deterministic or stochastic nodes. To this end, we first show that permutations can be approximated by the differentiable Sinkhorn operator. With this, we are able to define Sinkhorn networks for the supervised learning of permutations. Finally, for stochastic nodes (corresponding to latent distributions over permutations) we introduce two implicit distributions: Gumbel-Matching and its relaxation, the Gumbel-Sinkhorn, and we prescribe how to perform inferences. We demonstrate the effectiveness of our method by showing we achieve state-of-the-art results on several tasks involving both standard datasets and a scientific application."
"Hierarchical Planning for Device Placement.  We introduce a hierarchical model for efficient placement of computational graphs
onto hardware devices, especially in heterogeneous environments with a mixture of
CPUs, GPUs, and other computational devices. Our method learns to assign graph
operations to groups and to allocate those groups to available devices. The grouping
and device allocations are learned jointly. The proposed method is trained with
policy gradient and requires no human intervention. Experiments with widely-used
computer vision and natural language models show that our algorithm can find
optimized, non-trivial placements for TensorFlow computational graphs with over
80,000 operations. In addition, our approach outperforms placements by human
experts as well as a previous state-of-the-art placement method based on deep
reinforcement learning. Our method achieves runtime reductions of up to 60.6%
per training step when applied to models such as Neural Machine Translation."
Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling.  We do an empirical comparison of a variety of recent methods for decision making based on deep Bayesian Neural Networks with Thompson Sampling.
"Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models.  Deep generative neural networks have proven effective at both conditional and unconditional modeling of complex data distributions. Conditional generation enables interactive control, but creating new controls often requires expensive retraining. In this paper, we develop a method to condition generation without retraining the model. By post-hoc learning latent constraints, value functions that identify regions in latent space that generate outputs with desired attributes, we
can conditionally sample from these regions with gradient-based optimization or amortized actor functions. Combining attribute constraints with a universal ??realism?? constraint, which enforces similarity to the data distribution, we generate realistic conditional images from an unconditional variational autoencoder. Further, using gradient-based optimization, we demonstrate identity-preserving transformations that make the minimal adjustment in latent space to modify the attributes of an image. Finally, with discrete sequences of musical notes, we demonstrate zero-shot conditional generation, learning latent constraints in the absence of labeled data or a differentiable reward function."
"Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples.  The problem of detecting whether a test sample is from in-distribution (i.e., train-
ing distribution by a classifier) or out-of-distribution sufficiently different from it
arises in many real-world machine learning applications. However, the state-of-art
deep neural networks are known to be highly overconfident in their predictions,
i.e., do not distinguish in- and out-of-distributions. Recently, to handle this issue, 
several threshold-based detectors have been proposed given pre-trained neural classifiers. 
However, the performance of prior works highly depends on how
to train the classifiers since they only focus on improving inference procedures.
In this paper, we develop a novel training method for classifiers so that such inference 
algorithms can work better. In particular, we suggest two additional terms
added to the original loss (e.g., cross entropy). The first one forces samples from
out-of-distribution less confident by the classifier and the second one is for (implicitly) 
generating most effective training samples for the first one. In essence,
our method jointly trains both classification and generative neural networks for
out-of-distribution. We demonstrate its effectiveness using deep convolutional
neural networks on various popular image datasets."
"INITIALIZATION MATTERS: ORTHOGONAL PREDICTIVE STATE RECURRENT NEURAL NETWORKS.  Learning to predict complex time-series data is a fundamental challenge in a range of disciplines including Machine Learning, Robotics, and Natural Language Processing. Predictive State Recurrent Neural Networks (PSRNNs) Downey et al. (2017) are a state-of-the-art approach for modeling time-series data which combine the benefits of probabilistic filters and Recurrent Neural Networks in a single model. PSRNNs leverage the concept of Hilbert Space Embeddings of distributions Smola et al. (2007) to embed predictive states into a Reproducing Kernel Hilbert Space, then estimate, predict, and update these embedded states using Kernel Bayes Rule. Practical implementations of PSRNNs are made possible by the machinery of Random Features, where input features are mapped into a new space where dot products approximate the kernel well. Unfortunately it turns out that PSRNNs often require a large number of RFs to obtain good results, resulting in large models which are slow to execute and slow to train. Orthogonal Random Features (ORFs)Yu et al. (2016) is an improvement on RFs which has been shown to decrease the number of RFs required in a number of applications. Unfortunately it is not clear that ORFs can be applied to PSRNNs, as PSRNNs rely on Kernel Ridge Regression as a core component of their learning algorithm, and the theoretical guarantees of ORF do not apply in this setting. In this paper we extend the theory of ORFs to Kernel Ridge Regression and show that ORFs can be used to obtain Orthogonal PSRNNs (OPSRNNs), which are smaller and faster than PSRNNs."
"Matrix capsules with EM routing.  A capsule is a group of neurons whose outputs represent different properties of the same entity. We describe a version of capsules in which each capsule has a logistic unit to represent the presence of an entity and a 4x4 pose matrix to represent the relationship between that entity and the viewer. A capsule in one layer votes for the pose matrices of many different capsules in the layer above by multiplying its own pose matrix by viewpoint-invariant transformation matrices that represent part-whole relationships. Each of these votes is weighted by an assignment coefficient and these coefficients are iteratively updated using the EM algorithm so that the output of each capsule is routed to a capsule in the layer above that receives a cluster of similar votes. The whole system is trained discriminatively by unrolling the 3 iterations of EM between each pair of adjacent layers. On the small NORB benchmark, capsules reduce the number of test errors by 30\% compared with the best reported CNN. Capsules are also far more resistant to whitebox adversarial attack."
"Time-Dependent Representation for Neural Event Sequence Prediction.  Existing sequence prediction methods are mostly concerned with time-independent sequences, in which the actual time span between events is irrelevant and the distance between events is simply the difference between their order positions in the sequence. While this time-independent view of sequences is applicable for data such as natural languages, e.g., dealing with words in a sentence, it is inappropriate and inefficient for many real world events that are observed and collected at unequally spaced points of time as they naturally arise, e.g., when a person goes to a grocery store or makes a phone call. The time span between events can carry important information about the sequence dependence of human behaviors. To leverage continuous time in sequence prediction, we propose two methods for integrating time into event representation, based on the intuition on how time is tokenized in everyday life and previous work on embedding contextualization. We particularly focus on using these methods in recurrent neural networks, which have gained popularity in many sequence prediction tasks. We evaluated these methods as well as baseline models on two learning tasks: mobile app usage prediction and music recommendation. The experiments revealed that the proposed methods for time-dependent representation offer consistent gain on accuracy compared to baseline models that either directly use continuous time value in a recurrent neural network or do not use time."
"Cross-View Training for Semi-Supervised Learning.  We present a simple but effective technique for deep semi-supervised learning. On labeled examples, the model is trained with standard cross-entropy loss. On an unlabeled example, the model first performs inference (acting as a ??teacher??) and then learns from the resulting output distribution (acting as a ??student??). We deviate from prior work by adding multiple auxiliary student softmax layers to the model. The input to each student layer is a sub-network of the full model that has a restricted view of the input (e.g., only seeing one region of an image). The students can learn from the teacher because the teacher sees more of each example. Concurrently, the students improve the representations used by the teacher as they learn to make predictions with limited data. We propose variants of our method for CNN image classifiers and BiLSTM sequence taggers. When combined with Virtual Adversarial Training, it improves upon the current state-of-the-art on semi-supervised CIFAR-10 and semi-supervised SVHN. We also apply it to train semi-supervised sequence taggers for four Natural Language Processing tasks using hundreds of millions of sentences of unlabeled data. The resulting models improve upon or are competitive with the current state-of-the-art on every task."
"MaskGAN: Better Text Generation via Filling in the ____.  Recurrent neural networks (RNNs) are a common method of generating text token by token. These models are typically trained via maximum likelihood (known in this context as teacher forcing). However, this approach frequently suffers from problems when using a trained model to generate new text since when generating words later in the sequence the model often conditions on a sequence of 
words that was never observed at training time. We explore methods for using Generative Adversarial Networks (GANs) as an alternative to teacher forcing to generate discrete sequences. 
In particular, we consider a conditional GAN that fills in missing text conditioned on the surrounding context.  We show qualitatively and quantitatively evidence that this produces more realistic text samples compared to a maximum likelihood trained model. We also propose a new task that quantitatively measures the quality of RNN produced samples."
"Neumann Optimizer: A Practical Optimizer for Deep Neural Networks.  Progress in deep learning is slowed by the days or weeks it takes to train large models. The natural solution of using more hardware is limited by diminishing returns, and leads to inefficient use of additional resources. In this paper, we present a large batch, stochastic optimization algorithm that is both faster than widely used algorithms for fixed amounts of computation, and is also able to scale up substantially better as more computational resources become available. Our algorithm implicitly computes the inverse hessian of each mini-batch to produce descent directions. We demonstrate the effectiveness of our algorithm by successfully training large ImageNet models (Inception V3, Resnet-50, Resnet-101 and Inception-Resnet) with mini-batch sizes of up to 32000 with no loss in validation error relative to current baselines, and no increase in the total number of steps. At smaller mini-batch sizes, our optimizer improves the validation error in these models by 0.8-0.9%. Alternatively, we can trade off this accuracy to reduce the number of training steps needed by roughly 10-30%. Our work is practical and easily usable by others -- only one hyperparameter (learning rate) needs tuning, and furthermore, the algorithm is as computationally cheap as the commonly used adam optimizer."
"Analyzing Language Learned by an Active Question Answering Agent.  We analyze the language learned by an agent trained with reinforcement learning as a component of the ActiveQA system [Buck et al., 2017]. In ActiveQA, question answering is framed as a reinforcement learning task in which an agent sits between the user and a black box question-answering system. The agent learns to reformulate the user's questions to elicit the optimal answers. It probes the system with many versions of a question that are generated via a sequence-to-sequence question reformulation model, then aggregates the returned evidence to find the best answer. This process is an instance of machine-machine communication. The question reformulation model must adapt its language to increase the quality of the answers returned, matching the language of the question answering system. We find that the agent does not learn transformations that align with semantic intuitions but discovers through learning classical information retrieval techniques such as tf-idf re-weighting and stemming."
"ON USING BACKPROPAGATION FOR SPEECH TEXTURE GENERATION AND VOICE CONVERSION.  Inspired by recent work on neural network image generation which
rely on backpropagation towards the network inputs, we present
a proof-of-concept system for speech texture synthesis and voice
conversion based on two mechanisms: approximate inversion of
the representation learned by a speech recognition neural network,
and on matching statistics of neuron activations between different
source and target utterances. Similar to image texture synthesis and
neural style transfer, the system works by optimizing a cost function
with respect to the input waveform samples. To this end we use a
differentiable mel-filterbank feature extraction pipeline and train a
convolutional CTC speech recognition network. Our system is able
to extract speaker characteristics from very limited amounts of target
speaker data, as little as a few seconds, and can be used to generate
realistic speech babble or reconstruct an utterance in a different voice."
"YouTube-BoundingBoxes: A Large High-Precision Human-Annotated Dataset for Object Detection in Video.  We introduce a new large-scale data set of video URLs with densely-sampled object bounding box annotations called YouTube-BoundingBoxes (YT-BB). The data set consists of approximately 380,000 video segments about 19s long, automatically selected to feature objects in natural settings without editing or post-processing, with a recording quality often akin to that of a hand-held cell phone camera. The objects represent a subset of the MS COCO label set. All video segments were human-annotated with high-precision classification labels and bounding boxes at 1 frame per second. The use of a cascade of increasingly precise human annotations ensures a label accuracy above 95% for every class and tight bounding boxes. Finally, we train and evaluate well-known deep network architectures and report baseline figures for per-frame classification and localization to provide a point of comparison for future work. We also demonstrate how the temporal contiguity of video can potentially be used to improve such inferences. Please see the PDF file to find the URL to download the data. We hope the availability of such large curated corpus will spur new advances in video object detection and tracking."
"Path Consistency Learning in Tsallis Entropy Regularized MDPs.  We study the sparse entropy-regularized RL (ERL) problem in which the entropy term is a special form of the Tsallis entropy. The opti mal policy of this formulation is sparse, i.e., at each state, it has non-zero probability for only a small number of actions. This addresses the main drawback of standard (soft) ERL, namely having softmax optimal policy. The problem with a soft max policy is that at every state, it may assign a non-negligible probability mass to non-optimal actions. This problem is aggravated as the number of actions is increased. Lee et al. (2018) studied the properties of the sparse ERL problem and proposed value-based algorithms to solve it. In this paper, we follow the work of Nachum et al. (2017) in the soft ERL setting, and propose a class of novel path consistency learning (PCL) algorithms, called sparse PCL, for the sparse ERL problem that can work with both on-policy and off-policy data. We first derive a consistency equation for sparse ERL, called sparse consistency. We then prove that sparse consistency only implies sub-optimality (unlike the soft consistency in soft ERL). We then use the sparse consistency to derive our sparse PCL algorithms. We empirically compare sparse PCL with its soft counterpart, and show its advantage, especially in problems with large number of actions."
"SHAPED: Shared-Private Encoder-Decoder for Text Style Adaptation.  Supervised training of abstractive language generation models results in learning conditional probabilities over language sequences based on the supervised training signal. When the training signal contains a variety of writing styles, such models may end up learning an 'average' style that is directly influenced by the training data make-up and cannot be controlled by the needs of an application. We describe a family of model architectures capable of capturing both generic language characteristics via shared model parameters, as well as particular style characteristics via private model parameters. Such models are able to generate language according to a specific learned style, while still taking advantage of their power to model generic language phenomena. Furthermore, we describe an extension that uses a mixture of output distributions from all learned styles to perform on-the-fly style adaptation based on the textual input alone. Experimentally, we find that the proposed models consistently outperform models that encapsulate single-style or average-style language generation capabilities."
"Unsupervised Learning of Semantic Audio Representations.  Supervised training of abstractive language generation models results in learning conditional probabilities over language sequences based on the supervised training signal. When the training signal contains a variety of writing styles, such models may end up learning an 'average' style that is directly influenced by the training data make-up and cannot be controlled by the needs of an application. We describe a family of model architectures capable of capturing both generic language characteristics via shared model parameters, as well as particular style characteristics via private model parameters. Such models are able to generate language according to a specific learned style, while still taking advantage of their power to model generic language phenomena. Furthermore, we describe an extension that uses a mixture of output distributions from all learned styles to perform on-the-fly style adaptation based on the textual input alone. Experimentally, we find that the proposed models consistently outperform models that encapsulate single-style or average-style language generation capabilities."
"Decoding the auditory brain with canonical component analysis.  The relation between a stimulus and the evoked brain response can shed light on perceptual processes within the brain. Signals derived from this relation can also be harnessed to control external devices for Brain Computer Interface (BCI) applications. While the classic event-related potential (ERP) is appropriate for isolated stimuli, more sophisticated ??decoding?? strategies are needed to address continuous stimuli such as speech, music or environmental sounds. Here we describe an approach based on Canonical Correlation Analysis (CCA) that finds the optimal transform to apply to both the stimulus and the response to reveal correlations between the two. Compared to prior methods based on forward or backward models for stimulus-response mapping, CCA finds significantly higher correlation scores, thus providing increased sensitivity to relatively small effects, and supports classifier schemes that yield higher classification scores. CCA strips the brain response of variance unrelated to the stimulus, and the stimulus representation of variance that does not affect the response, and thus improves observations of the relation between stimulus and response."
"Clustering Small  Samples with Quality Guarantees: Adaptivity with One2all pps.  Clustering of data points is a fundamental tool in data analysis.  We consider points $X$ in a relaxed metric space, where the triangle inequality holds within a  constant factor.  A clustering of $X$ is a partition of $X$ defined by a set of  points $Q$ ({\em centroids}), according to the closest centroid.  The {\em cost} of clustering $X$ by $Q$ is  $V(Q)=\sum_{x\in X} d_{xQ}$. This formulation generalizes classic $k$-means clustering, which uses squared distances. Two basic tasks, parametrized by $k \geq 1$, are {\em cost estimation}, which returns (approximate) $V(Q)$ for queries $Q$ such that $|Q|=k$  and {\em clustering}, which returns an (approximate) minimizer of $V(Q)$ of size $|Q|=k$. When the data set $X$ is very large, we seek efficient constructions of small samples that can act as surrogates for performing these tasks.  Existing constructions that provide quality guarantees, however, are either worst-case, and unable to benefit from structure of real data sets, or make explicit strong assumptions on the structure.  We show here how to avoid both these pitfalls using adaptive designs. The core of our design are the novel {\em one2all} probabilities,  computed for a set $M$ of centroids and $\alpha \geq 1$: The clustering cost of  {\em each} $Q$ with cost  $V(Q) \geq V(M)/\alpha$  can be estimated well from a sample of size  $O(\alpha |M|\epsilon^{-2})$. For cost estimation, we apply  one2all with a bicriteria approximate $M$, while adaptively balancing $|M|$ and $\alpha$ to optimize sample size per quality. For clustering, we present a  wrapper that adaptively applies a base clustering algorithm to a sample $S$, using the smallest sample that provides the desired statistical guarantees on quality. We demonstrate experimentally the huge gains of using our  adaptive instead of  worst-case methods."
"Bootstrapped Graph Diffusions: Exposing the Power of Nonlinearity.  Recently,  non-linear methods such as node embeddings  and graph convolutional networks (GCN) demonstrated a large gain in quality for SSL tasks. These methods introduce  multiple components and greatly vary on how the graph structure, seed label information, and other features are used. We aim here to study  the contribution of non-linearity, as an isolated ingredient, to the performance gain.  To do so, we place classic linear graph diffusions in a self-training framework.  Surprisingly, we observe that the resulting {\em bootstrapped diffusions} not only significantly improve over the respective non-bootstrapped baselines but also outperform state-of-the-art non-linear methods.  Moreover, since the self-training wrapper retains the scalability of the base method, we obtain both higher quality and better scalability."
"The internet needs a competitive, royalty-free video codec.  In this paper, we present the argument in favor of an open source, a royalty-free video codec that will keep pace with the evolution of video traffic. Additionally, we argue that the availability of a state-of-the-art, royalty-free codec levels the playing field, allowing small content owners, and application developers to compete with the larger companies that operate in this space."
"Minimum Word Error Rate Training for Attention-based Sequence-to-Sequence Models.  Sequence-to-sequence models, such as attention-based models in automatic speech recognition (ASR), are typically trained to optimize the cross-entropy criterion which corresponds to improving the log-likelihood of the data. However, system performance is usually measured in terms of word error rate (WER), not log-likelihood. Traditional ASR systems benefit from discriminative sequence training which optimizes criteria such as the state-level minimum Bayes risk (sMBR) which are more closely related to WER. In the present work, we explore techniques to train attention-based models to directly minimize expected word error rate. We consider two loss functions which approximate the expected number of word errors: either by sampling from the model, or by using N-best lists of decoded hypotheses, which we find to be more effective than the sampling-based method. In experimental evaluations, we find that the proposed training procedure improves performance by up to 8.2% relative to the baseline system. This allows us to train grapheme-based, uni-directional attention-based models which match the performance of a traditional, state-of-the-art, discriminative sequence-trained system on a mobile voice-search task."
"Streaming Small-Footprint Keyword Spotting Using Sequence-to-Sequence Models.  We develop streaming keyword spotting systems using a recurrent neural network transducer (RNN-T) model: an all-neural, end-to-end trained, sequence-to-sequence model which jointly learns acoustic and language model components. Our models are trained to predict either phonemes or graphemes as subword units, thus allowing us to detect arbitrary keyword phrases, without any out-of-vocabulary words. In order to adapt the models to the requirements of keyword spotting, we propose a novel technique which biases the RNN-T system towards a specific keyword of interest. Our systems are compared against a strong sequence-trained, connectionist temporal classification (CTC) based ??keyword-filler?? baseline, which is augmented with a separate phoneme language model. Overall, our RNN-T system with the proposed biasing technique significantly improves performance over the baseline system."
"Preliminary Analysis of REST API Style Guidelines.  We studied a collection of 32 publicly published guidelines for designing RESTful Application Programming Interfaces (APIs), each from a different company, to identify similarities and differences to see if there are overall best practices across ten different topics. Our contribution includes providing a list of topics that API authors can reference when creating or evaluating their own guidelines. Additionally, we found that while some guidelines attempt to enforce consistency, simplicity, and intuitiveness in the APIs that use these guidelines, cross-guideline comparisons show a lack of consistency in some of the topics examined, and different interpretations of what is thought to be ??simple?? and ??intuitive.??"
"The Uses of Interactive Explorers for Web APIs.  Interactive method invocation has become a common interaction pattern in the documentation of web application programming interfaces (APIs). One of the earliest examples of this pattern being applied at scale is the Google APIs Explorer. In this paper, we describe eight ways developers use such tools in software development, grounded on empirical analyses of the Google APIs Explorer. We then explain the utility of these tools by tying the use cases back to extant literature on programming."
"Gradient descent efficiently learns positive definite deep linear residual networks.  We analyze algorithms
that aim to approximate 
a function
$f(x) = \Phi x$
mapping $\Re^d$ to $\Re^d$ using deep linear
neural networks, i.e.\ a function $h$ parameterized
by matrices $\Theta_1,...,\Theta_L$ defined by
$h(x) = \Theta_L \Theta_{L-1} ... \Theta_1 x$.  We focus
on algorithms that learn through gradient descent on the population
quadratic loss in the case that the distribution over the inputs is
isotropic.  We provide polynomial bounds on the number of
iterations for gradient descent to approximate the
optimum, in the case where
the initial hypothesis $\Theta_1 = ... = \Theta_L = I$ 
has loss bounded by a small enough
constant.  On the other hand,
we show that gradient descent fails to converge for
$\Phi$ whose distance from the identity
is a larger constant, and  we show that some forms
of regularization toward the identity in each layer do
not help.<br/>
If $\Phi$ is symmetric positive definite,
we show that an algorithm that initializes $\Theta_i = I$
learns an $\epsilon$-approximation of $f$ 
using a number of updates polynomial in $L$,
the condition number of $\Phi$, and $\log(d/\epsilon)$.  In contrast, we show
that if the $\Phi$ is symmetric and has a
negative eigenvalue, that all members of a class of algorithms
that perform gradient descent with identity initialization,
and optionally regularize toward the identity in each layer, fail to
converge.  We analyze an algorithm for
the case that $\Phi$ satisfies $u^{\top} \Phi u &gt; 0$ for all
$u$, but may not
be symmetric;  this algorithm uses two regularizers
that maintain the invariants that 
$u^{\top} \Theta_L \Theta_{L-1} ... \Theta_1 u &gt; 0$ for all $u$,
and that ``balance'' $\Theta_1 ... \Theta_L$ so that they
have the same singular values."
"Frame-Recurrent Video Super-Resolution.  Recent advances in video super-resolution have shown that convolutional neural networks combined with motion compensation are able to merge information from multiple low-resolution (LR) frames to create high-quality results. Current state-of-the-art methods process a batch of LR frames to generate a single high-resolution (HR) frame and run this scheme in a sliding window fashion over the entire video, effectively treating the problem as many independent multi-frame super-resolution tasks. This approach has two main weaknesses: 1) Each input frame is processed and warped multiple times, leading to redundant computations, and 2) each output frame is estimated independently, limiting the system's ability to produce temporally consistent results. In this work, we propose an end-to-end trainable frame-recursive video super-resolution framework that uses the previously inferred HR estimate to super-resolve the subsequent frame. This naturally encourages temporally consistent results and avoids redundant computations by warping only one image in each step. Furthermore, due to its recurrent nature, the proposed method has the ability to assimilate a large number of previous frames without increased computational demands. Extensive evaluations and comparisons with previous methods validate the strengths of our approach and demonstrate that the proposed framework is able to significantly outperform the current state of the art."
"Learning with Imprinted Weights.  Human vision is able to immediately recognize novel visual categories after seeing just one or a few training examples. We describe how to add a similar capability to ConvNet classifiers by directly setting the final layer weights from novel training examples during low-shot learning. We call this process \emph{weight imprinting} as it directly sets penultimate layer weights based on an appropriately scaled copy of their activations for that training example. The imprinting process provides a valuable complement to training with stochastic gradient descent, as it provides immediate good classification performance and an initialization for any further fine tuning in the future. We show how this imprinting process is related to proxy-based embeddings. However, it differs in that only a single imprinted weight vector is learned for each novel category, rather than relying on a nearest-neighbor distance to training instances as typically used with embedding methods. Our experiments show that using averaging of imprinted weights provides better generalization than using nearest-neighbor instance embeddings. A key change to traditional ConvNet classifiers is the introduction of a scaled normalization layer that allows activations to be directly imprinted as weights."
"Unsupervised Learning of Depth and Egomotion from Monocular Video Using 3D Geometric Constraints.  We present a novel approach for unsupervised learning of depth and ego-motion from monocular video. Unsupervised learning removes the need for separate supervisory signals (depth or ego-motion ground truth, or multi-view video). Prior work in unsupervised depth learning uses pixel-wise or gradient-based losses, which only consider pixels in small local neighborhoods. Our main contribution is to explicitly consider the inferred 3D geometry of the scene, enforcing consistency of the estimated 3D point clouds and ego-motion across consecutive frames. This is a challenging task and is solved by a novel (approximate) backpropagation algorithm for aligning 3D structures. 
We combine this novel 3D-based loss with 2D losses based on photometric quality of frame reconstructions using estimated depth and ego-motion from adjacent frames. We also incorporate validity masks to avoid penalizing areas in which no useful information exists. We test our algorithm on the KITTI dataset and on a video dataset captured on an uncalibrated mobile phone camera. Our proposed approach consistently improves depth estimates on both datasets, and outperforms the state-of-the-art for both depth and ego-motion. Because we only require a simple video, learning depth and ego-motion on large and varied datasets becomes possible. We demonstrate this by training on the low quality uncalibrated video dataset and evaluating on KITTI, ranking among top performing prior methods which are trained on KITTI itself."
"Predicting Human Performance in Vertical Menu Selection Using Deep Learning.  Predicting human performance in interaction tasks allows designers or developers to understand the expected performance of a target interface without actually testing it with real users. In this work, we present a deep neural net to model and predict human performance in performing a sequence of UI tasks. In particular, we focus on a dominant class of tasks, i.e., target selection from a vertical list or menu, which resembles many interaction scenarios in modern user interfaces. We experimented with our deep neural net using a public dataset collected from a desktop laboratory environment and a dataset collected from hundreds of touchscreen smartphone users via crowdsourcing. Our model significantly outperformed previous methods in various settings. Importantly, our method, as a deep model, can easily incorporate additional UI attributes such as visual appearance and content semantics without changing model architectures??these attributes were hard to capture with previous methods. We discussed our insights into the behaviors of our model."
"Attention-based Extraction of Structured Information from Street View Imagery.  We present a neural network model, based on
CNNs, RNNs and attention mechanisms, which achieves 84.04%
accuracy on the challenging French Street Name Signs (FSNS)
dataset, significantly outperforming the previous state of the
art (Smith??16), which achieved 72.46%. Furthermore, our new
method is much simpler and more general than the previous
approach. To demonstrate the generality of our model, we also
apply it to two datasets, derived from Google Street View, in
which the goal is to extract business names from store fronts,
and extract structured date/time information from parking signs.
Finally, we study the speed/accuracy tradeoff that results from
cutting pretrained inception CNNs at different depths and using
them as feature extractors for the attention mechanism. The
resulting model is not only accurate but efficient, allowing it
to be used at scale on a variety of challenging real-world text
extraction problems."
"State-of-the-art Speech Recognition With Sequence-to-Sequence Models.  We present a neural network model, based on
CNNs, RNNs and attention mechanisms, which achieves 84.04%
accuracy on the challenging French Street Name Signs (FSNS)
dataset, significantly outperforming the previous state of the
art (Smith??16), which achieved 72.46%. Furthermore, our new
method is much simpler and more general than the previous
approach. To demonstrate the generality of our model, we also
apply it to two datasets, derived from Google Street View, in
which the goal is to extract business names from store fronts,
and extract structured date/time information from parking signs.
Finally, we study the speed/accuracy tradeoff that results from
cutting pretrained inception CNNs at different depths and using
them as feature extractors for the attention mechanism. The
resulting model is not only accurate but efficient, allowing it
to be used at scale on a variety of challenging real-world text
extraction problems."
"Fast and Accurate Reading Comprehension by combining self-attention and convolution.  Current end-to-end Q&amp;A models are primarily based on recurrent neural networks with attention. Despite their success, these models are often slow for both training and inference. We propose a novel Q&amp;A model that does not require recurrent networks yet achieves equivalent or better performance than existing models. Our model is simple in that it consists exclusively of attention and convolutions. We present a thorough study of architectural choices that improve the accuracy of this simple model.
We also propose a novel data augmentation technique that not only enhances the training examples but also diversifies the phrasing of the sentences. It results in immediate improvement in the accuracy. This technique is of independent interest that it can be readily applied to other natural language processing tasks.
On the SQuAD dataset, our model is 3x faster in training and 10x faster in inference. The model achieves 82.2 F1 score on the development set, which is on par with best documented result of 81.8."
"Learning how to explain neural networks: PatternNet and PatternAttribution.  DeConvNet, Guided BackProp, LRP, were invented to better understand deep neural networks. We show that these methods do not produce the theoretically correct explanation for a linear model. Yet they are used on multi-layer networks with millions of parameters. This is a cause for concern since linear models are simple neural networks. We argue that explanation methods for neural nets should work reliably in the limit of  simplicity, the linear models. Based on our analysis of linear models we propose a generalization that yields two explanation techniques (PatternNet and PatternAttribution) that are theoretically sound for linear models and produce improved explanations for deep networks."
"Many Paths to Equilibrium: GANs do not need to decrease a divergence at every step.  Generative adversarial networks (GANs) are a family of generative models that
do not minimize a single training criterion. Unlike other generative models, the
data distribution is learned via a game between a generator (the generative model)
and a discriminator (a teacher providing training signal) that each minimize their
own cost. GANs are designed to reach a Nash equilibrium at which each player
cannot reduce their cost without changing the other players?? parameters. One
useful approach for the theory of GANs is to show that a divergence between
the training distribution and the model distribution obtains its minimum value at
equilibrium. Several recent research directions have been motivated by the idea
that this divergence is the primary guide for the learning process and that every
step of learning should decrease the divergence. We show that this view is overly
restrictive. During GAN training, the discriminator provides learning signal in
situations where the gradients of the divergences between distributions would not
be useful. We provide empirical counterexamples to the view of GAN training as
divergence minimization. Specifically, we demonstrate that GANs are able to learn
distributions in situations where the divergence minimization point of view predicts
they would fail. We also show that gradient penalties motivated from the divergence
minimization perspective are equally helpful when applied in other contexts in
which the divergence minimization perspective does not predict they would be
helpful. This contributes to a growing body of evidence that GAN training may be
more usefully viewed as approaching Nash equilibria via trajectories that do not
necessarily minimize a specific divergence at each step."
"An efficient framework for learning sentence representations.  In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time."
"Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games?.  Deep reinforcement learning has seen many remarkable successes over the past few years. However, progress is hindered by challenges faced in reinforcement learning, such as large variability in performance, catastrophic forgetting, and overfitting to particular states. We propose Erdos-Selfridge-Spencer games as a reinforcement learning testbed. We focus in particular on one of the best-known games in this genre, Spencer??s attacker-defender game, also known as the ??tenure game??. This game has several nice properties: it is (i) a low-dimensional, simply parametrized environment where (ii) there is a linear closed form solution for optimal behavior from any state, and (iii) the difficulty of the game can be tuned by changing environment parameters in an interpretable way. We compare several RL methods to the tenure game, examining their performance given varying environment difficulty and their generalization to environments outside the training set."
"A  Bayesian Perspective on Generalization and Stochastic Gradient Descent.  Zhang et al. (2016) argued that understanding deep learning requires rethinking
generalization. To justify this claim, they showed that deep networks can easily
memorize randomly labeled training data, despite generalizing well when shown
real labels of the same inputs. We show here that the same phenomenon occurs
in small linear models with fewer than a thousand parameters; however there is
no need to rethink anything, since our observations are explained by evaluating
the Bayesian evidence in favor of each model. This Bayesian evidence penalizes
sharp minima. We also explore the ??generalization gap?? observed between small
and large batch training, identifying an optimum batch size which scales linearly
with both the learning rate and the size of the training set. Surprisingly, in our
experiments the generalization gap was closed by regularizing the model."
"Towards learning a metric for neural prosthetics.  The goal of retinal prosthetics to build an electronic device to replaced diseased or damaged eye. One challenge of building this system is translating video information into the electrical signal sent from the eye to the brain. All electronic devices for interfacing with biology are imprecise and even if the desired electrical signal is known, the electronic device mostly likely will not be able to elicit this signal. This discrepancy requires devising a method for determining how close a given pair of neural firing patterns are in order to determine the best possible stimulation pattern to approximate the external visual world. In this work we propose to learn such a metric from recording of populations of retinal ganglion cells (RGCs) in the primate retina. We demonstrate that this metric outperforms existing techniques and demonstrate its application and utility in real stimulation experiments to mimic a retinal prosthetic device. Neural prosthetics systems requires measuring the distance between codes. We
4 build such a system."
"Appearance-and-Relation Networks for Video Classification.  Spatiotemporal feature learning in videos is a fundamental and difficult problem in computer vision. This paper presents a new architecture, termed as Appearance-and-Relation Networks (ARTNets), to learn video representation in an end-to-end manner. ARTNet is constructed by stacking multiple generic building blocks, called as SMART, whose goal is to simultaneously model appearance and relation from RGB input in a separate and explicit manner. Specifically, SMART block decouples the problem of spatiotemporal feature learning into an appearance branch for spatial modeling and a relation branch for temporal modeling.  Appearance branch is implemented based on the linear combination of pixels or filter responses in each frame, while relation branch is designed based on the multiplicative interactions between pixels or filter responses across multiple frames. We perform experiments on three action recognition benchmarks: Kinetics, UCF101, and HMDB51, demonstrating that SMART block obtains an evident improvement over 3D convolution for spatiotemporal feature learning. Under the same training setting, ARTNets achieve superior performance on these three datasets to the existing state-of-the-art methods. The code is at https://github.com/wanglimin/ARTNet"
"Affinity Clustering: Hierarchical Clustering at Scale.  Graph clustering is a fundamental task in many data-mining and machine-learning pipelines. In particular, identifying good hierarchical clustering structure is at the same time a fundamental and challenging problem for several applications. In many applications, the amount of data to analyze is increasing at an astonishing rate each day. Hence there is a need for new solutions to efficiently compute effective hierarchical clusterings on such huge data. In this paper, we propose algorithms to address this problem. First, we analyze minimum spanning tree-based clustering algorithms and their corresponding hierarchical clusterings. In particular we consider classic  single-linkage clustering based on Kruskal's algorithm and a variation of  Boruvka   algorithm that we call affinity clustering and prove new interesting properties of these clusterings via the concept of  certificates. Then we present new algorithms in the MapReduce model and their efficient real world implementations via Distributed Hash Tables (DHTs). Our MapReduce algorithms indeed improve upon the previous MapReduce algorithms for finding a minimum spanning tree in graphs as well. Finally we show experimentally that our algorithms are scalable for huge data and competitive with state-of-the-art algorithms. In particular we show that Affinity Clustering is in practice superior to several state-of-the-art clustering algorithms."
"Tunable inductive coupling of superconducting qubits in the strongly nonlinear regime.  For a variety of superconducting qubits, tunable interactions are achieved through mutual inductive coupling to a coupler circuit containing a nonlinear Josephson element. In this paper, we derive the general interaction mediated by such a circuit under the Born-Oppenheimer approximation. This interaction naturally decomposes into a classical part, with origin in the classical circuit equations, and a quantum part, associated with the coupler's zero-point energy. Our result is nonperturbative in the qubit-coupler coupling strengths and in the coupler nonlinearity. This can lead to significant departures from previous, linear theories for the interqubit coupling, including nonstoquastic and many-body interactions. Our analysis provides explicit and efficiently computable series for any term in the interaction Hamiltonian and can be applied to any superconducting qubit type. We conclude with a numerical investigation of our theory using a case study of two coupled flux qubits, and in particular study the regime of validity of the Born-Oppenheimer approximation."
"The Secret Sharer: Measuring Unintended Neural Network Memorization &amp; Extracting Secrets.  Machine learning models based on neural networks and deep learning are being rapidly adopted for many purposes. What those models learn, and what they may share, is a significant concern when the training data may contain secrets and the models are public -- e.g., when a model helps users compose text messages using models trained on all users' messages. 
This paper presents exposure: a simple-to-compute metric that can be applied to any deep learning model for measuring the memorization of secrets. Using this metric, we show how to extract those secrets efficiently using black-box API access. Further, we show that unintended memorization occurs early, is not due to over-fitting, and is a persistent issue across different types of models, hyperparameters, and training strategies. We experiment with both real-world models (e.g., a state-of-the-art translation model) and datasets (e.g., the Enron email dataset, which contains users' credit card numbers) to demonstrate both the utility of measuring exposure and the ability to extract secrets. 
Finally, we consider many defenses, finding some ineffective (like regularization), and others to lack guarantees. However, by instantiating our own differentially-private recurrent model, we validate that by appropriately investing in the use of state-of-the-art techniques, the problem can be resolved, with high utility."
"Acceleration and Averaging in Stochastic Descent Dynamics.  We formulate and study a general family of (continuous-time) stochastic dynamics for accelerated first-order minimization of smooth convex functions. Building on an averaging formulation of accelerated mirror descent, we propose a stochastic variant in which the gradient is contaminated by noise, and study the resulting stochastic differential equation. We prove a bound on the rate of change of an energy function associated with the problem, then use it to derive estimates of convergence rates of the function values (almost surely and in expectation), both for persistent and asymptotically vanishing noise. We discuss the interaction between the parameters of the dynamics (learning rate and averaging rates) and the covariation of the noise process. In particular, we show how the asymptotic rate of covariation affects the choice of parameters and, ultimately, the convergence rate."
"Crowdsourcing Ground Truth for Medical Relation Extraction.  Cognitive computing systems require human labeled data for evaluation, and often for training. The standard practice used in gathering this data minimizes disagreement between annotators, and we have found this results in data that fails to account for the ambiguity inherent in language. We have proposed the CrowdTruth method for collecting ground truth through crowdsourcing, that reconsiders the role of people in machine learning based on the observation that disagreement between annotators provides a useful signal for phenomena such as ambiguity in the text. We report on using this method to build an annotated data set for medical relation extraction for the $cause$ and $treat$ relations, and how this data performed in a supervised training experiment. We demonstrate that by modeling ambiguity, labeled data gathered from crowd workers can (1) reach the level of quality of domain experts for this task while reducing the cost, and (2) provide better training data at scale than distant supervision.  We further propose and validate new weighted measures for precision, recall, and F-measure, that account for ambiguity in both human and machine performance on this task."
"Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping.  Instrumenting and collecting annotated visual grasping datasets to train modern machine learning algorithms is prohibitively expensive. An appealing alternative is to use off-the-shelf simulators to render synthetic data for which ground-truth annotations are generated automatically. 
Unfortunately, models trained purely on simulated data often fail to generalize to the real world. To address this shortcoming, prior work introduced domain adaptation algorithms that attempt to make the resulting models domain-invariant. However, such works were evaluated primarily on offline image classification datasets. In this work, we adapt these techniques for learning,  primarily in simulation, robotic hand-eye coordination for grasping. Our approaches generalize to diverse and previously unseen real-world objects.
We show that, by using synthetic data and domain adaptation, we are able to reduce the amounts of real--world samples required for our goal and a certain level of performance by up to 50 times. We also show that by using our suggested methodology we are able to achieve good grasping results by using no real world labeled data."
"Quantum Simulation of Electronic Structure with Linear Depth and Connectivity.  As physical implementations of quantum architectures emerge, it is increasingly important to consider the cost of algorithms for practical connectivities between qubits. We show that by using an arrangement of gates that we term the fermionic swap network, we can simulate a Trotter step of the electronic structure Hamiltonian in exactly N depth and with N^2/2 two-qubit entangling gates, and prepare arbitrary Slater determinants in at most N/2 depth, all assuming only a minimal, linearly connected architecture. We conjecture that no explicit Trotter step of the electronic structure Hamiltonian is possible with fewer entangling gates, even with arbitrary connectivities. These results represent significant practical improvements on the cost of all current proposed algorithms for both variational and phase estimation based simulation of quantum chemistry."
"Improved Techniques for Preparing Eigenstates of Fermionic Hamiltonians.  Modeling low energy eigenstates of fermionic systems can provide insight into chemical reactions and material properties and is one of the most anticipated applications of quantum computing. We present three techniques for reducing the cost of preparing fermionic Hamiltonian eigenstates using phase estimation. First, we report a polylogarithmic-depth quantum algorithm for antisymmetrizing the initial states required for simulation of fermions in first quantization. This is an exponential improvement over the previous state-of-the-art. Next, we show how to reduce the overhead due to repeated state preparation in phase estimation when the goal is to prepare the ground state to high precision and one has knowledge of an upper bound on the ground state energy that is less than the excited state energy (often the case in quantum chemistry). Finally, we explain how one can perform the time evolution necessary for the phase estimation based preparation of Hamiltonian eigenstates with exactly zero error by using the recently introduced qubitization procedure."
"Application of Fermionic Marginal Constraints to Hybrid Quantum Algorithms.  Many quantum algorithms, including recently proposed hybrid classical/quantum algorithms, make use of restricted tomography of the quantum state that measures the reduced density matrices, or marginals, of the full state. The most straightforward approach to this algorithmic step estimates each component of the marginal independently without making use of the algebraic and geometric structure of the marginals. Within the field of quantum chemistry, this structure is termed the fermionic $n$-representability conditions, and is supported by a vast amount of literature on both theoretical and practical results related to their approximations. In this work, we introduce these conditions in the language of quantum computation, and utilize them to develop several techniques to accelerate and improve practical applications for quantum chemistry on quantum computers. As a general result, we demonstrate how these marginals concentrate to diagonal quantities when measured on random quantum states. We also show that one can use fermionic $n$-representability conditions to reduce the total number of measurements required by more than an order of magnitude for medium sized systems in chemistry. As a practical demonstration, we simulate an efficient restoration of the physicality of energy curves for the dilation of a four qubit diatomic hydrogen system in the presence of three distinct one qubit error channels, providing evidence these techniques are useful for pre-fault tolerant quantum chemistry experiments."
"False Positive and Cross-relation Signals in Distant Supervision Data.  Distant supervision (DS) is a well-established method for relation extraction from text, based on the assumption that when a knowledge-base contains a relation between a term pair, then sentences that contain that pair are likely to express the relation. In this paper, we use the results of a crowdsourcing relation extraction task to identify two problems with DS data quality: the widely varying degree of false positives across different relations, and the observed causal connection between relations that are not considered by the DS method. The crowdsourcing data aggregation is performed using ambiguity-aware CrowdTruth metrics, that are used to capture and interpret inter-annotator disagreement. We also present preliminary results of using the crowd to enhance DS training data for a relation classification model, without requiring the crowd to annotate the entire set."
"Grabity: A Wearable Haptic Interface for Simulating Weight and Grasping in Virtual Reality.  Ungrounded haptic devices for virtual reality (VR) applications
lack the ability to convincingly render the sensations of
a grasped virtual object??s rigidity and weight. We present Grabity,
a wearable haptic device designed to simulate kinesthetic
pad opposition grip forces and weight for grasping virtual
objects in VR. The device is mounted on the index finger
and thumb and enables precision grasps with a wide range of
motion. A unidirectional brake creates rigid grasping force
feedback. Two voice coil actuators create virtual force tangential
to each finger pad through asymmetric skin deformation.
These forces can be perceived as gravitational and inertial
forces of virtual objects. The rotational orientation of the
voice coil actuators is passively aligned with the real direction
of gravity through a revolute joint, causing the virtual forces to
always point downward. This paper evaluates the performance
of Grabity through two user studies, finding promising ability
to simulate different levels of weight with convincing object
rigidity. The first user study shows that Grabity can convey
various magnitudes of weight and force sensations to users by
manipulating the amplitude of the asymmetric vibration. The
second user study shows that users can differentiate different
weights in a virtual environment using Grabity."
"SmartSleeve: Real-time Sensing of Surface and Deformation Gestures on Flexible, Interactive Textiles, using a Hybrid Gesture Detection Pipeline.  Over the last decades, there have been numerous efforts in
wearable computing research to enable interactive textiles.
Most work focus, however, on integrating sensors for planar
touch gestures, and thus do not fully take advantage of the
flexible, deformable and tangible material properties of textile.
In this work, we introduce SmartSleeve, a deformable
textile sensor, which can sense both surface and deformation
gestures in real-time. It expands the gesture vocabulary with
a range of expressive interaction techniques, and we explore
new opportunities using advanced deformation gestures, such
as, Twirl, Twist, Fold, Push and Stretch. We describe our sensor
design, hardware implementation and its novel non-rigid
connector architecture. We provide a detailed description of
our hybrid gesture detection pipeline that uses learning-based
algorithms and heuristics to enable real-time gesture detection
and tracking. Its modular architecture allows us to derive new
gestures through the combination with continuous properties
like pressure, location, and direction. Finally, we report on
the promising results from our evaluations which demonstrate
real-time classification."
"Zensei: Embedded, Multi-electrode Bioimpedance Sensing for Implicit, Ubiquitous User Recognition.  Interactions and connectivity is increasingly expanding to
shared objects and environments, such as furniture, vehicles,
lighting, and entertainment systems. For transparent personalization
in such contexts, we see an opportunity for embedded
recognition, to complement traditional, explicit authentication. We introduce Zensei, an implicit sensing system that leverages
bio-sensing, signal processing and machine learning to classify
uninstrumented users by their body??s electrical properties.
Zensei could allow many objects to recognize users. E.g.,
phones that unlock when held, cars that automatically adjust
mirrors and seats, or power tools that restore user settings. We introduce wide-spectrum bioimpedance hardware that
measures both amplitude and phase. It extends previous approaches
through multi-electrode sensing and high-speed wireless
data collection for embedded devices. We implement
the sensing in devices and furniture, where unique electrode
configurations generate characteristic profiles based on user??s
unique electrical properties. Finally, we discuss results from
a comprehensive, longitudinal 22-day data collection experiment
with 46 subjects. Our analysis shows promising classifi-
cation accuracy and low false acceptance rate."
"StretchEBand: Enabling Fabric-Based Interactions through Rapid Fabrication of Textile Stretch Sensors.  The increased interest in interactive soft materials, such as
smart clothing and responsive furniture, means that there is a
need for flexible and deformable electronics. In this paper, we
focus on stitch-based elastic sensors, which have the benefit of
being manufacturable with textile craft tools that have been
used in homes for centuries. We contribute to the understanding
of stitch-based stretch sensors through four experiments
and one user study that investigate conductive yarns from textile
and technical perspectives, and analyze the impact of different
stitch types and parameters. The insights informed our
design of new stretch-based interaction techniques that emphasize
eyes-free or causal interactions. We demonstrate with
StretchEBand how soft, continuous sensors can be rapidly fabricated
with different parameters and capabilities to support
interaction with a wide range of performance requirements
across wearables, mobile devices, clothing, furniture, and toys."
"SkinMarks: Enabling Interactions on Body Landmarks Using Conformal Skin Electronics.  The body provides many recognizable landmarks due to the
underlying skeletal structure and variations in skin texture,
elasticity, and color. The visual and spatial cues of such body
landmarks can help in localizing on-body interfaces, guide
input on the body, and allow for easy recall of mappings.
Our main contribution are SkinMarks, novel skin-worn I/O
devices for precisely localized input and output on fine body
landmarks. SkinMarks comprise skin electronics on temporary
rub-on tattoos. They conform to fine wrinkles and are
compatible with strongly curved and elastic body locations.
We identify five types of body landmarks and demonstrate
novel interaction techniques that leverage SkinMarks?? unique
touch, squeeze and bend sensing with integrated visual output.
Finally, we detail on the conformality and evaluate
sub-millimeter electrodes for touch sensing. Taken together,
SkinMarks expands the on-body interaction space to more detailed,
highly curved and challenging areas on the body."
"WatchThru: Expanding Smartwatch Displays with Mid-air Visuals and Wrist-worn Augmented Reality.  We introduce WatchThru, an interactive method for extended
wrist-worn display on commercially-available smartwatches.
To address the limited visual and interaction space, WatchThru
expands the device into 3D through a transparent display. This
enables novel interactions that leverage and extend smartwatch
glanceability. We describe three novel interaction techniques,
Pop-up Visuals, Second Perspective and Peek-through, and discuss
how they can complement interaction on current devices.
We also describe two types of prototypes that helped us to
explore standalone interactions, as well as, proof-of-concept
AR interfaces using our platform."
"shiftIO: Reconfigurable Tactile Elements for Dynamic Affordances and Mobile Interaction.  Currently, virtual (i.e. touchscreen) controls are dynamic, but
lack the advantageous tactile feedback of physical controls.
Similarly, devices may also have dedicated physical controls,
but they lack the flexibility to adapt for different contexts and
applications. On mobile and wearable devices in particular,
space constraints further limit our input and output capabilities.
We propose utilizing reconfigurable tactile elements around
the edge of a mobile device to enable dynamic physical controls
and feedback. These tactile elements can be used for
physical touch input and output, and can reposition according
to the application both around the edge of and hidden within
the device. We present shiftIO, two implementations of such
a system which actuate physical controls around the edge of
a mobile device using magnetic locomotion. One version utilizes
PCB-manufactured electromagnetic coils, and the other
uses switchable permanent magnets. We perform a technical
evaluation of these prototypes and compare their advantages in
various applications. Finally, we demonstrate several mobile
applications which leverage shiftIO to create novel mobile
interactions."
"proCover: Sensory Augmentation of Prosthetic Limbs Using Smart Textile Covers.  Today??s commercially available prosthetic limbs lack tactile
sensation and feedback. Recent research in this domain focuses
on sensor technologies designed to be directly embedded
into future prostheses. We present a novel concept and
prototype of a prosthetic-sensing wearable that offers a noninvasive,
self-applicable and customizable approach for the
sensory augmentation of present-day and future low to midrange
priced lower-limb prosthetics. From consultation with
eight lower-limb amputees, we investigated the design space
for prosthetic sensing wearables and developed novel interaction
methods for dynamic, user-driven creation and mapping
of sensing regions on the foot to wearable haptic feedback actuators.
Based on a pilot-study with amputees, we assessed the
utility of our design in scenarios brought up by the amputees
and we summarize our findings to establish future directions
for research into using smart textiles for the sensory enhancement
of prosthetic limbs."
"Shape Displays: Spatial Interaction with Dynamic Physical Form.  Shape displays are a new class of I/O devices
that dynamically render physical shape and
geometry. They allow multiple users to experience
information through touch and deformation
of their surface topology. The rendered shapes
can react to user input or continuously update
their properties based on an underlying simulation.
Shape displays can be used by industrial
designers to quickly render physical CAD models
before 3D printing, urban planners to physically
visualize a site, medical experts to tactually explore
volumetric data sets, or students to learn and
understand parametric equations. Previous work on shape displays has mostly focused
on physical rendering of digital content to
overcome the limitations of single-point haptic
interfaces??examples include the Feelex and Lumen projects. In our research, we emphasize the
use of shape displays for designing new interaction
techniques that leverage tactile spatial qualities to
guide users. For this purpose, we designed, developed,
and engineered three shape display systems
that integrate physical rendering, synchronized
visual display, shape sensing, spatial tracking, and
object manipulation. This enabling technology has
allowed us to contribute numerous interaction
techniques for virtual, physical, and augmented
reality, in collocated settings as well as for remote
collaboration. Our systems are based on arrays of motorized
pins, which extend from a tabletop to form 2.5D
shapes: Relief consists of 120 pins in a circular
tabletop, a platform later augmented with spatial
graphics for the Sublimate system. Our next-generation
platform, inFORM renders higher resolution
shapes through 900 pins (see Figure 1).
The Transform system consists of 1,152 pins embedded
into the surface of domestic furniture. To capture objects and gestures and to control visual
appearance, we augment the shape displays with
overhead depth-sensing cameras and projectors. In this article, we wish to introduce readers
to some of the exciting interaction possibilities
that shape displays enable beyond those found
in traditional 3D displays or haptic interfaces.
We describe new means for physically displaying
3D graphics, interaction techniques that leverage
physical touch, enhanced collaboration through
physical telepresence and unique applications of
shape displays. Our current shape displays are
based on prototype hardware that enabled us to
design, develop, and explore a range of novel interaction
techniques. Although the general applicability
of these prototypes are limited by
resolution, mechanical complexity, and cost, we
believe that many of the techniques we introduce
can be transferred to a range of special-purpose
scenarios that have different sensing and actuation
needs, potentially even using a completely different
technical approach. We thus hope that our
work will inspire future researchers to start considering
dynamic physical form as an interesting
approach to enable new capabilities and expressiveness
beyond today??s flat displays."
"Shared Task Proposal: Multilingual Surface Realization Using Universal Dependency Trees.  We propose a shared task on multilingual Surface Realization, i.e., on mapping unordered
and uninflected universal dependency trees to correctly ordered and inflected sentences in a
number of languages. A second deeper input will be available in which, in addition,
functional words, fine-grained PoS and morphological information will be removed from
the input trees. The first shared task on Surface Realization was carried out in 2011 with
a similar setup, with a focus on English. We think that it is time for relaunching such a
shared task effort in view of the arrival of Universal Dependencies annotated treebanks for
a large number of languages on the one hand, and the increasing dominance of Deep Learning,
which proved to be a game changer for NLP,  on the other hand."
"Ask the Right Questions: Active Question Reformulation with Reinforcement Learning.  We frame Question Answering (QA) as a Reinforcement Learning task, an approach that we call Active Question Answering. We propose an agent that sits between the user and a black box QA system and learns to reformulate questions to elicit the best possible answers. The agent probes the system with, potentially many, natural language reformulations of an initial question and aggregates the returned evidence to yield the best answer. The reformulation system is trained end-to-end to maximize answer quality using policy gradient. We evaluate on SearchQA, a dataset of complex questions extracted from Jeopardy!. The agent outperforms a state-of-the-art base model, playing the role of the environment, and other benchmarks. We also analyze the language that the agent has learned while interacting with the question answering system. We find that successful question reformulations look quite different from natural language paraphrases. The agent is able to discover non-trivial reformulation strategies that resemble classic information retrieval techniques such as term re-weighting (tf-idf) and stemming."
"RFC 7871 - Client Subnet in DNS Queries.  This document describes an Extension Mechanisms for DNS (EDNS0)
option that is in active use to carry information about the network
that originated a DNS query and the network for which the subsequent
response can be cached.  Since it has some known operational and
privacy shortcomings, a revision will be worked through the IETF for
improvement."
RFC 8110 - Opportunistic Wireless Encryption.  This memo describes Opportunistic Wireless Encryption (OWE) -- a mode of opportunistic security [RFC7435] for IEEE Std 802.11 that provides encryption of the wireless medium but no authentication.
"RFC 8145 - Signaling Trust Anchor Knowledge in DNS Security Extensions (DNSSEC).  The DNS Security Extensions (DNSSEC) were developed to provide origin
authentication and integrity protection for DNS data by using digital
signatures.  These digital signatures can be verified by building a
chain of trust starting from a trust anchor and proceeding down to a
particular node in the DNS.  This document specifies two different
ways for validating resolvers to signal to a server which keys are
referenced in their chain of trust.  The data from such signaling
allow zone administrators to monitor the progress of rollovers in a
DNSSEC-signed zone."
"RFC 8198 - Aggressive Use of DNSSEC-Validated Cache.  The DNS relies upon caching to scale; however, the cache lookup
   generally requires an exact match.  This document specifies the use
   of NSEC/NSEC3 resource records to allow DNSSEC-validating resolvers
   to generate negative answers within a range and positive answers from
   wildcards.  This increases performance, decreases latency, decreases
   resource utilization on both authoritative and recursive servers, and
   increases privacy.  Also, it may help increase resilience to certain
   DoS attacks in some circumstances. This document updates RFC 4035 by allowing validating resolvers to
   generate negative answers based upon NSEC/NSEC3 records and positive
   answers in the presence of wildcards."
"RFC 8244 - Special-Use Domain Names Problem Statement.  The policy defined in RFC 6761 for IANA registrations in the
   ""Special-Use Domain Names"" registry has been shown, through
   experience, to present challenges that were not anticipated when RFC
   6761 was written.  This memo presents a list, intended to be
   comprehensive, of the problems that have since been identified.  In
   addition, it reviews the history of domain names and summarizes
   current IETF publications and some publications from other
   organizations relating to Special-Use Domain Names. This document should be considered required reading for IETF
   participants who wish to express an informed opinion on the topic of
   Special-Use Domain Names."
"What is in a Web View? An Analysis of Progressive Web App Features When the Means of Web Access is not a Web Browser.  Progressive Web Apps (PWA) are a new class of Web applications, enabled for the most part by the Service Workers APIs. Service Workers allow apps to work offline by intercepting network requests to deliver programmatic or cached responses, Service Workers can receive push notifications and synchronize data in the background even when the app is not running, and??together with Web App Manifests??allow users to install PWAs to their devices?? home screens. Service Workers being a Web standard, support has landed in several stand-alone Android Web browsers??among them (but not limited to) Chrome and its open-source foundation Chromium, Firefox, Edge, Opera, UC Browser, Samsung Internet, and??eagerly awaited??iOS Safari. In this paper, we examine the PWA feature support situation in Web Views, that is, in-app Web experiences that are explicitly not stand-alone browsers. Such in-app browsers can commonly be encountered in chat applications like WeChat or WhatsApp, online social networks like Facebook or Twitter, but also email clients like Gmail, or simply anywhere where Web content is displayed inside native apps. We have developed an open-source application called PWA Feature Detector that allows for easily testing in-app browsers (and naturally stand-alone browsers), and have evaluated the level of support for PWA features on different devices and Web Views. On the one hand, our results show that there are big differences between the various Web View technologies and the browser engines they are based upon, but on the other hand, that for Android the results are independent from the devices?? operating systems, which is good news given the problematic update policy of many device manufacturers. These findings help developers make educated choices when it comes to determining whether a PWA is the right approach given their target users?? means of Web access."
Test Selection Safety and Evaluation Framework.  Describes work we have done to evaluate the safety of new test selection algorithms for TAP.
"On-Skin Interaction Using Body Landmarks.  Recent research in human??computer interaction (HCI) has recognized the human skin as a promising surface for interacting with computing devices. The human skin is large, always available, and sensitive to touch. Leveraging it as an interface helps overcome the limited surface real estate of today??s wearable devices and allows for input to smart watches, smart glasses, mobile phones, and remote displays. Various technologies have been presented that transform the human skin into an interactive surface. For instance, touch input has been captured using cameras, body-worn sensors and slim skin-worn electronics. Output has been provided using projectors, thin displays, and computer-induced muscle movement. Researchers have also developed experimental interaction techniques for the human skin; for instance, allowing a user to activate an interface element by tapping on a specific finger location or by grabbing or squeezing the skin. To keep the design and engineering tractable, most existing work has approached the skin as a more or less planar surface. In that way, principles and models for designing interaction could be transferred from existing touch-based devices to the skin. However, this assumes that the resolution of sensing or visual output on the skin is as uniform and dense as on current touch devices. It is not; current on-skin interaction typically allows
only touch gestures or tapping on a few distinct locations with varying performance and, therefore, greatly
limits possible interaction styles. It might be acceptable
for answering or rejecting a phone call, but it is not powerful enough to allow expressive interaction with a wide
range of user interfaces and applications. More importantly, this line of
thinking does not consider the fact
that the human skin has unique properties that vary across body locations,
making it fundamentally different from planar touch surfaces. For instance, the skin contains many distinct geometries that users can feel and see during interactions, such as
the curvature of a finger or a protruding knuckle. Skin is also stretchable, which allows novel interactions based
on stretching and deforming. Additionally, skin provides a multitude of
sensory cells for direct tactile feedback, and proprioception guides the
user during interaction on the body."
"Measuring and Mitigating Unintended Bias in Text Classification.  We introduce and illustrate a new approach to measuring and
mitigating unintended bias in machine learning models. Our
definition of unintended bias is parameterized by a test set
and a subset of input features. We illustrate how this can
be used to evaluate text classifiers using a synthetic test set
and a public corpus of comments annotated for toxicity from
Wikipedia Talk pages. We also demonstrate how imbalances
in training data can lead to unintended bias in the resulting
models, and therefore potentially unfair applications. We use
a set of common demographic identity terms as the subset of
input features on which we measure bias. This technique permits
analysis in the common scenario where demographic information
on authors and readers is unavailable, so that bias
mitigation must focus on the content of the text itself. The
mitigation method we introduce is an unsupervised approach
based on balancing the training dataset. We demonstrate that
this approach reduces the unintended bias without compromising
overall model quality"
"Reliability When Everything Is a Platform: Why You Need to SRE Your Customers.  The general trend in software over the last several years is to give every system an API and turn every product into a platform. When these systems only served end users, their reliability depended solely on how well we did our jobs as SREs. Increasingly, however, our customers' perceptions of our reliability are being driven by the quality of the software they bring to our platforms. The normal boundaries between our platforms and our customers are being blurred and it's getting harder to deliver a consistent end user reliability experience. In this talk we'll discuss a provocative idea??that as SREs we should take joint operational responsibility and go on-call for the systems our customers build on our platforms. We'll discuss the specific technical and operational challenges in this approach and the results of an experiment we're running at Google to address this need. Finally, we'll try to take a glimpse into the future and see what these changes mean for the future of SRE as a discipline."
"Deep Music: Towards Musical Dialogue.  Computer dialogue systems are designed with the intention of supporting meaningful interactions with humans. Common modes of communication include speech, text, and physical gestures. In this work we explore a communication paradigm in which the input and output channels consist of music. Specifically, we examine the musical interaction scenario of call and response. We present a system that utilizes a deep autoencoder to learn semantic embeddings of musical input. The system learns to transform these embeddings in a manner such that reconstructing from these transformation vectors produces appropriate musical responses. In order to generate a response the system employs a combination of generation and unit selection. Selection is based on a nearest neighbor search within the embedding space and for real-time applica- tion the search space is pruned using vector quantization. The live demo consists of a person playing a midi keyboard and the computer generating a response that is played through a loudspeaker."
"Counterpoint by Convolution.  Machine learning models of music typically break down the task of composition into a chronological process, composing a piece of music in a single pass from beginning to end. On the contrary, human composers write music in a nonlinear fashion, scribbling motifs here and there, often revisiting choices previously made. We explore the use of blocked Gibbs sampling as an analogue to the human approach, and introduce COCONET, a convolutional neural network in the NADE family of generative models (Uria et al., 2016). Despite ostensibly sampling from the same distribution as the NADE ancestral sampling procedure, we find that a blocked Gibbs approach significantly improves sample quality. We provide evidence that this is due to some conditional distributions being poorly modeled. Moreover, we show that even the cheap approximate blocked Gibbs procedure from Yao et al. (2014) yields better samples than ancestral sampling. We demonstrate the versatility of our method on unconditioned polyphonic music generation."
"Methods, systems, and media for presenting a user interface customized for a predicted user activity.  Methods, systems, and media for presenting a user interface customized for a predicted user activity are provided. In some embodiments, the method comprises: selecting users of a content delivery service, causing user devices to prompt the associated users to provide subjective data related to the user's intent when requesting media content items, training a predictive model to identify a user's subjective intent in requesting a media content item based on objective data received from a user device associated with the user and the subjective data received from the user devices, wherein the predictive model is trained to identify whether to present the user with a first user interface associated with a first user intent or a second user interface associated with a second user intent, causing the first user interface or the second user interface to be presented."
"Learning to Create Piano Performances.  Nearly  all  previous  work  on  music  generation  has  focused  on  creating  pieces that are,  effectively,  musical scores.   In contrast,  we learn to create piano performances:  besides predicting the notes to be played, we also predict expressive variations in the timing and musical dynamics (loudness).  We provided samples generated by our system for informal feedback to a set of professional musicians and composers, and the samples were well-received. Overall, the comments indicate that our system is generating music that, while lacking high-level structure, does indeed sound very much like human performance, and is closely reminiscent of the classical piano repertoire."
"Aperture Supervision for Monocular Depth Estimation.  We present a novel method to train machine learning algorithms to estimate scene depths from a single image, by using the information provided by a camera's aperture as supervision. Prior works use a depth sensor's outputs or images of the same scene from alternate viewpoints as supervision, while our method instead uses images from the same viewpoint taken with a varying camera aperture. To enable learning algorithms to use aperture effects as supervision, we introduce two differentiable aperture rendering functions that use the input image and predicted depths to simulate the depth-of-field effects caused by real camera apertures. We train a monocular depth estimation network end-to-end to predict the scene depths that best explain these finite aperture images as defocus-blurred renderings of the input all-in-focus image."
"Burst Denoising with Kernel Prediction Networks.  We present a technique for jointly denoising bursts of images taken from a handheld camera. In particular, we propose a convolutional neural network architecture for predicting spatially varying kernels that can both align and denoise frames, a synthetic data generation approach based on a realistic noise formation model, and an optimization guided by an annealed loss function to avoid undesirable local minima. Our model matches or outperforms the state-of-the-art across a wide range of noise levels on both real and synthetic data."
Cascode Switching Modeling and Improvement in Flyback Converter for  LED Lighting Applications.  Modeling of the cascode switching structure used in Flyback converter for achieving fast startup in the deeply dimmed phase-cut LED driver is presented in this draft. The cascode structure??s inherent instability and oscillation issue is modeled and analyzed quantitatively. Three solutions are proposed to stabilize the structure and suppress the unstable voltage oscillation. Solutions are studied using the proposed model for design robustness. And this model can be further applied to the popular new high voltage (for example 650V) cascode GaN FET technology. Experimental results of a 20W phase-cut dimmable LED driver are demonstrated to verify the proposed modeling method and solutions.
"Side-Channel Inference Attacks on Mobile Keypads using Smartwatches.  Smartwatches enable many novel applications and are fast gaining popularity. However, the presence of a diverse set of on-board sensors provides an additional attack surface to malicious software and services on these devices. In this paper, we investigate the feasibility of key press inference attacks on handheld numeric touchpads by using smartwatch motion sensors as a side-channel. We consider different typing scenarios, and propose multiple attack approaches to exploit the characteristics of the observed wrist movements for inferring individual key presses. Experimental evaluation using a commercial off-the-shelf smartwatch and smartphone show that key press inference using smartwatch motion sensors is not only fairly accurate, but also better than similar attacks previously demonstrated using smartphone motion sensors. Additionally, hand movements captured by a combination of both smartwatch and smartphone motion sensors yields better inference accuracy than either device considered individually."
"Sequences with Low-Discrepancy Blue-Noise 2-D Projections.  Distributions of samples play a very important role in rendering, affecting variance, bias and aliasing in Monte-Carlo and Quasi-Monte Carlo evaluation of the rendering equation. In this paper, we propose an original sampler which inherits many important features of classical low-discrepancy sequences (LDS): a high degree of uniformity of the achieved distribution of samples, computational efficiency and progressive sampling capability. At the same time, we purposely tailor our sampler in order to improve its spectral characteristics, which in turn play a crucial role in variance reduction, anti-aliasing and improving visual appearance of rendering. Our sampler can efficiently generate sequences of multi-dimensional points, whose power spectra approach so-called Blue-Noise (BN) spectral property while preserving low discrepancy (LD) in certain 2-D projections. In our tile-based approach, we perform permutations on subsets of the original Sobol LDS. In a large space of all possible permutations, we choose those which better approach the target BN property, using pair-correlation statistics. We pre-calculate such ??good?? permutations for each possible Sobol pattern, and store them in a lookup table efficiently accessible in runtime. We provide a complete and rigorous proof that such permutations preserve dyadic partitioning and thus the LDS properties of the point set in 2-D projections. Our muti-dimensional construction is computationally efficient, has relatively low memory footprint and supports adaptive sampling. We validate our method by performing 
spectral/discrepancy/aliasing analysis of the achieved distributions, and provide variance analysis for several target integrands of theoretical and practical interest."
"2 Billion Devices and Counting: An Industry Perspective on the State of Mobile Computer Architecture.  Mobile computing has grown drastically over the past decade since the arrival of the smartphone. Despite the rapid pace of the advancements, mobile device benchmarking and evaluation are still in its infancy both in the industry and academia. Authors address this issue head-on and present an industrial perspective on the challenges facing mobile architecture research with the hope of fostering new research to solve many of the pending problems. This paper presents ??ten commandments?? that focus on raising awareness around mobile workloads, metrics and experimental methodology. These issues, as perceived from an industry perspective, are real challenges that if addressed can alleviate the entire mobile ecosystem to the next level."
"Telluride Decoding Toolbox.  This document introduces the Telluride Decoding Toolbox. The toolbox offers a test bed for algorithms for decoding brain signals in relation to a stimulus. Our goal is to provide a standard set of tools to allow users to decode brain signals into the signals that generated them??whether the signals come from visual or auditory stimuli, and whether they are measured with EEG, MEG, ECoG or any other response for decoding. In line with the dictionary definition of decoding, we want to convert (a coded) message into intelligible language, or at least determine which of several messages was intended. The tools in this toolbox allow any perceptual stimulus to be connected to any neural signal. Although the developers of this toolbox are largely researchers who meet in Telluride Colorado for a Neuromorphic workshop and use EEG to analyze auditory experiments, the tools in this toolbox allow any perceptual stimulus to be connected to any neural signal."
"Deep Neural Networks as Gaussian Processes.  It has long been known that a single-layer fully-connected neural network with an
i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit
of infinite network width. This correspondence enables exact Bayesian inference
for infinite width neural networks on regression tasks by means of evaluating the
corresponding GP. Recently, kernel functions which mimic multi-layer random
neural networks have been developed, but only outside of a Bayesian framework.
As such, previous work has not identified that these kernels can be used as covariance
functions for GPs and allow fully Bayesian prediction with a deep neural
network. In this work, we derive the exact equivalence between infinitely wide deep networks
and GPs. We further develop a computationally efficient pipeline to compute
the covariance function for these GPs. We then use the resulting GPs to perform
Bayesian inference for wide deep neural networks on MNIST and CIFAR10.
We observe that trained neural network accuracy approaches that of the corresponding
GP with increasing layer width, and that the GP uncertainty is strongly
correlated with trained network prediction error. We further find that test performance
increases as finite-width trained networks are made wider and more similar
to a GP, and thus that GP predictions typically outperform those of finite-width
networks. Finally we connect the performance of these GPs to the recent theory
of signal propagation in random neural networks."
"Reliability of nand-Based SSDs: What Field Studies Tell Us.  Solid-state drives (SSDs) based on NAND flash are making deep inroads into data centers as well as the consumer market. In 2016, manufacturers shipped more than 130 million units totaling around 50 Exabytes of storage capacity. As the amount of data stored on solid state drives keeps increasing, it is important to understand the reliability characteristics of these devices. For a long time, our knowledge about flash reliability was derived from controlled experiments in lab environments under synthetic workloads, often using methods for accelerated testing. However, within the last two years, three large-scale field studies have been published that report on the failure behavior of flash devices in production environments subjected to real workloads and operating conditions. The goal of this paper is to provide an overview of what we have learned about flash reliability in production, and where appropriate contrasting it with prior studies performing controlled experiments."
"Very Deep Convolutional Networks for End-to-End Speech Recognition.  Sequence-to-sequence models have shown success in end-to-end speech recognition. However these models have only used shallow acoustic encoder networks. In our work, we successively train very deep convolutional networks to add more expressive power and better generalization for end-to-end ASR models. We apply network-in-network principles, batch normalization, residual connections and convolutional LSTMs to build very deep recurrent and convolutional structures. Our models exploit the spectral structure in the feature space and add computational depth without overfitting issues. We experiment with the WSJ ASR task and achieve 10.5% word error rate without any dictionary or language using a 15 layer deep network."
"Unsupervised Morphology Induction Using Word Embeddings.  We present a language agnostic, unsupervised method for inducing morphological transformations between words.
The method relies heavily on certain regularities that manifest
in high-dimensional vector spaces.
We show that this method is capable of discovering a wide-range of morphological rules, which can be successfully used towards improved natural language processing.
We evaluate this method across six different languages and nine datasets,
and show significant improvements across all languages."
"Multilingual Word Embeddings using Multigraphs.  We present a family of neural-network??inspired models for computing continuous word representation, specifically designed to exploit monolingual and multilingual text, without and with annotations (syntactic dependencies, word alignments, etc.). 
We find that this framework allows us to train embeddings with significantly
higher accuracy on syntactic and semantic compositionality, as well as multilingual semantic similarity, compared to previous models. We also show that some of these embeddings can be used to improve the performance of a state-of-the-art machine translation system for words outside the vocabulary of the parallel training data."
"Understanding Image and Text Simultaneously: a Dual Vision-Language Machine Comprehension Task.  The paper makes several contributions: a generic mechanism for generating decoys from (human-created) image captions; an instance of applying this mechanism, yielding a large-scale machine comprehension dataset (based on the COCO images and captions) that we make publicly available;
results on a human evaluation on this dataset, thus providing a performance ceiling; and several baseline and competitive learning approaches that illustrate the utility of the proposed framework in advancing both image and language machine comprehension. In particular, there is a large gap between human performance and state-of-the-art learning methods, suggesting a fruitful direction for future research."
"Building Large Machine Reading-Comprehension Datasets using Paragraph Vectors.  We present a dual contribution to the task of machine reading-comprehension: a technique for creating large-sized machine-comprehension (MC) datasets using paragraph-vector models; and a novel, hybrid neural-network architecture that combines the representation power of recursive neural networks with the discriminative power of fully-connected multi-layered networks. We use the MC-dataset generation technique to build a dataset of around 2 million examples, for which we empirically determine the high-ceiling of human performance (around 91\% accuracy), as well as the performance of a variety of computer models. Among all the models we have experimented with, our hybrid neural-network architecture achieves the highest performance (83.2\% accuracy). The remaining gap to the human-performance ceiling provides enough room for future model improvements."
"Classification using Predictive State Smoothing (PRESS): A scalable kernel classifier for high-dimensional features with variable selection.  In this work we adapt the predictive state smoothing (PRESS) framework to
  classification, which leads to a fully probabilistic, non-linear classifier
  that estimates the minimal sufficient statistic for predicting class
  membership probabilities. It can be used for high-dimensional problems, both
  in number of observations and covariates, and allows for variable selection
  using LASSO or Ridge penalties.  We also establish a connection between the
  metric learning aspect of PRESS kernel smoothing and an equivalent
  state-dependent neural network representation.  Out-of-sample prediction
  performance is comparable to existing state-of-the-art classifiers on several
  benchmark datasets.  Yet a trained PRESS classifier provides meaningful
  domain-specific insights based on regression coefficients using standard
  frequentist as well Bayesian inference.  Algorithms scale linearly in the
  number of observations and can be easily implemented in R, STAN, or
  TensorFlow."
"Designing for Mobile Experience Beyond the Native Ad Click: Exploring Landing Page Presentation Style &amp; Media Usage.  Many free mobile applications are supported by advertising. Ads can greatly affect user perceptions and behavior. In mobile apps, ads often follow a ??native?? format: they are designed to conform in both format and style to the actual content and context of the application. Clicking on the ad leads users to a second destination, outside of the hosting app, where the unified experience provided by native ads within the app is not necessarily reflected by the landing page the user arrives at. Little is known about whether and how this type of mobile ads is impacting user experience. In this paper, we use both quantitative and qualitative methods to study the impact of two design decisions for the landing page of a native ad on the user experience: (i) native ad style (following the style of the application) versus a non-native ad style; and (ii) pages with multimedia versus static pages. We found consider-able variability in terms of user experience with mobile ad landing pages when varying presentation style and multimedia usage, especially interaction between presence of video and ad style (native or non-native). W e also discuss insights and recommendations for improving the user experience with mobile native ads."
"Activity Recognition at Google: Building and Deploying Machine Learning for a Billion Android Devices.  Activity recognition runs on over 1 billion Android devices today. Engineers and researchers at Google face many unique challenges when developing, launching, and iterating on activity recognition algorithms when compared to academia. This talk covers what Activity Recognition at Google is, discusses the value of applying software engineering techniques and sharing infrastructure, and explores two of these challenges: scale and battery life."
"Excluding pairs of tournaments.  The Erdos-Hajnal conjecture states that for every given undirected graph H there exists a constant c(H)&gt;0 such that every graph G that does not contain H as an induced subgraph contains a clique or a stable set of size at least |V(G)|^{c(H)}. The conjecture is still open. Its equivalent directed version states that for every given tournament H there exists a constant c(H)&gt;0 such that every H-free tournament T contains a transitive subtournament of order at least |V(T)|^{c(H)}. We prove in this paper that {H1,H2}-free tournaments T contain transitive subtournaments of size at least |V(T)|^{c(H1,H2)} for some c(H1,H2)&gt;0 and several pairs of tournaments: H1, H2. In particular we prove that {H,Hc}-freeness implies existence of the polynomial-size transitive subtournaments for several tournaments H for which the conjecture is still open (Hc stands for the complement of H). To the best of our knowledge these are first nontrivial results of this type."
"Tracking Ransomware End-to-end.  Ransomware is a type of malware that encrypts the
files of infected hosts and demands payment, often in a cryptocurrency
such as bitcoin. In this paper, we create a measurement
framework that we use to perform a large-scale, two-year,
end-to-end measurement of ransomware payments, victims, and
operators. By combining an array of data sources, including
ransomware binaries, seed ransom payments, victim telemetry
from infections, and a large database of bitcoin addresses
annotated with their owners, we sketch the outlines of this
burgeoning ecosystem and associated third-party infrastructure.
In particular, we trace the financial transactions, from the
moment victims acquire bitcoins, to when ransomware operators
cash them out. We find that many ransomware operators cashed
out using BTC-e, a now-defunct bitcoin exchange. In total we
are able to track over $16 million in likely ransom payments
made by 19,750 potential victims during a two-year period. While
our study focuses on ransomware, our methods are potentially
applicable to other cybercriminal operations that have similarly
adopted bitcoin as their payment channel."
"Assessing microscope image focus quality with deep learning.  Background: Large image datasets acquired on automated microscopes typically have some fraction of low quality, out-of-focus images, despite the use of hardware autofocus systems. Identification of these images using automated image analysis with high accuracy is important for obtaining a clean, unbiased image dataset. Complicating this task is the fact that image focus quality is only well-defined in foreground regions of images, and as a result, most previous approaches only enable a computation of the relative difference in quality between two or more images, rather than an absolute measure of quality. Results: We present a deep neural network model capable of predicting an absolute measure of image focus on a single image in isolation, without any user-specified parameters. The model operates at the image-patch level, and also outputs a measure of prediction certainty, enabling interpretable predictions. The model was trained on only 384 in-focus Hoechst (nuclei) stain images of U2OS cells, which were synthetically defocused to one of 11 absolute defocus levels during training. The trained model can generalize on previously unseen real Hoechst stain images, identifying the absolute image focus to within one defocus level (approximately 3 pixel blur diameter difference) with 95% accuracy. On a simpler binary in/out-of-focus classification task, the trained model outperforms previous approaches on both Hoechst and Phalloidin (actin) stain images (F-scores of 0.89 and 0.86, respectively over 0.84 and 0.83), despite only having been presented Hoechst stain images during training. Lastly, we observe qualitatively that the model generalizes to two additional stains, Hoechst and Tubulin, of an unseen cell type (Human MCF-7) acquired on a different instrument. Conclusions: Our deep neural network enables classification of out-of-focus microscope images with both higher accuracy and greater precision than previous approaches via interpretable patch-level focus and certainty predictions. The use of synthetically defocused images precludes the need for a manually annotated training dataset. The model also generalizes to different image and cell types. The framework for model training and image prediction is available as a free software library and the pre-trained model is available for immediate use in Fiji (ImageJ) and CellProfiler."
"Identifying Exoplanets with Deep Learning: A Five-planet Resonant Chain around Kepler-80 and an Eighth Planet around Kepler-90.  NASA's Kepler Space Telescope was designed to determine the frequency of Earth-sized planets orbiting Sun-like stars, but these planets are on the very edge of the mission's detection sensitivity. Accurately determining the occurrence rate of these planets will require automatically and accurately assessing the likelihood that individual candidates are indeed planets, even at low signal-to-noise ratios. We present a method for classifying potential planet signals using deep learning, a class of machine learning algorithms that have recently become state-of-the-art in a wide variety of tasks. We train a deep convolutional neural network to predict whether a given signal is a transiting exoplanet or a false positive caused by astrophysical or instrumental phenomena. Our model is highly effective at ranking individual candidates by the likelihood that they are indeed planets: 98.8% of the time it ranks plausible planet signals higher than false-positive signals in our test set. We apply our model to a new set of candidate signals that we identified in a search of known Kepler multi-planet systems. We statistically validate two new planets that are identified with high confidence by our model. One of these planets is part of a five-planet resonant chain around Kepler-80, with an orbital period closely matching the prediction by three-body Laplace relations. The other planet orbits Kepler-90, a star that was previously known to host seven transiting planets. Our discovery of an eighth planet brings Kepler-90 into a tie with our Sun as the star known to host the most planets."
"Round Compression for Parallel Matching Algorithms.  For over a decade now we have been witnessing the success of massive parallel computation (MPC) frameworks, such as MapReduce, Hadoop, Dryad, or Spark. One of the reasons for their success is the fact that these frameworks are able to accurately capture the nature of large-scale computation. In particular, compared to the classic distributed algorithms or PRAM models, these frameworks allow for much more local computation. The fundamental question that arises in this context is though: can we leverage this additional power to obtain even faster parallel algorithms? A prominent example here is the maximum matching problem---one of the most classic graph problems.
It is well known that in the PRAM model one can compute a 2-approximate maximum matching in O(log n) rounds. However, the exact complexity of this problem in the MPC framework is still far from understood. Lattanzi et al. (SPAA 2011) showed that if each machine has n^(1+Omega(1)) memory, this problem can also be solved 2-approximately in a constant number of rounds. These techniques, as well as the approaches developed in the follow up work, seem though to get stuck in a fundamental way at roughly O(log n) rounds once we enter the (at most) near-linear memory regime. It is thus entirely possible that in this regime, which captures in particular the case of sparse graph computations, the best MPC round complexity matches what one can already get in the PRAM model, without the need to take advantage of the extra local computation power. In this paper, we finally refute that perplexing possibility. That is, we break the above O(log n) round complexity bound even in the case of slightly sublinear memory per machine. In fact, our improvement here is {\em almost exponential}: we are able to deliver a (2+epsilon)-approximation to maximum matching, for any fixed constant epsilon&gt;0, in O((log log n)^2) rounds. To establish our result we need to deviate from the previous work in two important ways that are crucial for exploiting the power of the MPC model, as compared to the PRAM model. Firstly, we use vertex--based graph partitioning, instead of the edge--based approaches that were utilized so far. Secondly, we develop a technique of round compression. This technique enables one to take a (distributed) algorithm that computes an O(1)-approximation of maximum matching in O(log n) independent PRAM phases and implement a super-constant number of these phases in only a constant number of MPC rounds."
"Realistic Evaluation of Semi-Supervised Learning Algorithms.  Semi-supervised learning (SSL) provides a powerful framework for leveraging unlabeled data when labels are limited or expensive to obtain. Approaches based on deep neural networks have recently proven successful on standard benchmark tasks. However, we argue that these benchmarks do not reflect real-world requirements and are compared to weak baselines. We propose a set of new benchmarks and find that simple baselines that were previously underappreciated outperform more complicated research ideas that were previously regarded as state of the art. Using our new benchmarking procedures, we additionally find that SSL methods are highly sensitive to the amount of unlabeled data and the class distribution of the data. We encourage researchers studying SSL to adopt our improved methodology, and suggest readers and reviewers of SSL papers to familiarize
themselves with the experimental design concerns we identify."
"Learning Latent Representations of Music to Generate Interactive Musical Palettes.  Advances in machine learning have the potential to radically
reshape interactions between humans and computers. Deep
learning makes it possible to discover powerful representations
that are capable of capturing the latent structure of highdimensional
data such as music. By creating interactive latent
space ??palettes?? of musical sequences and timbres, we
demonstrate interfaces for musical creation made possible
by machine learning. We introduce an interface to the intuitive,
low-dimensional control spaces for high-dimensional
note sequences, allowing users to explore a compositional
space of melodies or drum beats in a simple 2-D grid. Furthermore,
users can define 1-D trajectories in the 2-D space
for autonomous, continuous morphing during improvisation.
Similarly for timbre, our interface to a learned latent space
of audio provides an intuitive and smooth search space for
morphing between the timbres of different instruments. We
remove technical and computational barriers by embedding
pre-trained networks into a browser-based GPU-accelerated
framework, making the systems accessible to a wide range of
users while maintaining potential for creative flexibility and
personalization."
"On the Ability of Mobile Sensor Networks to Diffuse Information.  We examine the ability of networks formed by mobile sensor nodes to diffuse information in the case when communication is only possible during opportunistic encounters. Our setting assumes that mobile nodes are continuously sensing the world and acquiring new information. We form an abstract model of this situation and show by theoretical analysis, simulation, and real mobility data that the diffusion of information in this setting cannot be as efficient as when we allow arbitrary contact patterns between the nodes with the same overall contact statistics. This establishes a fundamental asymptotic limitation on the information diffusion capacity of such opportunistic mobile sensor networks --- the encounter patterns arising out of physical motions in a geometric space are not ideal for information diffusion."
"Gradient Descent Quantizes ReLU Network Features.  Deep neural networks are often trained in the over-parametrized regime (i.e. with far more parameters than training examples), and understanding why the training converges to solutions that generalize remains an open problem. Several studies have highlighted the fact that the training procedure, i.e. mini-batch Stochastic Gradient Descent (SGD) leads to solutions that have specific properties in the loss landscape. However, even with plain Gradient Descent (GD) the solutions found in the over-parametrized regime are pretty good and this phenomenon is poorly understood. 
We propose an analysis of this behavior for feedforward networks with a ReLU activation function under the assumption of small initialization and learning rate and uncover a quantization effect: The weight vectors tend to concentrate at a small number of directions determined by the input data. As a consequence, we show that for given input data there are only finitely many, ""simple"" functions that can be obtained, independent of the network size. This puts these functions in analogy to linear interpolations (for given input data there are finitely many triangulations, which each determine a function by linear interpolation). We ask whether this analogy extends to the generalization properties - while the usual distribution-independent generalization property does not hold, it could be that for e.g. smooth functions with bounded second derivative an approximation property holds which could ""explain"" generalization of networks (of unbounded size) to unseen inputs."
"Improving homograph disambiguation with supervised machine learning.  We describe a pre-existing rule-based homograph disambiguation system used for text-to-speech synthesis at Google, and compare it to a novel system which performs disambiguation using classifiers trained on a small amount of labeled data. An evaluation of these systems, using a new, freely available English data set, finds that hybrid systems (making use of both rules and machine learning) are significantly more accurate than either hand-written rules or machine learning alone. The evaluation also finds minimal performance degradation when the hybrid system is configured to run on limited-resource mobile devices rather than on production servers. The two best systems described here are used for homograph disambiguation on all US English text-to-speech traffic at Google."
"Memory Tagging and how it improves C/C++ memory safety.  Memory safety in C and C++ remains largely unresolved. 
A technique usually called ??memory tagging?? may dramatically improve the situation if implemented in hardware with reasonable overhead. This paper describes two existing implementations of memory tagging: one is the full hardware implementation in SPARC; the other is a partially hardware-assisted compiler-based tool for AArch64. We describe the basic idea, evaluate the two implementations, and explain how they improve memory safety.
This paper is intended to initiate a wider discussion of memory tagging and to motivate the CPU and OS vendors to add support for it in the near future."
"TensorFlow-Serving: Flexible, High-Performance ML Serving.  We describe TensorFlow-Serving, a system to serve machine learning models inside
Google which is also available in the cloud and via open-source. It is extremely
flexible in terms of the types of ML platforms it supports, and ways to
integrate with systems that convey new models and updated versions from training
to serving. At the same time, the core code paths around model lookup and
inference have been carefully optimized to avoid performance pitfalls observed
in naive implementations. The paper covers the architecture of the extensible serving library, as well as
the distributed system for multi-tenant model hosting. Along the way it points
out which extensibility points and performance optimizations turned out to be
especially important based on production experience."
"Grader variability and the importance of reference standards for evaluating machine learning models for diabetic retinopathy.  Purpose
Use adjudication to quantify errors in diabetic retinopathy (DR) grading based on individual graders and majority decision, and to train an improved automated algorithm for DR grading. Design
Retrospective analysis. Participants
Retinal fundus images from DR screening programs. Methods
Images were each graded by the algorithm, U.S. board-certified ophthalmologists, and retinal specialists. The adjudicated consensus of the retinal specialists served as the reference standard. Main Outcome Measures
For agreement between different graders as well as between the graders and the algorithm, we measured the (quadratic-weighted) kappa score. To compare the performance of different forms of manual grading and the algorithm for various DR severity cutoffs (e.g., mild or worse DR, moderate or worse DR), we measured area under the curve (AUC), sensitivity, and specificity. Results
Of the 193 discrepancies between adjudication by retinal specialists and majority decision of ophthalmologists, the most common were missing microaneurysm (MAs) (36%), artifacts (20%), and misclassified hemorrhages (16%). Relative to the reference standard, the kappa for individual retinal specialists, ophthalmologists, and algorithm ranged from 0.82 to 0.91, 0.80 to 0.84, and 0.84, respectively. For moderate or worse DR, the majority decision of ophthalmologists had a sensitivity of 0.838 and specificity of 0.981. The algorithm had a sensitivity of 0.971, specificity of 0.923, and AUC of 0.986. For mild or worse DR, the algorithm had a sensitivity of 0.970, specificity of 0.917, and AUC of 0.986. By using a small number of adjudicated consensus grades as a tuning dataset and higher-resolution images as input, the algorithm improved in AUC from 0.934 to 0.986 for moderate or worse DR. Conclusions
Adjudication reduces the errors in DR grading. A small set of adjudicated DR grades allows substantial improvements in algorithm performance. The resulting algorithm's performance was on par with that of individual U.S. Board-Certified ophthalmologists and retinal specialists."
"Partitioning Orders in Online Shopping Services.  The rapid growth of the sharing economy has led to the widespread use of newer and richer models of online shopping and delivery services. The race to deliver fast has transformed such services into complex networks of shoppers,
stores, and consumers. Needless to say, the efficiency of the store order management is critical to the business. Motivated by this setting, we consider the following problem: given a set of online shopping orders each consisting of a few items, how to best partition the orders among a given number of pickers? Owing to logistical constraints the orders are typically unsplittable in the partition. This partitioning, taking the physical location of the items in the store , has to optimize the utilization and amount of work done by the shoppers in the store. Formulating this as a combinatorial optimization problem, we propose a family of simple and efficient algorithms that admit natural constraints arising in this setting. In addition to showing provable guarantees for the algorithms, we also demonstrate their efficiency in practice on real-world data from Google Express [1], outperforming natural baselines."
"Caching with Dual Costs.  Caching mechanisms in distributed and social settings face the issue that the items can frequently change, requiring the cached ver- sions to be updated to maintain coherence. There is thus a trade-off between incurring cache misses on read requests and cache hits on update requests. Motivated by this we consider the following dual cost variant of the classical caching problem: each request for an item can be either a read or a write. If the request is read and the item is not in the cache, then a read-miss cost is incurred and if the request is write and the item is in the cache, then a write-hit cost is incurred. The goal is to design a caching algorithm that minimizes the sum of read-miss and write-hit costs. We study online and offline algorithms for this problem.
For the online version of the problem, we obtain an efficient algorithm whose cost is provably close to near-optimal cost. This algorithm builds on online algorithms for classical caching and metrical task systems, using them as black boxes. For the offline ver- sion, we obtain an optimal deterministic algorithm that is based on a minimum cost flow. Experiments on real and synthetic data show that our online algorithm incurs much less cost compared to natural baselines, while utilizing cache even better; furthermore, they also show that the online algorithm is close to the offline optimum."
"Universal Sentence Encoder.  We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on TF Hub."
"Hierarchical Variational Autoencoders for Music.  In this work we develop recurrent variational autoencoders (VAEs) trained to
reproduce short musical sequences and demonstrate their use as a creative device
both via random sampling and data interpolation. Furthermore, by using a novel
hierarchical decoder, we show that we are able to model long sequences with
musical structure for both individual instruments and a three-piece band (lead, bass,
and drums). Finally, we demonstrate the effectiveness of scheduled sampling in
significantly improving our reconstruction accuracy."
"A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music.  The Variational Autoencoder (VAE) has proven to be an effective model for producing semantically
meaningful latent representations for natural data. However, it has thus far seen limited application to
sequential data, and, as we demonstrate, existing recurrent VAE models have difficulty modeling sequences with long-term structure. To address this issue, we propose the use of a hierarchical
decoder, which first outputs embeddings for subsequences of the input and then uses these embeddings
to generate each subsequence independently. This structure encourages the model to
utilize its latent code, thereby avoiding the ??posterior collapse?? problem which remains an issue
for recurrent VAEs. We apply this architecture to modeling sequences of musical notes and find
that it exhibits dramatically better sampling, interpolation, and reconstruction performance than a
??flat?? baseline model. An implementation of our ??MusicVAE?? is available online."
"Onsets and Frames: Dual-Objective Piano Transcription.  We consider the problem of transcribing polyphonic piano music with an emphasis on generalizing to unseen instruments. We use deep neural networks and propose a novel approach that predicts onsets and frames using both CNNs and LSTMs. This model predicts pitch onset events and then uses those predictions to condition framewise pitch predictions. During inference, we restrict the predictions from the framewise detector by not allowing a new note to start unless the onset detector also agrees that an onset for that pitch is present in the frame. We focus on improving onsets and offsets together instead of either in isolation as we believe it correlates better with human musical perception. This technique results in over a 100% relative improvement in note with offset score on the MAPS dataset."
"Targeting and Signaling in Ad Auctions.  Modern ad auctions allow advertisers to target more specific segments of the user population. Unfortunately,
this is not always in the best interest of the ad platform ?? partially hiding some information
could be more beneficial for the platform??s revenue. In this paper, we examine the following basic question
in the context of second-price ad auctions: how should an ad platform optimally reveal information
about the ad opportunity to the advertisers in order to maximize revenue? We consider a model in which
bidders?? valuations depend on a random state of the ad opportunity. Different from previous work, we
focus on a more practical, and challenging, situation where the space of possible realizations of ad opportunities
is extremely large. We thus focus on developing algorithms whose running time is polynomial
in the number of bidders, but is independent of the number of ad opportunity realizations.
We assume that the auctioneer can commit to a signaling scheme to reveal noisy information about
the realized state of the ad opportunity, and examine the auctioneer??s algorithmic question of designing
the optimal signaling scheme. We first consider that the auctioneer is restricted to send a public
signal to all bidders. As a warm-up, we start with a basic (though less realistic) setting in which the
auctioneer knows the bidders?? valuations, and show that an -optimal scheme can be implemented in
time polynomial in the number of bidders and 1/. We then move to a well-motivated Bayesian valuation
setting in which the auctioneer and bidders both have private information, and present two results.
First, we exhibit a characterization result regarding approximately optimal schemes and prove that any
constant-approximate public signaling scheme must use exponentially many signals. Second, we present
a ??simple?? public signaling scheme that serves as a constant approximation under mild assumptions.
Finally, we initiate an exploration on the power of being able to send different signals privately to
different bidders. In the basic setting where the auctioneer knows bidders?? valuations, we exhibit a
polynomial-time private scheme that extracts almost full surplus even in the worst Bayes Nash equilibrium.
This illustrates the surprising power of private signaling schemes in extracting revenue."
"Mixed-Initiative Generation of Multi-Channel Sequential Structures.  Modern ad auctions allow advertisers to target more specific segments of the user population. Unfortunately,
this is not always in the best interest of the ad platform ?? partially hiding some information
could be more beneficial for the platform??s revenue. In this paper, we examine the following basic question
in the context of second-price ad auctions: how should an ad platform optimally reveal information
about the ad opportunity to the advertisers in order to maximize revenue? We consider a model in which
bidders?? valuations depend on a random state of the ad opportunity. Different from previous work, we
focus on a more practical, and challenging, situation where the space of possible realizations of ad opportunities
is extremely large. We thus focus on developing algorithms whose running time is polynomial
in the number of bidders, but is independent of the number of ad opportunity realizations.
We assume that the auctioneer can commit to a signaling scheme to reveal noisy information about
the realized state of the ad opportunity, and examine the auctioneer??s algorithmic question of designing
the optimal signaling scheme. We first consider that the auctioneer is restricted to send a public
signal to all bidders. As a warm-up, we start with a basic (though less realistic) setting in which the
auctioneer knows the bidders?? valuations, and show that an -optimal scheme can be implemented in
time polynomial in the number of bidders and 1/. We then move to a well-motivated Bayesian valuation
setting in which the auctioneer and bidders both have private information, and present two results.
First, we exhibit a characterization result regarding approximately optimal schemes and prove that any
constant-approximate public signaling scheme must use exponentially many signals. Second, we present
a ??simple?? public signaling scheme that serves as a constant approximation under mild assumptions.
Finally, we initiate an exploration on the power of being able to send different signals privately to
different bidders. In the basic setting where the auctioneer knows bidders?? valuations, we exhibit a
polynomial-time private scheme that extracts almost full surplus even in the worst Bayes Nash equilibrium.
This illustrates the surprising power of private signaling schemes in extracting revenue."
"Recommendations for all : solving thousands of recommendation problems a day.  Recommendations are known to be an important part of several online experiences. Outside of media recommendation (music, movies, etc), online retailers have made use of product recommendations to help users make purchases. Product recommendation tends to be really hard because of the twin problems of sparsity and cold-start. Building a recommendation system that performs well in this setting is hard and is generally considered to need some expert tuning. However, all online retailers need to solve this problem well to provide good recommendations. In this paper, we tackle this problem and describe an industrial-scale system called Sigmund where we solve tens of thousands of instances of the recommendation problem as a service for various online retailers. for customers. Sigmund was deployed to production in early 2014 and has been serving thousands of retailers. We describe several design decisions that we made in building Sigmund. We also share some of the lessons we learned from this experience ??both from a machine learning perspective and a systems perspective. We hope that these lessons are useful for building future machine-learning services."
"The iNaturalist Species Classification and Detection Dataset.  Existing image classification datasets used in computer
vision tend to have a uniform distribution of images across
object categories. In contrast, the natural world is heavily
imbalanced, as some species are more abundant and easier
to photograph than others. To encourage further progress in
challenging real world conditions we present the iNaturalist
species classification and detection dataset, consisting of
859,000 images from over 5,000 different species of plants
and animals. It features visually similar species, captured
in a wide variety of situations, from all over the world. Images
were collected with different camera types, have varying
image quality, feature a large class imbalance, and have
been verified by multiple citizen scientists. We discuss the
collection of the dataset and present extensive baseline experiments
using state-of-the-art computer vision classification
and detection models. Results show that current nonensemble
based methods achieve only 67% top one classification
accuracy, illustrating the difficulty of the dataset.
Specifically, we observe poor results for classes with small
numbers of training examples suggesting more attention is
needed in low-shot learning."
"Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning.  Transferring the knowledge learned from large scale
datasets (e.g., ImageNet) via fine-tuning offers an effective
solution for domain-specific fine-grained visual categorization
(FGVC) tasks (e.g., recognizing bird species or car
make &amp; model). In such scenarios, data annotation often
calls for specialized domain knowledge and thus difficult to
scale. In this work, we first tackle a problem in large scale
FGVC. Our method won first place in iNaturalist 2017 large
scale species classification challenge. Central to the success
of our approach is a training scheme that uses higher
image resolution and deals with the long-tailed distribution
of training data. Next, we study transfer learning via
fine-tuning from large scale datasets to small scale, domainspecific
FGVC datasets. We propose a measure to estimate
domain similarity via Earth Mover??s Distance and demonstrate
that transfer learning benefits from pre-training on a
source domain that is similar to the target domain by this
measure. Our proposed transfer learning outperforms ImageNet
pre-training and obtains state-of-the-art results on
multiple commonly used FGVC datasets."
"Recent Books and Journals Articles in Public Opinion, Survey Methods, Survey Statistics, Big Data and User Experience Research. 2017 Update.  Welcome to the 9th edition of this column on recent books and journal articles in the field of public opinion, survey methods, survey statistics, Big Data, and user experience research. This article is an update of the 2016 article. Like the previous year, the books are organized by topic; this should help the readers to focus on their interests. Given that the last update listed books available as of August 2016, I added a few books, papers, and special issues that came out in late 2016, so there is no gap. This year I added a new section on user experience research. User experience research is a growing field with many applications to desktop and mobile platforms. Given almost all data collection methods in survey research rely heavily on technology, the learnings from the user experience field can be very beneficial to the survey researcher and practitioner. You will also note that I use very broad definitions of public opinion, survey methods, survey statistics, Big Data, and user experience research. This is because there are many books published in different outlets that can very useful to the readers of Survey Practice, even if they do not come from traditional sources of survey content. It is unlikely I have exhaustively listed all new books in each subcategory; I did my best scouting different resources and Websites, but I take full responsibility for any omission. The list is also focused only on books published in the English language and available for purchase (as an ebook or in print) at the time of this review (January 2018) and with copyright year as either 2016 or 2017. Books are listed based on the relevance to the topic, and no judgment is made in terms of quality of the content. We let the readers do so."
"Learning via social awareness: improving sketch representations with facial feedback.  In the quest towards general artificial intelligence (AI), researchers have explored developing loss functions that act as intrinsic motivators in the absence of external rewards. This paper argues that such research has overlooked an important and useful intrinsic motivator: social interaction. We posit that making an AI agent aware of implicit social feedback from humans can allow for faster learning of more generalizable and useful representations, and could potentially impact AI safety. We collect social feedback in the form of facial expression reactions to samples from Sketch RNN, an LSTM-based variational autoencoder (VAE) designed to produce sketch drawings. We use a Latent Constraints GAN (LC-GAN) to learn from the facial feedback of a small group of viewers, and then show in an independent evaluation with 76 users that this model produced sketches that lead to significantly more positive facial expressions. Thus, we establish that implicit social feedback can improve the output of a deep learning model."
"World Models.  We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment."
The Building Blocks of Interpretability.  Interpretability techniques are normally studied in isolation.  We explore the powerful interfaces that arise when you combine them -- and the rich structure of this combinatorial space.
"Q-LDA: Uncovering Latent Patterns in Text-based Sequential Decision Processes.  In sequential decision making, it is often important and useful for end users to understand the underlying patterns or causes that lead to the corresponding decisions.  However, typical deep reinforcement learning algorithms seldom provide such information due to their black-box nature. In this paper, we present a probabilistic model, Q-LDA, to uncover latent patterns in text-based sequential decision processes. The model can be understood as a variant of latent topic models that are tailored to maximize total rewards; we further draw an interesting connection between an approximate maximum-likelihood estimation of Q-LDA and the celebrated Q-learning algorithm. We demonstrate in the text-game domain that our proposed method not only provides a viable mechanism to uncover latent patterns in decision processes, but also obtains state-of-the-art rewards in these games."
"History-Independent Distributed Multi-agent Learning.  How should we evaluate a rumor? We address this question in a setting where multiple agents seek an estimate of the probability, b, of some future binary event. A common uniform prior on b is assumed. A rumor about b meanders through the network, evolving over time. The rumor evolves, not because of ill will or noise, but because agents incorporate private signals about b before passing on the (modified) rumor. The loss to an agent is the (realized) square error of her opinion. Our setting introduces strategic behavior based on evidence regarding an exogenous event to current models of rumor/influence propagation in social networks. We study a simple Exponential Moving Average (EMA) for combining experience evidence and trusted advice (rumor), quantifying its resulting performance and comparing it to the optimal achievable using Bayes posterior having access to the agents private signals. We study the quality of p_T, the prediction of the last agent along a chain of T rumor-mongering agents. The prediction p_T can be viewed as an aggregate estimator of b that depends on the private signals of T agents."
"Robust Domain Adaptation.  We derive a generalization bound for domain adaptation by using the properties of robust algorithms. Our new bound depends on ??-shift, a measure of prior knowledge regarding the similarity of source and target domain distributions. Based on the generalization bound, we design SVM variants for binary classification and regression domain adaptation algorithms."
"Barren Plateaus in Quantum Neural Network Training Landscapes.  Many experimental proposals for noisy intermediate scale quantum devices involve training a parameterized quantum circuit with a classical optimization loop. Such hybrid quantum-classical algorithms are popular for applications in quantum simulation, optimization, and machine learning. Due to its simplicity and hardware efficiency, random circuits are often proposed as initial guesses for exploring the space of quantum states. We show that the exponential dimension of Hilbert space and the gradient estimation complexity make this choice unsuitable for hybrid quantum-classical algorithms run on more than a few qubits. Specifically, we show that for a wide class of reasonable parameterized quantum circuits, the probability that the gradient along any reasonable direction is non-zero to some fixed precision is exponentially small as a function of the number of qubits. We argue that this is related to the 2-design characteristic of random circuits, and that solutions to this problem must be studied."
"Quantum Chemistry Calculations on a Trapped-Ion Quantum Simulator.  Quantum-classical hybrid algorithms are emerging as promising candidates for near-term practical applications of quantum information processors in a wide variety of fields ranging from chemistry to physics and materials science. We report on the experimental implementation of such an algorithm to solve a quantum chemistry problem, using a digital quantum simulator based on trapped ions. Specifically, we implement the variational quantum eigensolver algorithm to calculate the molecular ground state energies of two simple molecules and experimentally demonstrate and compare different encoding methods using up to four qubits. Furthermore, we discuss the impact of measurement noise as well as mitigation strategies and indicate the potential for adaptive implementations focused on reaching chemical accuracy, which may serve as a cross-platform benchmark for multi-qubit quantum simulators."
"Image Transformer.  Recent work demonstrated significant progress towards modeling the distribution of natural images with tractable likelihood using deep neural networks. This was achieved by modeling the joint distribution of pixels in the image as the product of conditional distributions, thereby turning it into a sequence modeling problem, and applying recurrent or convolutional neural networks to it. In this work we instead build on the Transformer, a recently proposed network architecture based on self-attention, to model the conditional distributions in similar factorizations. We present two extensions of the network architecture, allowing it to scale to images and to take advantage of their two-dimensional structure. While conceptually simple, our generative models trained on two image data sets are competitive with or outperform the current state of the art on two different data sets, CIFAR-10 and ImageNet, as measured by log-likelihood. We also present results on image super-resolution with large magnification ratio with an encoder-decoder configuration of our architecture. In a human evaluation study, we show that our super-resolution models improve over previously published autoregressive super-resolution models in how often they fool a naive human observer by a factor of three. Lastly, we provide examples of images generated or completed by our various models which, following previous work, we also believe to look pretty cool."
"Fast Decoding in Sequence Models Using Discrete Latent Variables.  Auto-regressive sequence models based on deep neural networks, such as
RNNs, Wavenet and Transformer are the state of the art on many tasks.
However, they lack parallelism and are thus slow for long sequences.
RNNs lack parallelism both during training and decoding, while 
architectures like WaveNet and Transformer are much more parallel
during training, but still lack parallelism during decoding. We present a method to extend sequence models using 
discrete latent variables that makes decoding much more parallel. 
The main idea behind this approach is to first autoencode the 
target sequence into a shorter discrete latent sequence,
which is generated auto-regressively,
and finally decode the full sequence from this shorter 
latent sequence in a parallel manner.
We verify that our method works on the task of neural machine
translation,  where our models are an order of magnitude faster than comparable
auto-regressive models.  We also introduce a new method for constructing discrete
latent variables that allows us to obtain good BLEU scores."
"Discrete Autoencoders for Sequence Models.  The contributions of this paper are:
* A discretization technique that works well without any extra losses or parameters to tune.
 * A way to measure performance of autoencoders for sequence models (and baselines).
 * An improved way to sample from sequence models (trained with an autoencoder part)."
"Xarray: N-D labeled arrays and datasets in Python.  Xarray is an open source project and Python package that provides data structures for N-dimensional labeled arrays inspired by Pandas. It provides a Pandas-like and Pandas-compatible toolkit for analytics on multi-dimensional arrays, rather than the tabular data format for which Pandas excels. Our approach adopts the Common Data Model for self-describing scientific data that is widely used in the geo-science community. Xarray builds on top of and seamlessly interoperates with the core scientific Python packages, such as NumPy, SciPy, Matplotlib, and Pandas."
"A Session Auction for Mobile App Advertising.  Xarray is an open source project and Python package that provides data structures for N-dimensional labeled arrays inspired by Pandas. It provides a Pandas-like and Pandas-compatible toolkit for analytics on multi-dimensional arrays, rather than the tabular data format for which Pandas excels. Our approach adopts the Common Data Model for self-describing scientific data that is widely used in the geo-science community. Xarray builds on top of and seamlessly interoperates with the core scientific Python packages, such as NumPy, SciPy, Matplotlib, and Pandas."
"Sim2Real Viewpoint Invariant Visual Servoing by Recurrent Control.  Humans are remarkably proficient at controlling their limbs and tools from a wide range of viewpoints. In robotics, this ability is referred to as visual servoing: moving a tool or end-point to a desired location using primarily visual feedback. In this paper, we propose learning viewpoint invariant visual servoing skills in a robot manipulation task. We train a deep recurrent controller that can automatically determine which actions move the end-effector of a robotic arm to a desired object. This problem is fundamentally ambiguous: under severe variation in viewpoint, it may be impossible to determine the actions in a single feedforward operation. Instead, our visual servoing approach uses its memory of past movements to understand how the actions affect the robot motion from the current viewpoint, correcting mistakes and gradually moving closer to the target. This ability is in stark contrast to previous visual servoing methods, which assume known dynamics or require a calibration phase. We learn our recurrent controller using simulated data, synthetic demonstrations and reinforcement learning. We then describe how the resulting model can be transferred to a real-world robot by disentangling perception from control and only adapting the visual layers. The adapted model can servo to previously unseen objects from novel viewpoints on a real-world Kuka IIWA robotic arm. For supplementary videos, see: \href{https://www.youtube.com/watch?v=oLgM2Bnb7fo}{https://www.youtube.com/watch?v=oLgM2Bnb7fo}"
"Factorized Recurrent Neural Architectures for Long Range Dependence.  The ability to capture Long Range Dependence (LRD) in a stochastic process is of prime importance in the context of predictive models. A sequential model with a longer-term memory is able to better contextualize recent observations. In this article, we apply the theory of LRD stochastic processes to modern recurrent architectures such as LSTM and GRU and prove they do not provide LRD behavior under homoscedasticity assumptions. After having proven that leaky gating mechanisms lead to memory loss in gated recurrent networks such as LSTMs and GRUs we provide an architecture that attempts at addressing the issue of faulty memory. The key insight of our theoretical study is to encourage memory redundancy. We show how the resulting architectures are more lightweight, parallelizable and able to leverage old observations. Experimental results on a synthetic copy task, the Youtube-8m video classification task and a latency sensitive recommender system show that our approach leads to better memorization"
"Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis.  In this work,  we propose ??global style tokens??(GSTs),  a bank of embeddings that are jointly trained within Tacotron,  a state-of-the-art end-to-end speech synthesis system.  The embeddings are trained in a completely unsupervised manner, and yet learn to model a large range of acoustic expressiveness. GSTs lead to a rich set of surprising results.  The soft interpretable ??labels?? they generate can be used to control synthesis in novel ways, such as varying speed and modifying speak-ing style ?? independently of the text content. The labels can also be used for style transfer, replicating the speaking style of one ??seed?? phrase across an entire long-form text corpus. Perhaps most surprisingly, when trained on noisy, unlabelled found data, GSTs learn to factorize noise and speaker identity, providing a path towards highly scaleable but robust speech synthesis."
"Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron.  We present an extension to the Tacotron speech synthesis architecture that learns a latent embedding space of prosody, derived from a reference acoustic  representation  containing  the  desired prosody.   We  show  that  conditioning  Tacotron on this learned embedding space results in synthesized audio that matches the reference signal??s prosody with fine time detail. We define several quantitative and subjective metrics for evaluating prosody transfer, and report results and audio samples  from  a  single-speaker  and  44-speaker Tacotron model on a prosody transfer task."
"Local Explanation Methods for Deep Neural Networks Lack Sensitivity to Parameter Values.  Explaining the output of a complicated machine learning model like a deep neural network (DNN) is a central challenge in machine learning.  Increasingly, explanations are required for debugging models, building trust prior to model deployment, and potentially identifying unwanted effects like model bias.
Several methods have been proposed to address this issue. Local explanation methods provide explanations of the output of a model on a single input. Given the importance of these explanations to the use and deployment of these models, we ask: can we trust local explanations for DNNs created using current methods? In particular, we seek to assess how specific local explanations are to the parameter values of DNNs.
We compare explanations generated using a fully trained DNNs to explanations of DNNs with some or all parameters replaced by random values. Somewhat surprisingly, we find that, for several local explanation methods, explanations derived from networks with randomized weights and trained weights are both visually and quantitatively similar; in some cases, virtually indistinguishable. By randomizing different portions of the network, we find that local explanations are significantly reliant on lower level features of the DNN."
"The Mirage of Action-Dependent Baselines in Reinforcement Learning.  Model-free reinforcement learning with flexible function approximators has shown recent success for solving goal-directed sequential decision-making problems. Policy gradient methods are a promising class of model-free algorithms, but they have high variance, which necessitates large batches resulting in low sample efficiency. Typically, a state-dependent control variate is used to reduce variance. Recently, several papers have introduced the idea of state and action-dependent control variates and showed that they significantly reduce variance and improve sample efficiency on continuous control tasks. We theoretically and numerically evaluate biases and variances of these policy gradient methods, and show that action-dependent control variates do not appreciably reduce variance in the tested domains. We show that seemingly insignificant implementation details enable these prior methods to achieve good empirical improvements, but at the cost of introducing further bias to the gradient. Our analysis indicates that biased methods tend to improve the performance significantly more than unbiased ones."
"Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs.  Deep learning models are often successfully
trained using gradient descent, despite the worst
case hardness of the underlying non-convex optimization
problem. The key question is then under
what conditions can one prove that optimization
will succeed. Here we provide a strong result
of this kind. We consider a neural net with
one hidden layer and a convolutional structure
with no overlap, and a ReLU activation function.
For this architecture we show that learning
is NP-complete in the general case, but that
when the input distribution is Gaussian, gradient
descent converges to the global optimum in polynomial
time. To the best of our knowledge, this
is the first global optimality guarantee of gradient
descent on a convolutional neural network with
ReLU activations"
"Collaborative Memory Network for Recommendation Systems.  Recommendation systems play a vital role to keep users engaged
with personalized content in modern online platforms. Deep learning
has revolutionized many research fields and collaborative filtering
(CF) is no different. However, existing methods compose deep
learning architectures with the latent factor model ignoring a major
class of CF models, neighborhood or memory-based approaches.
We propose Collaborative Memory Networks (CMN), a deep architecture
to unify the two classes of CF models capitalizing on the
strengths of the global structure of latent factor model and local
neighborhood-based structure in a nonlinear fashion. Motivated by
the success of Memory Networks, we fuse a memory component
and neural attention mechanism as the neighborhood component.
The associative addressing scheme with the user and item memories
in the memory module encodes complex user-item relations
coupled with the neural attention mechanism to learn a user-item
specific neighborhood. Finally, the output module jointly exploits
the neighborhood with the user and item memories to produce the
final ranking score. Stacking multiple memory modules together
yielding deeper architectures capturing additional complex useritem
relations. Furthermore, we show strong connections between
CMN components and two classes of CF models. Comprehensive
experimental results demonstrate the effectiveness of CMN on two
public datasets outperforming competitive baselines. Qualitative visualization
of the attention weights provide insight into the model??s
recommendation process and suggest the presence of higher order
interactions."
"Building a Conversational Agent Overnight with Dialogue Self-Play.  We propose Machines Talking To Machines (M2M), a framework combining automation and crowdsourcing to rapidly bootstrap end-to-end dialogue agents for goal-oriented dialogues in arbitrary domains. M2M scales to new tasks with just a task schema and an API client from the dialogue system developer, but it is also customizable to cater to task-specific interactions. Compared to the Wizard-of-Oz approach for data collection, M2M achieves greater diversity and coverage of salient dialogue flows while maintaining the naturalness of individual utterances. In the first phase, a simulated user bot and a domain-agnostic system bot converse to exhaustively generate dialogue ""outlines"", i.e. sequences of template utterances and their semantic parses. In the second phase, crowd workers provide contextual rewrites of the dialogues to make the utterances more natural while preserving their meaning. The entire process can finish within a few hours. We propose a new corpus of 3,000 dialogues spanning 2 domains collected with M2M, and present comparisons with popular dialogue datasets on the quality and diversity of the surface forms and dialogue flows."
"Deep learning of genomic variation and regulatory network data.  The human genome is now investigated through high-throughput functional assays, and through the generation of population genomic data. These advances support the identification of functional genetic variants and the prediction of traits (e.g. deleterious variants and disease). This review summarizes lessons learned from the large-scale analyses of genome and exome data sets, modeling of population data and machine-learning strategies to solve complex genomic sequence regions. The review also portrays the rapid adoption of artificial intelligence/deep neural networks in genomics; in particular, deep learning approaches are well suited to model the complex dependencies in the regulatory landscape of the genome, and to provide predictors for genetic variant calling and interpretation."
"Bias Correction For Paid Search In Media Mix Modeling.  Evaluating the return on ad spend (ROAS), the causal eect of advertising
on sales, is critical to advertisers for understanding the performance of their
existing marketing strategy as well as how to improve and optimize it. Media
Mix Modeling (MMM) has been used as a convenient analytical tool to address
the problem using observational data. However it is well recognized that MMM
suers from various fundamental challenges: data collection, model specication
and selection bias due to ad targeting, among others (Chan &amp; Perry 2017; Wolfe
2016). In this paper, we study the challenge associated with measuring the impact
of search ads in MMM, namely the selection bias due to ad targeting. Using
causal diagrams of the search ad environment, we derive a statistically principled
method for bias correction based on the back-door criterion (Pearl 2013).
We use case studies to show that the method provides promising results by
comparison with results from randomized experiments. We also report a more
complex case study where the advertiser had spent on more than a dozen media
channels but results from a randomized experiment are not available. Both our
theory and empirical studies suggest that in some common, practical scenarios,
one may be able to obtain an approximately unbiased estimate of search ad
ROAS."
"VidCrit: Video-based asynchronous video review.  Evaluating the return on ad spend (ROAS), the causal eect of advertising
on sales, is critical to advertisers for understanding the performance of their
existing marketing strategy as well as how to improve and optimize it. Media
Mix Modeling (MMM) has been used as a convenient analytical tool to address
the problem using observational data. However it is well recognized that MMM
suers from various fundamental challenges: data collection, model specication
and selection bias due to ad targeting, among others (Chan &amp; Perry 2017; Wolfe
2016). In this paper, we study the challenge associated with measuring the impact
of search ads in MMM, namely the selection bias due to ad targeting. Using
causal diagrams of the search ad environment, we derive a statistically principled
method for bias correction based on the back-door criterion (Pearl 2013).
We use case studies to show that the method provides promising results by
comparison with results from randomized experiments. We also report a more
complex case study where the advertiser had spent on more than a dozen media
channels but results from a randomized experiment are not available. Both our
theory and empirical studies suggest that in some common, practical scenarios,
one may be able to obtain an approximately unbiased estimate of search ad
ROAS."
"Doppio: Tracking UI Flows and Code Changes for App Development.  Developing interactive systems often involves a large set of callback functions for handling user interaction, which makes it challenging to manage UI behaviors, create descriptive documentation, and track revisions. We developed Doppio, a tool that automatically tracks and visualizes UI flows and their changes based on source code elements and their revisions. For each input event listener of a widget, e.g., onClick of an Android View class, Doppio captures and associates its UI output from an execution of the program with its code snippet from the source code. It automatically generates a screenflow diagram that is organized by the callback methods and interaction flow, where developers can review the code and UI revisions interactively. Doppio, implemented as an IDE plugin, is seamlessly integrated into a common development workflow. Our experiments show that Doppio was able to generate quality visual documentation and helped participants understand unfamiliar source code and track changes."
"A Hybrid Convolutional Variational Autoencoder for Text Generation.  In this paper we explore the effect of architectural choices on Variational Autoencoder models for text.
In contrast to the previously introduced VAE model for text where both the encoder and decoder are RNNs, we propose a novel hybrid architecture that blends a fully feed-forward convolutional and deconvolutional component with a recurrent language model. This architecture exhibits several attractive properties such as fast run time, ability to better handle long sequences and, more importantly, we demonstrate that our model helps to avoid some of the major difficulties posed by training VAE models on textual data."
"Eval all, trust a few, do wrong to none: Comparing sentence generation models.  In this paper we study various flavors of variational autoencoders and address the methodological issues with the current neural text generation research and also close some gaps by answering a few natural questions to the studies already published."
"Quadrature Compound: An approximating family of distributions.  Compound distributions allow construction of a rich set of distributions. Typically
they involve an intractable integral. Here we use a quadrature approximation to that
integral to define the quadrature compound family. Special care is taken that this
approximation is suitable for computation of gradients with respect to distribution pa-
rameters. This technique is applied to discrete (Poisson LogNormal) and continuous
distributions. In the continuous case, quadrature compound family naturally makes use
of parameterized transformations of unparameterized distributions (a.k.a ??reparame-
terization??), allowing for gradients of expectations to be estimated as the gradient of
a sample mean. This is demonstrated in a novel distribution, the diffeomixture, which
is is a reparameterizable approximation to a mixture distribution."
"The TensorFlow Distributions Library.  The  TensorFlow  Distributions  library  implements  building  blocks  for  probabilistic  models:   standard discrete and continuous distributions with methods including sampling, log densities, and statistics (mean, mode, variance, entropy, etc), as well as invertible transformations (Bijectors) that can generate more complex random structures.  Composing these in a TensorFlow computational graph allows us to represent sophisticated models while inheriting TensorFlow??s support for GPU acceleration and automatic differentiation, which enables gradient-based inference techniques such as HMC and ADVI.  The Distributions library is widely used in research codebases at Google and Deepmind (and elsewhere), and serves as the back-end for the probabilistic programming system Edward."
"The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation.  The past year has witnessed rapid advances in sequence-to-sequence (seq2seq)
modeling for Machine Translation (MT). The classic RNN-based approaches to MT
were first out-performed by the convolutional seq2seq model, which was then
out-performed by the more recent Transformer model. Each of these new
approaches consists of a fundamental architecture accompanied by a set of
modeling and training techniques that are in principle applicable to other
seq2seq architectures. In this paper, we tease apart the new architectures and
their accompanying techniques in two ways. First, we identify several key
modeling and training techniques, and apply them to the RNN architecture,
yielding a new RNMT+ model that outperforms all of the three fundamental architectures
on the benchmark WMT'14 English to French and
English to German tasks. Second, we analyze the properties of each
fundamental seq2seq architecture and devise new hybrid architectures intended
to combine their strengths. Our hybrid models obtain further improvements,
outperforming the RNMT+ model on both benchmark datasets."
"Semantic Location in Email Query Suggestion.  Mobile devices are pervasive, which means that users have access to web content and their personal documents at all locations, not just their home or office. Existing work has studied how locations can influence information needs, focusing on web queries. We explore whether or not location information can be helpful to users who are searching their own personal documents. We wish to study whether a users?? location can predict their queries over their own personal data, so we focus on the task of query suggestion. While we find that using location directly can be helpful, it does not generalize well to novel locations. To improve this situation, we explore using semantic location: that is, rather than memorizing location-query associations, we generalize our location information to names of the closest point of interest. By using short, semantic descriptions of locations, we find that we can more robustly improve query completion and observe that users are already using locations to extend their own queries in this domain. We present a simple but effective model that can use location to predict queries for a user even before they type anything into a search box, and which learns effectively even when not all queries have location information."
"Supervised Noise Reduction for Multichannel Keyword Spotting.  This paper presents a robust, small-footprint, far-field keyword spotting (KWS) algorithm, which was inspired by the human auditory system??s ability to achieve the so-called cocktail party effect in adverse acoustic environments. It introduces the idea of combining microphone-array speech enhancement with machine learning, by incorporating a feedback path from the neural network (NN) KWS classifier to its signal preprocessing frontend so that frontend noise reduction can benefit from, and in turn, better serve backend machine intelligence. We find that the new system can significantly improve KWS performance for Google Home when there is strong music or TV noise in the background. While this innovative and successfully validated strategy of combining signal processing and machine learning is developed for KWS, its technical feasibility is presumably extensible to many other applications, including noise robust speaker identification and automatic speech recognition."
"Thoracic Disease Identification and Localization with Limited Supervision.  Accurate identification and localization of abnormalities from radiology images play an integral part in clinical diagnosis and treatment planning. Building a highly accurate prediction model for these tasks usually requires a large number of images manually annotated with labels and finding sites of abnormalities. In reality, however, such annotated data are expensive to acquire, especially the ones with location annotations. We need methods that can work well with only a small amount of location annotations. To address this challenge, we present a unified approach that simultaneously performs disease identification and localization through the same underlying model for all images. We demonstrate that our approach can effectively leverage both class information as well as limited location annotation, and significantly outperforms the comparative reference baseline in both classification and localization tasks."
"PPP-Net: Platform-aware Progressive Search for Pareto-optimal Neural Architectures.  Recent breakthroughs in Neural Architectural Search (NAS) have achieved state-of-the-art performances in many applications such as image recognition. However, these techniques typically ignore platform-related constrictions (e.g., inference time and power consumptions) that can be critical for portable devices with limited computing resources. We propose PPP-Net: a multi-objective architectural search framework to automatically generate networks that achieve Pareto Optimality. PPP-Net employs a compact search space inspired by operations used in state-of-the-art mobile CNNs. PPP-Net has also adopted the progressive search strategy used in a recent literature (Liu et al. (2017a)).  Experimental results demonstrate that PPP-Net achieves better performances in both (a) higher accuracy and (b) shorter inference time, comparing to the state-of-the-art CondenseNet."
"RFC 8374 - BGPsec Design Choices.  This document captures the design rationale of the initial draft version of what became RFC 8205 (the BGPsec protocol specification). The designers needed to balance many competing factors, and this document lists the decisions that were made in favor of or against each design choice. This document also presents brief summaries of the arguments that aided the decision process. Where appropriate, this document also provides brief notes on design decisions that changed as the specification was reviewed and updated by the IETF SIDR Working Group and that resulted in RFC 8205. These notes highlight the differences and provide pointers to details and rationale regarding those design changes."
"Attribution Model Evaluation.  Many advertisers rely on attribution to make a variety of tactical and strategic marketing decisions, and there is no shortage of attribution models for advertisers to consider. In the end, most advertisers choose an attribution model based on their preconceived notions about how attribution credit should be allocated. A misguided selection can lead an advertiser to use erroneous information in making marketing decisions. In this paper, we address this issue by identifying a well-defined objective for attribution modeling and proposing a systematic approach for evaluating and comparing attribution model performance using simulation. Following this process also leads to a better understanding of the conditions under which attribution models are able to provide useful and reliable information for advertisers."
"Automatic prediction of discourse connectives.  Many advertisers rely on attribution to make a variety of tactical and strategic marketing decisions, and there is no shortage of attribution models for advertisers to consider. In the end, most advertisers choose an attribution model based on their preconceived notions about how attribution credit should be allocated. A misguided selection can lead an advertiser to use erroneous information in making marketing decisions. In this paper, we address this issue by identifying a well-defined objective for attribution modeling and proposing a systematic approach for evaluating and comparing attribution model performance using simulation. Following this process also leads to a better understanding of the conditions under which attribution models are able to provide useful and reliable information for advertisers."
"SRE Principles.  As Ben Treynor (VP of 24x7 at Google and founding father of SRE) puts it, ""SRE, fundamentally, it??s what happens when you ask a software engineer to design an operations function"". What does differentiate an SRE (Site Reliability Engineering) from DevOps? Aren't they the same? SRE is a job function that focuses on the reliability and maintainability of systems. It is also a mindset and a set of engineering practices to run better production services. An SRE has to be able to engineer creative solutions to problems, strike the right balance between reliability and feature velocity and target appropriate levels of service quality. This talk covers the principles under which all SRE teams operate at Google: consistency, design of systems, monitoring, automation, error budgets, blameless postmortems, etc."
"A Causal Framework for Digital Attribution.  In this paper we tackle the marketing problem of assigning credit for a successful outcome to events that occur prior to the success, otherwise known as the attribution problem. In the world of digital advertising, attribution is widely used to formulate and evaluate marketing but often without a clear specification of the measurement objective and the decision-making needs. We formalize the problem of attribution under a causal framework, note its shortcomings, and suggest an attribution algorithm evaluated via simulation."
"An Industrial Application of Mutation Testing: Lessons, Challenges, and Research Directions.  Mutation analysis evaluates a testing or debugging
technique by measuring how well it detects mutants, which
are systematically seeded, artificial faults. Mutation analysis is
inherently expensive due to the large number of mutants it
generates and due to the fact that many of these generated
mutants are not effective; they are redundant, equivalent, or
simply uninteresting and waste computational resources. A large
body of research has focused on improving the scalability of
mutation analysis and proposed numerous optimizations to, e.g.,
select effective mutants or efficiently execute a large number of
tests against a large number of mutants. However, comparatively
little research has focused on the costs and benefits of mutation
testing, in which mutants are presented as testing goals to a
developer, in the context of an industrial-scale software devel-
opment process. This paper aims to fill that gap. Specifically,
it first reports on a case study from an open source context,
which quantifies the costs of achieving a mutation adequate
test set. The results suggest that achieving mutation adequacy
is neither practical nor desirable. This paper then draws on
an industrial application of mutation testing, involving more
than 30,000+ developers and 1,890,442 change sets, written in
4 programming languages. It shows that mutation testing does
not add a significant overhead to the software development
process and reports on mutation testing benefits perceived by
developers. Finally, this paper describes lessons learned from
these studies, highlights the current challenges of efficiently
and effectively applying mutation testing in an industrial-scale
software development process, and outlines research directions."
"Canary Analysis Service.  Mutation analysis evaluates a testing or debugging
technique by measuring how well it detects mutants, which
are systematically seeded, artificial faults. Mutation analysis is
inherently expensive due to the large number of mutants it
generates and due to the fact that many of these generated
mutants are not effective; they are redundant, equivalent, or
simply uninteresting and waste computational resources. A large
body of research has focused on improving the scalability of
mutation analysis and proposed numerous optimizations to, e.g.,
select effective mutants or efficiently execute a large number of
tests against a large number of mutants. However, comparatively
little research has focused on the costs and benefits of mutation
testing, in which mutants are presented as testing goals to a
developer, in the context of an industrial-scale software devel-
opment process. This paper aims to fill that gap. Specifically,
it first reports on a case study from an open source context,
which quantifies the costs of achieving a mutation adequate
test set. The results suggest that achieving mutation adequacy
is neither practical nor desirable. This paper then draws on
an industrial application of mutation testing, involving more
than 30,000+ developers and 1,890,442 change sets, written in
4 programming languages. It shows that mutation testing does
not add a significant overhead to the software development
process and reports on mutation testing benefits perceived by
developers. Finally, this paper describes lessons learned from
these studies, highlights the current challenges of efficiently
and effectively applying mutation testing in an industrial-scale
software development process, and outlines research directions."
"Dynamic Control Flow in Large-Scale Machine Learning.  Many recent machine learning models rely on fine-grained dynamic control flow for training and inference. In particular, models based on recurrent neural networks and on reinforcement learning depend on recurrence relations, data-dependent conditional execution, and other features that call for dynamic control flow. These applications benefit from the ability to make rapid control-flow decisions across a set of computing devices in a distributed system. For performance, scalability, and expressiveness, a machine learning system must support dynamic control flow in distributed and heterogeneous environments. This paper presents a programming model for distributed machine learning that supports dynamic control flow. We describe the design of the programming model, and its implementation in TensorFlow, a distributed machine learning system. Our approach extends the use of dataflow graphs to represent machine learning models, offering several distinctive features. First, the branches of conditionals and bodies of loops can be partitioned across many machines to run on a set of heterogeneous devices, including CPUs, GPUs, and custom ASICs. Second, programs written in our model support automatic differentiation and distributed gradient computations, which are necessary for training machine learning models that use control flow. Third, our choice of non-strict semantics enables multiple loop iterations to execute in parallel across machines, and to overlap compute and I/O operations. We have done our work in the context of TensorFlow, and it has been used extensively in research and production. We evaluate it using several real-world applications, and demonstrate its performance and scalability."
"Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks.  Recurrent Neural Networks (RNNs) continue to show  outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models."
"Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks.  Recurrent Neural Networks (RNNs) continue to show outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models."
"Incentive-Aware Learning for Large Markets.  In a typical learning problem, one key step is to use training data to pick one model from a collection of models that optimizes an objective function. In many multi-agent settings, the training data is generated through the actions of the agents, and the model is
used to make a decision (e.g., how to sell an item) that affects the agents. An illustrative example of this is the problem of learning the
reserve price in an auction. In such cases, the agents have an incentive to influence the training data (e.g., by manipulating their bids in
the case of an auction) to game the system and achieve a more favorable outcome. In this paper, we study such incentive-aware learning
problem in a general setting and show that it is possible to approximately optimize the objective function under two assumptions:
(i) each individual agent is a ??small?? (part of the market); and (ii) there is a cost associated with manipulation. For our illustrative
application, this nicely translates to a mechanism for setting approximately optimal reserve prices in auctions where no individual
agent has significant market share. For this application, we also show that the second assumption (that manipulations are costly) is
not necessary since we can ??perturb?? any auction to make it costly for the agents to manipulate."
"Analysis and Modeling of Grid Performance on Touchscreen Mobile Devices.  Touchscreen mobile devices can afford rich interaction behaviors but they are complex to model. Scrollable two-dimensional grids are a common user interface on mobile devices that allow users to access a large number of items on a small screen by direct touch. By analyzing touch input and eye gaze of users during grid interaction, we reveal how multiple performance components come into play in such a task, including navigation, visual search and pointing. These findings inspired us to design a novel predictive model that combines these components for modeling grid tasks. We realized these model components by employing both traditional analytical methods and data-driven machine learning approaches. In addition to showing high accuracy achieved by our model in predicting human performance on a test dataset, we demonstrate how such a model can lead to a significant reduction in interaction time when used in a predictive user interface."
"Deep learning for predicting refractive error from retinal fundus images.  Objective: Refractive error, one of the leading cause of visual impairment, can be corrected by simple interventions like prescribing eyeglasses, which often starts with autorefraction to estimate the refractive error. In this study, using deep learning, we trained a network to estimate refractive error from fundus photos only. Design: Retrospective analysis. Subjects, Participants, and/or Controls: Retinal fundus images from participants in the UK Biobank cohort, which were 45 degree field of view images and the AREDS clinical trial, which contained 30 degree field of view images. Methods, Intervention, or Testing: Refractive error was measured by autorefraction in the UK Biobank dataset and subjective refraction in the AREDS dataset. We trained a deep learning algorithm to predict refractive error from the fundus photographs and tested the prediction of the algorithm to the documented refractive error measurement. Our model used attention for identifying features that are predictive for refractive error. Main Outcome Measures: Mean average error (MAE) of the algorithm??s prediction compared to the refractive error obtained in the AREDS and UK Biobank. Results: The resulting algorithm had a mean average error (MAE) of 0.56 diopters (95% CI: 0.55-0.56) for estimating spherical equivalent on the UK Biobank dataset and 0.91 diopters (95% CI: 0.89-0.92) for the AREDS dataset. The baseline expected MAE (obtained by simply predicting the mean of this population) is 1.81 diopters (95% CI: 1.79-1.84) for UK Biobank and 1.63 (95% CI: 1.60-1.67) for AREDS. Attention maps suggest that the foveal region is one of the most important areas that is used by the algorithm to make this prediction, though other regions also contribute to the prediction. Conclusions: The ability to estimate refractive error with high accuracy from retinal fundus photos has not been previously known and demonstrates that deep learning can be applied to make novel predictions from medical images. In addition, given that several groups have recently shown that it is feasible to obtain retinal fundus photos using mobile phones and inexpensive attachments, this work may be particularly relevant in regions of the world where autorefractors may not be readily available."
"Learning with Sparse and Biased Feedback for Personal Search.  Personal search, including email, on-device, and personal media search, has recently attracted a considerable attention from the information retrieval community. In this paper, we provide an overview of challenges and opportunities of learning with implicit user feedback (e.g., click data) in personal search. Implicit user feedback provides a convenient source of supervision for ranking models in personal search. This feedback, however, has two major drawbacks: it is highly sparse and biased due to the personal nature of queries and documents. We demonstrate how these drawbacks can be overcome, and empirically demonstrate the benefits of learning with implicit feedback in the context of a large-scale email search engine"
"Towards Neural Phrase-based Machine Translation.  In this paper, we present Neural Phrase-based Machine Translation (NPMT). Our method explicitly models the phrase structures in output sequences using SleepWAke Networks (SWAN), a recently proposed segmentation-based sequence modeling method. To mitigate the monotonic alignment requirement of SWAN, we introduce a new layer to perform (soft) local reordering of input sequences. Different from existing neural machine translation (NMT) approaches, NPMT does not use attention-based decoding mechanisms. Instead, it directly outputs phrases in a sequential order. Our experiments show that NPMT achieves superior performances on IWSLT 2014 German-English/English-German and IWSLT 2015 English-Vietnamese machine translation tasks compared with strong NMT baselines.  We also observe that our method produces meaningful phrases in output languages."
"Slicer: Auto-Sharding for Datacenter Applications.  Sharding is a fundamental building block of large-scale applications, but most have their own
custom, ad-hoc implementations. Our goal is to make sharding as easily reusable as a filesystem or
lock manager. Slicer is \Google's general purpose sharding service. It monitors signals such as
load hotspots and server health and dynamically shards work over a set of servers. Its goals are to
maintain high availability and reduce load imbalance while minimizing churn from moved work. In this paper, we describe Slicer's design and implementation. Slicer has the consistency and global
optimization of a centralized sharder while approaching the high availability, scalability, and low
latency of systems that make local decisions.
It achieves this by separating concerns:
a reliable data plane forwards requests, and
a smart control plane makes load-balancing decisions off the critical path.
Slicer's small but powerful API has proven useful and easy to adopt
in dozens of \Google applications.
It is used to allocate resources for web service front-ends,
coalesce writes to increase storage bandwidth, and
increase the efficiency of a web cache.
It currently handles 2-6M~req/s of production traffic.
Production workloads using Slicer
exhibit a most-loaded task 30\%--180\% of the mean load,
even for highly skewed and time-varying loads."
"BLADE: Filter Learning for General Purpose Image Processing.  The Rapid and Accurate Image Super Resolution (RAISR)
method of Romano, Isidoro, and Milanfar is a computationally efficient image
upscaling method using a trained set of filters. We describe a generalization of
RAISR, which we name Best Linear Adaptive Enhancement (BLADE). This
approach is a trainable edge-adaptive filtering framework that is general, simple,
computationally efficient, and useful for a wide range of image processing
problems. We show applications to denoising, compression artifact removal,
demosaicing, and approximation of anisotropic diffusion equations."
"Fast, Trainable, Multiscale Denoising.  Denoising is a fundamental imaging application. Versatile but fast filtering has been demanded for mobile camera systems. We present an approach to multiscale filtering which allows real-time applications on low-powered devices. The key idea is to learn a set of kernels that upscales, filters, and blends patches of different scales guided by local structure analysis. This approach is trainable so that learned filters are capable of treating diverse noise patterns and artifacts. Experimental results show that the presented approach produces comparable results to state-of-the-art algorithms while processing time is orders of magnitude faster."
"Multi-task learning for Joint Language Understanding and Dialogue State Tracking.  This paper presents a novel approach for multi-task learning of language understanding (LU) and dialogue state tracking (DST) in task-oriented dialogue systems. Multi-task training enables the sharing of lower layers of the neural network and improves the performance of LU and DST while reducing the number of network parameters. In our proposed framework, DST operates on a set of candidate values for each slot that has been mentioned so far. These candidate sets are generated using LU slot annotations for the current user utterance, dialogue acts corresponding to the preceding system utterance and the dialogue state estimated for the previous turn, enabling DST to handle slots with a large or unbounded set of possible values and deal with slot values not seen during training. Furthermore, to bridge the gap between training and inference, we investigate the use of scheduled sampling on LU output for the current user utterance as well as the DST output for the preceding turn."
"Encoding Electronic Spectra in Quantum Circuits with Linear T Complexity.  We construct quantum circuits which exactly encode the spectra of correlated electron models up to errors from rotation synthesis. By invoking these circuits as oracles within the recently introduced ""qubitization"" framework, one can use quantum phase estimation to sample states in the Hamiltonian eigenbasis with optimal query complexity O(lambda / epsilon) where lambda is an absolute sum of Hamiltonian coefficients and epsilon is target precision. For both the Hubbard model and electronic structure Hamiltonian in a second quantized basis diagonalizing the Coulomb operator, our circuits have T gate complexity O(N + \log (1/epsilon)) where N is number of orbitals in the basis. Compared to prior approaches, our algorithms are asymptotically more efficient in gate complexity and require fewer T gates near the classically intractable regime. Compiling to surface code fault-tolerant gates and assuming per gate error rates of one part in a thousand reveals that one can error-correct phase estimation on interesting instances of these problems beyond the current capabilities of classical methods using only a few times more qubits than would be required for magic state distillation."
"Fast Algorithms for Knapsack via Convolution and Prediction.  The knapsack problem is a fundamental problem in combinatorial optimization. It has been studied extensively from theoretical as well as practical perspectives as it is one of the most well-known NP-hard problems. The goal is to pack a knapsack of size t with the maximum value from a collection of n items with given sizes and values. Recent evidence suggests that a classic O(nt) dynamic-programming solution for the knapsack problem might be the fastest in the worst case. In fact, solving the knapsack problem was shown to be equivalent to the (min,+) convolution problem (Cygan et al., ICALP 2017), which is thought to be facing a quadratic-time barrier. This hardness is in contrast to the more famous (+,??) convolution (generally known as polynomial multiplication), that has an O(nlogn)-time solution via Fast Fourier Transform. Our main results are algorithms with near-linear running times for the knapsack problem, if either the values or sizes of items are small integers. More specifically, if item sizes are integers bounded by s_max, the running time of our algorithm is O~((n + t)s_max). If the item values are integers bounded by v_max, our algorithm runs in time O~(n + t v_max). Best previously known running times were O(nt), O(n^2 s_max) and O(n s_max v_max) (Pisinger, J. of Alg., 1999). At the core of our algorithms lies the prediction technique: Roughly speaking, this new technique enables us to compute the convolution of two vectors in time O (n e_max) when an approximation of the solution within an additive error of e_max is available. Our results also have implications regarding algorithms for several other problems including tree sparsity, tree separability and the unbounded knapsack problem, in the case when some of the relevant numerical input values are bounded."
"Building Open Javanese and Sundanese Corpora for Multilingual Text-to-Speech.  We present the multi-speaker text-to-speech corpora for Javanese and Sundanese
languages, the second and third biggest languages of Indonesia spoken by well
over a hundred million people. The key objectives were to collect the high-quality
data an affordable way and to share the data publicly with the speech
community. To achieve this, we collaborated with two local universities in Java and
streamlined our recording and crowdsourcing processes to produce the corpora
consisting of 5.8 thousand (Javanese) and 4.2 thousand (Sundanese) mixed-gender
recordings. We used these corpora to build several configurations of multi-speaker
neural network-based text-to-speech systems for Javanese and Sundanese. Subjective
evaluations performed on these configurations demonstrate that multilingual
configurations for which Javanese and Sundanese are trained jointly with a
larger Indonesian corpus significantly outperform the systems constructed
from a single language. We hope that sharing these corpora publicly and
presenting our multilingual approach to text-to-speech will help the community
to scale up the text-to-speech technologies to other lesser resourced languages
of Indonesia."
"FonBund: A Library for Combining Cross-lingual Phonological Segment Data.  In this paper, we present an open-source library that provides a way of mapping sequences of arbitrary phonetic segments in International Phonetic Association (IPA) alphabet into multiple articulatory feature representations. The library interfaces with several existing linguistic typology resources providing phonological segment inventories and their corresponding articulatory feature
systems. Our first goal was to facilitate the derivation of articulatory features without giving a special preference to any particular phonological segment inventory provided by freely available linguistic typology resources. The second goal was to build a very light-weight library that can be easily modified to support new phonological segment inventories. In order to support IPA segments unsuppored by the freely available resources the library provides a simple configuration language for performing segment rewrites and adding custom segments with the corresponding feature structures. In addition to introducing the library and the corresponding linguistic resources, we also describe some of the practical uses of this library (multilingual speech synthesis) in the hope that this software will help facilitate multilingual speech research."
"The Morpho-syntactic Annotation of Animacy for a Dependency Parser.  In this paper we present the annotation scheme and parser results of the animacy feature in Russian and Arabic, two morphologicallyrich languages, in the spirit of the universal dependency framework (McDonald et al., 2013; de Marneffe et al., 2014). We explain the animacy hierarchies in both languages and make the case for the existence of five animacy types. We train a morphological analyzer on the annotated data and the results show a prediction f-measure for animacy of 95.39% for Russian and 92.71% for Arabic. We also use animacy along with other morphological tags as features to train a dependency parser, and the results show a slight improvement gained from animacy. We compare the impact of animacy on improving the dependency parser to other features found in nouns, namely, ??gender??, ??number??, and ??case??. To our knowledge this is the first contrastive study of the impact of morphological features on the accuracy of a transition parser. A portion of our data (1,000 sentences for Arabic and Russian each, along with other languages) annotated according to the scheme described in this paper is made publicly available (https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-1983) as part of the CoNLL 2017 Shared Task on Multilingual Parsing (Zeman et al., 2017)."
"Multilingual Multi-class Sentiment Classification Using Convolutional Neural Networks.  This paper describes a language-independent model for multi-class sentiment analysis using a simple neural network architecture of five layers (Embedding, Conv1D, GlobalMaxPooling and two Fully-Connected). The advantage of the proposed model is that it does not rely on language-specific features such as ontologies, dictionaries, or morphological or syntactic pre-processing. Equally important, our system does not use pre-trained word2vec embeddings which can be costly to obtain and train for some languages. In this research, we also demonstrate that oversampling can be an effective approach for correcting class imbalance in the data. We evaluate our methods on three publicly available datasets for English, German and Arabic, and the results show that our system??s performance is comparable to, or even better than, the state of the art for these datasets. We make our source-code publicly available."
"Multi-Dialect Arabic POS Tagging: A CRF Approach.  This paper introduces a new dataset of POS-tagged Arabic tweets in four major dialects along with tagging guidelines. The data, which we are releasing publicly, includes tweets in Egyptian, Levantine, Gulf, and Maghrebi, with 350 tweets for each dialect with appropriate train/test/development splits for 5-fold cross validation. We use a Conditional Random Fields (CRF) sequence labeler to train POS taggers for each dialect and examine the effect of cross and joint dialect training, and give benchmark results for the datasets. Using clitic n-grams, clitic metatypes, and stem templates as features, we were able to train a joint model that can correctly tag four different dialects with an average accuracy of 89.3%."
"Diacritization of Moroccan and Tunisian Arabic Dialects: A CRF Approach.  Arabic is written as a sequence of consonants and long vowels, with short vowels normally omitted. Diacritization attempts to recover short vowels and is an essential step for Text-to-Speech (TTS) systems. Though Automatic diacritization of Modern Standard Arabic (MSA) has received significant attention, limited research has been conducted on dialectal Arabic (DA) diacritization. Phonemic patterns of DA vary greatly from MSA and even from one another, which accounts for the noted difficulty with mutual intelligibility between dialects. With the recent advent of spoken dialog systems (or intelligent personal assistants), dialect vowel restoration is crucial to allow systems to speak back to the users in their own language variant. In this paper we present our research and benchmark results on the automatic diacritization of Tunisian and Moroccan using linear Conditional Random Fields."
"How SRE relates to DevOps.  DevOps and Site Reliability Engineering (SRE) have emerged in recent years as solutions for managing operations in IT and software development. Is one method better than the other? Will one of them eventually win out? This article explains why these two disciplines??in both practice and philosophy??are much more alike than you may think. Humans have been thinking about better ways to operate things for millennia, but despite all of this effort and thought, running enterprise software operations well remains elusive for many organizations. In this article, IT operations experts provide the key tenets of DevOps and SRE, compare and contrast the two, and explain the incentives necessary to successfully adopt either approach."
"Building Successful SRE in Large Enterprises???One Year Later.  At SRECon2017 I talked about the formation of a special group of Google SREs who go into the world and teach enterprise customers??via actual production systems??how to ""do SRE"" in their orgs. It was new when I presented it. It's one year later and we have a lot of interesting data about how it's going. Some things that we thought would be hard, weren't. Others were nigh on impossible. We've written many postmortems and learned a bunch of lessons you can only learn the hard way. Things you can expect to learn: Why it's easier to bootstrap SRE in a large traditional enterprise than a cloud native!
Things enterprises assume are true, but aren't.
All the things we should have known better, but still learned the hard way??and how you can avoid them when bootstrapping SRE in your culture (or your customers' cultures)"
"Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings.  The rise of neural networks, and particularly recurrent neural networks, has produced significant advances in part-of-speech tagging accuracy. One characteristic common among these models is the presence of rich initial word encodings. These encodings typically are composed of a recurrent character-based representation with learned and pre-trained word embeddings. However, these encodings do not consider a context wider than a single word and it is only through subsequent recurrent layers that word or sub-word information interacts. In this paper, we investigate models that use recurrent neural networks with sentence-level context for initial character and word-based representations. In particular we show that optimal results are obtained by integrating these context sensitive representations through synchronized training with a meta-model that learns to combine their states. We present results on part-of-speech and morphological tagging with state-of-the-art performance on a number of languages."
